{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark是一个基于内存计算的开源集群计算系统，由一组功能强大的、高级别的库组成，目前这些库包括**SparkSQL、Spark Streaming、MLlib、GraphX**。Spark Core是一个基本引擎，用于大规模并行和分布式数据处理。Spark引入了弹性分布式数据集（RDD）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SparkSQL支持通过SQL或者hive查询语言来查询数据。  \n",
    "Spark Streaming支持对流数据的实时处理，会接受数据将其分成不同的批次，处理后根据批次的结果生成最终的流。  \n",
    "MLlib是一个机器学习库。  \n",
    "GraphX是一个图计算库，用来处理图，执行图的并行计算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark数据操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark RDD操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDD，即弹性分布式数据集，是一个容错的、并行的数据结构，可以让用户显式地将数据存储到磁盘和内存中，并能控制数据的分区。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDD的一个重要参数是将数据集划分成分片的数量，对每一个分片，Spark会在集群中运行一个对应的任务，一般情况，Spark会根据当前情况自行设定分片数量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T02:31:21.134414Z",
     "start_time": "2021-09-13T02:30:56.986376Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://caoyuyu:4040\n",
       "SparkContext available as 'sc' (version = 3.0.1, master = local[*], app id = local-1631500270884)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "data: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9)\r\n",
       "distData: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:26\r\n",
       "res0: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9)\r\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = Array(1,2,3,4,5,6,7,8,9)\n",
    "val distData = sc.parallelize(data, 3)//创建数据集合\n",
    "distData.collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "textFile方法会使用一个文件的地址或hdfs地址，然后读入这个文件建立一个文本行的集合。可读取多个文件，逗号分隔。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T01:34:22.877033Z",
     "start_time": "2021-09-13T01:34:21.260453Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "distFile1: org.apache.spark.rdd.RDD[String] = /D:/Dataset/stu_data/engdata.txt MapPartitionsRDD[2] at textFile at <console>:25\r\n",
       "res1: Array[String] = Array(???\t???, 1\tOnce we dreamt that we were strangers. We wake up to find that we were dear to each other., 2\tWe come nearest to the great when we are great in humility., 3\tI love you., 4\t\"My heart, the bird of the wilderness, has found its sky in your eyes.\", 5\tIt is the tears of the earth that keep her smiles in?bloom., 6\tThe perfect decks itself in beauty for the love of the Imperfect., 7\t\"What you are you do not see, what you see is your shadow.\", 8\t\"Like the meeting of the seagulls and the waves we meet and come near.The seagulls fly off, the waves roll away and we depart.\")\r\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val distFile1 = sc.textFile(\"/D:/Dataset/stu_data/engdata.txt\")\n",
    "distFile1.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T01:34:27.513116Z",
     "start_time": "2021-09-13T01:34:25.390552Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "distFile2: org.apache.spark.rdd.RDD[String] = hdfs://caoyuyu:8020/input/hadoop.txt MapPartitionsRDD[4] at textFile at <console>:25\r\n",
       "res2: Array[String] = Array(Hadoop???????, \"\", hadoop  dfsadmin -safemode leave  #???????, hadoop jar /opt/hadoop-2.7.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.0.jar  pi 10 10 #????pi?)\r\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val distFile2 = sc.textFile(\"hdfs://caoyuyu:8020/input/hadoop.txt\")\n",
    "distFile2.collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 转换操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "map是对RDD中每个元素都执行一个指定的函数来产生一个新的RDD，RDD之间的元素是一对一关系。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T01:34:54.784956Z",
     "start_time": "2021-09-13T01:34:54.026042Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[5] at parallelize at <console>:25\r\n",
       "rdd2: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[6] at map at <console>:26\r\n",
       "res3: Array[Int] = Array(2, 4, 6, 8, 10, 12, 14, 16, 18)\r\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd1 = sc.parallelize(1 to 9, 3)\n",
    "val rdd2 = rdd1.map(x => x*2)//映射\n",
    "rdd2.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T01:34:56.222158Z",
     "start_time": "2021-09-13T01:34:55.601184Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd3: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[7] at filter at <console>:26\r\n",
       "res4: Array[Int] = Array(12, 14, 16, 18)\r\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd3 = rdd2.filter(x => x > 10)//过滤\n",
    "rdd3.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T01:34:57.450926Z",
     "start_time": "2021-09-13T01:34:56.891285Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd4: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[8] at flatMap at <console>:26\r\n",
       "res5: Array[Int] = Array(12, 13, 14, 15, 16, 17, 18, 19, 20, 14, 15, 16, 17, 18, 19, 20, 16, 17, 18, 19, 20, 18, 19, 20)\r\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd4 = rdd3.flatMap(x => x to 20)//映射为序列，而不是单一的元素，一对多\n",
    "rdd4.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T01:34:58.730607Z",
     "start_time": "2021-09-13T01:34:57.873613Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "myfunc: [T](iter: Iterator[T])Iterator[(T, T)]\r\n",
       "rdd5: org.apache.spark.rdd.RDD[(Int, Int)] = MapPartitionsRDD[9] at mapPartitions at <console>:36\r\n",
       "res6: Array[(Int, Int)] = Array((2,3), (1,2), (5,6), (4,5), (8,9), (7,8))\r\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def myfunc[T](iter: Iterator[T]): Iterator[(T, T)] = {\n",
    "    var res = List[(T, T)]()\n",
    "    var pre = iter.next\n",
    "    while (iter.hasNext){\n",
    "        val cur = iter.next\n",
    "        res.::=(pre, cur)\n",
    "        pre = cur\n",
    "    }\n",
    "    res.iterator\n",
    "}\n",
    "val rdd5 = rdd1.mapPartitions(myfunc)\n",
    "rdd5.collect\n",
    "//最终的RDD是由所有分区经过输入函数处理后的结果合并起来的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T01:36:23.054360Z",
     "start_time": "2021-09-13T01:36:22.503690Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[10] at parallelize at <console>:25\r\n",
       "res7: Long = 943\r\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val a = sc.parallelize(1 to 10000, 3)\n",
    "a.sample(false, 0.1, 0).count //sample随机抽样，第一个参数是否放回抽样，第二个抽样比例，第三个随机种子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T01:39:54.647400Z",
     "start_time": "2021-09-13T01:39:54.242663Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd8: org.apache.spark.rdd.RDD[Int] = UnionRDD[12] at union at <console>:28\r\n",
       "res8: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 12, 14, 16, 18)\r\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd8 = rdd1.union(rdd3)\n",
    "rdd8.collect //数据合并"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T01:40:56.563314Z",
     "start_time": "2021-09-13T01:40:55.811206Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd9: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[18] at intersection at <console>:28\r\n",
       "res9: Array[Int] = Array(6, 1, 7, 8, 2, 3, 9, 4, 5)\r\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd9 = rdd8.intersection(rdd1)\n",
    "rdd9.collect //数据交集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T01:46:11.850716Z",
     "start_time": "2021-09-13T01:46:11.024153Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd10: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[22] at distinct at <console>:28\r\n",
       "res10: Array[Int] = Array(12, 1, 14, 2, 3, 4, 16, 5, 6, 18, 7, 8, 9)\r\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd10 = rdd8.union(rdd9).distinct\n",
    "rdd10.collect //数据去重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T01:58:09.271549Z",
     "start_time": "2021-09-13T01:58:08.660744Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd0: org.apache.spark.rdd.RDD[(Int, Int)] = ParallelCollectionRDD[23] at parallelize at <console>:25\r\n",
       "rdd11: org.apache.spark.rdd.RDD[(Int, Iterable[Int])] = ShuffledRDD[24] at groupByKey at <console>:26\r\n",
       "res11: Array[(Int, Iterable[Int])] = Array((1,CompactBuffer(1, 2, 3)), (2,CompactBuffer(1, 2, 3)))\r\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd0 = sc.parallelize(Array((1, 1), (1, 2), (1, 3), (2, 1), (2, 2), (2, 3)), 3)\n",
    "val rdd11 = rdd0.groupByKey() //数据分组\n",
    "rdd11.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T01:59:53.874085Z",
     "start_time": "2021-09-13T01:59:53.501348Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd12: org.apache.spark.rdd.RDD[(Int, Int)] = ShuffledRDD[26] at reduceByKey at <console>:28\r\n",
       "res14: Array[(Int, Int)] = Array((1,6), (2,6))\r\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd12 = rdd0.reduceByKey((x, y) => x+y)  //数据聚合\n",
    "rdd12.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T02:14:24.079456Z",
     "start_time": "2021-09-13T02:14:23.628281Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd14: org.apache.spark.rdd.RDD[(Int, Int)] = ShuffledRDD[29] at sortByKey at <console>:26\r\n",
       "res15: Array[(Int, Int)] = Array((1,1), (1,2), (1,3), (2,1), (2,2), (2,3))\r\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd14 = rdd0.sortByKey()\n",
    "rdd14.collect //默认升序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T02:16:17.162119Z",
     "start_time": "2021-09-13T02:16:16.807793Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd15: org.apache.spark.rdd.RDD[(Int, (Int, Int))] = MapPartitionsRDD[32] at join at <console>:26\r\n",
       "res16: Array[(Int, (Int, Int))] = Array((1,(1,1)), (1,(1,2)), (1,(1,3)), (1,(2,1)), (1,(2,2)), (1,(2,3)), (1,(3,1)), (1,(3,2)), (1,(3,3)), (2,(1,1)), (2,(1,2)), (2,(1,3)), (2,(2,1)), (2,(2,2)), (2,(2,3)), (2,(3,1)), (2,(3,2)), (2,(3,3)))\r\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd15 = rdd0.join(rdd0)\n",
    "rdd15.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T02:34:28.782062Z",
     "start_time": "2021-09-13T02:34:28.266012Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[7] at parallelize at <console>:28\r\n",
       "rdd2: Array[org.apache.spark.rdd.RDD[Int]] = Array(MapPartitionsRDD[8] at randomSplit at <console>:29, MapPartitionsRDD[9] at randomSplit at <console>:29)\r\n",
       "res5: Array[Int] = Array(3, 5, 7)\r\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd1 = sc.parallelize(1 to 9, 3)\n",
    "val rdd2 = rdd1.randomSplit(Array(0.3, 0.7), 1)//按权重分组，第一个参数为权重，第二个为随即种子\n",
    "rdd2(0).collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T02:34:36.172088Z",
     "start_time": "2021-09-13T02:34:35.815018Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res6: Array[Int] = Array(1, 2, 4, 6, 8, 9)\r\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2(1).collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T02:38:22.415569Z",
     "start_time": "2021-09-13T02:38:21.926465Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[16] at parallelize at <console>:29\r\n",
       "rdd2: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[17] at parallelize at <console>:30\r\n",
       "rdd3: org.apache.spark.rdd.RDD[Int] = MapPartitionsRDD[21] at subtract at <console>:31\r\n",
       "res8: Array[Int] = Array(6, 9, 4, 7, 5, 8)\r\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd1 = sc.parallelize(1 to 9, 3)\n",
    "val rdd2 = sc.parallelize(1 to 3, 3)\n",
    "val rdd3 = rdd1.subtract(rdd2)//减法，将输入的元素rdd1减去rdd2中包含的元素\n",
    "rdd3.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T02:42:45.656359Z",
     "start_time": "2021-09-13T02:42:45.227217Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[28] at parallelize at <console>:29\r\n",
       "rdd2: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[29] at parallelize at <console>:30\r\n",
       "rdd3: org.apache.spark.rdd.RDD[(Int, String)] = ZippedPartitionsRDD2[30] at zip at <console>:31\r\n",
       "res10: Array[(Int, String)] = Array((1,a), (2,b), (3,c), (4,d))\r\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd1 = sc.parallelize(1 to 4, 3)\n",
    "val rdd2 = sc.parallelize(Array(\"a\", \"b\", \"c\", \"d\"), 3)\n",
    "val rdd3 = rdd1.zip(rdd2)//拉链操作\n",
    "rdd3.collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 行动操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T02:45:41.442614Z",
     "start_time": "2021-09-13T02:45:40.937217Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rdd1: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[31] at parallelize at <console>:27\r\n",
       "rdd2: Int = 45\r\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rdd1 = sc.parallelize(1 to 9, 3)\n",
    "val rdd2 = rdd1.reduce(_ + _)//对所有元素执行聚集函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T02:47:06.659354Z",
     "start_time": "2021-09-13T02:47:06.311759Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res11: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9)\r\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.collect()//将数据集中的所有元素以一个array的形式返回"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T02:49:03.948700Z",
     "start_time": "2021-09-13T02:49:03.670991Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res13: Long = 9\r\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.count() //返回数据集中的元素个数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T02:50:59.697640Z",
     "start_time": "2021-09-13T02:50:59.454762Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res14: Int = 1\r\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.first() //返回数据集第一个元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T02:52:06.325426Z",
     "start_time": "2021-09-13T02:52:06.061673Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res15: Array[Int] = Array(1, 2, 3)\r\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.take(3) //返回一个包含数据集中前n个元素的数组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-13T02:54:22.370214Z",
     "start_time": "2021-09-13T02:54:22.029874Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res17: Array[Int] = Array(1, 2, 3, 4)\r\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd1.takeOrdered(4) //返回包含随机的n个元素的数组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//foreach(func)是对数据集中每个元素都执行func函数"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  },
  "notify_time": "30",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "185px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
