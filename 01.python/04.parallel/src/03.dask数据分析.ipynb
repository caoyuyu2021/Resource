{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6700b736",
   "metadata": {},
   "source": [
    "# Dask DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdb2c1b",
   "metadata": {},
   "source": [
    "pandas 已经成为 DataFrame 的标准，但无法利用多核和集群，Dask DataFrame 试图解决 pandas 并行计算的问题。Dask DataFrame 尽量提供与 pandas 一致的 API，但使用起来，Dask DataFrame 仍有很多不同。Dask DataFrame 将大数据切分成小的 Partition，每个 Partition 是一个 pandas DataFrame。Dask 会把 DataFrame 的元数据记录下来，存储在 \\_meta 中。多个 Partition 的信息存储在内置变量 partitions 里和 divisions 里。Dask 用 Task Graph 管理计算图。对于用户来说，其实不需要太深入理解 Dask DataFrame 具体如何实现的，只需要调用跟 pandas 类似的上层 API。本章假设用户已经了解并熟悉 pandas，并重点讨论 Dask DataFrame 与 pandas 的区别。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54ab32f",
   "metadata": {},
   "source": [
    "# 读写数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b831b17",
   "metadata": {},
   "source": [
    "Dask DataFrame 支持 pandas 中几乎所有的数据读写操作，包括从本地、NFS、HDFS 或 S3 上读写文本文件、Parquet、HDF、JSON 等格式的文件。 下表是几个常见的读写操作。\n",
    "\n",
    "||CSV|Parquet|HDF\n",
    "|---|---|---|---|\n",
    "|读|read_csv()|read_parquet()|read_hdf()\n",
    "|写|to_csv()|to_parquet()|to_hdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23921d44",
   "metadata": {},
   "source": [
    "## 文件系统"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea28af9",
   "metadata": {},
   "source": [
    "当我们在使用 Dask 集群读写数据时，数据源应该存储在 NFS、HDFS、S3 这种共享文件系统，这样所有 Dask Worker 都能访问该数据。\n",
    "\n",
    "业内经常使用文件系统前缀（Scheme）来标识不同的文件系统，并调用不同的库来读写文件系统。下表是几个 Scheme 例子。一个数据集地址，应该是一个统一资源标识（Uniform Resource Identifier，URI），URI 包括 Scheme 和具体的地址，URI 模板为：scheme://path/to/data。比如：file:///tmp/tripdata.parquet 或 s3://tmp/tripdata.parquet。\n",
    "\n",
    "||本地|S3|HDFS\n",
    "|---|---|---|---|\n",
    "|Scheme|file://|s3://|hdfs://"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f872356",
   "metadata": {},
   "source": [
    "如果数据集地址没有加任何 Scheme，会假设是本地可访问的文件系统，即该计算节点可以直接读写和访问的文件系统。比如，网络文件系统（Network File System，NFS）是分布式文件系统，假如被挂载到多个计算节点的 /mnt/nfs 目录，使用 Dask 读写该目录时，可直接使用 /mnt/nfs 这个目录。\n",
    "\n",
    "HDFS 和 S3 这样共享文件系统，在企业或组织中被多人共享，因此经常有用户验证的环节，以做好用户之间的数据隔离，避免用户互相修改或删除数据。不同的文件系统有自己的用户验证的方式，比如，S3 用户需要提供令牌（Token），Token 可以通过 read_*() 和 to_*() (包括 read_csv()、read_parquet、to_parquet() 等方法) 的 storage_options 参数传递进来。如果你对用户验证不熟悉，应咨询组织中负责运维管理的同事。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e135db4",
   "metadata": {},
   "source": [
    "# 数据切分与并行读取"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09375c42",
   "metadata": {},
   "source": [
    "## 案例：飞机起降数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee89b2c",
   "metadata": {},
   "source": [
    "飞机起降数据集由多个逗号分隔的数值（Comma-Separated Values，CSV）文件组成，每个文件对应一个年份，我们读取多个 CSV，来展示 Dask DataFrame 与 pandas 的区别。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a712bf4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T10:02:42.398542Z",
     "start_time": "2024-08-06T10:02:42.393525Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "file_path = os.path.join(\"../../../../data/20.others/nyc_flights/\", \"nyc-flights\", \"*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22f550bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T09:54:51.545514Z",
     "start_time": "2024-08-06T09:54:44.758755Z"
    }
   },
   "outputs": [],
   "source": [
    "import dask\n",
    "dask.config.set({'dataframe.query-planning': False})\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "from dask.distributed import LocalCluster, Client\n",
    "\n",
    "cluster = LocalCluster()\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46b620f",
   "metadata": {},
   "source": [
    "pandas 和 Dask DataFrame 都提供了 read_csv() 方法，用来读取 CSV 文件。Dask 的 read_csv() 参数与 pandas 几乎一致，可以参考 pandas 的 read_csv()。比如在这个例子中，原始数据有很多列，其中前三列分别为，年：Year，月：Month，日：DayofMonth，read_csv() 方法的 parse_dates 参数将这三列解析为时间 datetime64 类型，并生成一个新的列 Date。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f53cfea3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T10:02:51.228781Z",
     "start_time": "2024-08-06T10:02:51.194682Z"
    }
   },
   "outputs": [],
   "source": [
    "ddf = dd.read_csv(file_path, parse_dates={'Date': [0, 1, 2]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978cbd06",
   "metadata": {},
   "source": [
    "这里的 file_path 是 *.csv 的形式，匹配所有以 csv 结尾的文件；而 pandas 的 read_csv() 只能读取单个文件，并不支持 *.csv 这样的通配符。如果想用 pandas 读文件夹下面的所有以 csv 结尾的文件，应该："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22aed1fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T10:03:42.126486Z",
     "start_time": "2024-08-06T10:03:38.468627Z"
    }
   },
   "outputs": [],
   "source": [
    "file_list = glob.glob(file_path)\n",
    "data = []\n",
    "\n",
    "for p in file_list:\n",
    "    df = pd.read_csv(p, parse_dates={'Date': [0, 1, 2]})\n",
    "    data.append(df)\n",
    "\n",
    "pdf = pd.concat(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "355f277b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T10:02:55.868490Z",
     "start_time": "2024-08-06T10:02:54.569360Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>DepTime</th>\n",
       "      <th>CRSDepTime</th>\n",
       "      <th>ArrTime</th>\n",
       "      <th>CRSArrTime</th>\n",
       "      <th>UniqueCarrier</th>\n",
       "      <th>FlightNum</th>\n",
       "      <th>TailNum</th>\n",
       "      <th>ActualElapsedTime</th>\n",
       "      <th>...</th>\n",
       "      <th>AirTime</th>\n",
       "      <th>ArrDelay</th>\n",
       "      <th>DepDelay</th>\n",
       "      <th>Origin</th>\n",
       "      <th>Dest</th>\n",
       "      <th>Distance</th>\n",
       "      <th>TaxiIn</th>\n",
       "      <th>TaxiOut</th>\n",
       "      <th>Cancelled</th>\n",
       "      <th>Diverted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1991-01-08</td>\n",
       "      <td>2</td>\n",
       "      <td>1215.0</td>\n",
       "      <td>1215</td>\n",
       "      <td>1340.0</td>\n",
       "      <td>1336</td>\n",
       "      <td>US</td>\n",
       "      <td>121</td>\n",
       "      <td>NaN</td>\n",
       "      <td>85.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>EWR</td>\n",
       "      <td>PIT</td>\n",
       "      <td>319.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1991-01-09</td>\n",
       "      <td>3</td>\n",
       "      <td>1215.0</td>\n",
       "      <td>1215</td>\n",
       "      <td>1353.0</td>\n",
       "      <td>1336</td>\n",
       "      <td>US</td>\n",
       "      <td>121</td>\n",
       "      <td>NaN</td>\n",
       "      <td>98.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>EWR</td>\n",
       "      <td>PIT</td>\n",
       "      <td>319.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1991-01-10</td>\n",
       "      <td>4</td>\n",
       "      <td>1216.0</td>\n",
       "      <td>1215</td>\n",
       "      <td>1332.0</td>\n",
       "      <td>1336</td>\n",
       "      <td>US</td>\n",
       "      <td>121</td>\n",
       "      <td>NaN</td>\n",
       "      <td>76.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>EWR</td>\n",
       "      <td>PIT</td>\n",
       "      <td>319.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date  DayOfWeek  DepTime  CRSDepTime  ArrTime  CRSArrTime  \\\n",
       "0 1991-01-08          2   1215.0        1215   1340.0        1336   \n",
       "1 1991-01-09          3   1215.0        1215   1353.0        1336   \n",
       "2 1991-01-10          4   1216.0        1215   1332.0        1336   \n",
       "\n",
       "  UniqueCarrier  FlightNum  TailNum  ActualElapsedTime  ...  AirTime  \\\n",
       "0            US        121      NaN               85.0  ...      NaN   \n",
       "1            US        121      NaN               98.0  ...      NaN   \n",
       "2            US        121      NaN               76.0  ...      NaN   \n",
       "\n",
       "   ArrDelay  DepDelay  Origin Dest Distance  TaxiIn  TaxiOut  Cancelled  \\\n",
       "0       4.0       0.0     EWR  PIT    319.0     NaN      NaN          0   \n",
       "1      17.0       0.0     EWR  PIT    319.0     NaN      NaN          0   \n",
       "2      -4.0       1.0     EWR  PIT    319.0     NaN      NaN          0   \n",
       "\n",
       "   Diverted  \n",
       "0         0  \n",
       "1         0  \n",
       "2         0  \n",
       "\n",
       "[3 rows x 21 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddf.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b7765b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T10:03:44.471389Z",
     "start_time": "2024-08-06T10:03:44.452188Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>DepTime</th>\n",
       "      <th>CRSDepTime</th>\n",
       "      <th>ArrTime</th>\n",
       "      <th>CRSArrTime</th>\n",
       "      <th>UniqueCarrier</th>\n",
       "      <th>FlightNum</th>\n",
       "      <th>TailNum</th>\n",
       "      <th>ActualElapsedTime</th>\n",
       "      <th>...</th>\n",
       "      <th>AirTime</th>\n",
       "      <th>ArrDelay</th>\n",
       "      <th>DepDelay</th>\n",
       "      <th>Origin</th>\n",
       "      <th>Dest</th>\n",
       "      <th>Distance</th>\n",
       "      <th>TaxiIn</th>\n",
       "      <th>TaxiOut</th>\n",
       "      <th>Cancelled</th>\n",
       "      <th>Diverted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1991-01-08</td>\n",
       "      <td>2</td>\n",
       "      <td>1215.0</td>\n",
       "      <td>1215</td>\n",
       "      <td>1340.0</td>\n",
       "      <td>1336</td>\n",
       "      <td>US</td>\n",
       "      <td>121</td>\n",
       "      <td>NaN</td>\n",
       "      <td>85.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>EWR</td>\n",
       "      <td>PIT</td>\n",
       "      <td>319.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1991-01-09</td>\n",
       "      <td>3</td>\n",
       "      <td>1215.0</td>\n",
       "      <td>1215</td>\n",
       "      <td>1353.0</td>\n",
       "      <td>1336</td>\n",
       "      <td>US</td>\n",
       "      <td>121</td>\n",
       "      <td>NaN</td>\n",
       "      <td>98.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>EWR</td>\n",
       "      <td>PIT</td>\n",
       "      <td>319.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1991-01-10</td>\n",
       "      <td>4</td>\n",
       "      <td>1216.0</td>\n",
       "      <td>1215</td>\n",
       "      <td>1332.0</td>\n",
       "      <td>1336</td>\n",
       "      <td>US</td>\n",
       "      <td>121</td>\n",
       "      <td>NaN</td>\n",
       "      <td>76.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>EWR</td>\n",
       "      <td>PIT</td>\n",
       "      <td>319.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date  DayOfWeek  DepTime  CRSDepTime  ArrTime  CRSArrTime  \\\n",
       "0 1991-01-08          2   1215.0        1215   1340.0        1336   \n",
       "1 1991-01-09          3   1215.0        1215   1353.0        1336   \n",
       "2 1991-01-10          4   1216.0        1215   1332.0        1336   \n",
       "\n",
       "  UniqueCarrier  FlightNum TailNum  ActualElapsedTime  ...  AirTime  ArrDelay  \\\n",
       "0            US        121     NaN               85.0  ...      NaN       4.0   \n",
       "1            US        121     NaN               98.0  ...      NaN      17.0   \n",
       "2            US        121     NaN               76.0  ...      NaN      -4.0   \n",
       "\n",
       "   DepDelay  Origin Dest Distance  TaxiIn  TaxiOut  Cancelled  Diverted  \n",
       "0       0.0     EWR  PIT    319.0     NaN      NaN          0         0  \n",
       "1       0.0     EWR  PIT    319.0     NaN      NaN          0         0  \n",
       "2       1.0     EWR  PIT    319.0     NaN      NaN          0         0  \n",
       "\n",
       "[3 rows x 21 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dd8ff8",
   "metadata": {},
   "source": [
    "这里体现出 Dask DataFrame 与 pandas 的区别：Dask DataFrame 接受 \\*.csv 这样的通配符，可以批量读取文件夹下所有的以 csv 结尾的文件。Dask DataFrame 在实现时，先对 \\*.csv 目录进行了遍历，了解到目录下一共有多少个 CSV 文件，并在构建 Task Graph 时，根据文件数量，并行地启动多个 pandas 进程。\n",
    "\n",
    "ddf.visualize() 将 Task Graph 进行了可视化，可以看到：目录下有 m 个 CSV 文件，在 Task Graph 中生成 m 个 read_csv() 子图，执行时并行地启动 m 个 pandas 的 read_csv()；或者说，每个 CSV 文件对应一个 Partition。根据文件数量构建 Task Graph 中的并行粒度是一种最简单的图生成方式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8c6586ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-06T10:03:50.587613Z",
     "start_time": "2024-08-06T10:03:49.059897Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Could not load \"D:\\Anaconda\\Library\\bin\\gvplugin_pango.dll\" - It was found, so perhaps one of its dependents was not.  Try ldd.\r\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"674pt\" height=\"176pt\" viewBox=\"0.00 0.00 674.13 176.13\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 172.13)\">\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-172.13 670.13,-172.13 670.13,4 -4,4\"/>\n",
       "<!-- 7258259034073274107 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>7258259034073274107</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"48.07\" cy=\"-48.07\" rx=\"48.13\" ry=\"48.13\"/>\n",
       "<text text-anchor=\"middle\" x=\"48.07\" y=\"-43.87\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">read-csv</text>\n",
       "</g>\n",
       "<!-- 4275488917157992745 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>4275488917157992745</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"75.07,-168.13 21.07,-168.13 21.07,-132.13 75.07,-132.13 75.07,-168.13\"/>\n",
       "<text text-anchor=\"middle\" x=\"48.07\" y=\"-145.93\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">0</text>\n",
       "</g>\n",
       "<!-- 7258259034073274107&#45;&gt;4275488917157992745 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>7258259034073274107-&gt;4275488917157992745</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M48.07,-96.17C48.07,-104.95 48.07,-113.85 48.07,-121.79\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"44.57,-121.91 48.07,-131.91 51.57,-121.91 44.57,-121.91\"/>\n",
       "</g>\n",
       "<!-- 6221419621800953464 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>6221419621800953464</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"162.07\" cy=\"-48.07\" rx=\"48.13\" ry=\"48.13\"/>\n",
       "<text text-anchor=\"middle\" x=\"162.07\" y=\"-43.87\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">read-csv</text>\n",
       "</g>\n",
       "<!-- &#45;5015942494799071749 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>-5015942494799071749</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"189.07,-168.13 135.07,-168.13 135.07,-132.13 189.07,-132.13 189.07,-168.13\"/>\n",
       "<text text-anchor=\"middle\" x=\"162.07\" y=\"-145.93\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">1</text>\n",
       "</g>\n",
       "<!-- 6221419621800953464&#45;&gt;&#45;5015942494799071749 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>6221419621800953464-&gt;-5015942494799071749</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M162.07,-96.17C162.07,-104.95 162.07,-113.85 162.07,-121.79\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"158.57,-121.91 162.07,-131.91 165.57,-121.91 158.57,-121.91\"/>\n",
       "</g>\n",
       "<!-- 5417292716652380422 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>5417292716652380422</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"276.07\" cy=\"-48.07\" rx=\"48.13\" ry=\"48.13\"/>\n",
       "<text text-anchor=\"middle\" x=\"276.07\" y=\"-43.87\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">read-csv</text>\n",
       "</g>\n",
       "<!-- 1491698261302926676 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>1491698261302926676</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"303.07,-168.13 249.07,-168.13 249.07,-132.13 303.07,-132.13 303.07,-168.13\"/>\n",
       "<text text-anchor=\"middle\" x=\"276.07\" y=\"-145.93\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">2</text>\n",
       "</g>\n",
       "<!-- 5417292716652380422&#45;&gt;1491698261302926676 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>5417292716652380422-&gt;1491698261302926676</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M276.07,-96.17C276.07,-104.95 276.07,-113.85 276.07,-121.79\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"272.57,-121.91 276.07,-131.91 279.57,-121.91 272.57,-121.91\"/>\n",
       "</g>\n",
       "<!-- 5258254506106904902 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>5258254506106904902</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"390.07\" cy=\"-48.07\" rx=\"48.13\" ry=\"48.13\"/>\n",
       "<text text-anchor=\"middle\" x=\"390.07\" y=\"-43.87\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">read-csv</text>\n",
       "</g>\n",
       "<!-- &#45;3401375767669769690 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>-3401375767669769690</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"417.07,-168.13 363.07,-168.13 363.07,-132.13 417.07,-132.13 417.07,-168.13\"/>\n",
       "<text text-anchor=\"middle\" x=\"390.07\" y=\"-145.93\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">3</text>\n",
       "</g>\n",
       "<!-- 5258254506106904902&#45;&gt;&#45;3401375767669769690 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>5258254506106904902-&gt;-3401375767669769690</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M390.07,-96.17C390.07,-104.95 390.07,-113.85 390.07,-121.79\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"386.57,-121.91 390.07,-131.91 393.57,-121.91 386.57,-121.91\"/>\n",
       "</g>\n",
       "<!-- &#45;8751439262681442836 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>-8751439262681442836</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"504.07\" cy=\"-48.07\" rx=\"48.13\" ry=\"48.13\"/>\n",
       "<text text-anchor=\"middle\" x=\"504.07\" y=\"-43.87\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">read-csv</text>\n",
       "</g>\n",
       "<!-- 3106264988432228735 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>3106264988432228735</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"531.07,-168.13 477.07,-168.13 477.07,-132.13 531.07,-132.13 531.07,-168.13\"/>\n",
       "<text text-anchor=\"middle\" x=\"504.07\" y=\"-145.93\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">4</text>\n",
       "</g>\n",
       "<!-- &#45;8751439262681442836&#45;&gt;3106264988432228735 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>-8751439262681442836-&gt;3106264988432228735</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M504.07,-96.17C504.07,-104.95 504.07,-113.85 504.07,-121.79\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"500.57,-121.91 504.07,-131.91 507.57,-121.91 500.57,-121.91\"/>\n",
       "</g>\n",
       "<!-- &#45;8223348353006561304 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>-8223348353006561304</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"618.07\" cy=\"-48.07\" rx=\"48.13\" ry=\"48.13\"/>\n",
       "<text text-anchor=\"middle\" x=\"618.07\" y=\"-43.87\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">read-csv</text>\n",
       "</g>\n",
       "<!-- &#45;8832838329175324456 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>-8832838329175324456</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"645.07,-168.13 591.07,-168.13 591.07,-132.13 645.07,-132.13 645.07,-168.13\"/>\n",
       "<text text-anchor=\"middle\" x=\"618.07\" y=\"-145.93\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">5</text>\n",
       "</g>\n",
       "<!-- &#45;8223348353006561304&#45;&gt;&#45;8832838329175324456 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>-8223348353006561304-&gt;-8832838329175324456</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M618.07,-96.17C618.07,-104.95 618.07,-113.85 618.07,-121.79\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"614.57,-121.91 618.07,-131.91 621.57,-121.91 614.57,-121.91\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddf.visualize(filename=\"../images/nyc-flights-graph\", format=\"svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad050fa5",
   "metadata": {},
   "source": [
    "这个例子中，Dask 根据文件数量来确定 Partition 的数量。之前提到，数据切分得过细或者过粗都不是最好的。对于所有可能的场景来说，单纯根据文件数量来确定切分为多少个 Partition 的方式并不一定是最优的：因为如果很多小文件，切分粒度会过细；或者如果是单个大文件，切分粒度过粗，可能导致 OOM。这两种极端情况下生成的 Task Graph 和并行粒度都不是最优的。所以，Dask DataFrame 的 read_csv() 提供了自定义每个 Partition 大小的参数 blocksize，单个 Partition 的大小不会超过 blocksize。如果用户没有显式设置 `blocksize`，Dask DataFrame 会根据探测到的计算资源情况来确定 blocksize，最大不超过 64MB。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752b698d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "210.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
