{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b9ca483-043c-48f8-99a9-9dc0a49808d4",
   "metadata": {},
   "source": [
    "RuntimeError: CUDA out of memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcd615d-7a60-47f4-886b-536e0798b4f5",
   "metadata": {},
   "source": [
    "关于CUDA GPU显存管理的总结：\n",
    "- GPU显存占用率和存入的数据尺寸成正相关，越大的数据占用显存越多\n",
    "- 只要使用了GPU，就至少会占x xx M的显存，且这部分显存无法被释放\n",
    "- 当一块内存不再被变量所引用时，这块内存就由激活内存转为失活内存，但它仍然存在于这个数据队列中\n",
    "- 当数据队列达到某个阈值时，CUDA会触发垃圾回收机制，清理失活内存\n",
    "- 运行torch.cuda.empty_cache()可以手动清理失活内存\n",
    "\n",
    "那么根据上述理论，就可以得到对应的问题解决方案：\n",
    "- 调小batch_size  \n",
    "  本质上是防止GPU数据队列向显存申请的空间大于显存本身\n",
    "\n",
    "- 检查是否有数据持续存入GPU而未释放\n",
    "\n",
    "- 训练过程中的测试阶段和验证阶段前插入代码with torch.no_grad()  \n",
    "  原理是不计算梯度，从而不用GPU加速运算，不会把数据再加到数据队列中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86b9215-60f6-4375-a30e-c9d28a97b2e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
