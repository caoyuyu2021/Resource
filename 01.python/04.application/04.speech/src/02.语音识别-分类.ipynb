{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95fa2458-a463-4539-9783-2349725b8c50",
   "metadata": {},
   "source": [
    "# 基于TimesNet的说话人识别算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f35715dc-b5ab-4852-97c4-8d28b5c390f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T05:44:45.358664Z",
     "iopub.status.busy": "2024-12-16T05:44:45.358664Z",
     "iopub.status.idle": "2024-12-16T05:45:05.571485Z",
     "shell.execute_reply": "2024-12-16T05:45:05.571485Z",
     "shell.execute_reply.started": "2024-12-16T05:44:45.358664Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "from datetime import timedelta\n",
    "from numpy import ndarray\n",
    "from typing import Union, List, Dict\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import torchaudio\n",
    "from torchaudio.transforms import MFCC\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm  # 打印进度条\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List\n",
    "from pandas.tseries import offsets\n",
    "from pandas.tseries.frequencies import to_offset\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import joblib\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # 显示中文标签\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a34393-9f78-4c4a-8eca-b90503cfde59",
   "metadata": {},
   "source": [
    "## 基础知识"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fd535b-0e99-421b-996a-85fca6db54f4",
   "metadata": {},
   "source": [
    "### 梅尔频率倒谱系数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ec90e6-ea17-4fb4-a818-0414b8ffa71f",
   "metadata": {},
   "source": [
    "MFCC（Mel Frequency Cepstral Coefficients，梅尔频率倒谱系数）是语音信号处理中常用的一种特征提取方法，广泛应用于语音识别、音频处理等领域。MFCC的目的是将音频信号转换为一个更适合机器学习模型处理的特征表示。\n",
    "\n",
    "MFCC的基本步骤：\n",
    "- 分帧（Framing）：将音频信号分割成多个小的时间片段，每个片段通常为20-40毫秒。这是因为语音信号在短时间内可以近似为平稳信号。\n",
    "\n",
    "- 窗口加权（Windowing）：对每一帧信号应用窗函数（通常是汉明窗或汉宁窗），以减小帧之间的边界效应。\n",
    "\n",
    "- 快速傅里叶变换（FFT）：对每一帧信号进行FFT变换，得到信号的频谱表示。\n",
    "\n",
    "- Mel频率滤波器组：将频谱经过Mel频率尺度的滤波器组进行滤波。Mel尺度是一种对人耳听觉特性的模拟，它在低频段具有更高的分辨率，而在高频段分辨率较低。\n",
    "\n",
    "- 对数变换：对滤波器输出的能量进行对数变换，这一操作是为了模拟人耳对声音强度的感知。\n",
    "\n",
    "- 离散余弦变换（DCT）：对经过Mel频率滤波后的能量谱进行离散余弦变换，最终得到MFCC系数。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65acbd8-ce82-49e3-95ff-987779e8c283",
   "metadata": {},
   "source": [
    "### MFCC"
   ]
  },
  {
   "cell_type": "raw",
   "id": "45517c0c-9c40-4b2f-90bb-307e2df57a3e",
   "metadata": {},
   "source": [
    "class MFCC(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        sample_rate: int,\n",
    "        n_mfcc: int = 13,\n",
    "        melkwargs: Optional[dict] = None,\n",
    "    ) -> None:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e768a36b-8fc5-4cae-9dd5-f2034f56579c",
   "metadata": {},
   "source": [
    "构造函数：MFCC:\n",
    "- sample_rate（int）：音频的采样率。通常音频文件的采样率为16000、22050、44100等，这会影响特征提取的精度和速度。\n",
    "\n",
    "- n_mfcc（int）：要提取的MFCC系数的数量。通常选择13个MFCC系数，这些系数能够有效捕捉音频信号的特征。n_mfcc的默认值是13。\n",
    "\n",
    "- melkwargs（dict，可选）：一个字典，包含Mel滤波器组的参数配置。常见的参数包括：\n",
    "\n",
    "    - n_fft: 用于计算FFT的窗口大小（以样本为单位），默认值通常是400。\n",
    "    - hop_length: 每一帧的步长（以样本为单位），默认值通常是160。\n",
    "    - n_mels: Mel滤波器的数量，决定了Mel频率尺度的分辨率。默认通常为23。\n",
    "    - center: 是否将FFT窗口居中，默认为True。"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5493d5dd-f45b-4e10-b642-352978c238c8",
   "metadata": {},
   "source": [
    "def forward(self, waveform: Tensor) -> Tensor:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e905c32-df2c-4dbf-aedb-a947a5c6bafc",
   "metadata": {},
   "source": [
    "forward方法是MFCC类的核心功能，它执行音频信号的MFCC特征提取过程。调用transform(waveform)时，实际上是调用forward方法。\n",
    "- waveform（Tensor）：一个形状为 (channel, time) 的音频波形张量。通常情况下，channel表示音频的声道数（单声道是1，立体声是2），time表示音频的样本数。\n",
    "\n",
    "- 返回值：一个形状为 (n_mfcc, time) 的张量，包含了提取的MFCC系数。n_mfcc是MFCC系数的数量，time是帧数（即音频信号的时长被分割成的小帧数量，**每帧代表每个时间步**）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5bc0dc-ebf2-432c-96f9-3e6a0cbf857c",
   "metadata": {},
   "source": [
    "### 举例说明"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6a685239-8b6e-48fc-9567-e224d68fd56d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T07:18:10.934093Z",
     "iopub.status.busy": "2024-12-15T07:18:10.933091Z",
     "iopub.status.idle": "2024-12-15T07:18:11.031371Z",
     "shell.execute_reply": "2024-12-15T07:18:11.029370Z",
     "shell.execute_reply.started": "2024-12-15T07:18:10.934093Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13, 9981]) torch.Size([13, 9981])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from torchaudio.transforms import MFCC\n",
    "\n",
    "# 读取立体声音频（2通道）\n",
    "waveform, sample_rate = torchaudio.load('../data\\\\cyy1.wav')\n",
    "\n",
    "# 设置MFCC转换器\n",
    "mfcc_transform = MFCC(\n",
    "    sample_rate=sample_rate,\n",
    "    n_mfcc=13,\n",
    "    melkwargs={'n_fft': 400, 'hop_length': 160, 'n_mels': 23, 'center': False}\n",
    ")\n",
    "\n",
    "# 处理左右声道\n",
    "mfcc_left = mfcc_transform(waveform[0])  # 左声道\n",
    "mfcc_right = mfcc_transform(waveform[1])  # 右声道\n",
    "\n",
    "# 你可以将它们分别输入模型\n",
    "print(mfcc_left.shape, mfcc_right.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d38063b0-bcda-4dff-ab9e-4ef3e84d85cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T07:18:49.162953Z",
     "iopub.status.busy": "2024-12-15T07:18:49.161955Z",
     "iopub.status.idle": "2024-12-15T07:18:49.181955Z",
     "shell.execute_reply": "2024-12-15T07:18:49.181132Z",
     "shell.execute_reply.started": "2024-12-15T07:18:49.162953Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1597200])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "waveform[0].size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae8c723-2d9a-4d46-83a9-3d67f6c86ace",
   "metadata": {},
   "source": [
    "## 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bdad2b1-7941-4822-a67a-54ae0e68f6dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T05:45:09.911057Z",
     "iopub.status.busy": "2024-12-16T05:45:09.911057Z",
     "iopub.status.idle": "2024-12-16T05:45:09.926228Z",
     "shell.execute_reply": "2024-12-16T05:45:09.925230Z",
     "shell.execute_reply.started": "2024-12-16T05:45:09.911057Z"
    }
   },
   "outputs": [],
   "source": [
    "# 加载所有音频\n",
    "def loader(data_path):\n",
    "    \"\"\"data_path: 音频文件夹\"\"\"\n",
    "    # 将每个人的音频进行分段\n",
    "    def _select_data(wave_data):\n",
    "        length = int(0.2 * 48000)  # 0.2s为一段,每段9600个数据\n",
    "        seg = int(np.ceil(len(wave_data) / (length)))  # 分成的段数,向上取整\n",
    "        seg1 = seg - 1  # 不包含最后一段，因为有可能包含0\n",
    "        zeros = torch.zeros(length * seg - len(wave_data))\n",
    "        wave_data_new = torch.cat([wave_data, zeros\n",
    "                                        ]).reshape(seg,\n",
    "                                                   length)  # 将原数组变成seg*length的数组\n",
    "        wave_data_new = wave_data_new[:len(wave_data_new) - 1]\n",
    "        return wave_data_new\n",
    "    \n",
    "    # 遍历音频文件夹\n",
    "    for index, file in enumerate(os.listdir(data_path)):\n",
    "        if file.endswith('.wav'):\n",
    "            file_path = os.path.join(data_path, file)\n",
    "            # 读取立体声音频（2通道）\n",
    "            waveform, sample_rate = torchaudio.load(file_path)\n",
    "            waveform_left, waveform_right = _select_data(\n",
    "                waveform[0, :]), _select_data(waveform[1, :])\n",
    "            waveform_con = torch.cat([waveform_left, waveform_right])\n",
    "            label = torch.tensor([index] * waveform_con.shape[0]).reshape((-1))\n",
    "\n",
    "            # 合并每个人的音频\n",
    "            if index == 0:\n",
    "                waveforms = waveform_con\n",
    "                labels = label\n",
    "            else:\n",
    "                waveforms = torch.cat([waveforms, waveform_con])\n",
    "                labels = torch.cat([labels, label])\n",
    "\n",
    "    return waveforms, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2aa5ad90-a888-4794-b793-f0c40dea6404",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T05:45:12.408022Z",
     "iopub.status.busy": "2024-12-16T05:45:12.408022Z",
     "iopub.status.idle": "2024-12-16T05:45:12.998941Z",
     "shell.execute_reply": "2024-12-16T05:45:12.997847Z",
     "shell.execute_reply.started": "2024-12-16T05:45:12.408022Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: torch.Size([4066, 9600]) y shape: torch.Size([4066])\n"
     ]
    }
   ],
   "source": [
    "x, y = loader(\"../data\")\n",
    "print(\"x shape: {0} y shape: {1}\".format(x.shape, y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a234d04-e176-4d25-a0a5-2813ea257752",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T05:45:15.392002Z",
     "iopub.status.busy": "2024-12-16T05:45:15.392002Z",
     "iopub.status.idle": "2024-12-16T05:45:15.406821Z",
     "shell.execute_reply": "2024-12-16T05:45:15.406713Z",
     "shell.execute_reply.started": "2024-12-16T05:45:15.392002Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(y.reshape(-1).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5415afa-87fc-41c1-a2bc-626660692c05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T05:45:17.817371Z",
     "iopub.status.busy": "2024-12-16T05:45:17.815361Z",
     "iopub.status.idle": "2024-12-16T05:45:17.851250Z",
     "shell.execute_reply": "2024-12-16T05:45:17.850286Z",
     "shell.execute_reply.started": "2024-12-16T05:45:17.817371Z"
    }
   },
   "outputs": [],
   "source": [
    "# 数据归一化\n",
    "def divider(data_list, train_ratio, scaler_path):\n",
    "    \"\"\"\n",
    "    读取数据，并对数据进行归一化\n",
    "\n",
    "    参数说明\n",
    "    ----------\n",
    "    data_list : {list}\n",
    "        输入数据[x, y]\n",
    "    train_ratio : {float}\n",
    "        用于训练的数据集占比:将数据按照一定比例进行切分，取值范围为(0,1)\n",
    "    scaler_path : {str} \n",
    "        数据归一化模型保存地址\n",
    "\n",
    "    返回值\n",
    "    -------\n",
    "    x_scaler : {sklearn.preprocessing.MinMaxScaler}\n",
    "        训练特征列归一化器\n",
    "    train : {list[DataFrame]}\n",
    "        训练特征数据，目标特征数据，时间特征数据\n",
    "    valid : {list[DataFrame]}\n",
    "        验证特征数据，目标特征数据，时间特征数据\n",
    "    \"\"\"\n",
    "    # 获取数据\n",
    "    x, y = data_list\n",
    "    #归一化训练\n",
    "    x_scaler = MinMaxScaler() # 保证数据同分布\n",
    "    m, n = x.shape[0], x.shape[1]\n",
    "    x = x.reshape(-1, 1)\n",
    "    x_scaler = x_scaler.fit(x) \n",
    "\n",
    "    # 设置保存归一化参数路径\n",
    "    if not os.path.exists(scaler_path):\n",
    "        os.makedirs(scaler_path)\n",
    "\n",
    "    # 保存归一化参数\n",
    "    joblib.dump(x_scaler, scaler_path + \"/x_scaler.pkl\")\n",
    "\n",
    "    # 归一化\n",
    "    x = x_scaler.transform(x)\n",
    "    x = x.reshape(m, n)\n",
    "\n",
    "    # 划分数据集\n",
    "    x_train, x_valid, y_train, y_valid = train_test_split(x, y, test_size=1-train_ratio, random_state=42)\n",
    "    train = [x_train, y_train]\n",
    "    valid = [x_valid, y_valid]\n",
    "    \n",
    "    return x_scaler, train, valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b6ad62b-352d-43ce-adbe-003275f750b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T05:45:35.394832Z",
     "iopub.status.busy": "2024-12-16T05:45:35.393832Z",
     "iopub.status.idle": "2024-12-16T05:45:35.919376Z",
     "shell.execute_reply": "2024-12-16T05:45:35.918806Z",
     "shell.execute_reply.started": "2024-12-16T05:45:35.394832Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (3252, 9600) y_train shape: torch.Size([3252])\n",
      "x_valid shape: (814, 9600) y_valid shape: torch.Size([814])\n"
     ]
    }
   ],
   "source": [
    "# 构造参数字典\n",
    "params1 = {\n",
    "    \"data_list\": [x, y],\n",
    "    \"train_ratio\": 0.8,\n",
    "    \"scaler_path\": '../outputs/scalers/TimesNet_audio_C'\n",
    "}\n",
    "\n",
    "# 函数传参\n",
    "x_scaler, train_data, valid_data = divider(**params1)\n",
    "print(\"x_train shape: {0} y_train shape: {1}\".format(\n",
    "    train_data[0].shape, train_data[1].shape))\n",
    "print(\"x_valid shape: {0} y_valid shape: {1}\".format(\n",
    "    valid_data[0].shape, valid_data[1].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c816cf1-4ea6-48ea-a6a9-991d2400985f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ba9d7bf-f780-4916-95b9-0ccaffd8ce23",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T09:13:32.376581Z",
     "iopub.status.busy": "2024-12-15T09:13:32.374574Z",
     "iopub.status.idle": "2024-12-15T09:13:32.466383Z",
     "shell.execute_reply": "2024-12-15T09:13:32.464873Z",
     "shell.execute_reply.started": "2024-12-15T09:13:32.376581Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: torch.Size([3252, 9600]) y_train shape: torch.Size([3252])\n",
      "x_valid shape: torch.Size([814, 9600]) y_valid shape: torch.Size([814])\n"
     ]
    }
   ],
   "source": [
    "# 划分数据集\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "print(\"x_train shape: {0} y_train shape: {1}\".format(\n",
    "    x_train.shape, y_train.shape))\n",
    "print(\"x_valid shape: {0} y_valid shape: {1}\".format(\n",
    "    x_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8513ab04-a7d0-43fd-bcc7-7762db846ede",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1043c7-b97a-4dc7-bb1e-6532ce4848fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87a865ea-fb41-4851-ae1c-6840a7b5cddd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T05:45:39.052856Z",
     "iopub.status.busy": "2024-12-16T05:45:39.051628Z",
     "iopub.status.idle": "2024-12-16T05:45:39.086050Z",
     "shell.execute_reply": "2024-12-16T05:45:39.084030Z",
     "shell.execute_reply.started": "2024-12-16T05:45:39.052856Z"
    }
   },
   "outputs": [],
   "source": [
    "# 生成数据集\n",
    "def generator(data_list, batch_size):\n",
    "    # 获取数据\n",
    "    feature = data_list[0]  # 特征\n",
    "    target = data_list[1]  # 目标\n",
    "    \n",
    "    # 音频类\n",
    "    class AudioDataset(Dataset):\n",
    "        def __init__(self, waveforms, labels):\n",
    "            self.waveforms = waveforms\n",
    "            self.labels = labels\n",
    "            self.mfcc_transform = MFCC(\n",
    "                sample_rate=16000,  # 假设采样率为16000\n",
    "                n_mfcc=13,\n",
    "                melkwargs={'n_fft': 400, 'hop_length': 160, 'n_mels': 23, 'center': False}\n",
    "            )\n",
    "    \n",
    "        def __len__(self):\n",
    "            return len(self.waveforms)\n",
    "    \n",
    "        def __getitem__(self, idx):\n",
    "            # 获取音频和标签\n",
    "            waveforms = self.waveforms[idx]\n",
    "            label = self.labels[idx]\n",
    "            \n",
    "            # MFCC\n",
    "            mfcc_features = self.mfcc_transform(torch.tensor(waveforms, dtype = torch.float32))\n",
    "    \n",
    "            return mfcc_features.permute(1, 0), torch.tensor(label, dtype = torch.long), torch.ones(mfcc_features.shape[1])\n",
    "\n",
    "    # 生成数据集\n",
    "    dataset = AudioDataset(feature, target)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8053089-620d-497a-b089-de8d05bfca93",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T05:45:41.757406Z",
     "iopub.status.busy": "2024-12-16T05:45:41.754396Z",
     "iopub.status.idle": "2024-12-16T05:45:41.868050Z",
     "shell.execute_reply": "2024-12-16T05:45:41.866882Z",
     "shell.execute_reply.started": "2024-12-16T05:45:41.756479Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loader_len: 102\n",
      "test_loader_len: 26\n",
      "batch_x shape: torch.Size([32, 58, 13])\n",
      "batch_y shape: torch.Size([32])\n",
      "mask_x shape: torch.Size([32, 58])\n"
     ]
    }
   ],
   "source": [
    "# 构造参数字典\n",
    "params2 = {\n",
    "    \"batch_size\": 32,\n",
    "}\n",
    "train_loader = generator(train_data, **params2)\n",
    "valid_loader = generator(valid_data, **params2)\n",
    "print(\"train_loader_len: {}\".format(len(train_loader)))\n",
    "print(\"test_loader_len: {}\".format(len(valid_loader)))\n",
    "for batch_x, batch_y, mask_x in train_loader:\n",
    "    print(\"batch_x shape:\", batch_x.shape)\n",
    "    print(\"batch_y shape:\", batch_y.shape)\n",
    "    print(\"mask_x shape:\", mask_x.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19837aff-19bf-488e-b799-45a407ec7066",
   "metadata": {},
   "source": [
    "## 模型定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43cb9506-34ed-4e2d-8b25-c728ba6e0324",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T05:45:44.725671Z",
     "iopub.status.busy": "2024-12-16T05:45:44.723679Z",
     "iopub.status.idle": "2024-12-16T05:45:44.802176Z",
     "shell.execute_reply": "2024-12-16T05:45:44.800122Z",
     "shell.execute_reply.started": "2024-12-16T05:45:44.725671Z"
    }
   },
   "outputs": [],
   "source": [
    "# 位置编码，是对于每一条序列位置的编码，和具体的值无关\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model).float()\n",
    "        pe.require_grad = False\n",
    "\n",
    "        position = torch.arange(0, max_len).float().unsqueeze(1)\n",
    "        div_term = (torch.arange(0, d_model, 2).float() *\n",
    "                    -(math.log(10000.0) / d_model)).exp()\n",
    "\n",
    "        # 注意：d_model需要是偶数\n",
    "        # 0::2：从0开始，以步长为2进行取值，取到的都是偶数位置\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        # 1::2：从1开始，以步长为2进行取值，取到的都是奇数位置\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0)\n",
    "        # 作用是该组参数不会更新，但是保存模型时，该组参数又作为模型参数被保存\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pe[:, :x.size(1)]\n",
    "\n",
    "\n",
    "# 使用conv1d的目的是要对序列中的每一个时间点上的数据（也就是token）来做编码\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, c_in, d_model):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        padding = 1 if torch.__version__ >= '1.5.0' else 2\n",
    "        self.tokenConv = nn.Conv1d(in_channels=c_in,\n",
    "                                   out_channels=d_model,\n",
    "                                   kernel_size=3,\n",
    "                                   padding=padding,\n",
    "                                   padding_mode='circular',\n",
    "                                   bias=False)\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                nn.init.kaiming_normal_(m.weight,\n",
    "                                        mode='fan_in',\n",
    "                                        nonlinearity='leaky_relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.tokenConv(x.permute(0, 2, 1)).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DataEmbedding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 c_in,\n",
    "                 d_model,\n",
    "                 dropout=0.1):\n",
    "        super(DataEmbedding, self).__init__()\n",
    "\n",
    "        self.value_embedding = TokenEmbedding(c_in=c_in, d_model=d_model)\n",
    "        self.position_embedding = PositionalEmbedding(d_model=d_model)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.value_embedding(x) + self.position_embedding(x)\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "# 将不同的卷积层通过并联的方式结合在一起，经过不同卷积层处理的结果矩阵在深度这个维度拼接起来，形成一个更深的矩阵\n",
    "class Inception_Block_V1(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels,\n",
    "                 num_kernels=6,\n",
    "                 init_weight=True):\n",
    "        super(Inception_Block_V1, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.num_kernels = num_kernels  # 卷积核尺寸\n",
    "        kernels = []\n",
    "        for i in range(self.num_kernels):\n",
    "            kernels.append(\n",
    "                nn.Conv2d(in_channels,\n",
    "                          out_channels,\n",
    "                          kernel_size=2 * i + 1,\n",
    "                          padding=i))\n",
    "        self.kernels = nn.ModuleList(kernels)\n",
    "        if init_weight:\n",
    "            self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight,\n",
    "                                        mode='fan_out',\n",
    "                                        nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        res_list = []\n",
    "        for i in range(self.num_kernels):\n",
    "            res_list.append(self.kernels[i](x))\n",
    "        res = torch.stack(res_list, dim=-1).mean(-1)\n",
    "        return res\n",
    "\n",
    "\n",
    "# 快速傅里叶变换，返回周期长度和频率\n",
    "def FFT_for_Period(x, k=2):\n",
    "    # [B, T, C]\n",
    "    xf = torch.fft.rfft(x, dim=1)\n",
    "    # find period by amplitudes\n",
    "    frequency_list = abs(xf).mean(0).mean(-1)\n",
    "    frequency_list[0] = 0\n",
    "    _, top_list = torch.topk(frequency_list, k)\n",
    "    top_list = top_list.detach().cpu().numpy()\n",
    "    period = x.shape[1] // top_list\n",
    "    return period, abs(xf).mean(-1)[:, top_list]\n",
    "\n",
    "\n",
    "class TimesBlock(nn.Module):\n",
    "    def __init__(self, seq_len, pred_len, top_k, d_model, d_ff, num_kernels):\n",
    "        super(TimesBlock, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.pred_len = pred_len\n",
    "        self.k = top_k  # 强度最大的K个频率，不宜过大，容易出现0的情况\n",
    "        # parameter-efficient design\n",
    "        self.conv = nn.Sequential(\n",
    "            Inception_Block_V1(d_model, d_ff, num_kernels=num_kernels),\n",
    "            nn.GELU(),\n",
    "            Inception_Block_V1(d_ff, d_model, num_kernels=num_kernels))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        period_list, period_weight = FFT_for_Period(x, self.k)\n",
    "\n",
    "        res = []\n",
    "        for i in range(self.k):\n",
    "            period = period_list[i]\n",
    "            # padding\n",
    "            if (self.seq_len + self.pred_len) % period != 0:\n",
    "                length = ((\n",
    "                    (self.seq_len + self.pred_len) // period) + 1) * period\n",
    "                padding = torch.zeros([\n",
    "                    x.shape[0], (length - (self.seq_len + self.pred_len)),\n",
    "                    x.shape[2]\n",
    "                ]).to(x.device)\n",
    "                out = torch.cat([x, padding], dim=1)\n",
    "            else:\n",
    "                length = (self.seq_len + self.pred_len)\n",
    "                out = x\n",
    "            # reshape\n",
    "            out = out.reshape(B, length // period, period,\n",
    "                              C).permute(0, 3, 1, 2).contiguous()\n",
    "            # 2D conv: from 1d Variation to 2d Variation\n",
    "            out = self.conv(out)\n",
    "            # reshape back\n",
    "            out = out.permute(0, 2, 3, 1).reshape(B, -1, C)\n",
    "            res.append(out[:, :(self.seq_len + self.pred_len), :])\n",
    "        res = torch.stack(res, dim=-1)\n",
    "        # 自适应融合\n",
    "        period_weight = F.softmax(period_weight, dim=1)\n",
    "        period_weight = period_weight.unsqueeze(1).unsqueeze(1).repeat(\n",
    "            1, T, C, 1)\n",
    "        res = torch.sum(res * period_weight, -1)\n",
    "        # 残差连接\n",
    "        res = res + x\n",
    "        return res\n",
    "\n",
    "\n",
    "# TimesNet模型\n",
    "class TimesNet(nn.Module):\n",
    "    def __init__(self, seq_len, top_k, d_model, d_ff, num_kernels,\n",
    "                 e_layers, enc_in, dropout, num_class):\n",
    "        super(TimesNet, self).__init__()\n",
    "        self.seq_len = seq_len  # 输入序列长度\n",
    "        self.pred_len = 0  # 输出序列长度\n",
    "        self.model = nn.ModuleList([\n",
    "            TimesBlock(self.seq_len, self.pred_len, top_k, d_model, d_ff,\n",
    "                       num_kernels) for _ in range(e_layers)\n",
    "        ])\n",
    "        self.enc_embedding = DataEmbedding(\n",
    "            enc_in,  # 编码器维度\n",
    "            d_model,  # 隐藏层维度\n",
    "            dropout)\n",
    "        self.layer = e_layers  # TimesBlock层数\n",
    "        self.layer_norm = nn.LayerNorm(d_model)\n",
    "        self.act = F.gelu\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.projection = nn.Linear(d_model * seq_len, num_class)\n",
    "\n",
    "    def forward(self, x_enc, x_mark_enc):\n",
    "        # embedding\n",
    "        enc_out = self.enc_embedding(x_enc)  # [B,T,C]\n",
    "        # TimesNet\n",
    "        for i in range(self.layer):\n",
    "            enc_out = self.layer_norm(self.model[i](enc_out))\n",
    "        # Output\n",
    "        # the output transformer encoder/decoder embeddings don't include non-linearity\n",
    "        output = self.act(enc_out)\n",
    "        output = self.dropout(output)\n",
    "        # zero-out padding embeddings\n",
    "        output = output * x_mark_enc.unsqueeze(-1)\n",
    "        # (batch_size, seq_length * d_model)\n",
    "        output = output.reshape(output.shape[0], -1)\n",
    "        output = self.projection(output)  # (batch_size, num_classes)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5af6c8-299e-4e88-857d-748542b792e7",
   "metadata": {},
   "source": [
    "## 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d7e5878-cc76-47fe-9e84-fd5334f048e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T05:45:46.525960Z",
     "iopub.status.busy": "2024-12-16T05:45:46.523943Z",
     "iopub.status.idle": "2024-12-16T05:45:46.571538Z",
     "shell.execute_reply": "2024-12-16T05:45:46.570690Z",
     "shell.execute_reply.started": "2024-12-16T05:45:46.525960Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(train_args, model_args):\n",
    "    # 参数配置\n",
    "    model_name = train_args['model_name'] # 模型名称\n",
    "    train_loader = train_args['train_loader'] # 训练集\n",
    "    valid_loader = train_args['valid_loader'] # 验证集\n",
    "    n_epochs = train_args['n_epochs'] # 训练次数\n",
    "    learning_rate = train_args['learning_rate'] # 学习率\n",
    "    loss = train_args['loss'] # 损失函数\n",
    "    patience = train_args['patience'] # 最大早停次数阈值，超过就会早停\n",
    "    lradj = train_args['lradj'] # 学习率函数\n",
    "    model_path = train_args['model_path'] # 模型保存路径\n",
    "    verbose = train_args['verbose'] # 打印训练过程\n",
    "    plots = train_args['plots'] # 绘制损失图\n",
    "    device = train_args['device'] # 训练设备，可选'cuda'和'cpu'\n",
    "    \n",
    "    #检查是否可用GPU\n",
    "    device = torch.device(device)\n",
    "    \n",
    "    # 创建模型和优化器\n",
    "    model = model_name(**model_args).to(device)\n",
    "    optimizer = optim.RAdam(model.parameters(), lr=learning_rate)\n",
    "    criterion = loss\n",
    "    \n",
    "    # 调整学习率\n",
    "    def adjust_learning_rate(optimizer, epoch, lradj, learning_rate, train_epochs):\n",
    "        # lr = learning_rate * (0.2 ** (epoch // 2))\n",
    "        if lradj == 'type1':\n",
    "            lr_adjust = {epoch: learning_rate * (0.5 ** ((epoch - 1) // 1))}\n",
    "        elif lradj == 'type2':\n",
    "            lr_adjust = {\n",
    "                2: 5e-5, 4: 1e-5, 6: 5e-6, 8: 1e-6,\n",
    "                10: 5e-7, 15: 1e-7, 20: 5e-8\n",
    "            }\n",
    "        elif lradj == \"cosine\":\n",
    "            lr_adjust = {epoch: learning_rate /2 * (1 + math.cos(epoch / train_epochs * math.pi))}\n",
    "        if epoch in lr_adjust.keys():\n",
    "            lr = lr_adjust[epoch]\n",
    "            # 参数组(param_groups)是用来指定不同的参数组以便对它们进行不同的优化设置，比如'lr'\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "            print('Updating learning rate to {}'.format(lr))\n",
    "    \n",
    "    # 设置早停\n",
    "    class EarlyStopping():\n",
    "        def __init__(self, patience=7, verbose=False, delta=0):\n",
    "            self.patience = patience # 连续超限次数，如果满足条件，则早停\n",
    "            self.verbose = verbose\n",
    "            self.counter = 0\n",
    "            self.best_score = None\n",
    "            self.early_stop = False\n",
    "            self.val_loss_min = np.Inf\n",
    "            self.delta = delta\n",
    "\n",
    "        def __call__(self, val_loss, model, path):\n",
    "            score = -val_loss\n",
    "            if self.best_score is None:\n",
    "                self.best_score = score\n",
    "                self.save_checkpoint(val_loss, model, path)\n",
    "            elif score < self.best_score + self.delta:\n",
    "                self.counter += 1\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "                if self.counter >= self.patience:\n",
    "                    self.early_stop = True\n",
    "            else:\n",
    "                self.best_score = score\n",
    "                self.save_checkpoint(val_loss, model, path)\n",
    "                self.counter = 0\n",
    "\n",
    "        def save_checkpoint(self, val_loss, model, path):\n",
    "            if self.verbose:\n",
    "                print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "            torch.save(model.state_dict(), path + '/' + 'checkpoint.pth')\n",
    "            self.val_loss_min = val_loss\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=verbose)\n",
    "    \n",
    "    # 设置保存模型路径\n",
    "    if not os.path.exists(model_path):\n",
    "        os.makedirs(model_path)\n",
    "\n",
    "    # 模型训练和验证\n",
    "    train_losses, val_losses = [], []\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        train_correct_predictions = 0\n",
    "        for batch_x, batch_y, mask_x in train_loader:\n",
    "            #将数据移至 GPU\n",
    "            batch_x = batch_x.to(device) \n",
    "            batch_y = batch_y.to(device) \n",
    "            mask_x = mask_x.to(device)\n",
    "            # 清空梯度\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x, mask_x)\n",
    "            train_loss = criterion(outputs, batch_y)\n",
    "            # 反向传播计算得到每个参数的梯度值\n",
    "            train_loss.backward()\n",
    "            # 通过梯度下降执行一步参数更新\n",
    "            optimizer.step()\n",
    "            #每个batch的loss和\n",
    "            total_train_loss += train_loss.item() # .item()表示只包含一个元素的tensor中提取值\n",
    "            # 计算准确率\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            train_correct_predictions += torch.sum(preds == batch_y)\n",
    "                \n",
    "        #每个epoch的损失平均\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        \n",
    "        #所有epoch的loss\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # 计算准确率\n",
    "        train_accuracy = train_correct_predictions.double() / len(train_loader.dataset)\n",
    "        \n",
    "        #评估模型\n",
    "        model.eval()\n",
    "        preds = []\n",
    "        trues = []\n",
    "        #关闭自动求导功能，只使用训练好的模型进行预测或评估，不需要进行梯度计算和参数更新\n",
    "        with torch.no_grad():\n",
    "            total_val_loss = 0\n",
    "            val_correct_predictions = 0\n",
    "            for batch_x, batch_y, mask_x in valid_loader:\n",
    "                #将数据移至 GPU\n",
    "                batch_x = batch_x.to(device) \n",
    "                batch_y = batch_y.to(device) \n",
    "                mask_x = mask_x.to(device)\n",
    "                outputs = model(batch_x, mask_x)\n",
    "                val_loss = criterion(outputs, batch_y)\n",
    "                #每个batch的loss和\n",
    "                total_val_loss += val_loss.item()\n",
    "                # 计算准确率\n",
    "                _, preds = torch.max(outputs, dim=1)\n",
    "                val_correct_predictions += torch.sum(preds == batch_y)\n",
    "                \n",
    "        #每个epoch的损失平均\n",
    "        avg_val_loss = total_val_loss / len(valid_loader)\n",
    "        \n",
    "        #所有epoch的loss\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        # 计算准确率\n",
    "        val_accuracy = val_correct_predictions.double() / len(valid_loader.dataset)\n",
    "\n",
    "        # 打印训练过程\n",
    "        if verbose:\n",
    "            print(f'Epoch [{epoch+1}/{n_epochs}], Training Loss: {avg_train_loss:.4f}, Training Accuracy: {train_accuracy:.4f}, Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "        # 设置早停\n",
    "        early_stopping(avg_val_loss, model, model_path)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping!\")\n",
    "            break\n",
    "            \n",
    "        # 调整学习率\n",
    "        adjust_learning_rate(optimizer, epoch+1, lradj, learning_rate, n_epochs)\n",
    "\n",
    "    #绘制损失函数图\n",
    "    def plot_loss(train_losses, val_losses):\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.style.use('seaborn-v0_8-paper') #绘制背景色\n",
    "        plt.grid(axis = 'y',linewidth=0.35) #绘制网格\n",
    "        plt.plot(val_losses, linestyle='-',color = '#11b3b6')\n",
    "        plt.plot(train_losses, linestyle='-',color = '#f14643')\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Training and Validation Progress\")\n",
    "        plt.legend([\"Validation\", \"Training\"])\n",
    "        plt.show()\n",
    "    if plots:\n",
    "        plot_loss(train_losses, val_losses)\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "374743af-f3a6-44ee-b7dc-7357b5d2af38",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-16T05:45:54.360355Z",
     "iopub.status.busy": "2024-12-16T05:45:54.357048Z",
     "iopub.status.idle": "2024-12-16T09:37:20.306159Z",
     "shell.execute_reply": "2024-12-16T09:37:20.303149Z",
     "shell.execute_reply.started": "2024-12-16T05:45:54.360355Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▌                                                                             | 1/50 [13:48<11:16:40, 828.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Training Loss: 2.3253, Training Accuracy: 0.1780, Validation Loss: 2.0875, Validation Accuracy: 0.2936\n",
      "Validation loss decreased (inf --> 2.087543).  Saving model ...\n",
      "Updating learning rate to 0.0009990133642141358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|███▏                                                                           | 2/50 [28:35<11:30:18, 862.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/50], Training Loss: 2.0344, Training Accuracy: 0.2887, Validation Loss: 1.9288, Validation Accuracy: 0.3452\n",
      "Validation loss decreased (2.087543 --> 1.928755).  Saving model ...\n",
      "Updating learning rate to 0.000996057350657239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|████▋                                                                          | 3/50 [42:46<11:11:36, 857.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/50], Training Loss: 1.7759, Training Accuracy: 0.3896, Validation Loss: 1.7497, Validation Accuracy: 0.4189\n",
      "Validation loss decreased (1.928755 --> 1.749654).  Saving model ...\n",
      "Updating learning rate to 0.0009911436253643444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|██████▎                                                                        | 4/50 [56:56<10:55:10, 854.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/50], Training Loss: 1.6457, Training Accuracy: 0.4311, Validation Loss: 1.6323, Validation Accuracy: 0.4201\n",
      "Validation loss decreased (1.749654 --> 1.632253).  Saving model ...\n",
      "Updating learning rate to 0.0009842915805643156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|███████▋                                                                     | 5/50 [1:11:12<10:41:16, 855.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/50], Training Loss: 1.5023, Training Accuracy: 0.4760, Validation Loss: 1.5491, Validation Accuracy: 0.4558\n",
      "Validation loss decreased (1.632253 --> 1.549082).  Saving model ...\n",
      "Updating learning rate to 0.0009755282581475768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█████████▏                                                                   | 6/50 [1:25:31<10:28:06, 856.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/50], Training Loss: 1.3981, Training Accuracy: 0.5077, Validation Loss: 1.4740, Validation Accuracy: 0.4951\n",
      "Validation loss decreased (1.549082 --> 1.474049).  Saving model ...\n",
      "Updating learning rate to 0.0009648882429441257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|██████████▊                                                                  | 7/50 [1:39:55<10:15:31, 858.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/50], Training Loss: 1.2388, Training Accuracy: 0.5670, Validation Loss: 1.3876, Validation Accuracy: 0.5135\n",
      "Validation loss decreased (1.474049 --> 1.387619).  Saving model ...\n",
      "Updating learning rate to 0.0009524135262330098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|████████████▎                                                                | 8/50 [1:54:12<10:00:46, 858.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/50], Training Loss: 1.1725, Training Accuracy: 0.5935, Validation Loss: 1.4656, Validation Accuracy: 0.5504\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 0.0009381533400219318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|██████████████                                                                | 9/50 [2:08:30<9:46:27, 858.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/50], Training Loss: 1.0482, Training Accuracy: 0.6347, Validation Loss: 1.3086, Validation Accuracy: 0.5676\n",
      "Validation loss decreased (1.387619 --> 1.308612).  Saving model ...\n",
      "Updating learning rate to 0.0009221639627510075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|███████████████▍                                                             | 10/50 [2:22:47<9:31:53, 857.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/50], Training Loss: 0.9159, Training Accuracy: 0.6879, Validation Loss: 1.2856, Validation Accuracy: 0.5921\n",
      "Validation loss decreased (1.308612 --> 1.285593).  Saving model ...\n",
      "Updating learning rate to 0.0009045084971874737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|████████████████▉                                                            | 11/50 [2:37:15<9:19:37, 860.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/50], Training Loss: 0.8358, Training Accuracy: 0.7168, Validation Loss: 1.2949, Validation Accuracy: 0.5971\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 0.0008852566213878947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██████████████████▍                                                          | 12/50 [2:51:40<9:06:06, 862.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/50], Training Loss: 0.7417, Training Accuracy: 0.7482, Validation Loss: 1.2735, Validation Accuracy: 0.6069\n",
      "Validation loss decreased (1.285593 --> 1.273476).  Saving model ...\n",
      "Updating learning rate to 0.0008644843137107057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|████████████████████                                                         | 13/50 [3:05:59<8:51:07, 861.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/50], Training Loss: 0.6754, Training Accuracy: 0.7740, Validation Loss: 1.2012, Validation Accuracy: 0.6020\n",
      "Validation loss decreased (1.273476 --> 1.201211).  Saving model ...\n",
      "Updating learning rate to 0.0008422735529643444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|█████████████████████▌                                                       | 14/50 [3:20:23<8:37:07, 861.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/50], Training Loss: 0.6021, Training Accuracy: 0.7992, Validation Loss: 1.3377, Validation Accuracy: 0.5958\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 0.0008187119948743449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███████████████████████                                                      | 15/50 [3:34:47<8:23:09, 862.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/50], Training Loss: 0.5162, Training Accuracy: 0.8389, Validation Loss: 1.1895, Validation Accuracy: 0.6314\n",
      "Validation loss decreased (1.201211 --> 1.189550).  Saving model ...\n",
      "Updating learning rate to 0.0007938926261462366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|████████████████████████▋                                                    | 16/50 [3:49:08<8:08:37, 862.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/50], Training Loss: 0.4515, Training Accuracy: 0.8567, Validation Loss: 1.2048, Validation Accuracy: 0.6486\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Updating learning rate to 0.0007679133974894983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|████████████████████████▋                                                    | 16/50 [3:51:22<8:11:40, 867.65s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 29\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 构造参数字典\u001b[39;00m\n\u001b[0;32m      2\u001b[0m params3 \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_args\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: TimesNet,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     27\u001b[0m     },\n\u001b[0;32m     28\u001b[0m }\n\u001b[1;32m---> 29\u001b[0m model \u001b[38;5;241m=\u001b[39m train(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams3)\n",
      "Cell \u001b[1;32mIn[10], line 100\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(train_args, model_args)\u001b[0m\n\u001b[0;32m     98\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m#每个batch的loss和\u001b[39;00m\n\u001b[1;32m--> 100\u001b[0m total_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m train_loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;66;03m# .item()表示只包含一个元素的tensor中提取值\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;66;03m# 计算准确率\u001b[39;00m\n\u001b[0;32m    102\u001b[0m _, preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 构造参数字典\n",
    "params3 = {\n",
    "    \"train_args\": {\n",
    "        \"model_name\": TimesNet,\n",
    "        \"train_loader\": train_loader,\n",
    "        \"valid_loader\": valid_loader,\n",
    "        \"n_epochs\": 50,\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"loss\": nn.CrossEntropyLoss(),\n",
    "        \"patience\": 10,\n",
    "        \"lradj\": 'cosine',\n",
    "        \"model_path\": \"../outputs/best_models/TimesNet_audio_C\",\n",
    "        \"device\": 'cuda',\n",
    "        \"verbose\": True,\n",
    "        \"plots\": True,\n",
    "    },\n",
    "    \"model_args\": {\n",
    "        'seq_len': 58,\n",
    "        'top_k': 3,\n",
    "        'd_model': 128,\n",
    "        'num_kernels': 6,\n",
    "        'd_ff': 256,\n",
    "        'dropout': 0.1,\n",
    "        'e_layers': 2,\n",
    "        'enc_in': 13,\n",
    "        'num_class': 12,\n",
    "    },\n",
    "}\n",
    "model = train(**params3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4bf412-ce76-4bf9-bc4b-a3db17d82035",
   "metadata": {},
   "source": [
    "## 模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d76f21-e690-4da3-9764-efec1a9eaa72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
