{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1039d6e5-9600-43eb-b2cc-8c206b22b9ab",
   "metadata": {},
   "source": [
    "# 分布式训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d53648a-3496-4416-8144-36f272f56e7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-28T10:58:36.967030Z",
     "iopub.status.busy": "2024-12-28T10:58:36.967030Z",
     "iopub.status.idle": "2024-12-28T10:59:38.273190Z",
     "shell.execute_reply": "2024-12-28T10:59:38.272222Z",
     "shell.execute_reply.started": "2024-12-28T10:58:36.967030Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "from datetime import timedelta\n",
    "from numpy import ndarray\n",
    "from typing import Union, List, Dict\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm  # 打印进度条\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List\n",
    "from pandas.tseries import offsets\n",
    "from pandas.tseries.frequencies import to_offset\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import joblib\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # 显示中文标签\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5831d7b-473c-4d5c-be94-041879a6dc68",
   "metadata": {},
   "source": [
    "深度学习的分布式训练通常用于加速训练过程，特别是在处理大规模数据集时，或者需要大量计算资源的任务。PyTorch 提供了多种方法来进行分布式训练，包括数据并行、模型并行和混合并行等。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a2bdea-2dc6-403b-a0bb-575e08b10317",
   "metadata": {},
   "source": [
    "## 数据并行"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80499b4-9e96-4609-8ba6-54cbbd098705",
   "metadata": {},
   "source": [
    "数据并行是最常见的分布式训练方式。它的基本思想是将数据划分成多个小批次，并将每个批次分配给不同的计算节点（如 GPU）。每个节点计算梯度，并将梯度进行汇总更新。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c981d1-bce8-4166-94bb-62e00a6ed4ba",
   "metadata": {},
   "source": [
    "### 单机多卡"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07d7e94-5208-40c8-899e-1a1f973b1815",
   "metadata": {},
   "source": [
    "torch.nn.DataParallel 是 PyTorch 提供的一个简单的工具，用于在多个 GPU 上进行数据并行训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "760dcfac-c8af-485e-93db-e951df92eed8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-29T06:46:32.864855Z",
     "iopub.status.busy": "2024-12-29T06:46:32.863856Z",
     "iopub.status.idle": "2024-12-29T06:46:34.432570Z",
     "shell.execute_reply": "2024-12-29T06:46:34.431612Z",
     "shell.execute_reply.started": "2024-12-29T06:46:32.864855Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 使用DataParallel将模型并行化\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mDataParallel(\u001b[43mmodel\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# 使用DataParallel将模型并行化\n",
    "model = nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117f5a1a-a7c2-45ce-9903-22f2b44d67cb",
   "metadata": {},
   "source": [
    "### 多机多卡"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b515b5-0e39-4eaf-a0bb-b8125c2b7eae",
   "metadata": {},
   "source": [
    "torch.nn.parallel.DistributedDataParallel（DDP）是 PyTorch 中推荐的分布式训练方式，相比 DataParallel，DDP 在性能上有显著的提升，特别是在多机多卡的环境下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aaf0998-0332-4169-b244-610279634042",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2225b743-53e8-4468-a8c0-d9845a0b829f",
   "metadata": {},
   "source": [
    "## 模型并行"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2debfa27-4b71-4b08-b893-9a7db460566a",
   "metadata": {},
   "source": [
    "当模型过大，无法完全放入单个 GPU 时，可以使用模型并行。在这种方式下，模型的不同部分会分配到不同的设备（GPU）上，每个设备处理它自己的部分。\n",
    "\n",
    "例如，如果你有一个非常大的模型，可以将其分成多个部分，每个部分运行在不同的 GPU 上。PyTorch 允许通过将模型的不同层放到不同的设备上来实现模型并行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698da1bf-59a6-41db-a46d-a93d372d7434",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d4c489-f9e4-4628-826e-7ad8a0336987",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e39d0626-f9dd-499e-8dee-eb0f2b4703f9",
   "metadata": {},
   "source": [
    "## 混合并行"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921d3940-b688-4ac1-94c9-efee9a5adf1b",
   "metadata": {},
   "source": [
    "混合并行结合了数据并行和模型并行。它适用于非常大的模型或数据集，可以在多个节点和多个 GPU 上分配任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d63a1da-3190-4e60-bb73-f8ce7caef402",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
