{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa190d5d-4900-4715-a7dd-fe4a08ba4373",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-26T02:11:54.523917Z",
     "iopub.status.busy": "2024-12-26T02:11:54.522724Z",
     "iopub.status.idle": "2024-12-26T02:11:56.328181Z",
     "shell.execute_reply": "2024-12-26T02:11:56.327503Z",
     "shell.execute_reply.started": "2024-12-26T02:11:54.523917Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets\n",
    "data_path = '../../../../../data/02.cv/cifar-10/'\n",
    "cifar10 = datasets.CIFAR10(data_path, train=True, download=True) # <1>\n",
    "cifar10_val = datasets.CIFAR10(data_path, train=False, download=True) # <2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42dd03e9-aaf9-4491-86aa-b1c86f2bfa81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-26T02:18:10.869563Z",
     "iopub.status.busy": "2024-12-26T02:18:10.866345Z",
     "iopub.status.idle": "2024-12-26T02:18:11.900739Z",
     "shell.execute_reply": "2024-12-26T02:18:11.899896Z",
     "shell.execute_reply.started": "2024-12-26T02:18:10.869563Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bdc313ba-e966-4e52-bdfb-c4b147c7603b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-26T02:27:52.294707Z",
     "iopub.status.busy": "2024-12-26T02:27:52.293708Z",
     "iopub.status.idle": "2024-12-26T02:27:52.343865Z",
     "shell.execute_reply": "2024-12-26T02:27:52.343142Z",
     "shell.execute_reply.started": "2024-12-26T02:27:52.294707Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集大小: 50000\n",
      "验证集大小: 10000\n",
      "样本标签: 6\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAGEAAABhCAYAAADGBs+jAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAAArU0lEQVR4nO29244kSZKm94moqpm5e0Tkoaq6uqZ7ZnawIAHygk9AgHwfPgZfgg/GveEuwZ2Z3elTdR4j3N1MVUV4IWoeWQMQqBxmYuuiDPCMyAgPP6ioivzyyy/i4u7Or9d/00v/W7+AX69fjfCLuH41wi/g+tUIv4DrVyP8Aq5fjfALuH41wi/g+tUIv4DrVyP8Aq78c+/4P/8v/yvuzuVyZttWFCeLo+KcMkwKd3Pm5alQcuL+NFFy4rhMHOaCipI1ISJoLmjKIAKiNDPOT2dqa3QDc8g5czwsJFXEO5hhZrTWMDNqrbg7OWdSTiBgGO7OVld6b5RSWA4LKsD4nXWj9w4ISkIQROJWSmGeJlJKTNNEzhnVhGqOx20dcwckFkUURHGI19UdByx+yf/2v/8fX9YIZvEm3B0zAxxTRwC3eHLcwP35Kz+9uRug8ThmuAgo8b0/P77b88/MQNwQt0/uF4///P14TAwfz7PfFzfGq7y9BndDkHgtz0v6/Pgm47nDWCKG2XjN7iAyHjOWO97y/trid2NFvqwR/vTnP+HuXM9n1uuVrLAkJ6vAovQslFrYfEKmjJcTkOisbF1jxyHxpjSDKObQLYx6WVd6t/F/R5Ny+ZhREZI4Styv991Y8e7jJGTcjdrjdLReMe9M04T0AyoCEsvWe6e3joqQJSMIbRhfRLiqIipM00xKCZWEaIod7gASmwfB3OkG7k5v/bbJ4oT8bBv8fCO8efMmjPD0xHq9UhROBUqC6VSQKbF5oaWZ1AscBdGCuVBNYrsYY3+EEVrrccTN2Wq9GcA8FuyigggUdZISv4uViMURoaVEyhmzzlZXbOxgx/B5JlNRVWREP+sN62EE0RJGaH2c9HB5Is9GQBSRBAgMd8q4tW7UGi7Keh+GTGga9//SRtDxmEmhKEwJDkUpSThOmcOcOEzhU6cpdqemhGj4W3fH3OLoYvHCzcf/QVTDOUi4IxnfI5ByIifBzenmCILu8UVTLLII2W3szoZ7RySFe3Buvl81AxonLBVAUBeMjviID2O3220p45RoUkQU292m+/P3Y53MDauf44w+wwiTgrtAEnIOA7w6ZeasvL6fOc2Fu8PM/d2RUhLzPFNyAond7G50a5g7rcdi7dECETRn1IXuHufeHbwhCmUuLHPCDKyHEVIqqKS4m4O5ozkPd9TovZOSYhY+OuWMqoLGmxYVcioRLaRirYKD7H4eHUYQdGySUgqiGqe2GeZGt3ECxl/11qm1fYYJPsMIIrE7kwIKJQlzVuacmHNiykpOSlJBVW6hUNxxl5srud3cbscaQIcfNdt/5LegmZKSUkLEkYFOdLgqt92SjJgzHktj98Zr2H/P2BQS7mhskPiZxjO6fHrnT1bgE5Dhn9yet9LYbE63m9/9skYoGIhTcoC7+0PmuxcH5px4WOJETBkSDemdeuk01YGahN6dug2IJ44L5JIoUxlHfZyareLWQRwVUBWWkjjNJWKItzj+vUVg70brsSA2XmtKiVKmgXR6LPbw9TpuiAw0AyKOjpjjNgCmhMsShuG707criGDdwAxxJ8sn5nGnWee6NT6nVPazjZAk3qgmSKIsU+K0FOacOEzKlISkoN4RE3qLwxyQjWGE8aZT3PD4G1EhJY1gp+ONE0ZICiVruDZ3uuxv1nBzrBnW7flND4Mm1U9iSJxIGW4jgisBc9kDvYyF3FG+RlxirDCO9Xr7v5sjHrEykFM8lplT24CyX9oIswYsy5pQNw4lMZfMnBPLMILqvsvY/dcIvBpvIo8dI3Ea9qB923EA1rHe4nt1QFnXiiLD3/aRrHW6Oa13au+IKEkzgtLMcYvYErh+fxfxnJoidvRxEpSAoeqKJLvBXxuxaV/Q3b3V3mk9DLw1H3Eu7n9ZO5f1K52EQw6MV0hkjOOUWaY4CcdZWYreEBBEGuMIqOIIosMFAM3DLYnu6GYEP3ewjreKC5E0qXG9bPTaY+f3CIjXrdJ7p3qnWUc1MxdB1aEbNk6T4KhHPIsEXZGccTOsBTorksmSMCpKQObIzMO/974H63B517WytU5tzmVshnWzYRioXb5OTMhJEaCIkknkvAdhxo4GREjobRfsr8NFwMDUP41jt2O+f/1J1g03o3U31J5RkI0E7/b9yNLNwiU4RrOOYOgAE6k52Y2OxUJ3Y1tbnJicQBVIwISKjXzLRlA3bDy3u9O705pRu7ENI1zXRm1GN6HZ51FyP9sI98eAc5MKRTOHOTMVpWRl5DQk1fDFRCYc+DncWJdBdRh0PFCoGdY6LowT5FiruLXnxASh9kpHcAPzyBfW4e8jL4jnulpHcDardDNEnCSGKhzWQFkuhmnFeqdeApY+HDLHKTFNmcPhgCSHdMVprOuK9zD6tnW6GZe1sm6NtXWero2tGu8+XLleGyaK8ZUy5pLjJEwpUVQoOaHpGSruN9WxC9xuLsm5hYiAmXC77bv+xvP8FHMCcRLcwEwwHwF3JErht7mhLnC2ZtQOOgjGiDlGSuEmTcKttbUhLmwJJlFSUoQcLkxjc4i22wsxH1n7iAe9R87Texho3XoY+WvRFg+nCXDmlJiSMeXEMmeSCKoGz5Cf3ecIkc5368G7WBBnU05Mkig5MU8Fx2lbw9wCBTFjQBtvel0rtbc46n24u0G3ThqJZMDMFjHnCpfqP9kA56SoQkqFlDNznvjmxTeUVEh9QXtGmuAVNEGZFU0Jz0rPOuJLbB0dJ36eFE0T3ZxpWtiqsZmztq+UMZ8OGRGYkzGpkVNiKimw/I6lbwYYX90xa/TWbomXoKQ8oalQSmaagvex6hgW1PWUY6e1jptxuTYu60rtwtYkoK1HMUSLcsgS7jBF7tC2xnU1ugsDZ5EIDzdPE/M8UY4Trx5esZSF9YOzXQzpQIvXXlRIWWkjCXXzG6WhIiQNan6eCw4cT4I5nLfO49q+DkSdcoo/UCfJOOoqgeUlBbrZ6eGRJTuOaiLn3SzPO0kkcuqdwlZVEgnrQu8E8rg0Wjcu185lM1pX1h6LvxDY/nSa+P7+QErCPKVwge+upPPGtTmPa+f5TAxoPOoLvVW6KL1b5BtJBi5w8A5u8Y5Ubq43vlfSoLSD3xJyzogoklogu6+Bjo5LBGbtnWRGEqWookkoIzmybvSRqHTvuAslF3Kef4KYzBXzMEIfCVAqAX63c2etxuVq/PXtylYbj9fGtRrNna0nsjipKDnBb+9e8D/97W9Y5szLhwkD/s9/ecM/vXnizeOVf37zRLOOeQcYmyag8HZ9glrZLo22Rq7Rl6CyIzQ5SY1SxgYsiotTxBEd78UMTYn7+4VpnjlfK4fL+nWMkFMamafuxh/BVm70riM7Y/0Tck72QPWvsesnl9w4+h7wrxm1Glt1aneaRRjoNtDkiMNTUu4GwXc3XMNpmjhNlXPp5BRAodmehDE8pVFbQwxqixOXutItITsAGOhZREZOEyciud5oCoisPiUhJ6FkYS7p67ij+9MpfPxq2NaRlG48e20OdHqPxeOTINYcvO9UQNihmdHdyZLJOcqc3QO+Pq0Xfnz3xLp1Hi+d1o3mGltYdpcAc3aWDKfUuNMrsyemS8eBF1L4bnnAW+HDUdha57yeqb2iLrTaODfnj+uPqCi2Gd6Mo2X6PDN1xZcjs2bMBRNFEsxzIVu65SZmgY5EgjOjOUsW5rvysw3wWUZYphl3p7YrXcMAt91rUYbs5jSLrZYI6Np3nkXkBkn7MELCkRRVKyzojK0aT5eNrRprtQBBKsMIg1oWSMkpyZm0M0tjciPViEUHKdzlwrnAaWokqdS2DQjsWHM6je3awB3Z6YlcmKvRJbP0GbU03n2c5FwSanI70N0Mbbubi2yypEzO6V8v35cxwrQsUU6UjpUULGOO419rpVuH1ukBLwZa8ltCEBTGoJoTiEcNwZjo1nk8X9hq4+m6jvJkuB2RqGFIkPqQEkXgkI0pOWszfvxwIYuSPWEI7/qBqxUMY54Sok7rhU3DDRlRA+gSLsraSNTlmWp3G77vxmyPIP0JJ4aO4sROi8BPGYEvbYTj8QSAzwXaaXDx4fsu1wu1VdBKk23srudivzPoxkFX5zQKLGSMzNYrb96/5Xy98PHpwto2cCWlgMV5IJOcM3kqFIE7NWZxzlvjv/71EVywrjhKnaGXSPKOS2LqAj6ztYRZC9jsxtZqsJ4eOYkTpF63DvstLBC1FLjVl10C9uaUxukKWCqfnPwvbgQZmbBoAu1E7VVjwfegpRqL6xETdli4F1PG1kZTSEnMovJlHVrrtNoQd0oKAsxGlp1T4PKpBC7PCgtGEUdTog/ZSR90cid2O+Kkwf5OUxm15gmRKIFetjO9dy7SgB57BEfECf8YagvYQca+GM8FpP3HLp+JS/8tRogCAOFLUnq2tDN0P4YJ9PFjRQaKGhUwoIsH5VxmkhbWtbGtjXU1tqcr9XxmUed0LFGwaYM+L1Gzvj8deP3qniSQewtKPSUsJXrvrHKN3Zi2qKyRmHMGSRyPL4HE3f2R+4cTWz3z7v2/sG5X3rx95OPHCykLJTeyGsJKEHeKW7wO0TTAgcamdAY/FpmIsb/fr0Tg7VVU+Sk/ET+THcZFqo8PIyC330WJx8abyEjKID7cw1ArtE6alUNRzHYXIZQSiorjXLg/TCQRaIpYZNhIisJ8iqw1qSPSR2YbVbJcJlQnHh7uef36gXUrmL3lujmXy5V1VVSDa4oTbOzyEPdnLuh5FeT5ZOz1hs8hjP4tRmgDetIbYo09ArnbTfHWepBauODj5PShnOve2VpDVDiehGmC3jbcVpTK3ezMDg9H4e4YRqhVQJTjw0vmwx3LlLg7FPBRKu1OKRO5TKTW8BzFnOPxyDRNNIPNIKXM3YuXTNPC6f7A3f2BrYLm16zrkW3ttLXRvUfy6KCa0VQQTYinZzYsfOsnhR8b7vdTI30tI/Sd0O/DCLFT3Iy6G8Gg96AubBzJrbaRDDUu2/Umg5SU6FZx21Aqp9lxFV7dK6/ulG7CVkG08OK7FxwfvkHpZG+YdZ600ruT50JeDvTasJGI3N8/cDoc2Frjsm3kUvj2+wcOxxPzUpgPhVqVUl6zrlc+vnvi/OFMrZVza0P6klEtOBnxoEP6YHj3LMgHHe970X/30P6VArPvKeqndPMnGaWqkkewdoTWY7dstXNdN7p1Wo+i+7ZVUrrS1o26XqFvFHV0Eh7uJl6/mmndOV8NJHM8TSzHiSyh8rDeISdab5RlocwHrBvLNIURjieWeSbXCgk0J3J2NDUMp7ZGbRvrVtm2NuBwQlMQk5qCCdtFE7uD2oHonkn77afxz67qGAv25Y2wVwecUBrAJ2l7ysF+aka00Lrz4SkqTR/PK+8/PI6asqEqpPyR1q7085X6+ETRzjcH4zBlfv/DPf/wdy+5bo2/frjQXVle3jEd75mnmePhhJnx8fEjtW7My8S8TIjvqCC4rKyJ63rh8fEDqDMdGpovVOtcLo1t7bz7cGFbG7U6KU2AIsuoJZtQK0HGjfxmp19C7xqW2Hf8Hg+eDfYVjCBKvKKbXGR/qoEcRNnVzVFBM1rvtNaDo5FQajBogyTQWw18nWzwLsqyFI6nBcmVea00V8qUQh6zzMynMMJmHa2FZc7Mc2hKxWMHp0E3d89Mc8bFR/XPoHfce6gBCWipKZFLGQWqvTYy6hZyKz/F4vouL945Ydkx6ieVqs+LDD+/0H+YA7tXw+vQ+7QovgeKcFrtARVr5927J65r5fF8Zl3XSC6zYAJPbeMqUNyZDaaSubs78HAq3L94zeHhNXZdSVfHOkgJ0et0f8+L3/4t7k7+8J5aNyapFK3sIAE81HFmTFnR6XRLFl1AzRAzdAbPTmtOme+4f1xpdWW7PmHWWddQ8ZkkbBikWx+73AeVJZFQ+i7xHLFguLMvboRS4q5GxjFEeiAhiazKDayFFGVbK+fzOWqx15XW2o2CcAhBrjuSlCUnkiSW+cDhMDMf7piOd2wk0vREb47kBEnJy4Hjy9fxGJpo20q2M9nPhMOsgda2jjcjJSWXOTbILgggYqia4DlhHVQmlqVS1wvrU6K1ivsT6wajdDRE9+GCAwAEhaGqYYQ9GRpQ/XOun4+OtjV2Qd2gRfFb9ipyAKJIige30nujtYqKM5X0XA4EfKt470ylcDhMHI4zx5ffcnw40tIdb58mnp4q7z8ES/liycwygSW2Lfj76XBPXk5kP5L8inunW3zVobK21rFt0AlDIFZSYkmJ7kJpwZIuR6Nv0LYL69MH6rbS+QM8PrFVo9Ue6j6JyuDuslyEQWzQ92TNCcrma6Cj6+UMONpXtFecSMhGkQGApEIfOtReN+p2JefCYZ4oSTlMBcFpLnSvHOaJ0/2R+4c7Xnz/dzy8fMGG8cf3ztPjxp//GhD47kVh1iNihevVyFPm+OIbcikIFaHiVuntEfdYDnGnXjfWpzPeLRCVOXlZyIeF7srZMoaiHBAKvV6p5w9cz4+sNSpk/fFCv15ik6c0CNUhZpM9odwBu4TKz+z/YxX/fxphpI5goe+MILVb+9MyzuBvhhRFzBELHig0wDJ8qTJNE8e7E4fTHdPhjrLccbmcuVzOrNeOj5xjKORx69RtGyghoXkafQoT7hWShxEsSpPaFc0dVwOJejUpsD+aSDqjksh6JKWFtmVUorchlYwmiU4ibGAS/QSUxMaLdgm/CcljqT4PH/38/gSII2gNr+t4rnCOu1tyi+86jYs55w5qoA0mdbw1sgrHnChL4pvvvuXv/rt/4HR3z3e/+/csxzve/cf/i//6j39BbKNYCh2qO9oq9emJ1X9kPt5x/833pHwkzxNpnnA62AX3RrtesLriumGccDN63ULmYsZ1dSRNlMMrNE8cji+Z5hO9PVHXA+ePC9P9jHwEVsNyjQRtLyfuQuN9AzqD7Bu19f6VYsKIRXESev9Xv7WBjqO5wj2KNm3EbR1f26jNalGmIiyHAw8vX3K8u2c5PTAtJ3pXnj5eKNpZJqUMEQHesV6p1yuaJtwHB5UXdDoAHTyN+wlmiuSEluClxDTAxFbZWiUhTDqRdKZMh3juBiIXar2gRYOrTuMNwM3t3nrjcOSmk9rzKP2s0uZnGSFew97Yt/Pso0izC7rMoXqUM8drKyUxlcyscFdgysq339xxfzfz/e9/w7e//xvKfMCTsrUN7yvJVuasvLhbmOdMKQkn+tPK8ch0WFAEawZbx6UDzy1PbYVeFeuJ7oVuwsfLE9u68uHje95/eM9yOPK9w7IcmQ4T82HG7Mq2ndnqBbSTCkzHxKFFuXLnw7wHRxVrEQWgtm4BBHrGrHydQv9+7U+6G2EvWyoeJ7M5NItY4KHcW+bCkpzTZCxT4tvv7nj1+p7f/O47vv3970AS54+Vbd3wvoURdOLhNLMsE3lXO+TEcjxQlgXxMIJL1A7MQwln7lgVrAveE8ZEc/h4rpzPF3788Q1//vMfuLs7cThMdLvjrj0A92GE+kStF1w6WmA6KEeJCqLrgNhtdHia3KT5zTfM26BnnhutvqwRbMfYI+CKBr/OXm2CgKuGSsjmD91ZSmLK0dWzHArLlDk9PHD38mWwo6f7KJi/PXO9XoAeWfAyMZ9OTMtMPt6TDkfy4USa5lFW9VFS3QZe6KzbGbNOW8/0uuHWsFZprQbE7g3BSBlEO1t9Iq/Otp5p9UKra1DqFn3ToXvsUf9kdIwyfuaGYKE+dCOXvT91yGU+4/r5BN5gC0OO4s89XyrY3tXYDZXOlBOv7o4cp0qZMjkLx2Xim1cnDseFH/7+3/GbH77n5d/8HS9/+3suT2ee/tN/5v2PfwFfefXqyN3DPS9++IHpcCTffYMud2iZyfMJSYXuhtUrbV0H47ry+OE9rW5cHt9TrxGko7HDB9Jxcmrc3SVSanx8+gPrFghtmgKi9vVC39bIZWqFbUW2M+4dp7GrviGQXk5R58wKZom6Kuvl8/ijz3dHHqoKFac70fnoe88XI5sOvam7hXxeIGdlWhaWw5HldMdy98B8uCNPC7pWem/UbUPwiCNToSwHyuFAWo7ofATNI/A7tVXEhiapO9u2cnn6OIzwke06Fq7X6Es+FNLQo04loRo0h1kbbbVBkeND0rmrMHrHWwPvuNcbXAWPzFhimoBG6QNLkIp8HRY1jca62o3LdYNk0GLxS8kkTdHEIYYlOCyZUoTrVrleN073L3j1/b/n/sULXv3N/8DLH35guX8BegLfsM2wdSMjlHlhPj1w+va3TMd7bLrH8oHz0xMffnwzehPifW51Y9sqdV35+O5NSBu3K962UdZ2Uk7c2R3TXDgdMq9efBMLmDKaMoflJUkOwfCmjulG7o7Wip8v1HfvQy/rYSSTkSINnkhESHNGkpJTZnqY+SoxYadsrRtb63gH69G9PrmQ8rPkHXXKpGSDdavRUiqF48vvuX/9DXevfsfp1Q/kaUJkwT1HgKsNQUhloiwH5vtXzKd7Vj3iMlGfLrx/fKTVevPf63pl21bqeuXj2zdYq4h31IM2L0XJJXNYBNOZ+f6Blw/3IImuM6KZqZwQKRHPdKNoDm6rd3zdaE/ncMc9mFcbG90FTBVJyuROmjKpCPPhK4m/ojkvtDdpiLE8R/KSNcYePE91CNrak/Lqm5e8+Lbw/d/8ju9//wP3L15xfLinzAuOsZ0fqZcn3ILuNuvUvrFuV9anD7gbZ7uwkXj7pz/yp3/+z/TWhhzF2LaVra6Ru7QL6kYWQVME6+u1k1qiPBWqNabjgVMN7etyeElKE2U6olqw1mjVqJvRGiFM7tHv4D6ISmf0RoCJ0CQ23SqGbp2jGung8BnF/p9P4I2pKoIzZQ35StHRsBet13ujhwjIFAzjy+9/y8O3P/DNb3/HP/yP/z3HU5Qqp+XE9fE953dvuHx4h/cNkU7rUW3L50ce3/5IuTzxYXUuDf7ln/6J//s//Aes1UEKQm0brW8xWeZuJucRT0rmfLny8eNHRBNdnelyoBxOnF7AMc/c3f+WaTogTOCZ5hvrNQy3bc7WhGuFS911rEPXWqMbtTmsRDswa4dkvE6d6e6zbPAZ6Gj0hbHrLm5iAx+JpN+SOdhHGCjzsnB3f8fpdGKaZnLJoy69UdeVbb1St2vMo/A4CTsDW7crCNQ1pPJ1vdK2K9ZbDAdRGcKDjo523CSh6nDrWO8j2IauSVOj1udbqx1Vwy3gbF3XqIe3PjiAhKG0MUVGRwOAD9YYiIrerf02QMpe/PnyRhjwbMzBAdPA0RL9Zs+gLOoEKSVSKbz65hv+9u//nuPDa5Z5QgXev3nDdWusT+85v/sr6/kD18s5Fn5dqeczmpS3P/6JMs1cemJzpV0+knpFrTOJkqP1JuLSlLk/HlBVruuF6+USHZ6tI+rUdQWHpw+PvJ3fcHmqwIkyLdQtFl68IX1lXc9sPtHzPat85MkOgJNziRrClMiSSKJMEtn81ivdO6oBn/WrJGs3bmQs9g7jfHApvjdfBIklEhqkeZ453d2N4U9huOv1wtPTlfX8yOX8SL2c6T2GSfW2n4KN9XLGWqNS6GSsbcigqpN4jODRkK2XHO1XqsrlEidtH3/jJrdhU3XbuF6u4Jnz4xO5NLZ1pbYtRGUKdat0V1wnuhSqp1HVzbjqGJoVHfGiOTKHbZzKqAN/zTzBnxlEDYUdwsim/dbVLinEVqXMvHv7nv/nP/4npsMd93/5iKZC66FkqOuZernQtmBlkyZyLniZKbmQRJ9VHCjLVDgdD5i1AAC94aIoivfO9RKSmrq1MZJHmFLGRaF3+rZx/vAeq0aZZtaPF1LKbG2j9xaQNkXl7/HxHWtd8WSU+wLi6BR1ask9VBxjMwDkLrhlXrw+8Prbh68jednrCe4jrWdoTX2PF9FzZhjqiTxNlGni3du3vH33SJmPnB7+Si4Td/cvmZcDvW307Yq1FSE0qjlPSJnj6GvMl8gpoRIc1Ol0oNdKXc9YbzFyR4NUWy/BbtYxCk0QSso4Eietd56295w/fCRp5nH6ERGlWcX2tl3VgJ7D93tyyn2JmsQcrk1yR5KjHuN9BKGQSSivvzny3XcvnrtYv6QRQuHxLG1UkTGf6Flruqsu0qi74k6vlTakjkkLuUzMOZPEsNawFkExiaI5k5cF1JjmQkoFkRSiAvcxu45P8iC5bQJ3bs2L0cMwQPPYkbs+a99M4ob1Gpn+mI+0h7qoYY+ETCtpdlwNmxqo40noQ30iPU7ipEoSIU/KNJXPqjN/nhGGziaPXrUYZ+TPyjNJuKRnBULrrJcnLuuGauL64S05F1I9I3f38WZHwXyeJpJOTPcnpr0lV6NXYH184rxuXK9XbgNARCOvIHgtcRnlViFpJqcoMA1PiZg/AxYBoUMfdfNhwO6dtVdcjJYarh1bKsuh0rVRpyumneYW9EwX8iZkSZzm10z5wOG+cHd/F8jtSxvhJ1JY2U/DM1wNAiUyaB2KZUbtwVoF7TR3pDf6dqHXMlK7UDgLCdVEKcpcgh/qg6vZS4qxiM9qOG4doAyUFq9n7w7dEYrjt2ks+/8BzKK11/ckzHo0qIhhEsYQaWiJwk7LBjr4pjHNJjyBQwqSUJPG7WucBBktKSllyOUm8kJivlASQfNELgv7mEp6oyQ4zuk2UDCljrQL/SK4aJweTdTsuGeW+URaDuwzjXDnoRw5uvHur3/lfL7GglGxAQWj4uhYjy6hqQgpR6NjGqo5wcZCG92it+7xstK7s40MuXmj+grJmV86aXbmJBwWDYnNoO5ra6yboV1INYEKKc1MyzEUiHXvcfjCRoA4/ioha38enubj+GdymZjmQ+DmMV8iC6QpIR7UhqpBX7EaXTUuCXKm1zSMKqQyx64epc3jMQTE67VGob4ZweaPyh6xu1uLhC+lTPLRS7CjFN2n0EQMab3zeFnZaudyMbYt+tgaK1rg/iRMU8zxSFM0Nyo6GgahNUd6FLFSltEcvyCSaT1Y5i9uhKR7e2wEyL2XjLGQLjEzrlu/zTNSIbT9Y7cmUVTSraNfNKGje9Os4dVobQwH0UQ5nIAYmdmBPB94ePUNbVtZ55neNlpdaW0d3T6hBOyt0yQGn6iOkzDWpNXK5XrlujbOj1e21tmq0DqQnFQSqUh0BU0jIbQBx1u016brRLlCkcSsE0uaOeYTh3KAJjx9WH+2Af5NRqgjoRJNiE6BQiTjkuiAW43ERiN5S0nRMdYypSlw/3QMprQUlrnQzfj49EjrjfV0x9Y60zJxeniFaOLD05lt3ZhOL/n+94XeKuvjO/q28f7tj7x/94be1xhI1Q3VGFBYSkwaDjAdyqB1vfLh3Xsua+PtmwtbDRkMmimLMk+FMivH48LhmMm5I61BF+RpQroxnZW0Tixz4cXpyFJmXi+veDg88PhY+fHHj59V7P/5RkgCHu5EpA/px09HU4byjOciBwTptdei9z43uMn5b621Y1pXq2O8jSTm2pDk1G1jW1e81SFl3Muro22JncwZdPs+D2nc4medfe7pLurNSSO7z9EQkosylRz0twYtIWZQJRTfTZEGyRLiwsTEoSwsZWbKEyUV8BbDUuwrGOHhPgrbrBtcnzByQDVL1Aq9Z24qrXECRIQyRSULxpw7V3ptuHUUI6uGSqIJ3pR3P77l7dt3LMcjr58+oCnx/v1HzucroSSIRU304XoM1UxKRikzprEA21pptbFtGwwX6m602sZCF46HexAlTTMpF0ggBSQ5ORvaHbsY7QzWBa4Z6c6iE0mFl4c7fvfdd8zzzKuHVxyWIx/fbJw/XG7Twr6oEeYpCtjXbKQ0ZohaH0X2Fi0L4vgY4OGehtJuwtIoCg3GcZ+7bSkNpjMG0mJw3S5c28ZWN+YlBtw+vv/I5XJhjy06yqeqMjirnVKPYYXW20BB0HqIssza7dTmpDH4pBzi67yQpxJzkGQoOGQbTKzErJEuyCpgkKcYQbrkhfvDHfMyc5gWphx9EnWtX8cI9/eB/bUmFk2sVfh4iTe71TEpZZR1RARKkGutNUTXoAXot5mpItGztl63GOB3qbRmbH1l6yvWV+bJSSlhtZM9+h22Got5GVnWtlVqrTEec8QZtxaqu4H7cUdzVLv2GasqSs4R00gRNaIhZEyB9+cT2quiBmLx2u/mheNx4sX9kbvTgVxy0OPbmfN55XJuNzf4RY3w8kWMPJ7JrHPhfBmjCaqwbdBaonui2z42OQXL3BqI0VVw1tjJI42yPgRaHa5rSO2bXUdhJ5O4knNing+UNNHryvp4ppuxrjWmPo5MfiqZ+7tjDLEdY3K2beNyjd2vKU7RMse8I1Uhjxyi7w2Po9faEWhCt2GALY2TH5D3fjnw4sWRFw93PNwfERXevovxa48fL5zPG9a/QkzIWQcGj/EzKTm5BNGVp4CQ0h1v0Rbl9DG4L4od4bZCibGXSS0aZ8agjjZOUwcJ32+9h/K5NXCh9TYGko+hg33MfPTnYeYiFh0Fsk/oCr5/n3Nk7mPyvIRQwuU2MhoIsDFYMmUgO90n20RTe5knlmWOU+fRDH8+rzw+rjw+XTlf6tcxwnJKuEG/GP2yUWY43AmlK8wwN1ivxuXJR2Ya49JE52A5PcYwhHhq1IW8D0N0rvVC652cNbQ80mm1Rs25WRTmm90mvFsfQ2kRcMXEWK+VlJR5SmhOpAzLYRi51ZinurVoXFRlGh+ucesl2Ls/GFPpUyLvs5kk0FRKiRevHvj2N69IJdG7sK6V//LPb/jzXz7whz8/8s9//PB10FFKwUiqjgbAJOQCJKe44yl2dco2xGsRiP1WDHqe3u63EzJG1riFQbwBCZF4WbvWc+8v+/SDNJ65JAZHRfQhEKAAYJ8qLxh9wOTex1T5lEg9DQQ9OKDxWHuf2q07X3U0wsRAqpQzZSqgcpsI/HTe+PDhzOPTyuVa+YyQ8BnS+N0dzYm8JCYTDkXoLsjmTL2RJ0jF6R22a8ytSOqoKKqZko+IjNZUi8UIOjmU2ilFNU41kTSjOsX92eejSvSRfcqI/sQQMbnRG7jEKgSzGrjfReitsq0V0UYzQ8c0+JTT+PiAeKx9hEIXpZLwbrStklR4vDbO6/PU+MfHK//4z2/4x3/6C++fGm8+Vj7DBp9B4KXojkxZyVNkpPMURvBipA6aHFEPibmO2RSDYkqaSHlBBgfl6rhtCDH2TLOCRS0ijLbPVBrMkO9ubldEsyuwbkaIzszoD9h3uGrkKFnTmKkKrTYQoZnfJhfvI6Px0fw33FRHaegYQNsRgcsaI+Ja76zbxvuPV/745w/8l395x7nCY/0MC3yOEfoYhWmeQDIi8cFG4lAA7R7KA5cwAkLvgrXRHyANZ8XRMWoBWq+3OnAiB2yUHPO4JSYwsre0EiS2yl5s2UccRHDVJKSyj0iDfUbcreFbA5XlrJSSYoCJG94HjBZIQ+gcto14s26Nx8tG3Sof3z/hDt+9fuT+7jDyoj4aDcPtpqTMn0xJ/qJGWNeAqN0KyIxqZyqhwEjRDsY8SXzYRIfrYYin1h5DqZqzrY6ZsLVoXd0HF0YZco4PihhDDXHH+5jkOIr5aUgQ0U8Ft0Nek4Rp0iFlHB/f4s+zu1MOv16WzOITtXXaZY3sGsNqlGR1qDeahezlw9PKX96euZxX/vSHd1jrHOeJJM68JE4Pma2GLD4+2kBJZfr5FvgcI9QqQ7+j9L67iEElD5VFUvAUvcqTRw9yrNbecjuIMA239Vz2es6ER0gMJnYE4PHwY8n55Jt92spwJZ8MRb/9ySCpbhU5D65f5JOWWHM6HRnTfhFoOObCtjXWtbKuleu1Yi1ylLq1kNhTRh08Pl4g+p6/0vi1d2/iXbVLoq8ZnYRyjMK3JosxNQpljsU5HmMxtjXG7dcmXM8brQuXs1KrUFdnvQKupOi9gaHj2T9KzHGk73XiT6ap3LqERt+wjkHhe62pj07KoYmyWhE3Wt2wGlm6idA13VxiHuOXHeGyNWp33ry78Nc3Z9Zr4+njFczZrp22GemQuVvumbTy3YuXrK87zZTta/WsXS6Ev78IfUvhP7OGAfa6SbpVOKMfQISUjFyNXAE6rYFZvpVA25g/J7fPPhuM6E3NN9Qcwo393C0h7KMyw4/HwQt+B2LXp2EIrIJHqdV7iwBOir/zPfEL3ZA5XK8xDPH8dOHydInNtLXnDqEO4sqcJ9QTp2Xh/nhga5C+VmB2jUGEll7RlXAvbLgbaitIixxzDDIvSxBseXz+QOtQrtAbTIfMtibWFaanYGe9lSgA+Zgv9ImKIpQ1gYpunxaoY6zVGPlmQA0FzhirEPpY9cHWekLpiGzxQUYGPhCTpI5aDwRXCuowMSO5c2qFF32id+f+ISYZ/+aH73n17be8fHnHw8tvaa3z7W8+0D1zXTtPl73//4sbIWZEWHIsHena6d6GER5BtqAKCqSiLKdMKgoEK9k7bJvRu3A5F7YtsV2Fy5NiXaibjq7LhNv4nDJ/rg8wDFDrQDIpj4EgEYy7g9QxRX5reB1KPW+oOLPUoL9lw2QNaN0jG87jw/SyJnKKjztxbWQzmmxY3tg/e62UzO/+7ge+/e23vHi449U339J754e/uVCmA49PlQ8ft69T1NnFRT6Sn6Fh2JN8nhWy458x9SUyUY2PiNw5GP1kKozKGPK0F3/GINpdaDsS2L0p4zbaZi8SjWB8e+79210MtQsS9l+O14Tvr/PTP3wen/jTrHnwUGnkMfrJax/SmxjC+PyRNs99fD9jaf1zm25/vb749Xlh/Nfrq1y/GuEXcP1qhF/A9asRfgHXr0b4BVy/GuEXcP1qhF/A9asRfgHXr0b4BVz/L9SufIj06/NMAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 100x100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 查看数据集的大小\n",
    "print(f\"训练集大小: {len(cifar10)}\")\n",
    "print(f\"验证集大小: {len(cifar10_val)}\")\n",
    "\n",
    "# 查看单个样本\n",
    "image, label = cifar10[0]\n",
    "print(f\"样本标签: {label}\")\n",
    "\n",
    "fig = plt.figure(figsize=(1,1))\n",
    "\n",
    "# 显示缩放后的图像\n",
    "plt.imshow(image)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43708da2-52d8-4efd-beb2-b0d968f507c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-26T02:23:24.296684Z",
     "iopub.status.busy": "2024-12-26T02:23:24.296684Z",
     "iopub.status.idle": "2024-12-26T02:23:24.453769Z",
     "shell.execute_reply": "2024-12-26T02:23:24.452768Z",
     "shell.execute_reply.started": "2024-12-26T02:23:24.296684Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "图像大小: (32, 32)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGbCAYAAAAr/4yjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAfkUlEQVR4nO3da6ycBb3v8d8ztzWzZt1X16UXSimlbGrKZXMRezAUMUGj2cGEwDvTmBhjeEFIkOgLATkmxiiREIyQEC6KL4gGCR6MJkdKdnJELgqeXaC7XFro6mV13WatNWvuM8954fF/tgeE/z+US/f+fhJfOP777zPPPLN+M22fn0mapqkAAJCU+agPAADw8UEoAAAMoQAAMIQCAMAQCgAAQygAAAyhAAAwhAIAwBAKAABDKOA/pUOHDilJEv3whz88aTufeuopJUmip5566qTtBD5uCAV8bDz44INKkkTPP//8R30oH6hHHnlEn/rUp1QulzUyMqJdu3bpySef/KgPC5Ak5T7qAwD+K7ntttt0++2365prrtGePXvUbre1b98+HTly5KM+NEASoQB8aP74xz/q9ttv1x133KEbb7zxoz4c4B3xx0c4pbRaLd1yyy268MILNTw8rHK5rE9/+tPau3fvP/w1P/rRj3T66aerVCrp8ssv1759+942s3//fl1zzTUaGxtTsVjURRddpMcff/w9j6dWq2n//v2an59/z9k777xT09PTuuGGG5SmqarV6nv+GuDDRijglLKysqL77rtPu3fv1ve//33ddtttmpub01VXXaUXX3zxbfM//elPddddd+n666/Xt771Le3bt0+f+cxnNDs7azMvvfSSLr30Ur3yyiv65je/qTvuuEPlcllXX321fvWrX73r8Tz77LM655xzdPfdd7/nsf/+97/XxRdfrLvuuksTExMaHBzU+vXrXb8W+NCkwMfEAw88kEpKn3vuuX840+l00maz+XePLS0tpVNTU+lXvvIVe+zgwYOppLRUKqUzMzP2+DPPPJNKSm+88UZ77Morr0x37tyZNhoNe6zX66W7du1KzzrrLHts7969qaR07969b3vs1ltvfdfntri4mEpKx8fH04GBgfQHP/hB+sgjj6Sf+9znUknpPffc866/Hviw8E0Bp5RsNqtCoSBJ6vV6WlxcVKfT0UUXXaQ///nPb5u/+uqrtXHjRvvvl1xyiT75yU/qN7/5jSRpcXFRTz75pK699lqtrq5qfn5e8/PzWlhY0FVXXaVXX331Xf8SePfu3UrTVLfddtu7Hvff/qhoYWFB9913n2666SZde+21euKJJ7Rjxw5997vfjZ4K4ANBKOCU89BDD+ncc89VsVjU+Pi4JiYm9MQTT2h5eflts2edddbbHtu+fbsOHTokSXrttdeUpqm+/e1va2Ji4u/+c+utt0qSTpw48b6PuVQqSZLy+byuueYaezyTyei6667TzMyM3nrrrff9+wDvF//6CKeUhx9+WHv27NHVV1+tb3zjG5qcnFQ2m9X3vvc9vf766+F9vV5PknTTTTfpqquueseZbdu2va9jlmR/gT0yMqJsNvt3/9vk5KQkaWlpSZs3b37fvxfwfhAKOKX88pe/1NatW/Xoo48qSRJ7/G+f6v9/r7766tseO3DggLZs2SJJ2rp1q6S/foL/7Gc/e/IP+P/KZDI6//zz9dxzz6nVatkfgUnS0aNHJUkTExMf2O8PePHHRzil/O1Tdpqm9tgzzzyjp59++h3nH3vssb/7O4Fnn31WzzzzjD7/+c9L+uun9N27d+vee+/VsWPH3vbr5+bm3vV4Iv8k9brrrlO329VDDz1kjzUaDf385z/Xjh07tGHDhvfcAXzQ+KaAj537779fv/3tb9/2+A033KAvfvGLevTRR/WlL31JX/jCF3Tw4EHdc8892rFjxzv+u/9t27bpsssu09e//nU1m03deeedGh8f180332wzP/7xj3XZZZdp586d+upXv6qtW7dqdnZWTz/9tGZmZvSXv/zlHx7rs88+qyuuuEK33nrre/5l89e+9jXdd999uv7663XgwAFt3rxZP/vZz/Tmm2/q17/+tf8EAR8gQgEfOz/5yU/e8fE9e/Zoz549On78uO6991797ne/044dO/Twww/rF7/4xTsW1X35y19WJpPRnXfeqRMnTuiSSy7R3XffrfXr19vMjh079Pzzz+s73/mOHnzwQS0sLGhyclIXXHCBbrnllpP2vEqlkp588kndfPPNuv/++7W2tqbzzz9fTzzxxD/8+wzgw5ak//F7OADgvzT+TgEAYAgFAIAhFAAAhlAAABhCAQBgCAUAgHHfp3DZ5btDiyuVRfdsX6YX2j1W8P8r2s3j/aHdE2Nl9+y6kYHQ7kI2757N9ZVCu5WN3XKyuFRxz7Y6sX+1PDoy7J7NdNuh3c1m0z3baDRCu4ulYmi+q657tlaP/R/qDI8M+YdT/3FIUqvZcs9m5b9mJb2t1+ndDA7E3j/lsv+9KUn5vP/1rAfOiSSlSeDzdCb23oy8Pp00ee+h/+D6/37Pe87wTQEAYAgFAIAhFAAAhlAAABhCAQBgCAUAgCEUAACGUAAAGEIBAGAIBQCAIRQAAMZdyvHSyy+FFlfm592zY7HKGSXj/l+wrjsY212adM+u9fz9TpJU7fo7hNKkENpda8S6W2p1f4dQuxvrpprP+vtYirlYr1Kn4z+WbLBzpq+vLzRfa6y5Zzu92OuTNMbdsxl/3ZAkqR3ojyrlYm/OaqC3Z7HbCe3u7491HyUZf29TEuglkyRl/J+na41Yv1en7Z/P5mLXrAffFAAAhlAAABhCAQBgCAUAgCEUAACGUAAAGEIBAGAIBQCAIRQAAIZQAAAYdw9AKeevLpAkBe6+Pj1QWyFJW6aG3bOTE2Oh3aXArfRJEjsn9WbDPdto+6sIJCkNHkuhVPIPd2JVFGnPf+zDY/2h3Z22/1gK+cBzlNTthsaVLfgv8mbL/9pLUrvjfz37A8chSbmy/7wUg7s7ib/6I5PG6lM6il3jgbYVDZRj12F1reaebXdiNReZwHGvriyHdrt+/5O+EQBwyiIUAACGUAAAGEIBAGAIBQCAIRQAAIZQAAAYQgEAYAgFAIAhFAAAhlAAABh391Ex6YQWDw66V2v7xtHQ7vFS1j2b78U6Z6qLLfdstxfL1HrNfw4zhdBqDY0MhOZzgU6byvJqbLf/pdfYYKxzZnXF363TavhnJaneiHXUpIEunoGyv1NLktqtuns20w2ccEn5Pv9r3+3GzkkuUDjUbMZ2F/KxN0Wm53+/NatLod3q+ju4+vw/riRJnZ6/E2p5LdaR5sE3BQCAIRQAAIZQAAAYQgEAYAgFAIAhFAAAhlAAABhCAQBgCAUAgCEUAADGfX/8aF/sVvpS4Fb64XIptHtiKO+e7fa6od2R6WwueP96xp/BzV6wXiDSLSEpl/pvpe82/ZULkpRm/c/zxIlKaHe37X+FVmu10O5a119xIkkDpSH/cDN2HWblf30yib9yQZKyfUX3bH0tVhPTn/efk1waO+5GI/b61Nv+moueYsdSqfrPS6UWey9XA3U4jfbJ/1zPNwUAgCEUAACGUAAAGEIBAGAIBQCAIRQAAIZQAAAYQgEAYAgFAIAhFAAAhlAAABh3Yc7EiL8vRZIG8/5eoGIx1iGUyfp7SkqlWK9Su+PvqOkpCe1OU393S6sT62LptmL9Kr3UP58GO4HSXME9u9paC+3udv3XSq3r7w+SpE5wfnXNfw6PLMaeZz7jP5ahauw6bB+fd8/Wl2P9UZvXbXPPTk5uCu1OBpdD882lBfdstRp7fZZX/d1H88ux7rBDh/3Ps5uNdZ558E0BAGAIBQCAIRQAAIZQAAAYQgEAYAgFAIAhFAAAhlAAABhCAQBgCAUAgHHfI71hohxaPFTouGcH+v21CJKUBCoapFhdRJL66wWa9VgFQCZQizE+OBzaXS7HakhWlv1VB8NDQ6Hdqw3/6/PmEf9xSFK16a+5KMRaK7SxP1YZkMv76wsOLVRCu5up/3nmk9g1Pjw06J7dteOi0O6VY/6amLQWPO51+dB8s+Z/PavV2Ofjvrz/WE6b9p9vSZqcnHLPzq746za8+KYAADCEAgDAEAoAAEMoAAAMoQAAMIQCAMAQCgAAQygAAAyhAAAwhAIAwBAKAADjLgcZGyzFFrcq7tm+fKxzpr+v3z3brEd6kqR2z9/ZNDIyGtqdpv6ul1Y3ltftdqwDpX9gwD17dK4Z2v36m8vu2blV//mWpFpg/PSSvz9Ikq7+9Pmh+U3r/efwl396I7T76deOu2c7vVZody7jvw5XK3Oh3bWq/1oZHIx1Ganr7w6TpGLRv79QjF0r/Yl/d6cbu8Y3n7bBPTu4uBra7cE3BQCAIRQAAIZQAAAYQgEAYAgFAIAhFAAAhlAAABhCAQBgCAUAgCEUAADG3S8xOTYeWlxf9NcuZJJYzUW15q+uqLdit5jnEv/t7rV2N7Q7ksD1dqy6YGR0KDTf6vqrDt6YORravbjiPy9prhDanc36z+JQMfb6TOZilQHFRX+lw1lD06Hdx8b8z3O2ciK0u1nzX1svHDgQ2p3p9Nyz7XLsmtXwVGw+4/+5Mjzsr86RpMGe//3TaMWqdtLWint2y0Q5tNuDbwoAAEMoAAAMoQAAMIQCAMAQCgAAQygAAAyhAAAwhAIAwBAKAABDKAAADKEAADDucpDRdROhxaMDJfdsJpMP7a6sLLln22vV0O5M19+X05O/50WS0ry/i2VgoBja3VZs/pU3/J02a8210O5isc8/W4j1XpXK/o6a0Wys9+pPr82G5jst/7E3h2PdRxOj/tczUaxDqN3x95LVWvXQ7rWavxOo1Ym9PkmwD0yJfzSfCQxLSjP+jrR8LnaNd5r+Tq000GHmxTcFAIAhFAAAhlAAABhCAQBgCAUAgCEUAACGUAAAGEIBAGAIBQCAIRQAAIZQAAAYfylHsJ8oycfmI/qK/t39Kod25wI5mcnEMrUd6ErqKw2Hds8fXw3N1+b9/VFbx2K9Sk1/tY6KgS4jSTr7zI3u2UzkQCR1srFrdiXQwZXLLod2Dxb81+346Jmh3Weetdk9e/Ct50K79x844p4t5PwdP5KUprEes04n8OMtVwjtzhf810qvF+tI6wVKm5Lk5H+u55sCAMAQCgAAQygAAAyhAAAwhAIAwBAKAABDKAAADKEAADCEAgDAEAoAAOO+D7zeaIcWJ+16YLoT2r22tuKebbVjudfJ+CsdqrVYtcRKYH7jaf5b9CUp7cSO5fR1/lvpz9wQq3+oNfy7N24/L7S7kPqrK5aWY9dsaWQ8NK+FrHv0tOn1odWVtTX37NZ/Oiu0e2jUXy0yNHpOaPfSnP86XFqOVX/kA9UfkpRJ+9yz7V43tDvSXNFtx36+ZfxvH6VpGtrt+v1P+kYAwCmLUAAAGEIBAGAIBQCAIRQAAIZQAAAYQgEAYAgFAIAhFAAAhlAAABhCAQBg3AU73STWDZJ2/X0f0f6OUrHknh0Y9Pe8SNLROX9n08GZudDuXN7/PAuzR0O7G7OxYzlr0t9ndOXuWLfO60cW3bODGydCu9eNT7tnT8zNhnaPjAS7dXr+c1jI+HuSJOnE3BH3bK5YCe2eqxxzzx45Vg3tzuf977eRoUCBkKR6PfZzIs35P/MmkcIhSb1AV1Imie1OMv7j7p786iO+KQAA/h9CAQBgCAUAgCEUAACGUAAAGEIBAGAIBQCAIRQAAIZQAAAYQgEAYNw1FyMjA6HFnZy/5qJabYR2p23/LebLq8uh3W++5a9GqFZjFQCloj+Djx1cCe2eKhZC8xs3nu6eHdlwRmh3fjVQX1D0V0VI0qbzLvGvPu6vipCkUidWFdKV/7pdW4td4+v7/fUfrW6sLiIp+9/Lm8obQrsHR/w1JKsLx0O7T8wuhObbif/aarSaod3K+Pslyn3F0OpW3f9zJV+IvX88+KYAADCEAgDAEAoAAEMoAAAMoQAAMIQCAMAQCgAAQygAAAyhAAAwhAIAwBAKAADj7j5arcR6R3KtVfdsPglmUzZwHNnAsKRa1d+VNDpYDu0eKfs7UOpLse6jyQ3jofmN517unt030wrtPvCaf37X+rHQ7krFv3vqzPNCuzOqheZbTX9X0kga6ydaOeF/v5Va7dDu9WP+c17p9oV2588ddc/WK8dCu//Xbx4Pzc8c9r8+2XCHUOKerPtrkiRJ7cBn9Uw79tq7dp70jQCAUxahAAAwhAIAwBAKAABDKAAADKEAADCEAgDAEAoAAEMoAAAMoQAAMO6ai6z/rm5JUrdedc+mgVvGJSmjjv84kljNxVLgrvGVldj962nTX9GwfjhWoXHxFVeE5jedfal79tEH7g/tni4PuGezrXpo95E3Xvcfx9Ydod3F8W2h+XLqr3KpLZ4I7S71/HURrXqsnmN+1T8/MnFGaPf49Bb3bL06FNqdiY2rW2i4Z5NM7GdQu+1/Lyedbmh3kvrnOx33j3A3vikAAAyhAAAwhAIAwBAKAABDKAAADKEAADCEAgDAEAoAAEMoAAAMoQAAMIQCAMC4izOSWM2Pum1/iVCSiWVTLjCe1gNlRpKSnn92bLw/tHu639/Z9M8XbQ/tPmeXv8tIkpZO+Lup+jrLod1bN21yz/YiJ1zS9OSEe7bT8J9vSapV/H02ktTq+Pe367GOmq78/VGvH5kJ7f63fc+7Z3ddGjsn49Pj7tmV1VgfVD72dtO6Lf7+sF7wZ1C3FegnCnSeSdLyXMU921wNnhQHvikAAAyhAAAwhAIAwBAKAABDKAAADKEAADCEAgDAEAoAAEMoAAAMoQAAMIQCAMC4C1l6HX/XhyTVm/5Om0LZ3/MiSblc3j2bzcR6R7ZNj7pni6VYpm45/TT37HmXXRHavf7sc0PzLz79gHt282n+cyJJ05/Y6Z4tTJwZ2p3rH3bP1hr+fidJqq+shuZnjx52zy7NxvqJuu2ae7Y0WAztXrfO//45fPSF0O6p9Rvds51a7PVJ683QfLK25J7tpvXYsQTK4Ep9/vMtSYVp//xKXxLa7cE3BQCAIRQAAIZQAAAYQgEAYAgFAIAhFAAAhlAAABhCAQBgCAUAgCEUAADGXXORz7pHJUlLq/7b9LuN2K3apf6Sezab8d+OLkmT4/3u2cPHKqHdZ/7z59yzm3b6Z/8qVkXRXl1zzw4P+qslJGli+/nu2bXcWGj3Sy88555t1v3PUZJWViqh+fkjb7lns91Y3Uqx6H+/bTzDXy0hSedu3+ae7WTLod357Ih/ttAO7c41GqH52ptH3LPRGp9O4ON0NZsN7e4f95/zqQ3jod0efFMAABhCAQBgCAUAgCEUAACGUAAAGEIBAGAIBQCAIRQAAIZQAAAYQgEAYAgFAIBxF6w067Hekf4+f3dLUox1g+QzHfds2vXPSlJpwH8s/3Ldv4R27/r8le7ZoXVTod2zb7wSms8GzmFldTm0e+7Qv7tnj67GOmeeeuwx9+xAKR/a3WhWQ/PTU/5OqKHBWIfQwZnD7tlW4LWUpLENW9yz23deGNqtbp97dLEyE1pdC3akLdX95yVJY91ujXrPPVtNY/1radX/s/ackdBqF74pAAAMoQAAMIQCAMAQCgAAQygAAAyhAAAwhAIAwBAKAABDKAAADKEAADDue7t7aSu2ueevL0g6/lvGJamTtv27k9gt5sW+Iffs+RfGKgD68v7ahZdffCG0e+no66H5ZtN/K/3q0mJo9+HXXnbPVtNSaHe+6z/ugVysPmWoGKuimBj111wcmz0e2t1p+6/x2mqsnuPwwbcC0y+Fdlerq+7ZYi723uz0TYbmFzr+93KpVAzt7h/0X7elnL/6Q5JWayvu2U4vVnHiwTcFAIAhFAAAhlAAABhCAQBgCAUAgCEUAACGUAAAGEIBAGAIBQCAIRQAAIZQAAAYd/eRFOsn6nX8XUm5fH9od7fj71VqKdYNMjU86p793eP/I7R7bMrfIzO5/rTQ7lZtOTSfz/v7WAbK/g4ZScpl/J1D5UAflCRNT467Z+urS6HdpWyso2Zhbt492275r1lJGiz6u3Va1Vj30asvPO+ePbb/QGh3s1P3D+dj3VTdwHUlSeVNgS6rcqzbLdPn7+AqBvuJRuV/7c/5xBmh3R58UwAAGEIBAGAIBQCAIRQAAIZQAAAYQgEAYAgFAIAhFAAAhlAAABhCAQBg3DUXvV4SWlzI+W9JL+ZiFRrK+I8lzQZudZfUa7Xds/Pzx0O7q3P++VJ7JbS7p1gFwNiovy5iZMNEaHen23TPHjkaO4epUvdsJhNocZHU6sTqCLKJv6KjXIxVuXQCb4lsZFiSEv857LZi9SmZwM+JlVqshqTVF6jQkDS4wX8drpUqod2rPX8tRmMt9tl7fGire3ZdoPbFi28KAABDKAAADKEAADCEAgDAEAoAAEMoAAAMoQAAMIQCAMAQCgAAQygAAAyhAAAw7nKYTNIXWlzsK7lnU8U6Z8olf49MeXBdaHet3XDPjg8WQrtzgefZWp4N7e5lYsdSy/v7cqamzogdS8vfC3P2uZtCu/+w9/fu2VZaC+3OJ7F+r3rVv39ocCi0u5Dz9zZlk1j3UbXhv8YPHov1E1Uq/mu8mayFdk9sj32G3Tji/xnUSmPvn6V5/2tfaPg7siSpvNHfZ1SvdUO7PfimAAAwhAIAwBAKAABDKAAADKEAADCEAgDAEAoAAEMoAAAMoQAAMIQCAMC476Uv5GL5UWs23bPZYjm0u5f1V27U2vXQ7mw+dc/2Ffy30UtSPu9/noX+4dDu4aHYOTw+56/RqG2MVVFMnrbNPXvkxHxo9ycu/m/u2erc0dDuNw68FJpfq1bcs7ls7DocHvbXYiSK1VwcO+I/L2+9uRzanenzX4dDU/66GkmaGItVhSSBOo9kMfb+GV3y15BsnBwL7d404n+/vfby8dDuK7703jN8UwAAGEIBAGAIBQCAIRQAAIZQAAAYQgEAYAgFAIAhFAAAhlAAABhCAQBgCAUAgHEXeExNxPKjvbDgnq13Y90ta2v+2TTTDe3O5fydJkND46HdhXzePVtfWwntLuX9xy1Javnnn//DH0Krt57t71WamYl1t2QyiXu2v89/viUpG+jUkqRSyd+Xs1aNdR/V6/75TqcV2j1Q8j/PXRdsD+0uDvr7iTrZTmh3t10LzdcP+7uPMqvF0O7J/kH37AXbPxHbPTLlnv3TsYOh3R58UwAAGEIBAGAIBQCAIRQAAIZQAAAYQgEAYAgFAIAhFAAAhlAAABhCAQBgCAUAgHEX4Gw+rRBaPJz4u0ReOxzrNJmdS92zrW6sz2ZgwN8JtFZbDu3u9qru2Wwwrxfn/F1TkrRa9ffONNqx55lN/fODA6Oh3bPHF92zM2v+7htJ6qX+XiVJmprwd18lvXZo91JlyT3bV45d4yPD/t6eQjZ2HTZbga6xXKybaq0ZO5ZW1b+/3Ivt3nbatHt2w3SsI+3wjL87bGEu9rPTg28KAABDKAAADKEAADCEAgDAEAoAAEMoAAAMoQAAMIQCAMAQCgAAQygAAIy702FoNHZLej1w+/XoZDa0W+V+9+j8bDO0utFquWdzhaHQ7sBq9dqBugBJ7W7seS7X/TUK5VKsRqFR89dL1Bvzod2twHnpBs9hmsauw+qK/xofGiqFdg8NDbtn6/VY1cH8gv+1Hxgoh3YnGf/nzKTjr6uRpEIudg77/E07KhRir/2WbVvcs/Va7Hn+67++7J793wdOhHZ78E0BAGAIBQCAIRQAAIZQAAAYQgEAYAgFAIAhFAAAhlAAABhCAQBgCAUAgCEUAADG3X2UK7pHJUnFoYJ7dmwglk25ur/nJ1/qhXavLAWeZzd23KXipH91Pnbc3WYlNF/o9z/PfM7/WkpSNuvvpmqmsefZavsLpNI0Ce1OYhU1Slv+jqeuf1SSlM8FusYKsW6qypK/+6jeaod2D4/4+8BygZ4kScoEr8OaOu7Z2fnV0O6lqn/36tpyaPf/fGq/e3Y2VnvlwjcFAIAhFAAAhlAAABhCAQBgCAUAgCEUAACGUAAAGEIBAGAIBQCAIRQAAMbddVCtBm67l6TsgHt0oBzrAMiX/H0E5b5iaPfwsL92obpSD+2ursz6Z2vd0O52IzY/WBh3zxbzsde+0/TXkORysc8lhcB4vi8b2p0ksWPpH/BXhWRiLTHqdP01CoVSbPnQiL+GZHExVv+wGqgtGRrzX4OSVOv4K04k6dVDC+7Z/f92OLR7asxf5zG1yX++JUkZ/zlcNzwY2+357U/6RgDAKYtQAAAYQgEAYAgFAIAhFAAAhlAAABhCAQBgCAUAgCEUAACGUAAAGEIBAGDcpSkzb8YWNyv+zqHBCX/PiyQVS2337LC/gkmSNDbm75GprtVCuysV//zSQiG0e8lf8yJJyvb8vUC91N81JUndbqCHqRfrbIp8ikkySWh3NhfrEKp3/UeTxi5x5Xv+a7xTWwzt7tb912E3F+u9qlT9u1uxl16Lwa6xQ6/53xSVhbXQ7taa/+Cnh6dDu885faN7NnhKXPimAAAwhAIAwBAKAABDKAAADKEAADCEAgDAEAoAAEMoAAAMoQAAMIQCAMC47+vv5teFFrcLF7lnm71maHemM++eLQ7Hqg5GJvz1HKOZWHfBWK3nnq0slkK7K/P+2gpJqq/5Kx26nVjlhlL/Z41ex39OJKlRb7hnC4XYcWdzsXO42vAfe73qP25Jyqct9+xgZjC0u5dZcc+227Hqj76yvxKlmO8L7R4p+M+JJG3ViHt253nl0O6zzz3PPbtl27bQ7ksu9VeFzBythnZ78E0BAGAIBQCAIRQAAIZQAAAYQgEAYAgFAIAhFAAAhlAAABhCAQBgCAUAgCEUAAAmSdPUX1YCAPhPjW8KAABDKAAADKEAADCEAgDAEAoAAEMoAAAMoQAAMIQCAMAQCgAA838Avr5Z509BnA8AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 查看图像尺寸\n",
    "print(f\"图像大小: {image.size}\")  # 输出为 (宽, 高)，即 (32, 32)\n",
    "\n",
    "# 显示原始图像\n",
    "image.show()  # 使用系统默认图像查看器\n",
    "# 或用 Matplotlib 显示\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(image)\n",
    "plt.axis(\"off\")\n",
    "plt.title(f\"Label: {label}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5f530992-e30f-4b0e-8425-399daf527fbd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-26T03:03:15.531934Z",
     "iopub.status.busy": "2024-12-26T03:03:15.531607Z",
     "iopub.status.idle": "2024-12-26T03:03:30.247303Z",
     "shell.execute_reply": "2024-12-26T03:03:30.245417Z",
     "shell.execute_reply.started": "2024-12-26T03:03:15.531934Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Calculated padded input size per channel: (2 x 2). Kernel size: (4 x 4). Kernel size can't be greater than actual input size",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 111\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;66;03m# 训练判别器\u001b[39;00m\n\u001b[0;32m    110\u001b[0m optimizer_D\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 111\u001b[0m real_loss \u001b[38;5;241m=\u001b[39m adversarial_loss(\u001b[43mdiscriminator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m)\u001b[49m, real)  \u001b[38;5;66;03m# 判别真实图片\u001b[39;00m\n\u001b[0;32m    112\u001b[0m z \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(imgs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), latent_dim, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcuda()  \u001b[38;5;66;03m# 随机噪声\u001b[39;00m\n\u001b[0;32m    113\u001b[0m gen_imgs \u001b[38;5;241m=\u001b[39m generator(z)  \u001b[38;5;66;03m# 生成图片\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\cv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[21], line 88\u001b[0m, in \u001b[0;36mDiscriminator.forward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m---> 88\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 输出形状: [batch_size, 1, 1, 1]\u001b[39;00m\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\cv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\cv\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\cv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\cv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\cv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Calculated padded input size per channel: (2 x 2). Kernel size: (4 x 4). Kernel size can't be greater than actual input size"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# 超参数设置\n",
    "latent_dim = 100\n",
    "batch_size = 128\n",
    "num_epochs = 100\n",
    "lr = 0.0002\n",
    "beta1 = 0.5\n",
    "image_size = 32\n",
    "\n",
    "# CIFAR-10 数据预处理和加载\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "class FrogDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.data = [img for img, label in dataset if label == 6]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='../../../../../data/02.cv/cifar-10/', train=True, download=True, transform=transform)\n",
    "frog_dataset = FrogDataset(train_dataset)\n",
    "train_loader = DataLoader(frog_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 定义生成器\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.ConvTranspose2d(latent_dim, 512, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.ConvTranspose2d(64, 3, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.model(z)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(128, 256, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(256, 512, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(512, 1, 4, 1, 0, bias=False)  # 输出为 [batch_size, 1, 1, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        out = self.model(img)  # 输出形状: [batch_size, 1, 1, 1]\n",
    "        return out.view(-1)  # 展平为 [batch_size]\n",
    "\n",
    "\n",
    "# 初始化模型\n",
    "generator = Generator(latent_dim).cuda()\n",
    "discriminator = Discriminator().cuda()\n",
    "\n",
    "# 损失函数和优化器\n",
    "adversarial_loss = nn.BCELoss()\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, imgs in enumerate(train_loader):\n",
    "        imgs = imgs.cuda()\n",
    "\n",
    "        # 标签生成\n",
    "        real = torch.ones(imgs.size(0)).cuda()  # [batch_size]\n",
    "        fake = torch.zeros(imgs.size(0)).cuda()  # [batch_size]\n",
    "\n",
    "        # 训练判别器\n",
    "        optimizer_D.zero_grad()\n",
    "        real_loss = adversarial_loss(discriminator(imgs), real)  # 判别真实图片\n",
    "        z = torch.randn(imgs.size(0), latent_dim, 1, 1).cuda()  # 随机噪声\n",
    "        gen_imgs = generator(z)  # 生成图片\n",
    "        fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)  # 判别生成图片\n",
    "        d_loss = real_loss + fake_loss\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # 训练生成器\n",
    "        optimizer_G.zero_grad()\n",
    "        g_loss = adversarial_loss(discriminator(gen_imgs), real)  # 希望生成图片被判别为真实\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "    print(f\"[Epoch {epoch+1}/{num_epochs}] [D loss: {d_loss.item():.4f}] [G loss: {g_loss.item():.4f}]\")\n",
    "\n",
    "    # 每隔一定周期保存生成的图片\n",
    "    if epoch % 10 == 0:\n",
    "        torchvision.utils.save_image(gen_imgs.data[:25], f\"output_epoch_{epoch}.png\", nrow=5, normalize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8683a77c-be2a-4222-86fc-91122b427a97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d8a9eb-4de6-4aae-b48b-0241a9de7539",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5978272-efc9-468a-923e-d21004848658",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02886b6b-25be-4184-b944-95893e258005",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8955713-c8f7-4e11-8fe8-44e9dda5c164",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c7e7f48b-8760-4efe-b884-5ae967c9e179",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-26T03:05:42.765944Z",
     "iopub.status.busy": "2024-12-26T03:05:42.764943Z",
     "iopub.status.idle": "2024-12-26T03:05:43.129417Z",
     "shell.execute_reply": "2024-12-26T03:05:43.128420Z",
     "shell.execute_reply.started": "2024-12-26T03:05:42.765944Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=32, out_features=32768, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Unflatten(dim=1, unflattened_size=(128, 16, 16))\n",
      "    (3): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (5): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (6): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (7): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (8): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (9): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (11): Conv2d(256, 3, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
      "    (12): Tanh()\n",
      "  )\n",
      ")\n",
      "Generated image shape: torch.Size([1, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, channels):\n",
    "        super(Generator, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            # 输入层，将latent_dim映射为 128 x 16 x 16 的特征图\n",
    "            nn.Linear(latent_dim, 128 * 16 * 16),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Unflatten(1, (128, 16, 16)),\n",
    "\n",
    "            # 卷积层，保持 16x16 特征图大小\n",
    "            nn.Conv2d(128, 256, kernel_size=5, padding=2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # 转置卷积层，上采样到 32x32\n",
    "            nn.ConvTranspose2d(256, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # 连续两层卷积，保持 32x32 特征图大小\n",
    "            nn.Conv2d(256, 256, kernel_size=5, padding=2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=5, padding=2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # 输出层，生成具有指定通道数的图像\n",
    "            nn.Conv2d(256, channels, kernel_size=7, padding=3),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.model(z)\n",
    "\n",
    "# 测试生成器\n",
    "latent_dim = 32\n",
    "channels = 3\n",
    "\n",
    "# 初始化生成器\n",
    "generator = Generator(latent_dim, channels)\n",
    "generator = generator.cuda() if torch.cuda.is_available() else generator\n",
    "print(generator)\n",
    "\n",
    "# 输入随机噪声，生成样本\n",
    "z = torch.randn(1, latent_dim).cuda() if torch.cuda.is_available() else torch.randn(1, latent_dim)\n",
    "gen_imgs = generator(z)\n",
    "print(\"Generated image shape:\", gen_imgs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d6b2c1f6-967a-43b0-8e85-9f8098f529ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-26T03:07:12.394985Z",
     "iopub.status.busy": "2024-12-26T03:07:12.394985Z",
     "iopub.status.idle": "2024-12-26T03:07:12.418555Z",
     "shell.execute_reply": "2024-12-26T03:07:12.417586Z",
     "shell.execute_reply.started": "2024-12-26T03:07:12.394985Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator(\n",
      "  (model): Sequential(\n",
      "    (0): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2))\n",
      "    (3): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (4): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2))\n",
      "    (5): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (6): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2))\n",
      "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (8): Flatten(start_dim=1, end_dim=-1)\n",
      "    (9): Dropout(p=0.4, inplace=False)\n",
      "    (10): Linear(in_features=512, out_features=1, bias=True)\n",
      "    (11): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, height=32, width=32, channels=3):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            # 第一层卷积\n",
    "            nn.Conv2d(channels, 128, kernel_size=3, stride=1, padding=0),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # 第二层卷积，步幅为2，降采样\n",
    "            nn.Conv2d(128, 128, kernel_size=4, stride=2, padding=0),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # 第三层卷积，步幅为2，进一步降采样\n",
    "            nn.Conv2d(128, 128, kernel_size=4, stride=2, padding=0),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # 第四层卷积，步幅为2，进一步降采样\n",
    "            nn.Conv2d(128, 128, kernel_size=4, stride=2, padding=0),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # 展平为一维向量\n",
    "            nn.Flatten(),\n",
    "\n",
    "            # Dropout层，防止过拟合\n",
    "            nn.Dropout(0.4),\n",
    "\n",
    "            # 全连接层，用于最终判别\n",
    "            nn.Linear(128 * ((height // 16) * (width // 16)), 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# 示例使用\n",
    "height, width, channels = 32, 32, 3\n",
    "discriminator = Discriminator(height, width, channels)\n",
    "print(discriminator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1481e2-81cd-4d55-af85-bb32cf1c1041",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffe3531-0f91-42e3-861e-edffd84090ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aedd60a-0269-4bc9-b33f-5e27edc776e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229a878f-62b0-42d7-8a2a-ecbe2f44eabf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912bba9a-21b6-4c9f-b6b4-20f7be2c634a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7a4c30-ac9d-456b-8cfe-69e98934d6ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "67e5097c-80cc-48d1-b2ff-873795ac8439",
   "metadata": {},
   "source": [
    "# 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ac1152dd-9fa4-4e08-ac05-f51e1154bbba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-26T03:25:34.440661Z",
     "iopub.status.busy": "2024-12-26T03:25:34.439662Z",
     "iopub.status.idle": "2024-12-26T03:28:41.795577Z",
     "shell.execute_reply": "2024-12-26T03:28:41.794030Z",
     "shell.execute_reply.started": "2024-12-26T03:25:34.439662Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "Epoch [1/100], Step [0/391], D Loss: 1.3854, G Loss: 0.6990\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "Epoch [1/100], Step [50/391], D Loss: 1.0146, G Loss: 1.2427\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 3, 32, 32])\n",
      "torch.Size([128, 1])\n",
      "torch.Size([128, 1])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 89\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, (imgs, _) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(data_loader):\n\u001b[1;32m---> 89\u001b[0m         imgs \u001b[38;5;241m=\u001b[39m \u001b[43mimgs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m         \u001b[38;5;28mprint\u001b[39m(imgs\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     92\u001b[0m         \u001b[38;5;66;03m# 创建真实标签和假标签\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "# 定义生成器网络\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, img_channels):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            # 全连接层，展平为 (128, 128, 8, 8)\n",
    "            nn.Linear(latent_dim, 128 * 8 * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Unflatten(1, (128, 8, 8)),  # 将 (batch, 128*8*8) 转换为 (batch, 128, 8, 8)\n",
    "            # 第一次上采样，(128, 8, 8) -> (128, 16, 16)\n",
    "            nn.ConvTranspose2d(128, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # 第二次上采样，(128, 16, 16) -> (128, 32, 32)\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # 输出通道调整到 3，(64, 32, 32) -> (3, 32, 32)\n",
    "            nn.Conv2d(64, img_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Tanh()  # 输出范围在 [-1, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.model(z)\n",
    "\n",
    "        \n",
    "# 定义判别器网络\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(channels, 128, 3, padding=1),  # 输入 (3, 32, 32) -> (128, 32, 32)\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(128, 128, 4, stride=2, padding=1),  # (128, 32, 32) -> (128, 16, 16)\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(128, 128, 4, stride=2, padding=1),  # (128, 16, 16) -> (128, 8, 8)\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(128, 128, 4, stride=2, padding=1),  # (128, 8, 8) -> (128, 4, 4)\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Flatten(),  # 展平 -> (128, 2048)\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(128 * 4 * 4, 1),  # 将 `in_features` 调整为 2048\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        return self.model(img)\n",
    "\n",
    "\n",
    "\n",
    "# 参数设置\n",
    "latent_dim = 32\n",
    "channels = 3\n",
    "image_size = 32\n",
    "batch_size = 128\n",
    "epochs = 100\n",
    "lr = 0.0002\n",
    "\n",
    "# 数据加载和预处理\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(image_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "dataset = datasets.CIFAR10(root='../../../../../data/02.cv/cifar-10/', download=True, transform=transform)\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 初始化模型\n",
    "generator = Generator(latent_dim, channels).cuda()\n",
    "discriminator = Discriminator(channels).cuda()\n",
    "\n",
    "# 损失函数和优化器\n",
    "criterion = nn.BCELoss()\n",
    "g_optimizer = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "d_optimizer = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "# 开始训练\n",
    "for epoch in range(epochs):\n",
    "    for i, (imgs, _) in enumerate(data_loader):\n",
    "        imgs = imgs.cuda()\n",
    "\n",
    "        # 创建真实标签和假标签\n",
    "        real_labels = torch.ones(imgs.size(0), 1).cuda()\n",
    "        fake_labels = torch.zeros(imgs.size(0), 1).cuda()\n",
    "\n",
    "        # 训练判别器\n",
    "        z = torch.randn(imgs.size(0), latent_dim).cuda()\n",
    "        fake_imgs = generator(z)\n",
    "        \n",
    "        real_loss = criterion(discriminator(imgs), real_labels)\n",
    "        fake_loss = criterion(discriminator(fake_imgs.detach()), fake_labels)\n",
    "        d_loss = real_loss + fake_loss\n",
    "\n",
    "        d_optimizer.zero_grad()\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "\n",
    "        # 训练生成器\n",
    "        g_loss = criterion(discriminator(fake_imgs), real_labels)\n",
    "\n",
    "        g_optimizer.zero_grad()\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "\n",
    "        # 打印日志\n",
    "        if i % 50 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Step [{i}/{len(data_loader)}], D Loss: {d_loss.item():.4f}, G Loss: {g_loss.item():.4f}\")\n",
    "\n",
    "    # 每个epoch保存生成的图片\n",
    "    fake_imgs = fake_imgs.view(fake_imgs.size(0), channels, image_size, image_size)\n",
    "    vutils.save_image(fake_imgs[:25], f\"output_epoch_{epoch+1}.png\", nrow=5, normalize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356e48eb-61fe-4681-8dcb-cb40620ace2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b50b4b17-33ef-432e-8290-4fa6a2f90994",
   "metadata": {},
   "source": [
    "# 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "66f8fca1-7a9b-4fad-93be-9e8e2e5c8286",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-26T03:31:26.052931Z",
     "iopub.status.busy": "2024-12-26T03:31:26.050952Z",
     "iopub.status.idle": "2024-12-26T05:33:43.483190Z",
     "shell.execute_reply": "2024-12-26T05:33:43.481394Z",
     "shell.execute_reply.started": "2024-12-26T03:31:26.051930Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Epoch [1/100], Step [0/391], D Loss: 1.3855, G Loss: 0.6966\n",
      "Epoch [1/100], Step [50/391], D Loss: 1.3103, G Loss: 0.8835\n",
      "Epoch [1/100], Step [100/391], D Loss: 1.2646, G Loss: 0.8451\n",
      "Epoch [1/100], Step [150/391], D Loss: 1.3148, G Loss: 0.5637\n",
      "Epoch [1/100], Step [200/391], D Loss: 1.2801, G Loss: 0.8403\n",
      "Epoch [1/100], Step [250/391], D Loss: 0.6769, G Loss: 1.4675\n",
      "Epoch [1/100], Step [300/391], D Loss: 1.3419, G Loss: 1.2702\n",
      "Epoch [1/100], Step [350/391], D Loss: 1.3523, G Loss: 0.8073\n",
      "Epoch [2/100], Step [0/391], D Loss: 1.4065, G Loss: 0.7933\n",
      "Epoch [2/100], Step [50/391], D Loss: 1.3863, G Loss: 0.8207\n",
      "Epoch [2/100], Step [100/391], D Loss: 1.2818, G Loss: 0.8185\n",
      "Epoch [2/100], Step [150/391], D Loss: 1.2643, G Loss: 0.7714\n",
      "Epoch [2/100], Step [200/391], D Loss: 1.3200, G Loss: 0.8439\n",
      "Epoch [2/100], Step [250/391], D Loss: 1.3093, G Loss: 0.7690\n",
      "Epoch [2/100], Step [300/391], D Loss: 1.3448, G Loss: 0.7753\n",
      "Epoch [2/100], Step [350/391], D Loss: 1.2419, G Loss: 0.8551\n",
      "Epoch [3/100], Step [0/391], D Loss: 1.3525, G Loss: 0.7458\n",
      "Epoch [3/100], Step [50/391], D Loss: 1.3103, G Loss: 0.8736\n",
      "Epoch [3/100], Step [100/391], D Loss: 1.3449, G Loss: 0.9741\n",
      "Epoch [3/100], Step [150/391], D Loss: 1.3103, G Loss: 0.8969\n",
      "Epoch [3/100], Step [200/391], D Loss: 1.2573, G Loss: 0.9742\n",
      "Epoch [3/100], Step [250/391], D Loss: 1.2956, G Loss: 0.9255\n",
      "Epoch [3/100], Step [300/391], D Loss: 1.3399, G Loss: 0.9261\n",
      "Epoch [3/100], Step [350/391], D Loss: 1.2844, G Loss: 0.8188\n",
      "Epoch [4/100], Step [0/391], D Loss: 1.1984, G Loss: 0.9903\n",
      "Epoch [4/100], Step [50/391], D Loss: 1.1839, G Loss: 0.8629\n",
      "Epoch [4/100], Step [100/391], D Loss: 1.2338, G Loss: 1.0157\n",
      "Epoch [4/100], Step [150/391], D Loss: 1.2643, G Loss: 0.9988\n",
      "Epoch [4/100], Step [200/391], D Loss: 1.1668, G Loss: 0.9457\n",
      "Epoch [4/100], Step [250/391], D Loss: 1.4530, G Loss: 0.8448\n",
      "Epoch [4/100], Step [300/391], D Loss: 1.2215, G Loss: 0.8358\n",
      "Epoch [4/100], Step [350/391], D Loss: 1.1615, G Loss: 1.0638\n",
      "Epoch [5/100], Step [0/391], D Loss: 1.2159, G Loss: 0.9050\n",
      "Epoch [5/100], Step [50/391], D Loss: 1.1688, G Loss: 0.8551\n",
      "Epoch [5/100], Step [100/391], D Loss: 1.1954, G Loss: 1.2163\n",
      "Epoch [5/100], Step [150/391], D Loss: 1.2263, G Loss: 1.0344\n",
      "Epoch [5/100], Step [200/391], D Loss: 1.1297, G Loss: 1.0500\n",
      "Epoch [5/100], Step [250/391], D Loss: 1.2706, G Loss: 0.9974\n",
      "Epoch [5/100], Step [300/391], D Loss: 1.0664, G Loss: 1.1497\n",
      "Epoch [5/100], Step [350/391], D Loss: 1.0064, G Loss: 1.0687\n",
      "Epoch [6/100], Step [0/391], D Loss: 0.9544, G Loss: 1.0981\n",
      "Epoch [6/100], Step [50/391], D Loss: 1.0022, G Loss: 1.2532\n",
      "Epoch [6/100], Step [100/391], D Loss: 1.0372, G Loss: 1.0171\n",
      "Epoch [6/100], Step [150/391], D Loss: 1.1216, G Loss: 0.8274\n",
      "Epoch [6/100], Step [200/391], D Loss: 1.2224, G Loss: 0.9076\n",
      "Epoch [6/100], Step [250/391], D Loss: 1.0208, G Loss: 1.2590\n",
      "Epoch [6/100], Step [300/391], D Loss: 1.3347, G Loss: 0.9216\n",
      "Epoch [6/100], Step [350/391], D Loss: 1.1252, G Loss: 1.0965\n",
      "Epoch [7/100], Step [0/391], D Loss: 1.0172, G Loss: 1.3744\n",
      "Epoch [7/100], Step [50/391], D Loss: 0.8216, G Loss: 2.0208\n",
      "Epoch [7/100], Step [100/391], D Loss: 0.9754, G Loss: 1.1867\n",
      "Epoch [7/100], Step [150/391], D Loss: 0.9929, G Loss: 1.2936\n",
      "Epoch [7/100], Step [200/391], D Loss: 1.0012, G Loss: 1.1348\n",
      "Epoch [7/100], Step [250/391], D Loss: 1.1280, G Loss: 1.3840\n",
      "Epoch [7/100], Step [300/391], D Loss: 0.9075, G Loss: 1.3611\n",
      "Epoch [7/100], Step [350/391], D Loss: 0.9111, G Loss: 1.4097\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 100\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, (imgs, _) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(data_loader):\n\u001b[1;32m--> 100\u001b[0m         imgs \u001b[38;5;241m=\u001b[39m \u001b[43mimgs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 将数据移动到 GPU\u001b[39;00m\n\u001b[0;32m    102\u001b[0m         \u001b[38;5;66;03m# 创建真实标签和假标签\u001b[39;00m\n\u001b[0;32m    103\u001b[0m         real_labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(imgs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcuda()  \u001b[38;5;66;03m# 真实图像标签为 1\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "# 定义生成器网络\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, img_channels):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            # 1. 全连接层：将潜在向量 (latent_dim) 投影并展平为形状 (128, 8, 8)\n",
    "            nn.Linear(latent_dim, 128 * 8 * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Unflatten(1, (128, 8, 8)),  # 转换为形状 (batch, 128, 8, 8)\n",
    "\n",
    "            # 2. 转置卷积层：第一次上采样，(128, 8, 8) -> (128, 16, 16)\n",
    "            nn.ConvTranspose2d(128, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # 3. 转置卷积层：第二次上采样，(128, 16, 16) -> (64, 32, 32)\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # 4. 卷积层：输出通道调整为图像通道数 (64, 32, 32) -> (3, 32, 32)\n",
    "            nn.Conv2d(64, img_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.Tanh()  # 将输出范围限制在 [-1, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.model(z)\n",
    "\n",
    "# 定义判别器网络\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            # 1. 卷积层：初始通道从图像通道数增加到 128，(3, 32, 32) -> (128, 32, 32)\n",
    "            nn.Conv2d(channels, 128, 3, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # 2. 卷积层：下采样，(128, 32, 32) -> (128, 16, 16)\n",
    "            nn.Conv2d(128, 128, 4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # 3. 卷积层：下采样，(128, 16, 16) -> (128, 8, 8)\n",
    "            nn.Conv2d(128, 128, 4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # 4. 卷积层：下采样，(128, 8, 8) -> (128, 4, 4)\n",
    "            nn.Conv2d(128, 128, 4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            # 5. 展平层：将 (128, 4, 4) 展平为向量 (128 * 4 * 4)\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.4),  # 添加 Dropout 防止过拟合\n",
    "\n",
    "            # 6. 全连接层：映射到单个输出值 (即概率)\n",
    "            nn.Linear(128 * 4 * 4, 1),\n",
    "            nn.Sigmoid()  # 使用 Sigmoid 输出范围在 [0, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        return self.model(img)\n",
    "\n",
    "# 参数设置\n",
    "latent_dim = 32  # 潜在向量维度\n",
    "channels = 3  # 图像通道数（彩色图像为 3）\n",
    "image_size = 32  # 图像尺寸 (32x32)\n",
    "batch_size = 128  # 每批数据大小\n",
    "epochs = 100  # 训练轮数\n",
    "lr = 0.0002  # 学习率\n",
    "\n",
    "# 数据加载和预处理\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(image_size),  # 调整图像尺寸\n",
    "    transforms.ToTensor(),  # 转换为 PyTorch 张量\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # 归一化到 [-1, 1]\n",
    "])\n",
    "\n",
    "dataset = datasets.CIFAR10(root='../../../../../data/02.cv/cifar-10/', download=True, transform=transform)\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 初始化模型\n",
    "generator = Generator(latent_dim, channels).cuda()\n",
    "discriminator = Discriminator(channels).cuda()\n",
    "\n",
    "# 损失函数和优化器\n",
    "criterion = nn.BCELoss()  # 二分类交叉熵损失\n",
    "g_optimizer = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "d_optimizer = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "# 开始训练\n",
    "for epoch in range(epochs):\n",
    "    for i, (imgs, _) in enumerate(data_loader):\n",
    "        imgs = imgs.cuda()  # 将数据移动到 GPU\n",
    "\n",
    "        # 创建真实标签和假标签\n",
    "        real_labels = torch.ones(imgs.size(0), 1).cuda()  # 真实图像标签为 1\n",
    "        fake_labels = torch.zeros(imgs.size(0), 1).cuda()  # 生成图像标签为 0\n",
    "\n",
    "        # 训练判别器\n",
    "        z = torch.randn(imgs.size(0), latent_dim).cuda()  # 从标准正态分布中采样潜在向量\n",
    "        fake_imgs = generator(z)  # 使用生成器生成假图像\n",
    "\n",
    "        real_loss = criterion(discriminator(imgs), real_labels)  # 判别器对真实图像的损失\n",
    "        fake_loss = criterion(discriminator(fake_imgs.detach()), fake_labels)  # 判别器对假图像的损失\n",
    "        d_loss = real_loss + fake_loss  # 判别器总损失\n",
    "\n",
    "        d_optimizer.zero_grad()  # 清空梯度\n",
    "        d_loss.backward()  # 反向传播\n",
    "        d_optimizer.step()  # 更新判别器参数\n",
    "\n",
    "        # 训练生成器\n",
    "        g_loss = criterion(discriminator(fake_imgs), real_labels)  # 生成器希望生成的图像被判别为真实\n",
    "\n",
    "        g_optimizer.zero_grad()  # 清空梯度\n",
    "        g_loss.backward()  # 反向传播\n",
    "        g_optimizer.step()  # 更新生成器参数\n",
    "\n",
    "        # 打印日志\n",
    "        if i % 50 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Step [{i}/{len(data_loader)}], D Loss: {d_loss.item():.4f}, G Loss: {g_loss.item():.4f}\")\n",
    "\n",
    "    # 每个 epoch 保存生成的图片\n",
    "    fake_imgs = fake_imgs.view(fake_imgs.size(0), channels, image_size, image_size)\n",
    "    vutils.save_image(fake_imgs[:25], f\"output_epoch_{epoch+1}.png\", nrow=5, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbaca4c-1183-44c4-a314-bbab3a4783f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cfebb3-0c7e-43d0-8e75-3620b0568129",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "04dda9c7-ac67-40d8-8863-0a8b409960f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-26T07:43:43.669372Z",
     "iopub.status.busy": "2024-12-26T07:43:43.664063Z",
     "iopub.status.idle": "2024-12-26T07:43:43.872480Z",
     "shell.execute_reply": "2024-12-26T07:43:43.871547Z",
     "shell.execute_reply.started": "2024-12-26T07:43:43.669372Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator(\n",
      "  (model): Sequential(\n",
      "    (0): Linear(in_features=32, out_features=32768, bias=True)\n",
      "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (2): Unflatten(dim=1, unflattened_size=(128, 16, 16))\n",
      "    (3): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (5): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
      "    (6): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (7): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (8): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (9): Conv2d(256, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "    (11): Conv2d(256, 3, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
      "    (12): Tanh()\n",
      "  )\n",
      ")\n",
      "Output shape: torch.Size([1, 3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim, channels):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128 * 16 * 16),  # 全连接层，生成16*16*128的特征图\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Unflatten(1, (128, 16, 16)),  # 重塑为特征图\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=5, padding=2),  # padding='same' -> padding=2\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.ConvTranspose2d(256, 256, kernel_size=4, stride=2, padding=1),  # 上采样为32x32\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(256, 256, kernel_size=5, padding=2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(256, 256, kernel_size=5, padding=2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(256, channels, kernel_size=7, padding=3),  # 输出图像，padding=3保持输出尺寸不变\n",
    "            nn.Tanh()  # 输出激活函数\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# 参数定义\n",
    "latent_dim = 32\n",
    "height = 32\n",
    "width = 32\n",
    "channels = 3\n",
    "\n",
    "# 测试生成器\n",
    "generator = Generator(latent_dim, channels)\n",
    "generator_input = torch.randn(1, latent_dim)  # 假设一个batch，batch_size=1\n",
    "output = generator(generator_input)\n",
    "\n",
    "print(generator)\n",
    "print(f\"Output shape: {output.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7d5365cf-40bf-4c96-a8d8-df94e81e06b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-26T07:48:56.452980Z",
     "iopub.status.busy": "2024-12-26T07:48:56.451905Z",
     "iopub.status.idle": "2024-12-26T07:48:56.752340Z",
     "shell.execute_reply": "2024-12-26T07:48:56.750341Z",
     "shell.execute_reply.started": "2024-12-26T07:48:56.452980Z"
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x512 and 2048x1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 40\u001b[0m\n\u001b[0;32m     38\u001b[0m discriminator \u001b[38;5;241m=\u001b[39m Discriminator(channels)\n\u001b[0;32m     39\u001b[0m discriminator_input \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m, channels, height, width)  \u001b[38;5;66;03m# 假设一个batch，batch_size=1\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mdiscriminator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdiscriminator_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(discriminator)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOutput shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\cv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[37], line 30\u001b[0m, in \u001b[0;36mDiscriminator.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\cv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\cv\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\cv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\cv\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x512 and 2048x1)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(channels, 128, kernel_size=3),  # 输出维度: (batch_size, 128, height-2, width-2)\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(128, 128, kernel_size=4, stride=2),  # 输出维度: (batch_size, 128, (height-2-3)//2 + 1, (width-2-3)//2 + 1)\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(128, 128, kernel_size=4, stride=2),  # 输出维度: (batch_size, 128, (previous_height-3)//2 + 1, (previous_width-3)//2 + 1)\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Conv2d(128, 128, kernel_size=4, stride=2),  # 输出维度: (batch_size, 128, ...)\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Flatten(),  # 展平，适配全连接层\n",
    "\n",
    "            nn.Dropout(0.4),  # Dropout层\n",
    "\n",
    "            nn.Linear(128 * 4 * 4, 1),  # 假设最终特征图为4x4，调整输入维度\n",
    "            nn.Sigmoid()  # Sigmoid激活函数\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# 参数定义\n",
    "height = 32\n",
    "width = 32\n",
    "channels = 3\n",
    "\n",
    "# 测试判别器\n",
    "discriminator = Discriminator(channels)\n",
    "discriminator_input = torch.randn(1, channels, height, width)  # 假设一个batch，batch_size=1\n",
    "output = discriminator(discriminator_input)\n",
    "\n",
    "print(discriminator)\n",
    "print(f\"Output shape: {output.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea703e41-9a75-4978-9620-e19c95e0a404",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CV",
   "language": "python",
   "name": "cv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
