{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bf1d02d-8ef7-41e0-ad85-d4ef343b337f",
   "metadata": {},
   "source": [
    "# 基于BERT的自然语言处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "768031db-7df8-451a-a0b1-742256b4f03b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T07:24:34.637508Z",
     "start_time": "2024-11-15T07:24:20.953735Z"
    },
    "execution": {
     "iopub.execute_input": "2024-11-19T11:10:05.370628Z",
     "iopub.status.busy": "2024-11-19T11:10:05.370441Z",
     "iopub.status.idle": "2024-11-19T11:10:27.887027Z",
     "shell.execute_reply": "2024-11-19T11:10:27.885361Z",
     "shell.execute_reply.started": "2024-11-19T11:10:05.370628Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "from datetime import timedelta\n",
    "from numpy import ndarray\n",
    "from typing import Union, List, Dict\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm  # 打印进度条\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List\n",
    "from pandas.tseries import offsets\n",
    "from pandas.tseries.frequencies import to_offset\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import joblib\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # 显示中文标签\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c43347",
   "metadata": {},
   "source": [
    "在 BERT（Bidirectional Encoder Representations from Transformers）模型中，输入的维度通常是：\n",
    "\n",
    "input_ids: 这个是将文本转换成的词汇表索引（token ids）。它的形状是 [batch_size, seq_len]，其中 batch_size 是每个批次的样本数，seq_len 是输入文本的最大长度（即模型的输入长度，通常由 max_length 参数控制）。\n",
    "\n",
    "segment_info: 这个是一个用于表示句子 A 和句子 B 的信息（在BERT中通常用于句子对任务，如问答和自然语言推理）。它的形状也是 [batch_size, seq_len]，其中每个位置上的值通常是 0 或 1，分别代表句子 A 和句子 B。\n",
    "\n",
    "BERT 是一个双向 Transformer 模型，在预训练时使用了两种类型的任务：Masked Language Model（MLM）和 Next Sentence Prediction（NSP）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518cab59",
   "metadata": {},
   "source": [
    "BERT 预训练任务的核心：\n",
    "\n",
    "NextSentencePrediction（NSP）：判断两个句子是否有上下文关系，用于训练模型理解句子间的逻辑关系。  \n",
    "MaskedLanguageModel（MLM）：根据部分掩盖的句子预测掩盖的词汇，用于训练模型理解词汇的上下文信息。  \n",
    "\n",
    "这两个任务结合起来帮助 BERT 在大规模语料库上学习语言的上下文依赖性，并且在实际应用中可以通过微调（fine-tuning）来处理各种 NLP 任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969c0c42-8d0b-4f16-aeac-ae28c92ff5af",
   "metadata": {},
   "source": [
    "## 分类"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96f4171-3499-4732-8671-75e96a570ae1",
   "metadata": {},
   "source": [
    "### 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9364bd28-938c-4899-bc2f-d832467d365d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T11:10:30.902645Z",
     "iopub.status.busy": "2024-11-19T11:10:30.899223Z",
     "iopub.status.idle": "2024-11-19T11:10:30.936927Z",
     "shell.execute_reply": "2024-11-19T11:10:30.933912Z",
     "shell.execute_reply.started": "2024-11-19T11:10:30.902233Z"
    }
   },
   "outputs": [],
   "source": [
    "# 读取数据\n",
    "def loader(folder_paths=None):\n",
    "    \"\"\"\n",
    "    读取数据，并对输入数据时间列进行处理\n",
    "\n",
    "    参数说明\n",
    "    ----------\n",
    "    folder_paths : {list}\n",
    "        输入数据文件地址，支持多个文件\n",
    "\n",
    "    返回值\n",
    "    -------\n",
    "    contents : {list}\n",
    "        数据列表\n",
    "    \"\"\"\n",
    "    # 读取原始数据\n",
    "    if folder_paths == None:\n",
    "        raise ValueError(\"folder is not exist!\")\n",
    "\n",
    "    contents = []\n",
    "    # 遍历所有文件夹中的所有文件\n",
    "    for folder_path in folder_paths:\n",
    "        for filename in os.listdir(folder_path):\n",
    "            # 检查文件是否为.txt文件\n",
    "            if filename.endswith('.txt'):\n",
    "                file_path = os.path.join(folder_path, filename)\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    # 读取文件内容并加入到列表中\n",
    "                    contents.append(file.read())\n",
    "    return contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "513c434c-a97f-43f4-8ff5-40da1f59d84b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-19T11:10:32.599854Z",
     "iopub.status.busy": "2024-11-19T11:10:32.597859Z",
     "iopub.status.idle": "2024-11-19T11:14:31.648872Z",
     "shell.execute_reply": "2024-11-19T11:14:31.647329Z",
     "shell.execute_reply.started": "2024-11-19T11:10:32.599854Z"
    }
   },
   "outputs": [],
   "source": [
    "# 负面标签为0，正面标签为1\n",
    "folder_paths = ['../../../../../data/03.nlp/aclImdb/train/neg/', '../../../../../data/03.nlp/aclImdb/train/pos/']\n",
    "texts = loader(folder_paths)\n",
    "labels = [0]*12500 + [1]*12500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca8392e0-db65-4959-8b4f-3b41dab10e68",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-15T12:46:40.960704Z",
     "iopub.status.busy": "2024-11-15T12:46:40.958709Z",
     "iopub.status.idle": "2024-11-15T12:46:53.678994Z",
     "shell.execute_reply": "2024-11-15T12:46:53.677330Z",
     "shell.execute_reply.started": "2024-11-15T12:46:40.960704Z"
    }
   },
   "outputs": [],
   "source": [
    "# 定义并初始化分词器\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "531fc384-776b-48c6-a059-58551814b318",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T03:39:54.142472Z",
     "iopub.status.busy": "2024-11-16T03:39:54.140473Z",
     "iopub.status.idle": "2024-11-16T03:39:54.178067Z",
     "shell.execute_reply": "2024-11-16T03:39:54.176910Z",
     "shell.execute_reply.started": "2024-11-16T03:39:54.142472Z"
    }
   },
   "outputs": [],
   "source": [
    "def generator(data_list, train_ratio, valid_ratio, tokenizer, batch_size, max_length: int = 128):\n",
    "    \"\"\"\n",
    "    读取数据，并对数据进行划分，生成加载器\n",
    "\n",
    "    参数说明\n",
    "    ----------\n",
    "    data_list : {list[DataFrame]}\n",
    "        输入数据，包含数据和标签\n",
    "    train_ratio : {float}\n",
    "        用于训练的数据集占比:将数据按照一定比例进行切分，取值范围为(0,1)\n",
    "    valid_ratio : {float}\n",
    "        用于验证的数据集占比:将数据按照一定比例进行切分，取值范围为(0,1)\n",
    "    tokenizer : {}\n",
    "        分词器\n",
    "    batch_size : {int} \n",
    "        输入数据的批次大小，正整数\n",
    "    max_length : {int} \n",
    "        最大文本截取长度，正整数\n",
    "\n",
    "    返回值\n",
    "    -------\n",
    "    data_loader : {torch.utils.data.dataloader.DataLoader}\n",
    "        数据加载器\n",
    "    \"\"\"\n",
    "    # 获取数据\n",
    "    texts = data_list[0]  # 特征\n",
    "    labels = data_list[1]  # 目标\n",
    "    \n",
    "    # 划分数据\n",
    "    train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=train_ratio, random_state=42)\n",
    "\n",
    "    # 定义数据集的Dataset类\n",
    "    class CustomDataset(Dataset):\n",
    "        def __init__(self, texts, labels, tokenizer, max_length=max_length):\n",
    "            self.texts = texts\n",
    "            self.labels = labels\n",
    "            self.tokenizer = tokenizer\n",
    "            self.max_length = max_length\n",
    "    \n",
    "        def __len__(self):\n",
    "            return len(self.labels)\n",
    "    \n",
    "        def __getitem__(self, idx):\n",
    "            text = self.texts[idx]\n",
    "            label = self.labels[idx]\n",
    "            \n",
    "            # 对文本进行token化，并转换为输入id和attention mask\n",
    "            encoding = self.tokenizer(\n",
    "                text,\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                max_length=self.max_length,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            return {\n",
    "                'input_ids': encoding['input_ids'].flatten(),\n",
    "                'attention_mask': encoding['attention_mask'].flatten(),\n",
    "                'label': torch.tensor(label, dtype=torch.long)\n",
    "            }\n",
    "\n",
    "    # 创建Dataset和DataLoader\n",
    "    train_dataset = CustomDataset(train_texts, train_labels, tokenizer, max_length=max_length)\n",
    "    val_dataset = CustomDataset(val_texts, val_labels, tokenizer, max_length=max_length)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    return train_loader, valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "799f1d25-ef31-4cd7-a6c2-65186f57b78f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T03:41:16.054311Z",
     "iopub.status.busy": "2024-11-16T03:41:16.052313Z",
     "iopub.status.idle": "2024-11-16T03:41:16.081255Z",
     "shell.execute_reply": "2024-11-16T03:41:16.080254Z",
     "shell.execute_reply.started": "2024-11-16T03:41:16.054311Z"
    }
   },
   "outputs": [],
   "source": [
    "# 构造参数字典\n",
    "params1 = {\n",
    "    \"data_list\": [texts, labels],\n",
    "    \"train_ratio\": 0.8,\n",
    "    \"valid_ratio\": 0.1,\n",
    "    \"tokenizer\": tokenizer,\n",
    "    \"batch_size\": 64,\n",
    "    \"max_length\": 200,\n",
    "}\n",
    "\n",
    "# 函数传参\n",
    "train_loader, valid_loader = generator(**params1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17dad58d-6091-4629-9841-0dc3c6029458",
   "metadata": {},
   "source": [
    "### 模型定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4b315482",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T09:32:51.626470Z",
     "start_time": "2024-11-15T09:32:51.585325Z"
    },
    "execution": {
     "iopub.execute_input": "2024-11-16T03:41:17.034314Z",
     "iopub.status.busy": "2024-11-16T03:41:17.032315Z",
     "iopub.status.idle": "2024-11-16T03:41:17.108041Z",
     "shell.execute_reply": "2024-11-16T03:41:17.107043Z",
     "shell.execute_reply.started": "2024-11-16T03:41:17.034314Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "# 位置嵌入（PositionalEmbedding）：用于为输入的每个词汇位置添加位置信息（基于正弦和余弦函数）\n",
    "class PositionalEmbedding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, max_len=512):\n",
    "        super().__init__()\n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model).float()\n",
    "        pe.require_grad = False\n",
    "\n",
    "        position = torch.arange(0, max_len).float().unsqueeze(1)\n",
    "        div_term = (torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)).exp()\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pe[:, :x.size(1)]\n",
    "\n",
    "# 段落嵌入（SegmentEmbedding）：在BERT中用于表示输入的不同句子（如句子A和句子B）\n",
    "class SegmentEmbedding(nn.Embedding):\n",
    "    def __init__(self, embed_size=512):\n",
    "        super().__init__(3, embed_size, padding_idx=0)\n",
    "\n",
    "# 词汇嵌入（TokenEmbedding）：将输入的词汇ID映射到对应的词向量\n",
    "class TokenEmbedding(nn.Embedding):\n",
    "    def __init__(self, vocab_size, embed_size=512):\n",
    "        super().__init__(vocab_size, embed_size, padding_idx=0)\n",
    "\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    \"\"\"\n",
    "    Paper Section 3.4, last paragraph notice that BERT used the GELU instead of RELU\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
    "\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"Implements FFN equation.\"\n",
    "\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(self.activation(self.w_1(x))))\n",
    "\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    \"Construct a layernorm module (See citation for details).\"\n",
    "\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2\n",
    "\n",
    "# 残差连接和层归一化的组合\n",
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"Apply residual connection to any sublayer with the same size.\"\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    Compute 'Scaled Dot Product Attention\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, query, key, value, mask=None, dropout=None):\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) \\\n",
    "                 / math.sqrt(query.size(-1))\n",
    "\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        p_attn = F.softmax(scores, dim=-1)\n",
    "\n",
    "        if dropout is not None:\n",
    "            p_attn = dropout(p_attn)\n",
    "\n",
    "        return torch.matmul(p_attn, value), p_attn\n",
    "\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Take in model size and number of heads.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % h == 0\n",
    "\n",
    "        # We assume d_v always equals d_k\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "\n",
    "        self.linear_layers = nn.ModuleList([nn.Linear(d_model, d_model) for _ in range(3)])\n",
    "        self.output_linear = nn.Linear(d_model, d_model)\n",
    "        self.attention = Attention()\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k\n",
    "        query, key, value = [l(x).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\n",
    "                             for l, x in zip(self.linear_layers, (query, key, value))]\n",
    "\n",
    "        # 2) Apply attention on all the projected vectors in batch.\n",
    "        x, attn = self.attention(query, key, value, mask=mask, dropout=self.dropout)\n",
    "\n",
    "        # 3) \"Concat\" using a view and apply a final linear.\n",
    "        x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.h * self.d_k)\n",
    "\n",
    "        return self.output_linear(x)\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Bidirectional Encoder = Transformer (self-attention)\n",
    "    Transformer = MultiHead_Attention + Feed_Forward with sublayer connection\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden, attn_heads, feed_forward_hidden, dropout):\n",
    "        \"\"\"\n",
    "        :param hidden: hidden size of transformer\n",
    "        :param attn_heads: head sizes of multi-head attention\n",
    "        :param feed_forward_hidden: feed_forward_hidden, usually 4*hidden_size\n",
    "        :param dropout: dropout rate\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadedAttention(h=attn_heads, d_model=hidden)\n",
    "        self.feed_forward = PositionwiseFeedForward(d_model=hidden, d_ff=feed_forward_hidden, dropout=dropout)\n",
    "        self.input_sublayer = SublayerConnection(size=hidden, dropout=dropout)\n",
    "        self.output_sublayer = SublayerConnection(size=hidden, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.input_sublayer(x, lambda _x: self.attention.forward(_x, _x, _x, mask=mask))\n",
    "        x = self.output_sublayer(x, self.feed_forward)\n",
    "        return self.dropout(x)\n",
    "    \n",
    "# 将输入的ID序列转换为嵌入向量\n",
    "class BERTEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    BERT Embedding which is consisted with under features\n",
    "        1. TokenEmbedding : normal embedding matrix\n",
    "        2. PositionalEmbedding : adding positional information using sin, cos\n",
    "        2. SegmentEmbedding : adding sentence segment info, (sent_A:1, sent_B:2)\n",
    "\n",
    "        sum of all these features are output of BERTEmbedding\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, embed_size, dropout=0.1):\n",
    "        \"\"\"\n",
    "        :param vocab_size: total vocab size\n",
    "        :param embed_size: embedding size of token embedding\n",
    "        :param dropout: dropout rate\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.token = TokenEmbedding(vocab_size=vocab_size, embed_size=embed_size)\n",
    "        self.position = PositionalEmbedding(d_model=self.token.embedding_dim)\n",
    "        self.segment = SegmentEmbedding(embed_size=self.token.embedding_dim)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "    def forward(self, sequence, segment_label):\n",
    "        x = self.token(sequence) + self.position(sequence) + self.segment(segment_label)\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class BERT(nn.Module):\n",
    "    \"\"\"\n",
    "    BERT model : Bidirectional Encoder Representations from Transformers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, hidden=768, n_layers=12, attn_heads=12, dropout=0.1):\n",
    "        \"\"\"\n",
    "        :param vocab_size: vocab_size of total words\n",
    "        :param hidden: BERT model hidden size\n",
    "        :param n_layers: numbers of Transformer blocks(layers)\n",
    "        :param attn_heads: number of attention heads\n",
    "        :param dropout: dropout rate\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.hidden = hidden\n",
    "        self.n_layers = n_layers\n",
    "        self.attn_heads = attn_heads\n",
    "\n",
    "        # paper noted they used 4*hidden_size for ff_network_hidden_size\n",
    "        self.feed_forward_hidden = hidden * 4\n",
    "\n",
    "        # embedding for BERT, sum of positional, segment, token embeddings\n",
    "        self.embedding = BERTEmbedding(vocab_size=vocab_size, embed_size=hidden)\n",
    "\n",
    "        # multi-layers transformer blocks, deep network\n",
    "        self.transformer_blocks = nn.ModuleList(\n",
    "            [TransformerBlock(hidden, attn_heads, hidden * 4, dropout) for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, x, segment_info):\n",
    "        # attention masking for padded token\n",
    "        # 注意力编码，torch.ByteTensor([batch_size, 1, seq_len, seq_len)\n",
    "        mask = (x > 0).unsqueeze(1).repeat(1, x.size(1), 1).unsqueeze(1)\n",
    "\n",
    "        # embedding the indexed sequence to sequence of vectors\n",
    "        x = self.embedding(x, segment_info)\n",
    "\n",
    "        # running over multiple transformer blocks\n",
    "        for transformer in self.transformer_blocks:\n",
    "            x = transformer.forward(x, mask)\n",
    "\n",
    "        return x\n",
    "    \n",
    "# BERT完整模型，包含两大核心：NSP和MLM\n",
    "class BERTLM(nn.Module):\n",
    "    \"\"\"\n",
    "    BERT Language Model\n",
    "    Next Sentence Prediction Model + Masked Language Model\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, bert: BERT, vocab_size):\n",
    "        \"\"\"\n",
    "        :param bert: BERT model which should be trained\n",
    "        :param vocab_size: total vocab size for masked_lm\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.bert = bert\n",
    "        self.next_sentence = NextSentencePrediction(self.bert.hidden)\n",
    "        self.mask_lm = MaskedLanguageModel(self.bert.hidden, vocab_size)\n",
    "\n",
    "    def forward(self, x, segment_label):\n",
    "        x = self.bert(x, segment_label)\n",
    "        return self.next_sentence(x), self.mask_lm(x)\n",
    "\n",
    "# Next Sentence Prediction (NSP) 任务的目标是让 BERT 学会判断给定的两个句子是否具有上下文关联，即第二个句子是否是第一个句子的自然后续。\n",
    "class NextSentencePrediction(nn.Module):\n",
    "    \"\"\"\n",
    "    2-class classification model : is_next, is_not_next\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden):\n",
    "        \"\"\"\n",
    "        :param hidden: BERT model output size\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(hidden, 2)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.softmax(self.linear(x[:, 0]))\n",
    "\n",
    "# Masked Language Model (MLM) 任务的目标是通过给定一个部分掩盖的句子，预测被掩盖（[MASK]）的位置的词汇。\n",
    "# 这是 BERT 预训练过程中用于学习词汇表示和上下文信息的任务。\n",
    "class MaskedLanguageModel(nn.Module):\n",
    "    \"\"\"\n",
    "    predicting origin token from masked input sequence\n",
    "    n-class classification problem, n-class = vocab_size\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden, vocab_size):\n",
    "        \"\"\"\n",
    "        :param hidden: output size of BERT model\n",
    "        :param vocab_size: total vocab size\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(hidden, vocab_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.softmax(self.linear(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ca938928",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-15T09:33:12.472067Z",
     "start_time": "2024-11-15T09:33:12.466376Z"
    },
    "execution": {
     "iopub.execute_input": "2024-11-16T03:41:17.286524Z",
     "iopub.status.busy": "2024-11-16T03:41:17.283458Z",
     "iopub.status.idle": "2024-11-16T03:41:17.316408Z",
     "shell.execute_reply": "2024-11-16T03:41:17.310593Z",
     "shell.execute_reply.started": "2024-11-16T03:41:17.286524Z"
    }
   },
   "outputs": [],
   "source": [
    "# BERT分类任务\n",
    "class BERTC(nn.Module):\n",
    "    \"\"\"\n",
    "    BERTC : BERT分类模型.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, hidden_size, n_layers, attn_heads, dropout, num_classes=2):\n",
    "        super(BERTC, self).__init__()\n",
    "        self.bert = BERT(vocab_size=vocab_size, hidden=hidden_size, n_layers=n_layers, attn_heads=attn_heads, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)  # 分类头\n",
    "\n",
    "    def forward(self, input_ids, segment_ids):\n",
    "        # 获取 BERT 输出\n",
    "        bert_output = self.bert(input_ids, segment_ids)\n",
    "        # 取 BERT 输出的 [CLS] token 表示作为句子表示\n",
    "        cls_output = bert_output[:, 0, :]  # 取第一个 token（[CLS] token）的输出\n",
    "        output = self.fc(cls_output)  # 进行分类\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6f7b64-b9d5-4ea9-8d97-41e8d7bf3656",
   "metadata": {},
   "source": [
    "### 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6f070d43-23e6-499c-9d91-a0571821ade3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T03:41:17.930759Z",
     "iopub.status.busy": "2024-11-16T03:41:17.928003Z",
     "iopub.status.idle": "2024-11-16T03:41:18.029011Z",
     "shell.execute_reply": "2024-11-16T03:41:18.027488Z",
     "shell.execute_reply.started": "2024-11-16T03:41:17.930759Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(train_args, model_args):\n",
    "    # 参数配置\n",
    "    model_name = train_args['model_name']  # 模型名称\n",
    "    train_loader = train_args['train_loader']  # 训练集\n",
    "    valid_loader = train_args['valid_loader']  # 验证集\n",
    "    n_epochs = train_args['n_epochs']  # 训练次数\n",
    "    learning_rate = train_args['learning_rate']  # 学习率\n",
    "    loss = train_args['loss']  # 损失函数\n",
    "    patience = train_args['patience']  # 最大早停次数阈值，超过就会早停\n",
    "    lradj = train_args['lradj']  # 学习率函数\n",
    "    model_path = train_args['model_path']  # 模型保存路径\n",
    "    verbose = train_args['verbose']  # 打印训练过程\n",
    "    plots = train_args['plots']  # 绘制损失图\n",
    "    device = train_args['device']  # 训练设备，可选'cuda'和'cpu'\n",
    "\n",
    "    # 检查可用device\n",
    "    device = torch.device(device)\n",
    "\n",
    "    # 创建模型和优化器\n",
    "    model = model_name(**model_args).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = loss\n",
    "\n",
    "    # 调整学习率\n",
    "    def adjust_learning_rate(optimizer, epoch, lradj, learning_rate, train_epochs):\n",
    "        # lr = learning_rate * (0.2 ** (epoch // 2))\n",
    "        if lradj == 'type1':\n",
    "            lr_adjust = {epoch: learning_rate * (0.5 ** ((epoch - 1) // 1))}\n",
    "        elif lradj == 'type2':\n",
    "            lr_adjust = {\n",
    "                2: 5e-5, 4: 1e-5, 6: 5e-6, 8: 1e-6,\n",
    "                10: 5e-7, 15: 1e-7, 20: 5e-8\n",
    "            }\n",
    "        elif lradj == \"cosine\":\n",
    "            lr_adjust = {epoch: learning_rate / 2 *\n",
    "                         (1 + math.cos(epoch / train_epochs * math.pi))}\n",
    "        if epoch in lr_adjust.keys():\n",
    "            lr = lr_adjust[epoch]\n",
    "            # 参数组(param_groups)是用来指定不同的参数组以便对它们进行不同的优化设置，比如'lr'\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "            print('Updating learning rate to {}'.format(lr))\n",
    "\n",
    "    # 设置早停\n",
    "    class EarlyStopping():\n",
    "        def __init__(self, patience=7, verbose=False, delta=0):\n",
    "            self.patience = patience  # 连续超限次数，如果满足条件，则早停\n",
    "            self.verbose = verbose\n",
    "            self.counter = 0\n",
    "            self.best_score = None\n",
    "            self.early_stop = False\n",
    "            self.val_loss_min = np.Inf\n",
    "            self.delta = delta\n",
    "\n",
    "        def __call__(self, val_loss, model, path):\n",
    "            score = -val_loss\n",
    "            if self.best_score is None:\n",
    "                self.best_score = score\n",
    "                self.save_checkpoint(val_loss, model, path)\n",
    "            elif score < self.best_score + self.delta:\n",
    "                self.counter += 1\n",
    "                print(\n",
    "                    f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "                if self.counter >= self.patience:\n",
    "                    self.early_stop = True\n",
    "            else:\n",
    "                self.best_score = score\n",
    "                self.save_checkpoint(val_loss, model, path)\n",
    "                self.counter = 0\n",
    "\n",
    "        def save_checkpoint(self, val_loss, model, path):\n",
    "            if self.verbose:\n",
    "                print(\n",
    "                    f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "            torch.save(model.state_dict(), path + '/' + 'checkpoint.pth')\n",
    "            self.val_loss_min = val_loss\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=verbose)\n",
    "\n",
    "    # 设置保存模型路径\n",
    "    if not os.path.exists(model_path):\n",
    "        os.makedirs(model_path)\n",
    "\n",
    "    # 模型训练和验证\n",
    "    train_losses, val_losses = [], []\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        train_correct_predictions = 0\n",
    "        for batch in train_loader:\n",
    "            # 将数据移至 device\n",
    "            batch_x = batch['input_ids'].to(device)\n",
    "            batch_y = batch['label'].to(device)\n",
    "            segment_ids = torch.zeros_like(batch_x).to(device)\n",
    "            # 清空梯度\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x, segment_ids)\n",
    "            train_loss = criterion(outputs, batch_y)\n",
    "            # 反向传播计算得到每个参数的梯度值\n",
    "            train_loss.backward()\n",
    "            # 通过梯度下降执行一步参数更新\n",
    "            optimizer.step()\n",
    "            # 每个batch的loss和\n",
    "            total_train_loss += train_loss.item()  # .item()表示只包含一个元素的tensor中提取值\n",
    "            # 计算准确率\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            train_correct_predictions += torch.sum(preds == batch_y)\n",
    "\n",
    "        # 每个epoch的损失平均\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "        # 所有epoch的loss\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # 计算准确率\n",
    "        train_accuracy = train_correct_predictions.double() / len(train_loader.dataset)\n",
    "\n",
    "        # 评估模型\n",
    "        model.eval()\n",
    "        # 关闭自动求导功能，只使用训练好的模型进行预测或评估，不需要进行梯度计算和参数更新\n",
    "        with torch.no_grad():\n",
    "            total_val_loss = 0\n",
    "            val_correct_predictions = 0\n",
    "            for batch in valid_loader:\n",
    "                # 将数据移至 device\n",
    "                batch_x = batch['input_ids'].to(device)\n",
    "                batch_y = batch['label'].to(device)\n",
    "                segment_ids = torch.zeros_like(batch_x).to(device)\n",
    "                outputs = model(batch_x, segment_ids)\n",
    "                val_loss = criterion(outputs, batch_y)\n",
    "                # 每个batch的loss和\n",
    "                total_val_loss += val_loss.item()\n",
    "                # 计算准确率\n",
    "                _, preds = torch.max(outputs, dim=1)\n",
    "                val_correct_predictions += torch.sum(preds == batch_y)\n",
    "\n",
    "        # 每个epoch的损失平均\n",
    "        avg_val_loss = total_val_loss / len(valid_loader)\n",
    "\n",
    "        # 所有epoch的loss\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        # 计算准确率\n",
    "        val_accuracy = val_correct_predictions.double() / len(valid_loader.dataset)\n",
    "\n",
    "        # 打印训练过程\n",
    "        if verbose:\n",
    "            print(f'Epoch [{epoch+1}/{n_epochs}], Training Loss: {avg_train_loss:.4f}, Training Accuracy: {train_accuracy:.4f}, Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "        # 设置早停\n",
    "        early_stopping(avg_val_loss, model, model_path)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping!\")\n",
    "            break\n",
    "\n",
    "        # 调整学习率\n",
    "        adjust_learning_rate(optimizer, epoch+1, lradj,\n",
    "                             learning_rate, n_epochs)\n",
    "\n",
    "    # 绘制损失函数图\n",
    "    def plot_loss(train_loss, val_loss):\n",
    "        \"\"\"\n",
    "        绘制训练和验证损失曲线\n",
    "\n",
    "        参数:\n",
    "        - train_loss: 训练损失数组\n",
    "        - val_loss: 验证损失数组\n",
    "        \"\"\"\n",
    "        # 自动生成 epochs（假设train_loss和val_loss长度一致）\n",
    "        epochs = np.arange(len(train_loss))\n",
    "\n",
    "        # 使用 Seaborn 设置白色背景样式\n",
    "        sns.set(style=\"white\")\n",
    "\n",
    "        # 创建图形并优化细节\n",
    "        plt.figure(figsize=(10, 6))\n",
    "\n",
    "        # 绘制训练和验证曲线\n",
    "        plt.plot(epochs, train_loss, label='Training', color='#d62728', linewidth=2, marker='o', markersize=6)\n",
    "        plt.plot(epochs, val_loss, label='Validation', color='#1f77b4', linewidth=2, marker='s', markersize=6)\n",
    "\n",
    "        # 添加标题和标签\n",
    "        plt.title('Training and Validation Loss', fontsize=18, fontweight='bold', color='black')\n",
    "        plt.xlabel('Epoch', fontsize=14)\n",
    "        plt.ylabel('Loss', fontsize=14)\n",
    "\n",
    "        # 添加图例\n",
    "        plt.legend(fontsize=12, loc='upper right')\n",
    "\n",
    "        # 启用横向网格线\n",
    "        plt.grid(True, axis='y', linestyle='--', linewidth=0.5, alpha=0.7)\n",
    "\n",
    "        # 去掉顶部和右侧的边框，仅显示左侧和底部的边框\n",
    "        plt.gca().spines['top'].set_visible(False)\n",
    "        plt.gca().spines['right'].set_visible(False)\n",
    "\n",
    "        # 可选：修改左侧和底部边框的样式\n",
    "        plt.gca().spines['left'].set_linewidth(1.5)\n",
    "        plt.gca().spines['bottom'].set_linewidth(1.5)\n",
    "\n",
    "        plt.gca().spines['left'].set_visible(True)\n",
    "        plt.gca().spines['bottom'].set_visible(True)\n",
    "\n",
    "        plt.gca().tick_params(axis='both', which='both', length=0)\n",
    "\n",
    "        # 调整布局以防止标签重叠\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # 展示图形\n",
    "        plt.show()\n",
    "\n",
    "    if plots:\n",
    "        plot_loss(train_losses, val_losses)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1dd75124-1797-499e-a37f-43d6ced79c85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-16T03:41:18.339538Z",
     "iopub.status.busy": "2024-11-16T03:41:18.336538Z",
     "iopub.status.idle": "2024-11-16T03:41:23.609976Z",
     "shell.execute_reply": "2024-11-16T03:41:23.607975Z",
     "shell.execute_reply.started": "2024-11-16T03:41:18.338539Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/20 [00:04<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 150.00 MiB (GPU 0; 2.00 GiB total capacity; 8.74 GiB already allocated; 0 bytes free; 9.03 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 26\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 构造参数字典\u001b[39;00m\n\u001b[0;32m      2\u001b[0m params2 \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_args\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: BERTC,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     24\u001b[0m     },\n\u001b[0;32m     25\u001b[0m }\n\u001b[1;32m---> 26\u001b[0m model \u001b[38;5;241m=\u001b[39m train(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams2)\n",
      "Cell \u001b[1;32mIn[27], line 96\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(train_args, model_args)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;66;03m# 清空梯度\u001b[39;00m\n\u001b[0;32m     95\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 96\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msegment_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     97\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m criterion(outputs, batch_y)\n\u001b[0;32m     98\u001b[0m \u001b[38;5;66;03m# 反向传播计算得到每个参数的梯度值\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\nlp\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[26], line 13\u001b[0m, in \u001b[0;36mBERTC.forward\u001b[1;34m(self, input_ids, segment_ids)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, segment_ids):\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m# 获取 BERT 输出\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m     bert_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msegment_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;66;03m# 取 BERT 输出的 [CLS] token 表示作为句子表示\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     cls_output \u001b[38;5;241m=\u001b[39m bert_output[:, \u001b[38;5;241m0\u001b[39m, :]  \u001b[38;5;66;03m# 取第一个 token（[CLS] token）的输出\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\nlp\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[25], line 241\u001b[0m, in \u001b[0;36mBERT.forward\u001b[1;34m(self, x, segment_info)\u001b[0m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;66;03m# running over multiple transformer blocks\u001b[39;00m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m transformer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_blocks:\n\u001b[1;32m--> 241\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "Cell \u001b[1;32mIn[25], line 170\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[1;34m(self, x, mask)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, mask):\n\u001b[0;32m    169\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_sublayer(x, \u001b[38;5;28;01mlambda\u001b[39;00m _x: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention\u001b[38;5;241m.\u001b[39mforward(_x, _x, _x, mask\u001b[38;5;241m=\u001b[39mmask))\n\u001b[1;32m--> 170\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_sublayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    171\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\nlp\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[25], line 89\u001b[0m, in \u001b[0;36mSublayerConnection.forward\u001b[1;34m(self, x, sublayer)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, sublayer):\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mApply residual connection to any sublayer with the same size.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[43msublayer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\nlp\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[25], line 58\u001b[0m, in \u001b[0;36mPositionwiseFeedForward.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mw_2(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivation\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mw_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\nlp\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\nlp\\lib\\site-packages\\torch\\nn\\modules\\dropout.py:59\u001b[0m, in \u001b[0;36mDropout.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\nlp\\lib\\site-packages\\torch\\nn\\functional.py:1252\u001b[0m, in \u001b[0;36mdropout\u001b[1;34m(input, p, training, inplace)\u001b[0m\n\u001b[0;32m   1250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[0;32m   1251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(p))\n\u001b[1;32m-> 1252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 150.00 MiB (GPU 0; 2.00 GiB total capacity; 8.74 GiB already allocated; 0 bytes free; 9.03 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# 构造参数字典\n",
    "params2 = {\n",
    "    \"train_args\": {\n",
    "        \"model_name\": BERTC,\n",
    "        \"train_loader\": train_loader,\n",
    "        \"valid_loader\": valid_loader,\n",
    "        \"n_epochs\": 20,\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"loss\": nn.CrossEntropyLoss(),\n",
    "        \"patience\": 5,\n",
    "        \"lradj\": 'cosine',\n",
    "        \"model_path\": \"../outputs/best_models/BERTC\",\n",
    "        \"device\": 'cuda',\n",
    "        \"verbose\": True,\n",
    "        \"plots\": True,\n",
    "    },\n",
    "    \"model_args\": {\n",
    "        'hidden_size': 768,\n",
    "        'vocab_size': tokenizer.vocab_size, \n",
    "        'dropout': 0.1,\n",
    "        'attn_heads': 12,\n",
    "        'n_layers': 2,\n",
    "        'num_classes': 2,\n",
    "    },\n",
    "}\n",
    "model = train(**params2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aca31fa-d0c6-4483-a0ba-2ad748a6ddf4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "186.6px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
