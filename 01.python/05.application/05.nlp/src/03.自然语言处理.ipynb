{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0021130a-a861-4311-ad7e-62c41527d9ec",
   "metadata": {},
   "source": [
    "# 大模型介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cec41a0-e8aa-46b1-bbbd-10bc9d186edc",
   "metadata": {},
   "source": [
    "## 大模型的定义"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98411ab1-4f3e-4c63-b7b6-603c8861d625",
   "metadata": {},
   "source": [
    "LLM是人工智能大模型，大多数情况下源自Transformer架构，旨在理解和生成人类语言、代码等。通常有两种语言建模任务：自编码任务和自回归任务。\n",
    "\n",
    "词元是语义意义的最小单位，是通过将句子或文本分解成更小的单位而创建的；它是LLM的基本输入。\n",
    "\n",
    "**自编码语言模型**让模型从已知词汇表中的单词来填充短语的缺失部分。 例如BERT   \n",
    "**自回归语言模型**要求模型从已知词汇中生成给定短语的下一个最可能的词。例如GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2439361e-40db-4b8a-b59f-85d25559dd56",
   "metadata": {},
   "source": [
    "## 大模型是如何工作的"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c86f62-e939-42f0-9dd5-84b0547e6801",
   "metadata": {},
   "source": [
    "### 预训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d64a5e2-2bdd-4217-b39e-3c29a51b9bd7",
   "metadata": {},
   "source": [
    "LLM的表现效果仅是合格还是最佳，取决于其预训练和微调的方式。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec09606-1636-430d-8e10-a523e4e52549",
   "metadata": {},
   "source": [
    "### 迁移学习"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5763e76e-1cd2-4b95-9a0b-6a05cdc9ed7c",
   "metadata": {},
   "source": [
    "迁移学习背后的思想是预训练模型已经学习了大量的语言和单词之间的关系，这些信息可以作为起点，以提高新任务的性能。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc86b87-7f40-4311-bba4-d346767afec8",
   "metadata": {},
   "source": [
    "### 微调"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e662c9f7-a2aa-4c8e-add7-527fa628fcf5",
   "metadata": {},
   "source": [
    "微调涉及在较小的特定任务数据集上训练LLM，以调整其参数，使其适应特定任务。\n",
    "- 定义要微调的模型以及全部微调参数；  \n",
    "- 聚合一些训练数据；  \n",
    "- 计算损失和梯度；  \n",
    "- 通过反向传播来更新模型-这是一种更新模型参数以最小化误差的机制。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160edaad-e108-4ca3-b708-21a833cb34f4",
   "metadata": {},
   "source": [
    "### 注意力"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c73334d-8a20-411a-ba6b-32b52aedd1a9",
   "metadata": {},
   "source": [
    "注意力机制是深度学习模型中使用的一种机制，允许模型动态的聚焦输入的不同部分，从而提高性能和结果准确性。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52fda39-a607-4158-995d-1e8210806d69",
   "metadata": {},
   "source": [
    "### 嵌入"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595eeaff-57b8-460f-9feb-d5b8d3ef3c9b",
   "metadata": {},
   "source": [
    "嵌入是在高维空间中对单词，词语或词元的数学表示，在NLP中，嵌入用于表示单词、短语或词元，以捕捉它们的语义含义以及与其他单词的关系，可能有多种类型的嵌入，包括对句子中位置进行编码的位置嵌入，以及对词的语义进行编码的词嵌入。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc8d0e5-cf25-4e06-be90-661ac83de3aa",
   "metadata": {},
   "source": [
    "### 词元化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d529fdee-2525-4af8-aa85-067073481ec6",
   "metadata": {},
   "source": [
    "词元化过程是句子按照语义切割为小片段，并以嵌入方式参与注意力机制的计算过程。在传统NLP中使用的停用词删除、词干提取等技术，对于LLM来说不是必要的，反而可能会损害模型的性能。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6233fe39-0be6-40cb-8d3d-31e856d8650b",
   "metadata": {},
   "source": [
    "## 当前流行的大模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37717fc-13a3-4c6a-93ee-9545421246fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
