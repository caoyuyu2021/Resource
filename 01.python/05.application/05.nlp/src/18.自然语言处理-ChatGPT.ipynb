{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0021130a-a861-4311-ad7e-62c41527d9ec",
   "metadata": {},
   "source": [
    "# 大模型介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cec41a0-e8aa-46b1-bbbd-10bc9d186edc",
   "metadata": {},
   "source": [
    "## 大模型的定义"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98411ab1-4f3e-4c63-b7b6-603c8861d625",
   "metadata": {},
   "source": [
    "LLM是人工智能大模型，大多数情况下源自Transformer架构，旨在理解和生成人类语言、代码等。通常有两种语言建模任务：自编码任务和自回归任务。\n",
    "\n",
    "词元是语义意义的最小单位，是通过将句子或文本分解成更小的单位而创建的；它是LLM的基本输入。\n",
    "\n",
    "**自编码语言模型**让模型从已知词汇表中的单词来填充短语的缺失部分。 例如BERT   \n",
    "**自回归语言模型**要求模型从已知词汇中生成给定短语的下一个最可能的词。例如GPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2439361e-40db-4b8a-b59f-85d25559dd56",
   "metadata": {},
   "source": [
    "## 大模型是如何工作的"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c86f62-e939-42f0-9dd5-84b0547e6801",
   "metadata": {},
   "source": [
    "### 预训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d64a5e2-2bdd-4217-b39e-3c29a51b9bd7",
   "metadata": {},
   "source": [
    "LLM的表现效果仅是合格还是最佳，取决于其预训练和微调的方式。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec09606-1636-430d-8e10-a523e4e52549",
   "metadata": {},
   "source": [
    "### 迁移学习"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5763e76e-1cd2-4b95-9a0b-6a05cdc9ed7c",
   "metadata": {},
   "source": [
    "迁移学习背后的思想是预训练模型已经学习了大量的语言和单词之间的关系，这些信息可以作为起点，以提高新任务的性能。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc86b87-7f40-4311-bba4-d346767afec8",
   "metadata": {},
   "source": [
    "### 微调"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e662c9f7-a2aa-4c8e-add7-527fa628fcf5",
   "metadata": {},
   "source": [
    "微调涉及在较小的特定任务数据集上训练LLM，以调整其参数，使其适应特定任务。\n",
    "- 定义要微调的模型以及全部微调参数；  \n",
    "- 聚合一些训练数据；  \n",
    "- 计算损失和梯度；  \n",
    "- 通过反向传播来更新模型-这是一种更新模型参数以最小化误差的机制。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160edaad-e108-4ca3-b708-21a833cb34f4",
   "metadata": {},
   "source": [
    "### 注意力"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c73334d-8a20-411a-ba6b-32b52aedd1a9",
   "metadata": {},
   "source": [
    "注意力机制是深度学习模型中使用的一种机制，允许模型动态的聚焦输入的不同部分，从而提高性能和结果准确性。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52fda39-a607-4158-995d-1e8210806d69",
   "metadata": {},
   "source": [
    "### 嵌入"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595eeaff-57b8-460f-9feb-d5b8d3ef3c9b",
   "metadata": {},
   "source": [
    "嵌入是在高维空间中对单词，词语或词元的数学表示，在NLP中，嵌入用于表示单词、短语或词元，以捕捉它们的语义含义以及与其他单词的关系，可能有多种类型的嵌入，包括对句子中位置进行编码的位置嵌入，以及对词的语义进行编码的词嵌入。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc8d0e5-cf25-4e06-be90-661ac83de3aa",
   "metadata": {},
   "source": [
    "### 词元化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d529fdee-2525-4af8-aa85-067073481ec6",
   "metadata": {},
   "source": [
    "词元化过程是句子按照语义切割为小片段，并以嵌入方式参与注意力机制的计算过程。在传统NLP中使用的停用词删除、词干提取等技术，对于LLM来说不是必要的，反而可能会损害模型的性能。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6233fe39-0be6-40cb-8d3d-31e856d8650b",
   "metadata": {},
   "source": [
    "## 当前流行的大模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2681bae5",
   "metadata": {},
   "source": [
    "BERT、GPT-3、ChatGPT、ChatGLM3等。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d67e1fd",
   "metadata": {},
   "source": [
    "# 大模型微调"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d7cf90",
   "metadata": {},
   "source": [
    "## GPT-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ac9f35",
   "metadata": {},
   "source": [
    "### 环境配置"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83316af5",
   "metadata": {},
   "source": [
    "要安装jupyter_ai，则需要安装 Python 3.8 至 3.10 和 JupyterLab 3。可以jupyter_ai使用 conda 和 pip 进行安装。\n",
    "\n",
    "使用 Python 3.10 创建一个新的虚拟环境并安装jupyter_ai.\n",
    "\n",
    "`pip install jupyter_ai`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272da73a",
   "metadata": {},
   "source": [
    "请注意，要将 Jupyter AI 与特定提供商一起使用，必须安装其 Python 包并在环境或聊天界面中设置其 API 密钥。\n",
    "\n",
    "假设使用 OpenAI 的 ChatGPT 模型。则须安装 openai 软件包。\n",
    "\n",
    "`pip install openai`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb34bc8b",
   "metadata": {},
   "source": [
    "### 使用指南"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f5b3677",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-06T13:04:24.680369Z",
     "start_time": "2024-11-06T13:04:21.933533Z"
    },
    "execution": {
     "iopub.execute_input": "2024-11-12T05:33:05.117906Z",
     "iopub.status.busy": "2024-11-12T05:33:05.117906Z",
     "iopub.status.idle": "2024-11-12T05:33:11.802092Z",
     "shell.execute_reply": "2024-11-12T05:33:11.801310Z",
     "shell.execute_reply.started": "2024-11-12T05:33:05.117906Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 加载功能，并连接openapi api key，淘宝可买\n",
    "import os \n",
    "%load_ext jupyter_ai\n",
    "os.environ[\"OPENAI_API_KEY\"]='sk-jFBLOMLTmN4Xs81hKFiHBlHqZFs2tU3WD5hzvVvTCKtGJVE8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ce2ab4b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T05:33:12.317463Z",
     "iopub.status.busy": "2024-11-12T05:33:12.316949Z",
     "iopub.status.idle": "2024-11-12T05:33:12.346546Z",
     "shell.execute_reply": "2024-11-12T05:33:12.345579Z",
     "shell.execute_reply.started": "2024-11-12T05:33:12.317463Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| Provider | Environment variable | Set? | Models |\n",
       "|----------|----------------------|------|--------|\n",
       "| `ai21` | `AI21_API_KEY` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">❌</abbr> | <ul><li>`ai21:j1-large`</li><li>`ai21:j1-grande`</li><li>`ai21:j1-jumbo`</li><li>`ai21:j1-grande-instruct`</li><li>`ai21:j2-large`</li><li>`ai21:j2-grande`</li><li>`ai21:j2-jumbo`</li><li>`ai21:j2-grande-instruct`</li><li>`ai21:j2-jumbo-instruct`</li></ul> |\n",
       "| `bedrock` | Not applicable. | <abbr title=\"Not applicable\">N/A</abbr> | <ul><li>`bedrock:amazon.titan-text-express-v1`</li><li>`bedrock:amazon.titan-text-lite-v1`</li><li>`bedrock:amazon.titan-text-premier-v1:0`</li><li>`bedrock:ai21.j2-ultra-v1`</li><li>`bedrock:ai21.j2-mid-v1`</li><li>`bedrock:ai21.jamba-instruct-v1:0`</li><li>`bedrock:cohere.command-light-text-v14`</li><li>`bedrock:cohere.command-text-v14`</li><li>`bedrock:cohere.command-r-v1:0`</li><li>`bedrock:cohere.command-r-plus-v1:0`</li><li>`bedrock:meta.llama2-13b-chat-v1`</li><li>`bedrock:meta.llama2-70b-chat-v1`</li><li>`bedrock:meta.llama3-8b-instruct-v1:0`</li><li>`bedrock:meta.llama3-70b-instruct-v1:0`</li><li>`bedrock:meta.llama3-1-8b-instruct-v1:0`</li><li>`bedrock:meta.llama3-1-70b-instruct-v1:0`</li><li>`bedrock:meta.llama3-1-405b-instruct-v1:0`</li><li>`bedrock:mistral.mistral-7b-instruct-v0:2`</li><li>`bedrock:mistral.mixtral-8x7b-instruct-v0:1`</li><li>`bedrock:mistral.mistral-large-2402-v1:0`</li><li>`bedrock:mistral.mistral-large-2407-v1:0`</li></ul> |\n",
       "| `bedrock-chat` | Not applicable. | <abbr title=\"Not applicable\">N/A</abbr> | <ul><li>`bedrock-chat:amazon.titan-text-express-v1`</li><li>`bedrock-chat:amazon.titan-text-lite-v1`</li><li>`bedrock-chat:amazon.titan-text-premier-v1:0`</li><li>`bedrock-chat:anthropic.claude-v2`</li><li>`bedrock-chat:anthropic.claude-v2:1`</li><li>`bedrock-chat:anthropic.claude-instant-v1`</li><li>`bedrock-chat:anthropic.claude-3-sonnet-20240229-v1:0`</li><li>`bedrock-chat:anthropic.claude-3-haiku-20240307-v1:0`</li><li>`bedrock-chat:anthropic.claude-3-opus-20240229-v1:0`</li><li>`bedrock-chat:anthropic.claude-3-5-haiku-20241022-v1:0`</li><li>`bedrock-chat:anthropic.claude-3-5-sonnet-20240620-v1:0`</li><li>`bedrock-chat:anthropic.claude-3-5-sonnet-20241022-v2:0`</li><li>`bedrock-chat:meta.llama2-13b-chat-v1`</li><li>`bedrock-chat:meta.llama2-70b-chat-v1`</li><li>`bedrock-chat:meta.llama3-8b-instruct-v1:0`</li><li>`bedrock-chat:meta.llama3-70b-instruct-v1:0`</li><li>`bedrock-chat:meta.llama3-1-8b-instruct-v1:0`</li><li>`bedrock-chat:meta.llama3-1-70b-instruct-v1:0`</li><li>`bedrock-chat:meta.llama3-1-405b-instruct-v1:0`</li><li>`bedrock-chat:mistral.mistral-7b-instruct-v0:2`</li><li>`bedrock-chat:mistral.mixtral-8x7b-instruct-v0:1`</li><li>`bedrock-chat:mistral.mistral-large-2402-v1:0`</li><li>`bedrock-chat:mistral.mistral-large-2407-v1:0`</li></ul> |\n",
       "| `bedrock-custom` | Not applicable. | <abbr title=\"Not applicable\">N/A</abbr> | Specify the ARN (Amazon Resource Name) of the custom/provisioned model as the model ID. For more information, see the [Amazon Bedrock model IDs documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids.html).\n",
       "\n",
       "The model provider must also be specified below. This is the provider of your foundation model *in lowercase*, e.g. `amazon`, `anthropic`, `meta`, or `mistral`. |\n",
       "| `anthropic-chat` | `ANTHROPIC_API_KEY` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">❌</abbr> | <ul><li>`anthropic-chat:claude-2.0`</li><li>`anthropic-chat:claude-2.1`</li><li>`anthropic-chat:claude-3-opus-20240229`</li><li>`anthropic-chat:claude-3-sonnet-20240229`</li><li>`anthropic-chat:claude-3-haiku-20240307`</li><li>`anthropic-chat:claude-3-5-haiku-20241022`</li><li>`anthropic-chat:claude-3-5-sonnet-20240620`</li><li>`anthropic-chat:claude-3-5-sonnet-20241022`</li></ul> |\n",
       "| `azure-chat-openai` | `AZURE_OPENAI_API_KEY` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">❌</abbr> | This provider does not define a list of models. |\n",
       "| `cohere` | `COHERE_API_KEY` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">❌</abbr> | <ul><li>`cohere:command`</li><li>`cohere:command-nightly`</li><li>`cohere:command-light`</li><li>`cohere:command-light-nightly`</li><li>`cohere:command-r-plus`</li><li>`cohere:command-r`</li></ul> |\n",
       "| `gemini` | `GOOGLE_API_KEY` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">❌</abbr> | <ul><li>`gemini:gemini-1.5-pro`</li><li>`gemini:gemini-1.5-flash`</li><li>`gemini:gemini-1.0-pro`</li><li>`gemini:gemini-1.0-pro-001`</li><li>`gemini:gemini-1.0-pro-latest`</li><li>`gemini:gemini-1.0-pro-vision-latest`</li><li>`gemini:gemini-pro`</li><li>`gemini:gemini-pro-vision`</li></ul> |\n",
       "| `gpt4all` | Not applicable. | <abbr title=\"Not applicable\">N/A</abbr> | <ul><li>`gpt4all:ggml-gpt4all-j-v1.2-jazzy`</li><li>`gpt4all:ggml-gpt4all-j-v1.3-groovy`</li><li>`gpt4all:ggml-gpt4all-l13b-snoozy`</li><li>`gpt4all:mistral-7b-openorca.Q4_0`</li><li>`gpt4all:mistral-7b-instruct-v0.1.Q4_0`</li><li>`gpt4all:gpt4all-falcon-q4_0`</li><li>`gpt4all:wizardlm-13b-v1.2.Q4_0`</li><li>`gpt4all:nous-hermes-llama2-13b.Q4_0`</li><li>`gpt4all:gpt4all-13b-snoozy-q4_0`</li><li>`gpt4all:mpt-7b-chat-merges-q4_0`</li><li>`gpt4all:orca-mini-3b-gguf2-q4_0`</li><li>`gpt4all:starcoder-q4_0`</li><li>`gpt4all:rift-coder-v0-7b-q4_0`</li><li>`gpt4all:em_german_mistral_v01.Q4_0`</li></ul> |\n",
       "| `huggingface_hub` | `HUGGINGFACEHUB_API_TOKEN` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">❌</abbr> | See [https://huggingface.co/models](https://huggingface.co/models) for a list of models. Pass a model's repository ID as the model ID; for example, `huggingface_hub:ExampleOwner/example-model`. |\n",
       "| `mistralai` | `MISTRAL_API_KEY` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">❌</abbr> | <ul><li>`mistralai:open-mistral-7b`</li><li>`mistralai:open-mixtral-8x7b`</li><li>`mistralai:open-mixtral-8x22b`</li><li>`mistralai:mistral-small-latest`</li><li>`mistralai:mistral-medium-latest`</li><li>`mistralai:mistral-large-latest`</li><li>`mistralai:codestral-latest`</li></ul> |\n",
       "| `nvidia-chat` | `NVIDIA_API_KEY` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">❌</abbr> | <ul><li>`nvidia-chat:playground_llama2_70b`</li><li>`nvidia-chat:playground_nemotron_steerlm_8b`</li><li>`nvidia-chat:playground_mistral_7b`</li><li>`nvidia-chat:playground_nv_llama2_rlhf_70b`</li><li>`nvidia-chat:playground_llama2_13b`</li><li>`nvidia-chat:playground_steerlm_llama_70b`</li><li>`nvidia-chat:playground_llama2_code_13b`</li><li>`nvidia-chat:playground_yi_34b`</li><li>`nvidia-chat:playground_mixtral_8x7b`</li><li>`nvidia-chat:playground_neva_22b`</li><li>`nvidia-chat:playground_llama2_code_34b`</li></ul> |\n",
       "| `ollama` | Not applicable. | <abbr title=\"Not applicable\">N/A</abbr> | See [https://www.ollama.com/library](https://www.ollama.com/library) for a list of models. Pass a model's name; for example, `deepseek-coder-v2`. |\n",
       "| `openai` | `OPENAI_API_KEY` | <abbr title=\"You have set this environment variable, so you can use this provider's models.\">✅</abbr> | <ul><li>`openai:babbage-002`</li><li>`openai:davinci-002`</li><li>`openai:gpt-3.5-turbo-instruct`</li></ul> |\n",
       "| `openai-chat` | `OPENAI_API_KEY` | <abbr title=\"You have set this environment variable, so you can use this provider's models.\">✅</abbr> | <ul><li>`openai-chat:gpt-3.5-turbo`</li><li>`openai-chat:gpt-3.5-turbo-0125`</li><li>`openai-chat:gpt-3.5-turbo-0301`</li><li>`openai-chat:gpt-3.5-turbo-0613`</li><li>`openai-chat:gpt-3.5-turbo-1106`</li><li>`openai-chat:gpt-3.5-turbo-16k`</li><li>`openai-chat:gpt-3.5-turbo-16k-0613`</li><li>`openai-chat:gpt-4`</li><li>`openai-chat:gpt-4-turbo`</li><li>`openai-chat:gpt-4-turbo-preview`</li><li>`openai-chat:gpt-4-0613`</li><li>`openai-chat:gpt-4-32k`</li><li>`openai-chat:gpt-4-32k-0613`</li><li>`openai-chat:gpt-4-0125-preview`</li><li>`openai-chat:gpt-4-1106-preview`</li><li>`openai-chat:gpt-4o`</li><li>`openai-chat:gpt-4o-mini`</li></ul> |\n",
       "| `openrouter` | `OPENROUTER_API_KEY` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">❌</abbr> | This provider does not define a list of models. |\n",
       "| `qianfan` | `QIANFAN_AK`, `QIANFAN_SK` | <abbr title=\"You have not set all of these environment variables, so you cannot use this provider's models.\">❌</abbr> | <ul><li>`qianfan:ERNIE-Bot`</li><li>`qianfan:ERNIE-Bot-4`</li></ul> |\n",
       "| `sagemaker-endpoint` | Not applicable. | <abbr title=\"Not applicable\">N/A</abbr> | Specify an endpoint name as the model ID. In addition, you must specify a region name, request schema, and response path. For more information, see the documentation about [SageMaker endpoints deployment](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-deploy-models.html) and about [using magic commands with SageMaker endpoints](https://jupyter-ai.readthedocs.io/en/latest/users/index.html#using-magic-commands-with-sagemaker-endpoints). |\n",
       "| `togetherai` | `TOGETHER_API_KEY` | <abbr title=\"You have not set this environment variable, so you cannot use this provider's models.\">❌</abbr> | <ul><li>`togetherai:Austism/chronos-hermes-13b`</li><li>`togetherai:DiscoResearch/DiscoLM-mixtral-8x7b-v2`</li><li>`togetherai:EleutherAI/llemma_7b`</li><li>`togetherai:Gryphe/MythoMax-L2-13b`</li><li>`togetherai:Meta-Llama/Llama-Guard-7b`</li><li>`togetherai:Nexusflow/NexusRaven-V2-13B`</li><li>`togetherai:NousResearch/Nous-Capybara-7B-V1p9`</li><li>`togetherai:NousResearch/Nous-Hermes-2-Yi-34B`</li><li>`togetherai:NousResearch/Nous-Hermes-Llama2-13b`</li><li>`togetherai:NousResearch/Nous-Hermes-Llama2-70b`</li></ul> |\n",
       "\n",
       "Aliases and custom commands:\n",
       "\n",
       "| Name | Target |\n",
       "|------|--------|\n",
       "| `gpt2` | `huggingface_hub:gpt2` |\n",
       "| `gpt3` | `openai:davinci-002` |\n",
       "| `chatgpt` | `openai-chat:gpt-3.5-turbo` |\n",
       "| `gpt4` | `openai-chat:gpt-4` |\n",
       "| `ernie-bot` | `qianfan:ERNIE-Bot` |\n",
       "| `ernie-bot-4` | `qianfan:ERNIE-Bot-4` |\n",
       "| `titan` | `bedrock:amazon.titan-tg1-large` |\n",
       "| `openrouter-claude` | `openrouter:anthropic/claude-3.5-sonnet:beta` |\n"
      ],
      "text/plain": [
       "ai21\n",
       "Requires environment variable: AI21_API_KEY (not set)\n",
       "* ai21:j1-large\n",
       "* ai21:j1-grande\n",
       "* ai21:j1-jumbo\n",
       "* ai21:j1-grande-instruct\n",
       "* ai21:j2-large\n",
       "* ai21:j2-grande\n",
       "* ai21:j2-jumbo\n",
       "* ai21:j2-grande-instruct\n",
       "* ai21:j2-jumbo-instruct\n",
       "\n",
       "bedrock\n",
       "* bedrock:amazon.titan-text-express-v1\n",
       "* bedrock:amazon.titan-text-lite-v1\n",
       "* bedrock:amazon.titan-text-premier-v1:0\n",
       "* bedrock:ai21.j2-ultra-v1\n",
       "* bedrock:ai21.j2-mid-v1\n",
       "* bedrock:ai21.jamba-instruct-v1:0\n",
       "* bedrock:cohere.command-light-text-v14\n",
       "* bedrock:cohere.command-text-v14\n",
       "* bedrock:cohere.command-r-v1:0\n",
       "* bedrock:cohere.command-r-plus-v1:0\n",
       "* bedrock:meta.llama2-13b-chat-v1\n",
       "* bedrock:meta.llama2-70b-chat-v1\n",
       "* bedrock:meta.llama3-8b-instruct-v1:0\n",
       "* bedrock:meta.llama3-70b-instruct-v1:0\n",
       "* bedrock:meta.llama3-1-8b-instruct-v1:0\n",
       "* bedrock:meta.llama3-1-70b-instruct-v1:0\n",
       "* bedrock:meta.llama3-1-405b-instruct-v1:0\n",
       "* bedrock:mistral.mistral-7b-instruct-v0:2\n",
       "* bedrock:mistral.mixtral-8x7b-instruct-v0:1\n",
       "* bedrock:mistral.mistral-large-2402-v1:0\n",
       "* bedrock:mistral.mistral-large-2407-v1:0\n",
       "\n",
       "bedrock-chat\n",
       "* bedrock-chat:amazon.titan-text-express-v1\n",
       "* bedrock-chat:amazon.titan-text-lite-v1\n",
       "* bedrock-chat:amazon.titan-text-premier-v1:0\n",
       "* bedrock-chat:anthropic.claude-v2\n",
       "* bedrock-chat:anthropic.claude-v2:1\n",
       "* bedrock-chat:anthropic.claude-instant-v1\n",
       "* bedrock-chat:anthropic.claude-3-sonnet-20240229-v1:0\n",
       "* bedrock-chat:anthropic.claude-3-haiku-20240307-v1:0\n",
       "* bedrock-chat:anthropic.claude-3-opus-20240229-v1:0\n",
       "* bedrock-chat:anthropic.claude-3-5-haiku-20241022-v1:0\n",
       "* bedrock-chat:anthropic.claude-3-5-sonnet-20240620-v1:0\n",
       "* bedrock-chat:anthropic.claude-3-5-sonnet-20241022-v2:0\n",
       "* bedrock-chat:meta.llama2-13b-chat-v1\n",
       "* bedrock-chat:meta.llama2-70b-chat-v1\n",
       "* bedrock-chat:meta.llama3-8b-instruct-v1:0\n",
       "* bedrock-chat:meta.llama3-70b-instruct-v1:0\n",
       "* bedrock-chat:meta.llama3-1-8b-instruct-v1:0\n",
       "* bedrock-chat:meta.llama3-1-70b-instruct-v1:0\n",
       "* bedrock-chat:meta.llama3-1-405b-instruct-v1:0\n",
       "* bedrock-chat:mistral.mistral-7b-instruct-v0:2\n",
       "* bedrock-chat:mistral.mixtral-8x7b-instruct-v0:1\n",
       "* bedrock-chat:mistral.mistral-large-2402-v1:0\n",
       "* bedrock-chat:mistral.mistral-large-2407-v1:0\n",
       "\n",
       "bedrock-custom\n",
       "* Specify the ARN (Amazon Resource Name) of the custom/provisioned model as the model ID. For more information, see the [Amazon Bedrock model IDs documentation](https://docs.aws.amazon.com/bedrock/latest/userguide/model-ids.html).\n",
       "\n",
       "The model provider must also be specified below. This is the provider of your foundation model *in lowercase*, e.g. `amazon`, `anthropic`, `meta`, or `mistral`.\n",
       "\n",
       "anthropic-chat\n",
       "Requires environment variable: ANTHROPIC_API_KEY (not set)\n",
       "* anthropic-chat:claude-2.0\n",
       "* anthropic-chat:claude-2.1\n",
       "* anthropic-chat:claude-3-opus-20240229\n",
       "* anthropic-chat:claude-3-sonnet-20240229\n",
       "* anthropic-chat:claude-3-haiku-20240307\n",
       "* anthropic-chat:claude-3-5-haiku-20241022\n",
       "* anthropic-chat:claude-3-5-sonnet-20240620\n",
       "* anthropic-chat:claude-3-5-sonnet-20241022\n",
       "\n",
       "azure-chat-openai\n",
       "Requires environment variable: AZURE_OPENAI_API_KEY (not set)\n",
       "* This provider does not define a list of models.\n",
       "\n",
       "cohere\n",
       "Requires environment variable: COHERE_API_KEY (not set)\n",
       "* cohere:command\n",
       "* cohere:command-nightly\n",
       "* cohere:command-light\n",
       "* cohere:command-light-nightly\n",
       "* cohere:command-r-plus\n",
       "* cohere:command-r\n",
       "\n",
       "gemini\n",
       "Requires environment variable: GOOGLE_API_KEY (not set)\n",
       "* gemini:gemini-1.5-pro\n",
       "* gemini:gemini-1.5-flash\n",
       "* gemini:gemini-1.0-pro\n",
       "* gemini:gemini-1.0-pro-001\n",
       "* gemini:gemini-1.0-pro-latest\n",
       "* gemini:gemini-1.0-pro-vision-latest\n",
       "* gemini:gemini-pro\n",
       "* gemini:gemini-pro-vision\n",
       "\n",
       "gpt4all\n",
       "* gpt4all:ggml-gpt4all-j-v1.2-jazzy\n",
       "* gpt4all:ggml-gpt4all-j-v1.3-groovy\n",
       "* gpt4all:ggml-gpt4all-l13b-snoozy\n",
       "* gpt4all:mistral-7b-openorca.Q4_0\n",
       "* gpt4all:mistral-7b-instruct-v0.1.Q4_0\n",
       "* gpt4all:gpt4all-falcon-q4_0\n",
       "* gpt4all:wizardlm-13b-v1.2.Q4_0\n",
       "* gpt4all:nous-hermes-llama2-13b.Q4_0\n",
       "* gpt4all:gpt4all-13b-snoozy-q4_0\n",
       "* gpt4all:mpt-7b-chat-merges-q4_0\n",
       "* gpt4all:orca-mini-3b-gguf2-q4_0\n",
       "* gpt4all:starcoder-q4_0\n",
       "* gpt4all:rift-coder-v0-7b-q4_0\n",
       "* gpt4all:em_german_mistral_v01.Q4_0\n",
       "\n",
       "huggingface_hub\n",
       "Requires environment variable: HUGGINGFACEHUB_API_TOKEN (not set)\n",
       "* See [https://huggingface.co/models](https://huggingface.co/models) for a list of models. Pass a model's repository ID as the model ID; for example, `huggingface_hub:ExampleOwner/example-model`.\n",
       "\n",
       "mistralai\n",
       "Requires environment variable: MISTRAL_API_KEY (not set)\n",
       "* mistralai:open-mistral-7b\n",
       "* mistralai:open-mixtral-8x7b\n",
       "* mistralai:open-mixtral-8x22b\n",
       "* mistralai:mistral-small-latest\n",
       "* mistralai:mistral-medium-latest\n",
       "* mistralai:mistral-large-latest\n",
       "* mistralai:codestral-latest\n",
       "\n",
       "nvidia-chat\n",
       "Requires environment variable: NVIDIA_API_KEY (not set)\n",
       "* nvidia-chat:playground_llama2_70b\n",
       "* nvidia-chat:playground_nemotron_steerlm_8b\n",
       "* nvidia-chat:playground_mistral_7b\n",
       "* nvidia-chat:playground_nv_llama2_rlhf_70b\n",
       "* nvidia-chat:playground_llama2_13b\n",
       "* nvidia-chat:playground_steerlm_llama_70b\n",
       "* nvidia-chat:playground_llama2_code_13b\n",
       "* nvidia-chat:playground_yi_34b\n",
       "* nvidia-chat:playground_mixtral_8x7b\n",
       "* nvidia-chat:playground_neva_22b\n",
       "* nvidia-chat:playground_llama2_code_34b\n",
       "\n",
       "ollama\n",
       "* See [https://www.ollama.com/library](https://www.ollama.com/library) for a list of models. Pass a model's name; for example, `deepseek-coder-v2`.\n",
       "\n",
       "openai\n",
       "Requires environment variable: OPENAI_API_KEY (set)\n",
       "* openai:babbage-002\n",
       "* openai:davinci-002\n",
       "* openai:gpt-3.5-turbo-instruct\n",
       "\n",
       "openai-chat\n",
       "Requires environment variable: OPENAI_API_KEY (set)\n",
       "* openai-chat:gpt-3.5-turbo\n",
       "* openai-chat:gpt-3.5-turbo-0125\n",
       "* openai-chat:gpt-3.5-turbo-0301\n",
       "* openai-chat:gpt-3.5-turbo-0613\n",
       "* openai-chat:gpt-3.5-turbo-1106\n",
       "* openai-chat:gpt-3.5-turbo-16k\n",
       "* openai-chat:gpt-3.5-turbo-16k-0613\n",
       "* openai-chat:gpt-4\n",
       "* openai-chat:gpt-4-turbo\n",
       "* openai-chat:gpt-4-turbo-preview\n",
       "* openai-chat:gpt-4-0613\n",
       "* openai-chat:gpt-4-32k\n",
       "* openai-chat:gpt-4-32k-0613\n",
       "* openai-chat:gpt-4-0125-preview\n",
       "* openai-chat:gpt-4-1106-preview\n",
       "* openai-chat:gpt-4o\n",
       "* openai-chat:gpt-4o-mini\n",
       "\n",
       "openrouter\n",
       "Requires environment variable: OPENROUTER_API_KEY (not set)\n",
       "* This provider does not define a list of models.\n",
       "\n",
       "qianfan\n",
       "Requires environment variables: QIANFAN_AK (not set), QIANFAN_SK (not set)\n",
       "* qianfan:ERNIE-Bot\n",
       "* qianfan:ERNIE-Bot-4\n",
       "\n",
       "sagemaker-endpoint\n",
       "* Specify an endpoint name as the model ID. In addition, you must specify a region name, request schema, and response path. For more information, see the documentation about [SageMaker endpoints deployment](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-deploy-models.html) and about [using magic commands with SageMaker endpoints](https://jupyter-ai.readthedocs.io/en/latest/users/index.html#using-magic-commands-with-sagemaker-endpoints).\n",
       "\n",
       "togetherai\n",
       "Requires environment variable: TOGETHER_API_KEY (not set)\n",
       "* togetherai:Austism/chronos-hermes-13b\n",
       "* togetherai:DiscoResearch/DiscoLM-mixtral-8x7b-v2\n",
       "* togetherai:EleutherAI/llemma_7b\n",
       "* togetherai:Gryphe/MythoMax-L2-13b\n",
       "* togetherai:Meta-Llama/Llama-Guard-7b\n",
       "* togetherai:Nexusflow/NexusRaven-V2-13B\n",
       "* togetherai:NousResearch/Nous-Capybara-7B-V1p9\n",
       "* togetherai:NousResearch/Nous-Hermes-2-Yi-34B\n",
       "* togetherai:NousResearch/Nous-Hermes-Llama2-13b\n",
       "* togetherai:NousResearch/Nous-Hermes-Llama2-70b\n",
       "\n",
       "\n",
       "Aliases and custom commands:\n",
       "gpt2 - huggingface_hub:gpt2\n",
       "gpt3 - openai:davinci-002\n",
       "chatgpt - openai-chat:gpt-3.5-turbo\n",
       "gpt4 - openai-chat:gpt-4\n",
       "ernie-bot - qianfan:ERNIE-Bot\n",
       "ernie-bot-4 - qianfan:ERNIE-Bot-4\n",
       "titan - bedrock:amazon.titan-tg1-large\n",
       "openrouter-claude - openrouter:anthropic/claude-3.5-sonnet:beta\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看可用框架\n",
    "%ai list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f33e5d6e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T05:33:41.963954Z",
     "iopub.status.busy": "2024-11-12T05:33:41.963261Z",
     "iopub.status.idle": "2024-11-12T05:33:41.973645Z",
     "shell.execute_reply": "2024-11-12T05:33:41.972599Z",
     "shell.execute_reply.started": "2024-11-12T05:33:41.963954Z"
    }
   },
   "outputs": [],
   "source": [
    "# %%ai chatgpt\n",
    "# 讲个故事：林黛玉倒拔垂杨柳，请用中文"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e561f5-ad32-4953-80ed-4cc5b35c2222",
   "metadata": {},
   "source": [
    "## ChatGLM3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3d0bff49-8595-48c7-8134-fd44b53a164f",
   "metadata": {},
   "source": [
    "自诞生以来，ChatGLM便以其卓越的性能和广泛的应用而闻名于世，成为了国产大语言模型中最强大、最著名的模型。2023年3月，第一代ChatGLM-6B的推出，开源之后便获得了广泛的关注和使用。短短3个月后，2023年6月，ChatGLM2的发布再次引发了科技界的热议。而就在不久前的2023年10月27日，清华大学再次发布了第三代基础大语言模型ChatGLM3系列，这一系列共包含了3个模型：基础大语言模型ChatGLM3-6B-Base、对话调优大语言模型ChatGLM3-6B和长文本对话大语言模型ChatGLM3-6B-32K。\n",
    "\n",
    "ChatGLM3系列的发布，标志着中国在自然语言处理领域的研究取得了新的突破。这一系列模型不仅具备了更强大的语言理解能力，更能在不同场景下进行高效、精准的应用。其中，ChatGLM3-6B-Base作为基础大语言模型，凭借其出色的性能表现，成为了众多领域中的得力助手。而ChatGLM3-6B则通过对话调优，实现了更加智能、自然的交互体验。最后，ChatGLM3-6B-32K作为长文本对话大语言模型，则以其在长文本处理方面的卓越表现，广泛应用于新闻媒体、科技文献等领域。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6855a2fc-e559-4f7f-8c58-75c305b51093",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T10:45:43.282185Z",
     "iopub.status.busy": "2024-11-12T10:45:43.279490Z",
     "iopub.status.idle": "2024-11-12T14:33:18.610367Z",
     "shell.execute_reply": "2024-11-12T14:33:18.604904Z",
     "shell.execute_reply.started": "2024-11-12T10:45:43.282185Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\nlp\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model to directory: C:\\Users\\caoyuyu\\.cache\\modelscope\\hub\\ZhipuAI/chatglm3-6b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-12 18:45:49,906 - modelscope - INFO - Use user-specified model revision: v1.0.0\n",
      "Loading checkpoint shards:   0%|                                                                 | 0/7 [00:00<?, ?it/s]D:\\Anaconda\\envs\\nlp\\lib\\site-packages\\torch\\_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████| 7/7 [00:20<00:00,  2.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "晚上睡不着，你可以尝试以下方法来帮助自己入睡：\n",
      "\n",
      "1. 保持冷静：不要焦虑，接受当前的状态。试着将注意力从难以入睡的想法上转移开来。\n",
      "\n",
      "2. 放松身心：尝试进行深呼吸练习、渐进性肌肉松弛或冥想等放松技巧，帮助身体和心灵放松。\n",
      "\n",
      "3. 避免刺激性活动：避免在晚上进行过于刺激的活动，如剧烈运动、观看刺激性电影或进行紧张的游戏。\n",
      "\n",
      "4. 规律作息：尽量保持作息规律，每天按时上床睡觉和起床。这有助于调整你的生物钟。\n",
      "\n",
      "5. 改善睡眠环境：确保你的卧室具备良好的睡眠条件。保持房间温度适中、光线柔和、空气流通并选择舒适的床垫和枕头。\n",
      "\n",
      "6. 限制使用电子产品：睡前一小时避免使用手机、电脑等电子产品，这些设备发出的蓝光可能会影响你的睡眠。\n",
      "\n",
      "7. 尝试睡前按摩：睡前进行简单的按摩，如手指按摩或脚部按摩，可以帮助放松紧张的肌肉。\n",
      "\n",
      "8. 制定睡前例行程序：建立一个简单的睡前例行程序，如刷牙、洗脸、阅读等，可以帮助 signal 身体准备进入睡眠状态。\n",
      "\n",
      "9. 避免过度兴奋：尽量避免在晚上进行过于兴奋的的活动，如激烈的争吵、运行速度过快的游戏等。\n",
      "\n",
      "10. 如果长时间睡不着，可以考虑咨询专业医生。医生可能会为你提供一些针对性的建议或药物帮助。\n",
      "\n",
      "每个人的情况不同，你需要找到最适合自己的方法。尝试上述方法，看看哪些能够帮助你改善睡眠质量。\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from modelscope import AutoTokenizer, AutoModel, snapshot_download\n",
    "model_dir = snapshot_download(\"ZhipuAI/chatglm3-6b\", revision = \"v1.0.0\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    #采用量化方案的ChatGLM3\n",
    "    model = AutoModel.from_pretrained(model_dir, trust_remote_code=True).quantize(4).cuda()\n",
    "\n",
    "model = model.eval()\n",
    "response, history = model.chat(tokenizer, \"晚上睡不着应该怎么办\", history=[])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1853b49-69a9-4e9c-979d-afbd229d418a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "203.667px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
