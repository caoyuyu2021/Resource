{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bf1d02d-8ef7-41e0-ad85-d4ef343b337f",
   "metadata": {},
   "source": [
    "# 基于Transformer的自然语言处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "768031db-7df8-451a-a0b1-742256b4f03b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-14T11:28:53.671768Z",
     "iopub.status.busy": "2024-11-14T11:28:53.670777Z",
     "iopub.status.idle": "2024-11-14T11:28:54.022242Z",
     "shell.execute_reply": "2024-11-14T11:28:54.017327Z",
     "shell.execute_reply.started": "2024-11-14T11:28:53.671768Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import DataFrame\n",
    "from datetime import timedelta\n",
    "from numpy import ndarray\n",
    "from typing import Union, List, Dict\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm  # 打印进度条\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List\n",
    "from pandas.tseries import offsets\n",
    "from pandas.tseries.frequencies import to_offset\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import joblib\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # 显示中文标签\n",
    "plt.rcParams['axes.unicode_minus'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ebb0eea-c0da-489a-8b58-74968e5350de",
   "metadata": {},
   "source": [
    "在自然语言处理（NLP）领域，Transformer模型是一种深度学习模型架构，广泛应用于机器翻译、文本生成、情感分析等任务中。Transformer在2017年由Vaswani等人提出，突破了传统RNN模型的限制。它的核心特点是完全基于自注意力机制（Self-Attention Mechanism），并且可以并行处理输入数据，从而显著提高了模型的效率和性能。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d877469-a19d-45bb-a059-d0197b66fe0f",
   "metadata": {},
   "source": [
    "Transformer的主要组件包括编码器（Encoder）和解码器（Decoder），两者都由多个相同的层叠加而成。每一层的结构相似，包含自注意力和前馈神经网络（Feed-Forward Neural Network）。常见的NLP任务通常只使用编码器部分，比如BERT，而生成任务（如机器翻译）则会用到完整的编码-解码结构，例如GPT和T5等模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969c0c42-8d0b-4f16-aeac-ae28c92ff5af",
   "metadata": {},
   "source": [
    "## 分类"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96f4171-3499-4732-8671-75e96a570ae1",
   "metadata": {},
   "source": [
    "### 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9364bd28-938c-4899-bc2f-d832467d365d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-14T12:32:13.439177Z",
     "iopub.status.busy": "2024-11-14T12:32:13.438174Z",
     "iopub.status.idle": "2024-11-14T12:32:13.457632Z",
     "shell.execute_reply": "2024-11-14T12:32:13.456632Z",
     "shell.execute_reply.started": "2024-11-14T12:32:13.439177Z"
    }
   },
   "outputs": [],
   "source": [
    "# 读取数据\n",
    "def loader(folder_paths=None):\n",
    "    \"\"\"\n",
    "    读取数据，并对输入数据时间列进行处理\n",
    "\n",
    "    参数说明\n",
    "    ----------\n",
    "    folder_paths : {list}\n",
    "        输入数据文件地址，支持多个文件\n",
    "\n",
    "    返回值\n",
    "    -------\n",
    "    contents : {list}\n",
    "        数据列表\n",
    "    \"\"\"\n",
    "    # 读取原始数据\n",
    "    if folder_paths == None:\n",
    "        raise ValueError(\"folder is not exist!\")\n",
    "\n",
    "    contents = []\n",
    "    # 遍历所有文件夹中的所有文件\n",
    "    for folder_path in folder_paths:\n",
    "        for filename in os.listdir(folder_path):\n",
    "            # 检查文件是否为.txt文件\n",
    "            if filename.endswith('.txt'):\n",
    "                file_path = os.path.join(folder_path, filename)\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    # 读取文件内容并加入到列表中\n",
    "                    contents.append(file.read())\n",
    "    return contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "513c434c-a97f-43f4-8ff5-40da1f59d84b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-14T10:47:56.598228Z",
     "iopub.status.busy": "2024-11-14T10:47:56.596181Z",
     "iopub.status.idle": "2024-11-14T10:50:46.538893Z",
     "shell.execute_reply": "2024-11-14T10:50:46.537891Z",
     "shell.execute_reply.started": "2024-11-14T10:47:56.597305Z"
    }
   },
   "outputs": [],
   "source": [
    "# 负面标签为0，正面标签为1\n",
    "folder_paths = ['../../../../../data/03.nlp/aclImdb/train/neg/', '../../../../../data/03.nlp/aclImdb/train/pos/']\n",
    "texts = loader(folder_paths)\n",
    "labels = [0]*12500 + [1]*12500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca8392e0-db65-4959-8b4f-3b41dab10e68",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-14T10:50:58.843976Z",
     "iopub.status.busy": "2024-11-14T10:50:58.841975Z",
     "iopub.status.idle": "2024-11-14T10:51:19.187168Z",
     "shell.execute_reply": "2024-11-14T10:51:19.187168Z",
     "shell.execute_reply.started": "2024-11-14T10:50:58.843976Z"
    }
   },
   "outputs": [],
   "source": [
    "# 定义并初始化分词器\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "531fc384-776b-48c6-a059-58551814b318",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-14T12:32:19.135293Z",
     "iopub.status.busy": "2024-11-14T12:32:19.134288Z",
     "iopub.status.idle": "2024-11-14T12:32:19.151391Z",
     "shell.execute_reply": "2024-11-14T12:32:19.150472Z",
     "shell.execute_reply.started": "2024-11-14T12:32:19.135293Z"
    }
   },
   "outputs": [],
   "source": [
    "def generator(data_list, train_ratio, valid_ratio, tokenizer, batch_size, max_length: int = 128):\n",
    "    \"\"\"\n",
    "    读取数据，并对数据进行划分，生成加载器\n",
    "\n",
    "    参数说明\n",
    "    ----------\n",
    "    data_list : {list[DataFrame]}\n",
    "        输入数据，包含数据和标签\n",
    "    train_ratio : {float}\n",
    "        用于训练的数据集占比:将数据按照一定比例进行切分，取值范围为(0,1)\n",
    "    valid_ratio : {float}\n",
    "        用于验证的数据集占比:将数据按照一定比例进行切分，取值范围为(0,1)\n",
    "    tokenizer : {}\n",
    "        分词器\n",
    "    batch_size : {int} \n",
    "        输入数据的批次大小，正整数\n",
    "    max_length : {int} \n",
    "        最大文本截取长度，正整数\n",
    "\n",
    "    返回值\n",
    "    -------\n",
    "    data_loader : {torch.utils.data.dataloader.DataLoader}\n",
    "        数据加载器\n",
    "    \"\"\"\n",
    "    # 获取数据\n",
    "    texts = data_list[0]  # 特征\n",
    "    labels = data_list[1]  # 目标\n",
    "    \n",
    "    # 划分数据\n",
    "    train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=train_ratio, random_state=42)\n",
    "\n",
    "    # 定义数据集的Dataset类\n",
    "    class CustomDataset(Dataset):\n",
    "        def __init__(self, texts, labels, tokenizer, max_length=max_length):\n",
    "            self.texts = texts\n",
    "            self.labels = labels\n",
    "            self.tokenizer = tokenizer\n",
    "            self.max_length = max_length\n",
    "    \n",
    "        def __len__(self):\n",
    "            return len(self.labels)\n",
    "    \n",
    "        def __getitem__(self, idx):\n",
    "            text = self.texts[idx]\n",
    "            label = self.labels[idx]\n",
    "            \n",
    "            # 对文本进行token化，并转换为输入id和attention mask\n",
    "            encoding = self.tokenizer(\n",
    "                text,\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                max_length=self.max_length,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            return {\n",
    "                'input_ids': encoding['input_ids'].flatten(),\n",
    "                'attention_mask': encoding['attention_mask'].flatten(),\n",
    "                'label': torch.tensor(label, dtype=torch.long)\n",
    "            }\n",
    "\n",
    "    # 创建Dataset和DataLoader\n",
    "    train_dataset = CustomDataset(train_texts, train_labels, tokenizer, max_length=max_length)\n",
    "    val_dataset = CustomDataset(val_texts, val_labels, tokenizer, max_length=max_length)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    return train_loader, valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "799f1d25-ef31-4cd7-a6c2-65186f57b78f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-14T12:32:25.988337Z",
     "iopub.status.busy": "2024-11-14T12:32:25.987269Z",
     "iopub.status.idle": "2024-11-14T12:32:26.009536Z",
     "shell.execute_reply": "2024-11-14T12:32:26.008571Z",
     "shell.execute_reply.started": "2024-11-14T12:32:25.988337Z"
    }
   },
   "outputs": [],
   "source": [
    "# 构造参数字典\n",
    "params1 = {\n",
    "    \"data_list\": [texts, labels],\n",
    "    \"train_ratio\": 0.7,\n",
    "    \"valid_ratio\": 0.1,\n",
    "    \"tokenizer\": tokenizer,\n",
    "    \"batch_size\": 64,\n",
    "    \"max_length\": 128,\n",
    "}\n",
    "\n",
    "# 函数传参\n",
    "train_loader, valid_loader = generator(**params1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17dad58d-6091-4629-9841-0dc3c6029458",
   "metadata": {},
   "source": [
    "### 模型定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "538de166-4221-4998-8872-193f16183b0f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-14T12:32:30.257193Z",
     "iopub.status.busy": "2024-11-14T12:32:30.255152Z",
     "iopub.status.idle": "2024-11-14T12:32:30.324221Z",
     "shell.execute_reply": "2024-11-14T12:32:30.322552Z",
     "shell.execute_reply.started": "2024-11-14T12:32:30.257193Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "# Token Embedding\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        self.tokenEmbedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.tokenEmbedding(x)\n",
    "\n",
    "# Positional Embedding\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).float().unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pe[:x.size(1), :]\n",
    "\n",
    "# 数据嵌入层：包括词嵌入和位置编码\n",
    "class DataEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, dropout=0.1):\n",
    "        super(DataEmbedding, self).__init__()\n",
    "        self.token_embedding = TokenEmbedding(vocab_size, d_model)\n",
    "        self.position_embedding = PositionalEmbedding(d_model=d_model)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.token_embedding(x) + self.position_embedding(x)\n",
    "        return self.dropout(x)\n",
    "\n",
    "# 多头注意力机制\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "        self.d_k = d_model // n_heads\n",
    "        self.n_heads = n_heads\n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, q, k, v, attn_mask=None):\n",
    "        bs = q.size(0)\n",
    "        q = self.q_linear(q).view(bs, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        k = self.k_linear(k).view(bs, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        v = self.v_linear(v).view(bs, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        if attn_mask is not None:\n",
    "            scores = scores.masked_fill(attn_mask == 0, float('-inf'))\n",
    "        scores = torch.nn.functional.softmax(scores, dim=-1)\n",
    "        \n",
    "        attn_output = torch.matmul(scores, v).transpose(1, 2).contiguous().view(bs, -1, self.d_k * self.n_heads)\n",
    "        return self.out(attn_output)\n",
    "        \n",
    "# Encoder Layer\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, self_attention, d_model, d_ff, dropout=0.1, activation='relu'):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attention = self_attention\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU() if activation == 'relu' else nn.GELU(),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        attn_output = self.self_attention(x, x, x, attn_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        \n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.norm2(x + self.dropout(ffn_output))\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, layers, norm_layer):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.norm = norm_layer\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, attn_mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "# Transformer模型\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, seq_len, vocab_size, d_model, dropout, n_heads, d_ff, \n",
    "                 e_layers, num_classes):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        # 嵌入层\n",
    "        self.enc_embedding = DataEmbedding(vocab_size, d_model, dropout)\n",
    "        \n",
    "        # 编码器\n",
    "        self.encoder = Encoder(\n",
    "            [\n",
    "                EncoderLayer(\n",
    "                    MultiHeadAttention(d_model, n_heads), d_model, d_ff,\n",
    "                    dropout=dropout, activation='relu'\n",
    "                ) for _ in range(e_layers)\n",
    "            ],\n",
    "            norm_layer=torch.nn.LayerNorm(d_model)\n",
    "        )\n",
    "\n",
    "        # 分类头部\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_enc):\n",
    "        enc_out = self.enc_embedding(x_enc)\n",
    "        enc_out = self.encoder(enc_out)\n",
    "        \n",
    "        output = enc_out[:, -1, :]  # 使用序列的最后一个token表示\n",
    "        output = self.classifier(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6f7b64-b9d5-4ea9-8d97-41e8d7bf3656",
   "metadata": {},
   "source": [
    "### 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6f070d43-23e6-499c-9d91-a0571821ade3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-14T12:32:31.387301Z",
     "iopub.status.busy": "2024-11-14T12:32:31.386073Z",
     "iopub.status.idle": "2024-11-14T12:32:31.435134Z",
     "shell.execute_reply": "2024-11-14T12:32:31.434128Z",
     "shell.execute_reply.started": "2024-11-14T12:32:31.387301Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(train_args, model_args):\n",
    "    # 参数配置\n",
    "    model_name = train_args['model_name']  # 模型名称\n",
    "    train_loader = train_args['train_loader']  # 训练集\n",
    "    valid_loader = train_args['valid_loader']  # 验证集\n",
    "    n_epochs = train_args['n_epochs']  # 训练次数\n",
    "    learning_rate = train_args['learning_rate']  # 学习率\n",
    "    loss = train_args['loss']  # 损失函数\n",
    "    patience = train_args['patience']  # 最大早停次数阈值，超过就会早停\n",
    "    lradj = train_args['lradj']  # 学习率函数\n",
    "    model_path = train_args['model_path']  # 模型保存路径\n",
    "    verbose = train_args['verbose']  # 打印训练过程\n",
    "    plots = train_args['plots']  # 绘制损失图\n",
    "    device = train_args['device']  # 训练设备，可选'cuda'和'cpu'\n",
    "\n",
    "    # 检查可用device\n",
    "    device = torch.device(device)\n",
    "\n",
    "    # 创建模型和优化器\n",
    "    model = model_name(**model_args).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = loss\n",
    "\n",
    "    # 调整学习率\n",
    "    def adjust_learning_rate(optimizer, epoch, lradj, learning_rate, train_epochs):\n",
    "        # lr = learning_rate * (0.2 ** (epoch // 2))\n",
    "        if lradj == 'type1':\n",
    "            lr_adjust = {epoch: learning_rate * (0.5 ** ((epoch - 1) // 1))}\n",
    "        elif lradj == 'type2':\n",
    "            lr_adjust = {\n",
    "                2: 5e-5, 4: 1e-5, 6: 5e-6, 8: 1e-6,\n",
    "                10: 5e-7, 15: 1e-7, 20: 5e-8\n",
    "            }\n",
    "        elif lradj == \"cosine\":\n",
    "            lr_adjust = {epoch: learning_rate / 2 *\n",
    "                         (1 + math.cos(epoch / train_epochs * math.pi))}\n",
    "        if epoch in lr_adjust.keys():\n",
    "            lr = lr_adjust[epoch]\n",
    "            # 参数组(param_groups)是用来指定不同的参数组以便对它们进行不同的优化设置，比如'lr'\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "            print('Updating learning rate to {}'.format(lr))\n",
    "\n",
    "    # 设置早停\n",
    "    class EarlyStopping():\n",
    "        def __init__(self, patience=7, verbose=False, delta=0):\n",
    "            self.patience = patience  # 连续超限次数，如果满足条件，则早停\n",
    "            self.verbose = verbose\n",
    "            self.counter = 0\n",
    "            self.best_score = None\n",
    "            self.early_stop = False\n",
    "            self.val_loss_min = np.Inf\n",
    "            self.delta = delta\n",
    "\n",
    "        def __call__(self, val_loss, model, path):\n",
    "            score = -val_loss\n",
    "            if self.best_score is None:\n",
    "                self.best_score = score\n",
    "                self.save_checkpoint(val_loss, model, path)\n",
    "            elif score < self.best_score + self.delta:\n",
    "                self.counter += 1\n",
    "                print(\n",
    "                    f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "                if self.counter >= self.patience:\n",
    "                    self.early_stop = True\n",
    "            else:\n",
    "                self.best_score = score\n",
    "                self.save_checkpoint(val_loss, model, path)\n",
    "                self.counter = 0\n",
    "\n",
    "        def save_checkpoint(self, val_loss, model, path):\n",
    "            if self.verbose:\n",
    "                print(\n",
    "                    f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "            torch.save(model.state_dict(), path + '/' + 'checkpoint.pth')\n",
    "            self.val_loss_min = val_loss\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=verbose)\n",
    "\n",
    "    # 设置保存模型路径\n",
    "    if not os.path.exists(model_path):\n",
    "        os.makedirs(model_path)\n",
    "\n",
    "    # 模型训练和验证\n",
    "    train_losses, val_losses = [], []\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        train_correct_predictions = 0\n",
    "        for batch in train_loader:\n",
    "            # 将数据移至 device\n",
    "            batch_x = batch['input_ids'].to(device)\n",
    "            batch_y = batch['label'].to(device)\n",
    "            # 清空梯度\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x)\n",
    "            train_loss = criterion(outputs, batch_y)\n",
    "            # 反向传播计算得到每个参数的梯度值\n",
    "            train_loss.backward()\n",
    "            # 通过梯度下降执行一步参数更新\n",
    "            optimizer.step()\n",
    "            # 每个batch的loss和\n",
    "            total_train_loss += train_loss.item()  # .item()表示只包含一个元素的tensor中提取值\n",
    "            # 计算准确率\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            train_correct_predictions += torch.sum(preds == batch_y)\n",
    "\n",
    "        # 每个epoch的损失平均\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "        # 所有epoch的loss\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # 计算准确率\n",
    "        train_accuracy = train_correct_predictions.double() / len(train_loader.dataset)\n",
    "\n",
    "        # 评估模型\n",
    "        model.eval()\n",
    "        # 关闭自动求导功能，只使用训练好的模型进行预测或评估，不需要进行梯度计算和参数更新\n",
    "        with torch.no_grad():\n",
    "            total_val_loss = 0\n",
    "            val_correct_predictions = 0\n",
    "            for batch in valid_loader:\n",
    "                # 将数据移至 device\n",
    "                batch_x = batch['input_ids'].to(device)\n",
    "                batch_y = batch['label'].to(device)\n",
    "                outputs = model(batch_x)\n",
    "                val_loss = criterion(outputs, batch_y)\n",
    "                # 每个batch的loss和\n",
    "                total_val_loss += val_loss.item()\n",
    "                # 计算准确率\n",
    "                _, preds = torch.max(outputs, dim=1)\n",
    "                val_correct_predictions += torch.sum(preds == batch_y)\n",
    "\n",
    "        # 每个epoch的损失平均\n",
    "        avg_val_loss = total_val_loss / len(valid_loader)\n",
    "\n",
    "        # 所有epoch的loss\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        # 计算准确率\n",
    "        val_accuracy = val_correct_predictions.double() / len(valid_loader.dataset)\n",
    "\n",
    "        # 打印训练过程\n",
    "        if verbose:\n",
    "            print(f'Epoch [{epoch+1}/{n_epochs}], Training Loss: {avg_train_loss:.4f}, Training Accuracy: {train_accuracy:.4f}, Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "        # 设置早停\n",
    "        early_stopping(avg_val_loss, model, model_path)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping!\")\n",
    "            break\n",
    "\n",
    "        # 调整学习率\n",
    "        adjust_learning_rate(optimizer, epoch+1, lradj,\n",
    "                             learning_rate, n_epochs)\n",
    "\n",
    "    # 绘制损失函数图\n",
    "    def plot_loss(train_loss, val_loss):\n",
    "        \"\"\"\n",
    "        绘制训练和验证损失曲线\n",
    "\n",
    "        参数:\n",
    "        - train_loss: 训练损失数组\n",
    "        - val_loss: 验证损失数组\n",
    "        \"\"\"\n",
    "        # 自动生成 epochs（假设train_loss和val_loss长度一致）\n",
    "        epochs = np.arange(len(train_loss))\n",
    "\n",
    "        # 使用 Seaborn 设置白色背景样式\n",
    "        sns.set(style=\"white\")\n",
    "\n",
    "        # 创建图形并优化细节\n",
    "        plt.figure(figsize=(10, 6))\n",
    "\n",
    "        # 绘制训练和验证曲线\n",
    "        plt.plot(epochs, train_loss, label='Training', color='#d62728', linewidth=2, marker='o', markersize=6)\n",
    "        plt.plot(epochs, val_loss, label='Validation', color='#1f77b4', linewidth=2, marker='s', markersize=6)\n",
    "\n",
    "        # 添加标题和标签\n",
    "        plt.title('Training and Validation Loss', fontsize=18, fontweight='bold', color='black')\n",
    "        plt.xlabel('Epoch', fontsize=14)\n",
    "        plt.ylabel('Loss', fontsize=14)\n",
    "\n",
    "        # 添加图例\n",
    "        plt.legend(fontsize=12, loc='upper right')\n",
    "\n",
    "        # 启用横向网格线\n",
    "        plt.grid(True, axis='y', linestyle='--', linewidth=0.5, alpha=0.7)\n",
    "\n",
    "        # 去掉顶部和右侧的边框，仅显示左侧和底部的边框\n",
    "        plt.gca().spines['top'].set_visible(False)\n",
    "        plt.gca().spines['right'].set_visible(False)\n",
    "\n",
    "        # 可选：修改左侧和底部边框的样式\n",
    "        plt.gca().spines['left'].set_linewidth(1.5)\n",
    "        plt.gca().spines['bottom'].set_linewidth(1.5)\n",
    "\n",
    "        plt.gca().spines['left'].set_visible(True)\n",
    "        plt.gca().spines['bottom'].set_visible(True)\n",
    "\n",
    "        plt.gca().tick_params(axis='both', which='both', length=0)\n",
    "\n",
    "        # 调整布局以防止标签重叠\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # 展示图形\n",
    "        plt.show()\n",
    "\n",
    "    if plots:\n",
    "        plot_loss(train_losses, val_losses)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd75124-1797-499e-a37f-43d6ced79c85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-14T13:33:44.620770Z",
     "iopub.status.busy": "2024-11-14T13:33:44.620770Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/20 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# 构造参数字典\n",
    "params2 = {\n",
    "    \"train_args\": {\n",
    "        \"model_name\": Transformer,\n",
    "        \"train_loader\": train_loader,\n",
    "        \"valid_loader\": valid_loader,\n",
    "        \"n_epochs\": 20,\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"loss\": nn.CrossEntropyLoss(),\n",
    "        \"patience\": 5,\n",
    "        \"lradj\": 'cosine',\n",
    "        \"model_path\": \"../outputs/best_models/Transformer\",\n",
    "        \"device\": 'cuda',\n",
    "        \"verbose\": True,\n",
    "        \"plots\": True,\n",
    "    },\n",
    "    \"model_args\": {\n",
    "        'seq_len': 128,\n",
    "        'vocab_size': tokenizer.vocab_size, \n",
    "        'd_model': 512,\n",
    "        'dropout': 0.1,\n",
    "        'n_heads': 8,\n",
    "        'd_ff': 1024,\n",
    "        'e_layers': 6,\n",
    "        'num_classes': 2,\n",
    "    },\n",
    "}\n",
    "model = train(**params2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aca31fa-d0c6-4483-a0ba-2ad748a6ddf4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
