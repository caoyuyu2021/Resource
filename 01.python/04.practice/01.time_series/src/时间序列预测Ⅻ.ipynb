{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48b9aa0d",
   "metadata": {},
   "source": [
    "**时间序列预测前沿算法汇总Ⅱ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2a49cf9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:06:47.477968Z",
     "start_time": "2024-04-14T13:06:43.510208Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.fft as fft\n",
    "import os\n",
    "from tqdm import tqdm # 打印进度条\n",
    "import math\n",
    "from einops import rearrange, repeat, reduce\n",
    "from scipy.fftpack import next_fast_len\n",
    "import matplotlib.pyplot as plt\n",
    "import functools\n",
    "from functools import partial, wraps\n",
    "from sympy import Poly, legendre, Symbol, chebyshevt\n",
    "from scipy.special import eval_legendre\n",
    "from scipy import signal\n",
    "from torch.nn.modules.linear import Linear\n",
    "from operator import mul\n",
    "from typing import List\n",
    "from pandas.tseries import offsets\n",
    "from pandas.tseries.frequencies import to_offset\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f85599",
   "metadata": {},
   "source": [
    "# 基于Crossformer的时间序列预测"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2df08a",
   "metadata": {},
   "source": [
    "## 多变量多步预测"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9c3731",
   "metadata": {},
   "source": [
    "### 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aff33a11",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:07:25.801132Z",
     "start_time": "2024-04-14T13:07:25.780850Z"
    }
   },
   "outputs": [],
   "source": [
    "# 读取数据\n",
    "def get_dataset(path, time_col=None):\n",
    "    data = pd.read_csv(path)\n",
    "    rows = data.shape[0]\n",
    "    now = datetime.now()\n",
    "    newtime = now.replace(microsecond=0)\n",
    "    if time_col == None:\n",
    "        # 如果没有时间列，生成时间戳范围\n",
    "        time_index = pd.date_range(start=datetime.now() -\n",
    "                                   timedelta(seconds=rows - 1),\n",
    "                                   end=datetime.now(),\n",
    "                                   freq='S')\n",
    "        full_data = pd.DataFrame(data=data.values,\n",
    "                                 index=pd.to_datetime(time_index, unit='s'),\n",
    "                                 columns=data.columns)\n",
    "    else:\n",
    "        columns = [i for i in data.columns if i != time_col] # 去除时间列\n",
    "        full_data = pd.DataFrame(data=data.drop([time_col], axis=1).values,\n",
    "                                 index=pd.to_datetime(data[time_col].values),\n",
    "                                 columns=columns)\n",
    "    return full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6fa305a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:07:26.866013Z",
     "start_time": "2024-04-14T13:07:26.722686Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>load</th>\n",
       "      <th>temp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-01-01 00:00:00</th>\n",
       "      <td>2698.0</td>\n",
       "      <td>32.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-01 01:00:00</th>\n",
       "      <td>2558.0</td>\n",
       "      <td>32.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-01 02:00:00</th>\n",
       "      <td>2444.0</td>\n",
       "      <td>30.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-01 03:00:00</th>\n",
       "      <td>2402.0</td>\n",
       "      <td>31.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-01 04:00:00</th>\n",
       "      <td>2403.0</td>\n",
       "      <td>32.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-12-31 19:00:00</th>\n",
       "      <td>4012.0</td>\n",
       "      <td>18.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-12-31 20:00:00</th>\n",
       "      <td>3856.0</td>\n",
       "      <td>16.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-12-31 21:00:00</th>\n",
       "      <td>3671.0</td>\n",
       "      <td>17.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-12-31 22:00:00</th>\n",
       "      <td>3499.0</td>\n",
       "      <td>15.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-12-31 23:00:00</th>\n",
       "      <td>3345.0</td>\n",
       "      <td>15.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26304 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       load   temp\n",
       "2012-01-01 00:00:00  2698.0  32.00\n",
       "2012-01-01 01:00:00  2558.0  32.67\n",
       "2012-01-01 02:00:00  2444.0  30.00\n",
       "2012-01-01 03:00:00  2402.0  31.00\n",
       "2012-01-01 04:00:00  2403.0  32.00\n",
       "...                     ...    ...\n",
       "2014-12-31 19:00:00  4012.0  18.00\n",
       "2014-12-31 20:00:00  3856.0  16.67\n",
       "2014-12-31 21:00:00  3671.0  17.00\n",
       "2014-12-31 22:00:00  3499.0  15.33\n",
       "2014-12-31 23:00:00  3345.0  15.33\n",
       "\n",
       "[26304 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"../data/energy.csv\"\n",
    "ts_data = get_dataset(path, time_col='time')\n",
    "ts_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "960ea005",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:07:33.017950Z",
     "start_time": "2024-04-14T13:07:32.975597Z"
    }
   },
   "outputs": [],
   "source": [
    "# 时间格式编码\n",
    "def time_features_from_frequency_str(freq_str: str):\n",
    "    \"\"\"\n",
    "    Returns a list of time features that will be appropriate for the given frequency string.\n",
    "    Parameters\n",
    "    ----------\n",
    "    freq_str\n",
    "        Frequency string of the form [multiple][granularity] such as \"12H\", \"5min\", \"1D\" etc.\n",
    "    \"\"\"\n",
    "\n",
    "    class TimeFeature:\n",
    "        def __init__(self):\n",
    "            pass\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            pass\n",
    "\n",
    "        def __repr__(self):\n",
    "            return self.__class__.__name__ + \"()\"\n",
    "\n",
    "\n",
    "    class SecondOfMinute(TimeFeature):\n",
    "        \"\"\"Minute of hour encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return index.second / 59.0 - 0.5\n",
    "\n",
    "\n",
    "    class MinuteOfHour(TimeFeature):\n",
    "        \"\"\"Minute of hour encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return index.minute / 59.0 - 0.5\n",
    "\n",
    "\n",
    "    class HourOfDay(TimeFeature):\n",
    "        \"\"\"Hour of day encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return index.hour / 23.0 - 0.5\n",
    "\n",
    "\n",
    "    class DayOfWeek(TimeFeature):\n",
    "        \"\"\"Hour of day encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return index.dayofweek / 6.0 - 0.5\n",
    "\n",
    "\n",
    "    class DayOfMonth(TimeFeature):\n",
    "        \"\"\"Day of month encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return (index.day - 1) / 30.0 - 0.5\n",
    "\n",
    "\n",
    "    class DayOfYear(TimeFeature):\n",
    "        \"\"\"Day of year encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return (index.dayofyear - 1) / 365.0 - 0.5\n",
    "\n",
    "\n",
    "    class MonthOfYear(TimeFeature):\n",
    "        \"\"\"Month of year encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return (index.month - 1) / 11.0 - 0.5\n",
    "\n",
    "\n",
    "    class WeekOfYear(TimeFeature):\n",
    "        \"\"\"Week of year encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return (index.isocalendar().week - 1) / 52.0 - 0.5\n",
    "\n",
    "    \n",
    "    features_by_offsets = {\n",
    "        offsets.YearEnd: [],\n",
    "        offsets.QuarterEnd: [MonthOfYear],\n",
    "        offsets.MonthEnd: [MonthOfYear],\n",
    "        offsets.Week: [DayOfMonth, WeekOfYear],\n",
    "        offsets.Day: [DayOfWeek, DayOfMonth, DayOfYear],\n",
    "        offsets.BusinessDay: [DayOfWeek, DayOfMonth, DayOfYear],\n",
    "        offsets.Hour: [HourOfDay, DayOfWeek, DayOfMonth, DayOfYear],\n",
    "        offsets.Minute: [\n",
    "            MinuteOfHour,\n",
    "            HourOfDay,\n",
    "            DayOfWeek,\n",
    "            DayOfMonth,\n",
    "            DayOfYear,\n",
    "        ],\n",
    "        offsets.Second: [\n",
    "            SecondOfMinute,\n",
    "            MinuteOfHour,\n",
    "            HourOfDay,\n",
    "            DayOfWeek,\n",
    "            DayOfMonth,\n",
    "            DayOfYear,\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    offset = to_offset(freq_str)\n",
    "\n",
    "    for offset_type, feature_classes in features_by_offsets.items():\n",
    "        if isinstance(offset, offset_type):\n",
    "            return [cls() for cls in feature_classes]\n",
    "\n",
    "    supported_freq_msg = f\"\"\"\n",
    "    Unsupported frequency {freq_str}\n",
    "    The following frequencies are supported:\n",
    "        Y   - yearly\n",
    "            alias: A\n",
    "        M   - monthly\n",
    "        W   - weekly\n",
    "        D   - daily\n",
    "        B   - business days\n",
    "        H   - hourly\n",
    "        T   - minutely\n",
    "            alias: min\n",
    "        S   - secondly\n",
    "    \"\"\"\n",
    "    raise RuntimeError(supported_freq_msg)\n",
    "    \n",
    "def time_features(dates, freq='h'):\n",
    "    return np.vstack([feat(dates) for feat in time_features_from_frequency_str(freq)])\n",
    "\n",
    "# 包含时间维度的数据集划分\n",
    "def divide_dataset(df, valid_date, test_date, x_feature_list, y_feature_list, freq):\n",
    "    #归一化\n",
    "    x_scaler = MinMaxScaler()\n",
    "    y_scaler = MinMaxScaler()\n",
    "\n",
    "    #测试集\n",
    "    train = df.copy()[df.index < valid_date][x_feature_list]\n",
    "    train_stamp = time_features(pd.to_datetime(train.index), freq=freq)\n",
    "    train_stamp = train_stamp.transpose(1, 0)\n",
    "    train[x_feature_list] = x_scaler.fit_transform(train)\n",
    "    xtr = train.values.astype('float32')\n",
    "\n",
    "    #验证集\n",
    "    valid = df.copy()[(df.index >= valid_date) & (df.index < test_date)][x_feature_list]\n",
    "    valid_stamp = time_features(pd.to_datetime(valid.index), freq=freq)\n",
    "    valid_stamp = valid_stamp.transpose(1, 0)\n",
    "    valid[x_feature_list] = x_scaler.fit_transform(valid)\n",
    "    xva = valid.values.astype('float32')\n",
    "\n",
    "    #测试集\n",
    "    test = df.copy()[test_date:][x_feature_list]\n",
    "    test_stamp = time_features(pd.to_datetime(test.index), freq=freq)\n",
    "    test_stamp = test_stamp.transpose(1, 0)\n",
    "    test[x_feature_list] = x_scaler.fit_transform(test)\n",
    "    xte = test.values.astype('float32')\n",
    "\n",
    "    #标签\n",
    "    ytr = df.copy()[df.index < valid_date][y_feature_list]\n",
    "    ytr[y_feature_list] = y_scaler.fit_transform(ytr)\n",
    "    ytr = ytr.values.astype('float32')\n",
    "\n",
    "    yva = df.copy()[(df.index >= valid_date) & (df.index < test_date)][y_feature_list]\n",
    "    yva[y_feature_list] = y_scaler.fit_transform(yva)\n",
    "    yva = yva.values.astype('float32')\n",
    "\n",
    "    yte = df.copy()[test_date:][y_feature_list]\n",
    "    yte[y_feature_list] = y_scaler.fit_transform(yte)\n",
    "    yte = yte.values.astype('float32')\n",
    "    \n",
    "    # 数据合并，[训练集，测试集，时间戳]\n",
    "    train = [xtr, ytr, train_stamp]\n",
    "    valid = [xva, yva, valid_stamp]\n",
    "    test = [xte, yte, test_stamp]\n",
    "    \n",
    "    return y_scaler, train, valid, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "893c75d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:07:34.013901Z",
     "start_time": "2024-04-14T13:07:33.922063Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (23376, 2) y_train shape: (23376, 1) stamp_train shape: (23376, 4)\n",
      "x_valid shape: (1464, 2) y_valid shape: (1464, 1) stamp_valid shape: (1464, 4)\n",
      "x_test shape: (1464, 2) y_test shape: (1464, 1) stamp_test shape: (1464, 4)\n"
     ]
    }
   ],
   "source": [
    "# 构造参数字典\n",
    "params1 = {\n",
    "    \"df\": ts_data,\n",
    "    \"valid_date\": \"2014-09-01 00:00:00\",\n",
    "    \"test_date\": \"2014-11-01 00:00:00\",\n",
    "    \"x_feature_list\": [\"load\", 'temp'],\n",
    "    \"y_feature_list\": [\"load\"],\n",
    "    \"freq\": 'h'\n",
    "}\n",
    "\n",
    "#函数传参\n",
    "scaler, train, valid, test = divide_dataset(**params1)\n",
    "print(\"x_train shape: {0} y_train shape: {1} stamp_train shape: {2}\".format(train[0].shape, train[1].shape, train[2].shape))\n",
    "print(\"x_valid shape: {0} y_valid shape: {1} stamp_valid shape: {2}\".format(valid[0].shape, valid[1].shape, valid[2].shape))\n",
    "print(\"x_test shape: {0} y_test shape: {1} stamp_test shape: {2}\".format(test[0].shape, test[1].shape, test[2].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1fecd39e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:07:35.137838Z",
     "start_time": "2024-04-14T13:07:35.117710Z"
    }
   },
   "outputs": [],
   "source": [
    "#利用前seq_len个数据，预测下pred_len个数据\n",
    "def create_dataset(data_list, seq_len, pred_len, label_len, batch_size):\n",
    "    \"\"\"\n",
    "    @参数设置：\n",
    "    data_list：特征，目标，时间戳\n",
    "    seq_len：输入数据包含过去多少个时间步\n",
    "    pred_len：目标应该在未来多少个时间步之后\n",
    "    label_len：先验时间步\n",
    "    \"\"\"\n",
    "    # 获取数据\n",
    "    feature = data_list[0] # 特征\n",
    "    target = data_list[1] # 目标\n",
    "    stamp = data_list[2] # 时间戳\n",
    "    \n",
    "    # 循环生成数据\n",
    "    X, y = [], []\n",
    "    X_stamp, y_stamp = [], []\n",
    "    seq_len = seq_len - 1 # 包含当前时间点\n",
    "    for i in range(seq_len, len(feature) - pred_len):\n",
    "        # 数据维度\n",
    "        feat = feature[i - seq_len:i + 1]\n",
    "        tar = target[i + 1:i + 1 + pred_len]\n",
    "        X.append(feat)\n",
    "        y.append(tar)\n",
    "        \n",
    "        # 时间维度\n",
    "        xs = stamp[i - seq_len:i + 1]\n",
    "        ys = stamp[i + 1 - label_len:i + 1 + pred_len]\n",
    "        X_stamp.append(xs)\n",
    "        y_stamp.append(ys)\n",
    "        \n",
    "    # 转为张量，数据维度\n",
    "    X = torch.as_tensor(X).float()\n",
    "    y = torch.as_tensor(y).float()\n",
    "    \n",
    "    # 转为张量，时间维度\n",
    "    X_stamp = torch.as_tensor(X_stamp).float()\n",
    "    y_stamp = torch.as_tensor(y_stamp).float()\n",
    "    \n",
    "    # 创建dataloader，[特征，目标，特征时间编码，目标时间编码]\n",
    "    data_loader = DataLoader(TensorDataset(X, y, X_stamp, y_stamp), shuffle=True, batch_size=batch_size)\n",
    "    \n",
    "    return X, y, data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a52dd2a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:07:37.115398Z",
     "start_time": "2024-04-14T13:07:36.247176Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_size: torch.Size([23368, 6, 2]),y_size: torch.Size([23368, 3, 1]),loader_len: 731\n",
      "X_size: torch.Size([1456, 6, 2]),y_size: torch.Size([1456, 3, 1]),loader_len: 46\n",
      "X_size: torch.Size([1456, 6, 2]),y_size: torch.Size([1456, 3, 1]),loader_len: 46\n"
     ]
    }
   ],
   "source": [
    "# 构造参数字典\n",
    "params2 = {\n",
    "    \"seq_len\": 6,\n",
    "    \"pred_len\": 3,\n",
    "    \"label_len\": 0,\n",
    "    \"batch_size\": 32,\n",
    "}\n",
    "X_train, y_train, train_loader = create_dataset(train, **params2)\n",
    "X_valid, y_valid, valid_loader = create_dataset(valid, **params2)\n",
    "X_test, y_test, test_loader = create_dataset(test, **params2)\n",
    "print(\"X_size: {0},y_size: {1},loader_len: {2}\".format(X_train.shape, y_train.shape, len(train_loader)))\n",
    "print(\"X_size: {0},y_size: {1},loader_len: {2}\".format(X_valid.shape, y_valid.shape, len(valid_loader)))\n",
    "print(\"X_size: {0},y_size: {1},loader_len: {2}\".format(X_test.shape, y_test.shape, len(test_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391071f0",
   "metadata": {},
   "source": [
    "### 模型定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c56b6f08",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:09:59.790415Z",
     "start_time": "2024-04-14T13:09:59.719045Z"
    }
   },
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model).float()\n",
    "        pe.require_grad = False\n",
    "\n",
    "        position = torch.arange(0, max_len).float().unsqueeze(1)\n",
    "        div_term = (torch.arange(0, d_model, 2).float()\n",
    "                    * -(math.log(10000.0) / d_model)).exp()\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pe[:, :x.size(1)]\n",
    "    \n",
    "    \n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, patch_len, stride, padding, dropout):\n",
    "        super(PatchEmbedding, self).__init__()\n",
    "        # Patching\n",
    "        self.patch_len = patch_len\n",
    "        self.stride = stride\n",
    "        self.padding_patch_layer = nn.ReplicationPad1d((0, padding))\n",
    "\n",
    "        # Backbone, Input encoding: projection of feature vectors onto a d-dim vector space\n",
    "        self.value_embedding = nn.Linear(patch_len, d_model, bias=False)\n",
    "\n",
    "        # Positional embedding\n",
    "        self.position_embedding = PositionalEmbedding(d_model)\n",
    "\n",
    "        # Residual dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # do patching\n",
    "        n_vars = x.shape[1]\n",
    "        x = self.padding_patch_layer(x)\n",
    "        x = x.unfold(dimension=-1, size=self.patch_len, step=self.stride)\n",
    "        x = torch.reshape(x, (x.shape[0] * x.shape[1], x.shape[2], x.shape[3]))\n",
    "        # Input encoding\n",
    "        x = self.value_embedding(x) + self.position_embedding(x)\n",
    "        return self.dropout(x), n_vars\n",
    "    \n",
    "    \n",
    "class TwoStageAttentionLayer(nn.Module):\n",
    "    '''\n",
    "    The Two Stage Attention (TSA) Layer\n",
    "    input/output shape: [batch_size, Data_dim(D), Seg_num(L), d_model]\n",
    "    '''\n",
    "\n",
    "    def __init__(self, seg_num, factor, d_model, n_heads, output_attention, d_ff=None, dropout=0.1):\n",
    "        super(TwoStageAttentionLayer, self).__init__()\n",
    "        d_ff = d_ff or 4 * d_model\n",
    "        self.time_attention = AttentionLayer(FullAttention(False, factor, attention_dropout=dropout,\n",
    "                                                           output_attention=output_attention), d_model, n_heads)\n",
    "        self.dim_sender = AttentionLayer(FullAttention(False, factor, attention_dropout=dropout,\n",
    "                                                       output_attention=output_attention), d_model, n_heads)\n",
    "        self.dim_receiver = AttentionLayer(FullAttention(False, factor, attention_dropout=dropout,\n",
    "                                                         output_attention=output_attention), d_model, n_heads)\n",
    "        self.router = nn.Parameter(torch.randn(seg_num, factor, d_model))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.norm4 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.MLP1 = nn.Sequential(nn.Linear(d_model, d_ff),\n",
    "                                  nn.GELU(),\n",
    "                                  nn.Linear(d_ff, d_model))\n",
    "        self.MLP2 = nn.Sequential(nn.Linear(d_model, d_ff),\n",
    "                                  nn.GELU(),\n",
    "                                  nn.Linear(d_ff, d_model))\n",
    "\n",
    "    def forward(self, x, attn_mask=None, tau=None, delta=None):\n",
    "        # Cross Time Stage: Directly apply MSA to each dimension\n",
    "        batch = x.shape[0]\n",
    "        time_in = rearrange(x, 'b ts_d seg_num d_model -> (b ts_d) seg_num d_model')\n",
    "        time_enc, attn = self.time_attention(\n",
    "            time_in, time_in, time_in, attn_mask=None, tau=None, delta=None\n",
    "        )\n",
    "        dim_in = time_in + self.dropout(time_enc)\n",
    "        dim_in = self.norm1(dim_in)\n",
    "        dim_in = dim_in + self.dropout(self.MLP1(dim_in))\n",
    "        dim_in = self.norm2(dim_in)\n",
    "\n",
    "        # Cross Dimension Stage: use a small set of learnable vectors to aggregate and distribute messages to build the D-to-D connection\n",
    "        dim_send = rearrange(dim_in, '(b ts_d) seg_num d_model -> (b seg_num) ts_d d_model', b=batch)\n",
    "        batch_router = repeat(self.router, 'seg_num factor d_model -> (repeat seg_num) factor d_model', repeat=batch)\n",
    "        dim_buffer, attn = self.dim_sender(batch_router, dim_send, dim_send, attn_mask=None, tau=None, delta=None)\n",
    "        dim_receive, attn = self.dim_receiver(dim_send, dim_buffer, dim_buffer, attn_mask=None, tau=None, delta=None)\n",
    "        dim_enc = dim_send + self.dropout(dim_receive)\n",
    "        dim_enc = self.norm3(dim_enc)\n",
    "        dim_enc = dim_enc + self.dropout(self.MLP2(dim_enc))\n",
    "        dim_enc = self.norm4(dim_enc)\n",
    "\n",
    "        final_out = rearrange(dim_enc, '(b seg_num) ts_d d_model -> b ts_d seg_num d_model', b=batch)\n",
    "\n",
    "        return final_out\n",
    "\n",
    "    \n",
    "class SegMerging(nn.Module):\n",
    "    def __init__(self, d_model, win_size, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.win_size = win_size\n",
    "        self.linear_trans = nn.Linear(win_size * d_model, d_model)\n",
    "        self.norm = norm_layer(win_size * d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, ts_d, seg_num, d_model = x.shape\n",
    "        pad_num = seg_num % self.win_size\n",
    "        if pad_num != 0:\n",
    "            pad_num = self.win_size - pad_num\n",
    "            x = torch.cat((x, x[:, :, -pad_num:, :]), dim=-2)\n",
    "\n",
    "        seg_to_merge = []\n",
    "        for i in range(self.win_size):\n",
    "            seg_to_merge.append(x[:, :, i::self.win_size, :])\n",
    "        x = torch.cat(seg_to_merge, -1)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        x = self.linear_trans(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class scale_block(nn.Module):\n",
    "    def __init__(self, win_size, d_model, n_heads, d_ff, depth, dropout, output_attention, \n",
    "                 seg_num=10, factor=10):\n",
    "        super(scale_block, self).__init__()\n",
    "\n",
    "        if win_size > 1:\n",
    "            self.merge_layer = SegMerging(d_model, win_size, nn.LayerNorm)\n",
    "        else:\n",
    "            self.merge_layer = None\n",
    "\n",
    "        self.encode_layers = nn.ModuleList()\n",
    "\n",
    "        for i in range(depth):\n",
    "            self.encode_layers.append(TwoStageAttentionLayer(seg_num, factor, d_model, n_heads, output_attention, \n",
    "                                                             d_ff, dropout))\n",
    "\n",
    "    def forward(self, x, attn_mask=None, tau=None, delta=None):\n",
    "        _, ts_dim, _, _ = x.shape\n",
    "\n",
    "        if self.merge_layer is not None:\n",
    "            x = self.merge_layer(x)\n",
    "\n",
    "        for layer in self.encode_layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        return x, None\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, attn_layers):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.encode_blocks = nn.ModuleList(attn_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        encode_x = []\n",
    "        encode_x.append(x)\n",
    "\n",
    "        for block in self.encode_blocks:\n",
    "            x, attns = block(x)\n",
    "            encode_x.append(x)\n",
    "\n",
    "        return encode_x, None\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, self_attention, cross_attention, seg_len, d_model, d_ff=None, dropout=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attention = self_attention\n",
    "        self.cross_attention = cross_attention\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.MLP1 = nn.Sequential(nn.Linear(d_model, d_model),\n",
    "                                  nn.GELU(),\n",
    "                                  nn.Linear(d_model, d_model))\n",
    "        self.linear_pred = nn.Linear(d_model, seg_len)\n",
    "\n",
    "    def forward(self, x, cross):\n",
    "        batch = x.shape[0]\n",
    "        x = self.self_attention(x)\n",
    "        x = rearrange(x, 'b ts_d out_seg_num d_model -> (b ts_d) out_seg_num d_model')\n",
    "\n",
    "        cross = rearrange(cross, 'b ts_d in_seg_num d_model -> (b ts_d) in_seg_num d_model')\n",
    "        tmp, attn = self.cross_attention(x, cross, cross, None, None, None,)\n",
    "        x = x + self.dropout(tmp)\n",
    "        y = x = self.norm1(x)\n",
    "        y = self.MLP1(y)\n",
    "        dec_output = self.norm2(x + y)\n",
    "\n",
    "        dec_output = rearrange(dec_output, '(b ts_d) seg_dec_num d_model -> b ts_d seg_dec_num d_model', b=batch)\n",
    "        layer_predict = self.linear_pred(dec_output)\n",
    "        layer_predict = rearrange(layer_predict, 'b out_d seg_num seg_len -> b (out_d seg_num) seg_len')\n",
    "\n",
    "        return dec_output, layer_predict\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.decode_layers = nn.ModuleList(layers)\n",
    "\n",
    "\n",
    "    def forward(self, x, cross):\n",
    "        final_predict = None\n",
    "        i = 0\n",
    "\n",
    "        ts_d = x.shape[1]\n",
    "        for layer in self.decode_layers:\n",
    "            cross_enc = cross[i]\n",
    "            x, layer_predict = layer(x, cross_enc)\n",
    "            if final_predict is None:\n",
    "                final_predict = layer_predict\n",
    "            else:\n",
    "                final_predict = final_predict + layer_predict\n",
    "            i += 1\n",
    "\n",
    "        final_predict = rearrange(final_predict, 'b (out_d seg_num) seg_len -> b (seg_num seg_len) out_d', out_d=ts_d)\n",
    "\n",
    "        return final_predict\n",
    "\n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, attention, d_model, n_heads, d_keys=None,\n",
    "                 d_values=None):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "\n",
    "        d_keys = d_keys or (d_model // n_heads)\n",
    "        d_values = d_values or (d_model // n_heads)\n",
    "\n",
    "        self.inner_attention = attention\n",
    "        self.query_projection = nn.Linear(d_model, d_keys * n_heads)\n",
    "        self.key_projection = nn.Linear(d_model, d_keys * n_heads)\n",
    "        self.value_projection = nn.Linear(d_model, d_values * n_heads)\n",
    "        self.out_projection = nn.Linear(d_values * n_heads, d_model)\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "    def forward(self, queries, keys, values, attn_mask, tau=None, delta=None):\n",
    "        B, L, _ = queries.shape\n",
    "        _, S, _ = keys.shape\n",
    "        H = self.n_heads\n",
    "\n",
    "        queries = self.query_projection(queries).view(B, L, H, -1)\n",
    "        keys = self.key_projection(keys).view(B, S, H, -1)\n",
    "        values = self.value_projection(values).view(B, S, H, -1)\n",
    "\n",
    "        out, attn = self.inner_attention(\n",
    "            queries,\n",
    "            keys,\n",
    "            values,\n",
    "            attn_mask,\n",
    "            tau=tau,\n",
    "            delta=delta\n",
    "        )\n",
    "        out = out.view(B, L, -1)\n",
    "\n",
    "        return self.out_projection(out), attn\n",
    "    \n",
    "    \n",
    "class TriangularCausalMask():\n",
    "    def __init__(self, B, L, device=\"cpu\"):\n",
    "        mask_shape = [B, 1, L, L]\n",
    "        with torch.no_grad():\n",
    "            self._mask = torch.triu(torch.ones(mask_shape, dtype=torch.bool), diagonal=1).to(device)\n",
    "\n",
    "    @property\n",
    "    def mask(self):\n",
    "        return self._mask\n",
    "    \n",
    "    \n",
    "class FullAttention(nn.Module):\n",
    "    def __init__(self, mask_flag=True, factor=5, scale=None, attention_dropout=0.1, output_attention=False):\n",
    "        super(FullAttention, self).__init__()\n",
    "        self.scale = scale\n",
    "        self.mask_flag = mask_flag\n",
    "        self.output_attention = output_attention\n",
    "        self.dropout = nn.Dropout(attention_dropout)\n",
    "\n",
    "    def forward(self, queries, keys, values, attn_mask, tau=None, delta=None):\n",
    "        B, L, H, E = queries.shape\n",
    "        _, S, _, D = values.shape\n",
    "        scale = self.scale or 1. / math.sqrt(E)\n",
    "\n",
    "        scores = torch.einsum(\"blhe,bshe->bhls\", queries, keys)\n",
    "\n",
    "        if self.mask_flag:\n",
    "            if attn_mask is None:\n",
    "                attn_mask = TriangularCausalMask(B, L, device=queries.device)\n",
    "\n",
    "            scores.masked_fill_(attn_mask.mask, -np.inf)\n",
    "\n",
    "        A = self.dropout(torch.softmax(scale * scores, dim=-1))\n",
    "        V = torch.einsum(\"bhls,bshd->blhd\", A, values)\n",
    "\n",
    "        if self.output_attention:\n",
    "            return V.contiguous(), A\n",
    "        else:\n",
    "            return V.contiguous(), None\n",
    "        \n",
    "        \n",
    "class FlattenHead(nn.Module):\n",
    "    def __init__(self, n_vars, nf, target_window, head_dropout=0):\n",
    "        super().__init__()\n",
    "        self.n_vars = n_vars\n",
    "        self.flatten = nn.Flatten(start_dim=-2)\n",
    "        self.linear = nn.Linear(nf, target_window)\n",
    "        self.dropout = nn.Dropout(head_dropout)\n",
    "\n",
    "    def forward(self, x):  # x: [bs x nvars x d_model x patch_num]\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "    \n",
    "# Crossformer模型\n",
    "class Crossformer(nn.Module):\n",
    "    def __init__(self, enc_in, seq_len, pred_len, label_len, e_layers, d_layers, d_model, \n",
    "                 output_attention, n_heads, d_ff, dropout, factor):\n",
    "        super(Crossformer, self).__init__()\n",
    "        self.enc_in = enc_in\n",
    "        self.seq_len = seq_len\n",
    "        self.pred_len = pred_len\n",
    "        self.seg_len = 12\n",
    "        self.win_size = 2\n",
    "\n",
    "        # The padding operation to handle invisible sgemnet length\n",
    "        self.pad_in_len = math.ceil(1.0 * seq_len / self.seg_len) * self.seg_len\n",
    "        self.pad_out_len = math.ceil(1.0 * pred_len / self.seg_len) * self.seg_len\n",
    "        self.in_seg_num = self.pad_in_len // self.seg_len\n",
    "        self.out_seg_num = math.ceil(self.in_seg_num / (self.win_size ** (e_layers - 1)))\n",
    "        self.head_nf = d_model * self.out_seg_num\n",
    "\n",
    "        # Embedding\n",
    "        self.enc_value_embedding = PatchEmbedding(d_model, self.seg_len, self.seg_len, self.pad_in_len - seq_len, 0)\n",
    "        self.enc_pos_embedding = nn.Parameter(\n",
    "            torch.randn(1, enc_in, self.in_seg_num, d_model))\n",
    "        self.pre_norm = nn.LayerNorm(d_model)\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = Encoder(\n",
    "            [\n",
    "                scale_block(1 if l is 0 else self.win_size, d_model, n_heads, d_ff, 1, dropout, output_attention, \n",
    "                            self.in_seg_num if l is 0 else math.ceil(self.in_seg_num / self.win_size ** l), factor\n",
    "                            ) for l in range(e_layers)\n",
    "            ]\n",
    "        )\n",
    "        # Decoder\n",
    "        self.dec_pos_embedding = nn.Parameter(\n",
    "            torch.randn(1, enc_in, (self.pad_out_len // self.seg_len), d_model))\n",
    "\n",
    "        self.decoder = Decoder(\n",
    "            [\n",
    "                DecoderLayer(\n",
    "                    TwoStageAttentionLayer((self.pad_out_len // self.seg_len), factor, d_model, n_heads, output_attention,\n",
    "                                           d_ff, dropout),\n",
    "                    AttentionLayer(\n",
    "                        FullAttention(False, factor, attention_dropout=dropout, output_attention=False),\n",
    "                        d_model, n_heads),\n",
    "                    self.seg_len,\n",
    "                    d_model,\n",
    "                    d_ff,\n",
    "                    dropout=dropout,\n",
    "                    # activation=configs.activation,\n",
    "                )\n",
    "                for l in range(d_layers + 1)\n",
    "            ],\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec):\n",
    "        # embedding\n",
    "        x_enc, n_vars = self.enc_value_embedding(x_enc.permute(0, 2, 1))\n",
    "        x_enc = rearrange(x_enc, '(b d) seg_num d_model -> b d seg_num d_model', d = n_vars)\n",
    "        x_enc += self.enc_pos_embedding\n",
    "        x_enc = self.pre_norm(x_enc)\n",
    "        enc_out, attns = self.encoder(x_enc)\n",
    "\n",
    "        dec_in = repeat(self.dec_pos_embedding, 'b ts_d l d -> (repeat b) ts_d l d', repeat=x_enc.shape[0])\n",
    "        dec_out = self.decoder(dec_in, enc_out)\n",
    "        \n",
    "        output = dec_out[:, -self.pred_len:, :]  # [B, L, D]\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366f0c58",
   "metadata": {},
   "source": [
    "### 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f44a61ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:10:01.927297Z",
     "start_time": "2024-04-14T13:10:01.882785Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(task_args, train_args, model_args):\n",
    "    # 参数配置\n",
    "    columns = task_args['columns'] # 模型全部特征\n",
    "    target = task_args['target'] # 模型预测特征\n",
    "    features = task_args['features'] # 模型预测模式\n",
    "    model_name = train_args['model_name'] # 模型名称\n",
    "    train_loader = train_args['train_loader'] # 训练集\n",
    "    valid_loader = train_args['valid_loader'] # 验证集\n",
    "    n_epochs = train_args['n_epochs'] # 训练次数\n",
    "    learning_rate = train_args['learning_rate'] # 学习率\n",
    "    loss = train_args['loss'] # 损失函数\n",
    "    patience = train_args['patience'] # 最大早停次数阈值，超过就会早停\n",
    "    lradj = train_args['lradj'] # 学习率函数\n",
    "    path = train_args['path'] # 模型保存路径\n",
    "    verbose = train_args['verbose'] # 打印训练过程\n",
    "    plots = train_args['plots'] # 绘制损失图\n",
    "    pred_len = model_args['pred_len'] # 预测长度\n",
    "    label_len = model_args['label_len']\n",
    "    \n",
    "    #检查是否可用GPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # 创建模型和优化器\n",
    "    model = model_name(**model_args).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_fn = loss\n",
    "    \n",
    "    # 损失函数值\n",
    "    train_losses, val_losses = [], []\n",
    "    \n",
    "    # 调整学习率\n",
    "    def adjust_learning_rate(optimizer, epoch, lradj, learning_rate, train_epochs):\n",
    "        # lr = learning_rate * (0.2 ** (epoch // 2))\n",
    "        if lradj == 'type1':\n",
    "            lr_adjust = {epoch: learning_rate * (0.5 ** ((epoch - 1) // 1))}\n",
    "        elif lradj == 'type2':\n",
    "            lr_adjust = {\n",
    "                2: 5e-5, 4: 1e-5, 6: 5e-6, 8: 1e-6,\n",
    "                10: 5e-7, 15: 1e-7, 20: 5e-8\n",
    "            }\n",
    "        elif lradj == \"cosine\":\n",
    "            lr_adjust = {epoch: learning_rate /2 * (1 + math.cos(epoch / train_epochs * math.pi))}\n",
    "        if epoch in lr_adjust.keys():\n",
    "            lr = lr_adjust[epoch]\n",
    "            # 参数组(param_groups)是用来指定不同的参数组以便对它们进行不同的优化设置，比如'lr'\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "            print('Updating learning rate to {}'.format(lr))\n",
    "    \n",
    "    # 设置早停\n",
    "    class EarlyStopping():\n",
    "        def __init__(self, patience=7, verbose=False, delta=0):\n",
    "            self.patience = patience # 连续超限次数，如果满足条件，则早停\n",
    "            self.verbose = verbose\n",
    "            self.counter = 0\n",
    "            self.best_score = None\n",
    "            self.early_stop = False\n",
    "            self.val_loss_min = np.Inf\n",
    "            self.delta = delta\n",
    "\n",
    "        def __call__(self, val_loss, model, path):\n",
    "            score = -val_loss\n",
    "            if self.best_score is None:\n",
    "                self.best_score = score\n",
    "                self.save_checkpoint(val_loss, model, path)\n",
    "            elif score < self.best_score + self.delta:\n",
    "                self.counter += 1\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "                if self.counter >= self.patience:\n",
    "                    self.early_stop = True\n",
    "            else:\n",
    "                self.best_score = score\n",
    "                self.save_checkpoint(val_loss, model, path)\n",
    "                self.counter = 0\n",
    "\n",
    "        def save_checkpoint(self, val_loss, model, path):\n",
    "            if self.verbose:\n",
    "                print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "            torch.save(model.state_dict(), path + '/' + 'checkpoint.pth')\n",
    "            self.val_loss_min = val_loss\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=verbose)\n",
    "    \n",
    "    # 训练任务\n",
    "    def forecasting_task(columns, target, features):\n",
    "        \"\"\"\n",
    "        features: [M, S, MS]; \n",
    "            M:multivariate predict multivariate, \n",
    "            S:univariate predict univariate, \n",
    "            MS:multivariate predict univariate'\n",
    "        \"\"\"\n",
    "        # 字典索引生成\n",
    "        col_dict = {}\n",
    "        for i,j in enumerate(columns):\n",
    "            col_dict[j] = i\n",
    "\n",
    "        if features == 'MS':\n",
    "            target = target[0]\n",
    "            f_dim = col_dict[target]\n",
    "        elif features == 'S':\n",
    "            f_dim = 0\n",
    "        else:\n",
    "            f_dim = 0\n",
    "        return f_dim\n",
    "    f_dim = forecasting_task(columns, target, features)\n",
    "    \n",
    "    # 设置保存模型路径\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    # 模型训练和验证\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for batch_x, batch_y, batch_x_mark, batch_y_mark in train_loader:\n",
    "            #将数据移至 GPU\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            batch_x_mark = batch_x_mark.to(device)\n",
    "            batch_y_mark = batch_y_mark.to(device)\n",
    "            # 清空梯度\n",
    "            optimizer.zero_grad()\n",
    "            # decoder输入\n",
    "            dec_inp = torch.zeros_like(batch_y[:, -pred_len:, :]).float()\n",
    "            dec_inp = torch.cat([batch_y[:, :label_len, :], dec_inp], dim=1).float().to(device)\n",
    "            outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "            if features == 'MS':\n",
    "                outputs = outputs[:, :, f_dim: f_dim+1] \n",
    "            else:\n",
    "                outputs = outputs[:, :, f_dim:]\n",
    "            train_loss = loss_fn(outputs, batch_y)\n",
    "            # 反向传播计算得到每个参数的梯度值\n",
    "            train_loss.backward()\n",
    "            # 通过梯度下降执行一步参数更新\n",
    "            optimizer.step()\n",
    "            #每个batch的loss和\n",
    "            total_train_loss += train_loss.item() # .item()表示只包含一个元素的tensor中提取值\n",
    "                \n",
    "        #每个epoch的损失平均\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        \n",
    "        #所有epoch的loss\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        #评估模型\n",
    "        model.eval()\n",
    "        #关闭自动求导功能，只使用训练好的模型进行预测或评估，不需要进行梯度计算和参数更新\n",
    "        with torch.no_grad():\n",
    "            total_val_loss = 0\n",
    "            for batch_x, batch_y, batch_x_mark, batch_y_mark in valid_loader:\n",
    "                #将数据移至 GPU\n",
    "                batch_x = batch_x.to(device)\n",
    "                batch_y = batch_y.to(device)\n",
    "                batch_x_mark = batch_x_mark.to(device)\n",
    "                batch_y_mark = batch_y_mark.to(device)\n",
    "                # decoder输入\n",
    "                dec_inp = torch.zeros_like(batch_y[:, -pred_len:, :]).float()\n",
    "                dec_inp = torch.cat([batch_y[:, :label_len, :], dec_inp], dim=1).float().to(device)\n",
    "                val_outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "                if features == 'MS':\n",
    "                    val_outputs = val_outputs[:, :, f_dim: f_dim+1] \n",
    "                else:\n",
    "                    val_outputs = val_outputs[:, :, f_dim:]\n",
    "                val_loss = loss_fn(val_outputs, batch_y)\n",
    "                #每个batch的loss和\n",
    "                total_val_loss += val_loss.item()\n",
    "                \n",
    "        #每个epoch的损失平均\n",
    "        avg_val_loss = total_val_loss / len(valid_loader)\n",
    "        \n",
    "        #所有epoch的loss\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        #打印训练过程\n",
    "        if verbose:\n",
    "            print(f'Epoch [{epoch+1}/{n_epochs}], Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}')\n",
    "            \n",
    "        # 设置早停\n",
    "        early_stopping(avg_val_loss, model, path)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping!\")\n",
    "            break\n",
    "            \n",
    "        # 调整学习率\n",
    "        adjust_learning_rate(optimizer, epoch+1, lradj, learning_rate, n_epochs)\n",
    "        \n",
    "    # 加载最佳模型\n",
    "    best_model_path = path + '/' + 'checkpoint.pth'\n",
    "    model.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "    #绘制损失函数图\n",
    "    def plot_loss(train_losses, val_losses):\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.style.use('seaborn-v0_8-paper') #绘制背景色\n",
    "        plt.grid(axis = 'y',linewidth=0.35) #绘制网格\n",
    "        plt.plot(val_losses, linestyle='-',color = '#11b3b6')\n",
    "        plt.plot(train_losses, linestyle='-',color = '#f14643')\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Training and Validation Progress\")\n",
    "        plt.legend([\"Validation\", \"Training\"])\n",
    "        plt.show()\n",
    "    if plots:\n",
    "        plot_loss(train_losses, val_losses)\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c271620a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:10:42.650170Z",
     "start_time": "2024-04-14T13:10:06.959944Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|████▏                                                                              | 1/20 [00:26<08:14, 26.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Training Loss: 0.0185, Validation Loss: 0.0093\n",
      "Validation loss decreased (inf --> 0.009289).  Saving model ...\n",
      "Updating learning rate to 0.0009938441702975688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|████▏                                                                              | 1/20 [00:33<10:42, 33.81s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 36\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 构造参数字典\u001b[39;00m\n\u001b[0;32m      2\u001b[0m params3 \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtask_args\u001b[39m\u001b[38;5;124m\"\u001b[39m:{\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mload\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemp\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     34\u001b[0m     },\n\u001b[0;32m     35\u001b[0m }\n\u001b[1;32m---> 36\u001b[0m model \u001b[38;5;241m=\u001b[39m train(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams3)\n",
      "Cell \u001b[1;32mIn[13], line 125\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(task_args, train_args, model_args)\u001b[0m\n\u001b[0;32m    123\u001b[0m dec_inp \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(batch_y[:, \u001b[38;5;241m-\u001b[39mpred_len:, :])\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m    124\u001b[0m dec_inp \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([batch_y[:, :label_len, :], dec_inp], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m--> 125\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_x_mark\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdec_inp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_y_mark\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m features \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMS\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    127\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m outputs[:, :, f_dim: f_dim\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m] \n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[12], line 390\u001b[0m, in \u001b[0;36mCrossformer.forward\u001b[1;34m(self, x_enc, x_mark_enc, x_dec, x_mark_dec)\u001b[0m\n\u001b[0;32m    387\u001b[0m enc_out, attns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(x_enc)\n\u001b[0;32m    389\u001b[0m dec_in \u001b[38;5;241m=\u001b[39m repeat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdec_pos_embedding, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb ts_d l d -> (repeat b) ts_d l d\u001b[39m\u001b[38;5;124m'\u001b[39m, repeat\u001b[38;5;241m=\u001b[39mx_enc\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m--> 390\u001b[0m dec_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdec_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc_out\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    392\u001b[0m output \u001b[38;5;241m=\u001b[39m dec_out[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpred_len:, :]  \u001b[38;5;66;03m# [B, L, D]\u001b[39;00m\n\u001b[0;32m    393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[12], line 223\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[1;34m(self, x, cross)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode_layers:\n\u001b[0;32m    222\u001b[0m     cross_enc \u001b[38;5;241m=\u001b[39m cross[i]\n\u001b[1;32m--> 223\u001b[0m     x, layer_predict \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcross_enc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    224\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m final_predict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    225\u001b[0m         final_predict \u001b[38;5;241m=\u001b[39m layer_predict\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[12], line 193\u001b[0m, in \u001b[0;36mDecoderLayer.forward\u001b[1;34m(self, x, cross)\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, cross):\n\u001b[0;32m    192\u001b[0m     batch \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m--> 193\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    194\u001b[0m     x \u001b[38;5;241m=\u001b[39m rearrange(x, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb ts_d out_seg_num d_model -> (b ts_d) out_seg_num d_model\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    196\u001b[0m     cross \u001b[38;5;241m=\u001b[39m rearrange(cross, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb ts_d in_seg_num d_model -> (b ts_d) in_seg_num d_model\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[12], line 85\u001b[0m, in \u001b[0;36mTwoStageAttentionLayer.forward\u001b[1;34m(self, x, attn_mask, tau, delta)\u001b[0m\n\u001b[0;32m     83\u001b[0m batch \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     84\u001b[0m time_in \u001b[38;5;241m=\u001b[39m rearrange(x, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb ts_d seg_num d_model -> (b ts_d) seg_num d_model\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 85\u001b[0m time_enc, attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtime_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtime_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtime_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtau\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[0;32m     87\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     88\u001b[0m dim_in \u001b[38;5;241m=\u001b[39m time_in \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(time_enc)\n\u001b[0;32m     89\u001b[0m dim_in \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(dim_in)\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[12], line 258\u001b[0m, in \u001b[0;36mAttentionLayer.forward\u001b[1;34m(self, queries, keys, values, attn_mask, tau, delta)\u001b[0m\n\u001b[0;32m    255\u001b[0m keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_projection(keys)\u001b[38;5;241m.\u001b[39mview(B, S, H, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    256\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_projection(values)\u001b[38;5;241m.\u001b[39mview(B, S, H, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 258\u001b[0m out, attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minner_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m    \u001b[49m\u001b[43mqueries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    260\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    261\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    262\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtau\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtau\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    264\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdelta\u001b[49m\n\u001b[0;32m    265\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    266\u001b[0m out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mview(B, L, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_projection(out), attn\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[12], line 304\u001b[0m, in \u001b[0;36mFullAttention.forward\u001b[1;34m(self, queries, keys, values, attn_mask, tau, delta)\u001b[0m\n\u001b[0;32m    301\u001b[0m     scores\u001b[38;5;241m.\u001b[39mmasked_fill_(attn_mask\u001b[38;5;241m.\u001b[39mmask, \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf)\n\u001b[0;32m    303\u001b[0m A \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(torch\u001b[38;5;241m.\u001b[39msoftmax(scale \u001b[38;5;241m*\u001b[39m scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m--> 304\u001b[0m V \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbhls,bshd->blhd\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_attention:\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m V\u001b[38;5;241m.\u001b[39mcontiguous(), A\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\functional.py:378\u001b[0m, in \u001b[0;36meinsum\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    373\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m einsum(equation, \u001b[38;5;241m*\u001b[39m_operands)\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(operands) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39menabled:\n\u001b[0;32m    376\u001b[0m     \u001b[38;5;66;03m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[0;32m    377\u001b[0m     \u001b[38;5;66;03m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[1;32m--> 378\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperands\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m    380\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39mis_available():\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 构造参数字典\n",
    "params3 = {\n",
    "    \"task_args\":{\n",
    "        \"columns\": ['load', 'temp'],\n",
    "        \"target\": ['load'],\n",
    "        \"features\": 'MS'\n",
    "    },\n",
    "    \"train_args\": {\n",
    "        \"model_name\": Crossformer,\n",
    "        \"train_loader\": train_loader,\n",
    "        \"valid_loader\": valid_loader,\n",
    "        \"n_epochs\": 20,\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"loss\": nn.MSELoss(),\n",
    "        \"patience\": 7,\n",
    "        \"lradj\": 'cosine',\n",
    "        \"path\": \"../models/test\",\n",
    "        \"verbose\": True,\n",
    "        \"plots\": True,\n",
    "    },\n",
    "    \"model_args\": {\n",
    "        'enc_in': 2, \n",
    "        'seq_len': 6,\n",
    "        'pred_len': 3,\n",
    "        'label_len': 0,\n",
    "        'output_attention': False,\n",
    "        'd_model': 32,\n",
    "        'n_heads': 8,\n",
    "        'd_ff': 32,\n",
    "        'dropout': 0.1,\n",
    "        'e_layers': 1,\n",
    "        'd_layers': 1,\n",
    "        'factor': 3\n",
    "    },\n",
    "}\n",
    "model = train(**params3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d795bcea",
   "metadata": {},
   "source": [
    "# 基于ETSformer的时间序列预测"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0b3b4c",
   "metadata": {},
   "source": [
    "## 多变量多步预测"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f458f952",
   "metadata": {},
   "source": [
    "### 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ea9a8c31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:17:14.770003Z",
     "start_time": "2024-04-14T13:17:14.751557Z"
    }
   },
   "outputs": [],
   "source": [
    "# 读取数据\n",
    "def get_dataset(path, time_col=None):\n",
    "    data = pd.read_csv(path)\n",
    "    rows = data.shape[0]\n",
    "    now = datetime.now()\n",
    "    newtime = now.replace(microsecond=0)\n",
    "    if time_col == None:\n",
    "        # 如果没有时间列，生成时间戳范围\n",
    "        time_index = pd.date_range(start=datetime.now() -\n",
    "                                   timedelta(seconds=rows - 1),\n",
    "                                   end=datetime.now(),\n",
    "                                   freq='S')\n",
    "        full_data = pd.DataFrame(data=data.values,\n",
    "                                 index=pd.to_datetime(time_index, unit='s'),\n",
    "                                 columns=data.columns)\n",
    "    else:\n",
    "        columns = [i for i in data.columns if i != time_col] # 去除时间列\n",
    "        full_data = pd.DataFrame(data=data.drop([time_col], axis=1).values,\n",
    "                                 index=pd.to_datetime(data[time_col].values),\n",
    "                                 columns=columns)\n",
    "    return full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f2b06190",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:17:15.580590Z",
     "start_time": "2024-04-14T13:17:15.513934Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>load</th>\n",
       "      <th>temp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-01-01 00:00:00</th>\n",
       "      <td>2698.0</td>\n",
       "      <td>32.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-01 01:00:00</th>\n",
       "      <td>2558.0</td>\n",
       "      <td>32.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-01 02:00:00</th>\n",
       "      <td>2444.0</td>\n",
       "      <td>30.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-01 03:00:00</th>\n",
       "      <td>2402.0</td>\n",
       "      <td>31.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-01 04:00:00</th>\n",
       "      <td>2403.0</td>\n",
       "      <td>32.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-12-31 19:00:00</th>\n",
       "      <td>4012.0</td>\n",
       "      <td>18.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-12-31 20:00:00</th>\n",
       "      <td>3856.0</td>\n",
       "      <td>16.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-12-31 21:00:00</th>\n",
       "      <td>3671.0</td>\n",
       "      <td>17.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-12-31 22:00:00</th>\n",
       "      <td>3499.0</td>\n",
       "      <td>15.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-12-31 23:00:00</th>\n",
       "      <td>3345.0</td>\n",
       "      <td>15.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26304 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       load   temp\n",
       "2012-01-01 00:00:00  2698.0  32.00\n",
       "2012-01-01 01:00:00  2558.0  32.67\n",
       "2012-01-01 02:00:00  2444.0  30.00\n",
       "2012-01-01 03:00:00  2402.0  31.00\n",
       "2012-01-01 04:00:00  2403.0  32.00\n",
       "...                     ...    ...\n",
       "2014-12-31 19:00:00  4012.0  18.00\n",
       "2014-12-31 20:00:00  3856.0  16.67\n",
       "2014-12-31 21:00:00  3671.0  17.00\n",
       "2014-12-31 22:00:00  3499.0  15.33\n",
       "2014-12-31 23:00:00  3345.0  15.33\n",
       "\n",
       "[26304 rows x 2 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"../data/energy.csv\"\n",
    "ts_data = get_dataset(path, time_col='time')\n",
    "ts_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7d134ab8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:17:16.764466Z",
     "start_time": "2024-04-14T13:17:16.723570Z"
    }
   },
   "outputs": [],
   "source": [
    "# 时间格式编码\n",
    "def time_features_from_frequency_str(freq_str: str):\n",
    "    \"\"\"\n",
    "    Returns a list of time features that will be appropriate for the given frequency string.\n",
    "    Parameters\n",
    "    ----------\n",
    "    freq_str\n",
    "        Frequency string of the form [multiple][granularity] such as \"12H\", \"5min\", \"1D\" etc.\n",
    "    \"\"\"\n",
    "\n",
    "    class TimeFeature:\n",
    "        def __init__(self):\n",
    "            pass\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            pass\n",
    "\n",
    "        def __repr__(self):\n",
    "            return self.__class__.__name__ + \"()\"\n",
    "\n",
    "\n",
    "    class SecondOfMinute(TimeFeature):\n",
    "        \"\"\"Minute of hour encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return index.second / 59.0 - 0.5\n",
    "\n",
    "\n",
    "    class MinuteOfHour(TimeFeature):\n",
    "        \"\"\"Minute of hour encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return index.minute / 59.0 - 0.5\n",
    "\n",
    "\n",
    "    class HourOfDay(TimeFeature):\n",
    "        \"\"\"Hour of day encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return index.hour / 23.0 - 0.5\n",
    "\n",
    "\n",
    "    class DayOfWeek(TimeFeature):\n",
    "        \"\"\"Hour of day encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return index.dayofweek / 6.0 - 0.5\n",
    "\n",
    "\n",
    "    class DayOfMonth(TimeFeature):\n",
    "        \"\"\"Day of month encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return (index.day - 1) / 30.0 - 0.5\n",
    "\n",
    "\n",
    "    class DayOfYear(TimeFeature):\n",
    "        \"\"\"Day of year encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return (index.dayofyear - 1) / 365.0 - 0.5\n",
    "\n",
    "\n",
    "    class MonthOfYear(TimeFeature):\n",
    "        \"\"\"Month of year encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return (index.month - 1) / 11.0 - 0.5\n",
    "\n",
    "\n",
    "    class WeekOfYear(TimeFeature):\n",
    "        \"\"\"Week of year encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return (index.isocalendar().week - 1) / 52.0 - 0.5\n",
    "\n",
    "    \n",
    "    features_by_offsets = {\n",
    "        offsets.YearEnd: [],\n",
    "        offsets.QuarterEnd: [MonthOfYear],\n",
    "        offsets.MonthEnd: [MonthOfYear],\n",
    "        offsets.Week: [DayOfMonth, WeekOfYear],\n",
    "        offsets.Day: [DayOfWeek, DayOfMonth, DayOfYear],\n",
    "        offsets.BusinessDay: [DayOfWeek, DayOfMonth, DayOfYear],\n",
    "        offsets.Hour: [HourOfDay, DayOfWeek, DayOfMonth, DayOfYear],\n",
    "        offsets.Minute: [\n",
    "            MinuteOfHour,\n",
    "            HourOfDay,\n",
    "            DayOfWeek,\n",
    "            DayOfMonth,\n",
    "            DayOfYear,\n",
    "        ],\n",
    "        offsets.Second: [\n",
    "            SecondOfMinute,\n",
    "            MinuteOfHour,\n",
    "            HourOfDay,\n",
    "            DayOfWeek,\n",
    "            DayOfMonth,\n",
    "            DayOfYear,\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    offset = to_offset(freq_str)\n",
    "\n",
    "    for offset_type, feature_classes in features_by_offsets.items():\n",
    "        if isinstance(offset, offset_type):\n",
    "            return [cls() for cls in feature_classes]\n",
    "\n",
    "    supported_freq_msg = f\"\"\"\n",
    "    Unsupported frequency {freq_str}\n",
    "    The following frequencies are supported:\n",
    "        Y   - yearly\n",
    "            alias: A\n",
    "        M   - monthly\n",
    "        W   - weekly\n",
    "        D   - daily\n",
    "        B   - business days\n",
    "        H   - hourly\n",
    "        T   - minutely\n",
    "            alias: min\n",
    "        S   - secondly\n",
    "    \"\"\"\n",
    "    raise RuntimeError(supported_freq_msg)\n",
    "    \n",
    "def time_features(dates, freq='h'):\n",
    "    return np.vstack([feat(dates) for feat in time_features_from_frequency_str(freq)])\n",
    "\n",
    "# 包含时间维度的数据集划分\n",
    "def divide_dataset(df, valid_date, test_date, x_feature_list, y_feature_list, freq):\n",
    "    #归一化\n",
    "    x_scaler = MinMaxScaler()\n",
    "    y_scaler = MinMaxScaler()\n",
    "\n",
    "    #测试集\n",
    "    train = df.copy()[df.index < valid_date][x_feature_list]\n",
    "    train_stamp = time_features(pd.to_datetime(train.index), freq=freq)\n",
    "    train_stamp = train_stamp.transpose(1, 0)\n",
    "    train[x_feature_list] = x_scaler.fit_transform(train)\n",
    "    xtr = train.values.astype('float32')\n",
    "\n",
    "    #验证集\n",
    "    valid = df.copy()[(df.index >= valid_date) & (df.index < test_date)][x_feature_list]\n",
    "    valid_stamp = time_features(pd.to_datetime(valid.index), freq=freq)\n",
    "    valid_stamp = valid_stamp.transpose(1, 0)\n",
    "    valid[x_feature_list] = x_scaler.fit_transform(valid)\n",
    "    xva = valid.values.astype('float32')\n",
    "\n",
    "    #测试集\n",
    "    test = df.copy()[test_date:][x_feature_list]\n",
    "    test_stamp = time_features(pd.to_datetime(test.index), freq=freq)\n",
    "    test_stamp = test_stamp.transpose(1, 0)\n",
    "    test[x_feature_list] = x_scaler.fit_transform(test)\n",
    "    xte = test.values.astype('float32')\n",
    "\n",
    "    #标签\n",
    "    ytr = df.copy()[df.index < valid_date][y_feature_list]\n",
    "    ytr[y_feature_list] = y_scaler.fit_transform(ytr)\n",
    "    ytr = ytr.values.astype('float32')\n",
    "\n",
    "    yva = df.copy()[(df.index >= valid_date) & (df.index < test_date)][y_feature_list]\n",
    "    yva[y_feature_list] = y_scaler.fit_transform(yva)\n",
    "    yva = yva.values.astype('float32')\n",
    "\n",
    "    yte = df.copy()[test_date:][y_feature_list]\n",
    "    yte[y_feature_list] = y_scaler.fit_transform(yte)\n",
    "    yte = yte.values.astype('float32')\n",
    "    \n",
    "    # 数据合并，[训练集，测试集，时间戳]\n",
    "    train = [xtr, ytr, train_stamp]\n",
    "    valid = [xva, yva, valid_stamp]\n",
    "    test = [xte, yte, test_stamp]\n",
    "    \n",
    "    return y_scaler, train, valid, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a5e38278",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:17:17.829968Z",
     "start_time": "2024-04-14T13:17:17.755630Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (23376, 2) y_train shape: (23376, 1) stamp_train shape: (23376, 4)\n",
      "x_valid shape: (1464, 2) y_valid shape: (1464, 1) stamp_valid shape: (1464, 4)\n",
      "x_test shape: (1464, 2) y_test shape: (1464, 1) stamp_test shape: (1464, 4)\n"
     ]
    }
   ],
   "source": [
    "# 构造参数字典\n",
    "params1 = {\n",
    "    \"df\": ts_data,\n",
    "    \"valid_date\": \"2014-09-01 00:00:00\",\n",
    "    \"test_date\": \"2014-11-01 00:00:00\",\n",
    "    \"x_feature_list\": [\"load\", 'temp'],\n",
    "    \"y_feature_list\": [\"load\"],\n",
    "    \"freq\": 'h'\n",
    "}\n",
    "\n",
    "#函数传参\n",
    "scaler, train, valid, test = divide_dataset(**params1)\n",
    "print(\"x_train shape: {0} y_train shape: {1} stamp_train shape: {2}\".format(train[0].shape, train[1].shape, train[2].shape))\n",
    "print(\"x_valid shape: {0} y_valid shape: {1} stamp_valid shape: {2}\".format(valid[0].shape, valid[1].shape, valid[2].shape))\n",
    "print(\"x_test shape: {0} y_test shape: {1} stamp_test shape: {2}\".format(test[0].shape, test[1].shape, test[2].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8620e43b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:17:19.221166Z",
     "start_time": "2024-04-14T13:17:19.200105Z"
    }
   },
   "outputs": [],
   "source": [
    "#利用前seq_len个数据，预测下pred_len个数据\n",
    "def create_dataset(data_list, seq_len, pred_len, label_len, batch_size):\n",
    "    \"\"\"\n",
    "    @参数设置：\n",
    "    data_list：特征，目标，时间戳\n",
    "    seq_len：输入数据包含过去多少个时间步\n",
    "    pred_len：目标应该在未来多少个时间步之后\n",
    "    label_len：先验时间步\n",
    "    \"\"\"\n",
    "    # 获取数据\n",
    "    feature = data_list[0] # 特征\n",
    "    target = data_list[1] # 目标\n",
    "    stamp = data_list[2] # 时间戳\n",
    "    \n",
    "    # 循环生成数据\n",
    "    X, y = [], []\n",
    "    X_stamp, y_stamp = [], []\n",
    "    seq_len = seq_len - 1 # 包含当前时间点\n",
    "    for i in range(seq_len, len(feature) - pred_len):\n",
    "        # 数据维度\n",
    "        feat = feature[i - seq_len:i + 1]\n",
    "        tar = target[i + 1:i + 1 + pred_len]\n",
    "        X.append(feat)\n",
    "        y.append(tar)\n",
    "        \n",
    "        # 时间维度\n",
    "        xs = stamp[i - seq_len:i + 1]\n",
    "        ys = stamp[i + 1 - label_len:i + 1 + pred_len]\n",
    "        X_stamp.append(xs)\n",
    "        y_stamp.append(ys)\n",
    "        \n",
    "    # 转为张量，数据维度\n",
    "    X = torch.as_tensor(X).float()\n",
    "    y = torch.as_tensor(y).float()\n",
    "    \n",
    "    # 转为张量，时间维度\n",
    "    X_stamp = torch.as_tensor(X_stamp).float()\n",
    "    y_stamp = torch.as_tensor(y_stamp).float()\n",
    "    \n",
    "    # 创建dataloader，[特征，目标，特征时间编码，目标时间编码]\n",
    "    data_loader = DataLoader(TensorDataset(X, y, X_stamp, y_stamp), shuffle=True, batch_size=batch_size)\n",
    "    \n",
    "    return X, y, data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cd9450fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:17:20.854727Z",
     "start_time": "2024-04-14T13:17:20.005614Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_size: torch.Size([23368, 6, 2]),y_size: torch.Size([23368, 3, 1]),loader_len: 731\n",
      "X_size: torch.Size([1456, 6, 2]),y_size: torch.Size([1456, 3, 1]),loader_len: 46\n",
      "X_size: torch.Size([1456, 6, 2]),y_size: torch.Size([1456, 3, 1]),loader_len: 46\n"
     ]
    }
   ],
   "source": [
    "# 构造参数字典\n",
    "params2 = {\n",
    "    \"seq_len\": 6,\n",
    "    \"pred_len\": 3,\n",
    "    \"label_len\": 0,\n",
    "    \"batch_size\": 32,\n",
    "}\n",
    "X_train, y_train, train_loader = create_dataset(train, **params2)\n",
    "X_valid, y_valid, valid_loader = create_dataset(valid, **params2)\n",
    "X_test, y_test, test_loader = create_dataset(test, **params2)\n",
    "print(\"X_size: {0},y_size: {1},loader_len: {2}\".format(X_train.shape, y_train.shape, len(train_loader)))\n",
    "print(\"X_size: {0},y_size: {1},loader_len: {2}\".format(X_valid.shape, y_valid.shape, len(valid_loader)))\n",
    "print(\"X_size: {0},y_size: {1},loader_len: {2}\".format(X_test.shape, y_test.shape, len(test_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51480be9",
   "metadata": {},
   "source": [
    "### 模型定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "86260892",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:20:03.743409Z",
     "start_time": "2024-04-14T13:20:03.647178Z"
    }
   },
   "outputs": [],
   "source": [
    "# DataEmbedding编码类\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model).float()\n",
    "        pe.require_grad = False\n",
    "\n",
    "        position = torch.arange(0, max_len).float().unsqueeze(1)\n",
    "        div_term = (torch.arange(0, d_model, 2).float()\n",
    "                    * -(math.log(10000.0) / d_model)).exp()\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pe[:, :x.size(1)]\n",
    "\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, c_in, d_model):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        padding = 1 if torch.__version__ >= '1.5.0' else 2\n",
    "        self.tokenConv = nn.Conv1d(in_channels=c_in, out_channels=d_model,\n",
    "                                   kernel_size=3, padding=padding, padding_mode='circular', bias=False)\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                nn.init.kaiming_normal_(\n",
    "                    m.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.tokenConv(x.permute(0, 2, 1)).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class FixedEmbedding(nn.Module):\n",
    "    def __init__(self, c_in, d_model):\n",
    "        super(FixedEmbedding, self).__init__()\n",
    "\n",
    "        w = torch.zeros(c_in, d_model).float()\n",
    "        w.require_grad = False\n",
    "\n",
    "        position = torch.arange(0, c_in).float().unsqueeze(1)\n",
    "        div_term = (torch.arange(0, d_model, 2).float()\n",
    "                    * -(math.log(10000.0) / d_model)).exp()\n",
    "\n",
    "        w[:, 0::2] = torch.sin(position * div_term)\n",
    "        w[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        self.emb = nn.Embedding(c_in, d_model)\n",
    "        self.emb.weight = nn.Parameter(w, requires_grad=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.emb(x).detach()\n",
    "\n",
    "\n",
    "class TemporalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, embed_type='fixed', freq='h'):\n",
    "        super(TemporalEmbedding, self).__init__()\n",
    "\n",
    "        minute_size = 4\n",
    "        hour_size = 24\n",
    "        weekday_size = 7\n",
    "        day_size = 32\n",
    "        month_size = 13\n",
    "\n",
    "        Embed = FixedEmbedding if embed_type == 'fixed' else nn.Embedding\n",
    "        if freq == 't':\n",
    "            self.minute_embed = Embed(minute_size, d_model)\n",
    "        self.hour_embed = Embed(hour_size, d_model)\n",
    "        self.weekday_embed = Embed(weekday_size, d_model)\n",
    "        self.day_embed = Embed(day_size, d_model)\n",
    "        self.month_embed = Embed(month_size, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.long()\n",
    "        minute_x = self.minute_embed(x[:, :, 4]) if hasattr(\n",
    "            self, 'minute_embed') else 0.\n",
    "        hour_x = self.hour_embed(x[:, :, 3])\n",
    "        weekday_x = self.weekday_embed(x[:, :, 2])\n",
    "        day_x = self.day_embed(x[:, :, 1])\n",
    "        month_x = self.month_embed(x[:, :, 0])\n",
    "\n",
    "        return hour_x + weekday_x + day_x + month_x + minute_x\n",
    "\n",
    "\n",
    "class TimeFeatureEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, embed_type='timeF', freq='h'):\n",
    "        super(TimeFeatureEmbedding, self).__init__()\n",
    "\n",
    "        freq_map = {'h': 4, 't': 5, 's': 6,\n",
    "                    'm': 1, 'a': 1, 'w': 2, 'd': 3, 'b': 3}\n",
    "        d_inp = freq_map[freq]\n",
    "        self.embed = nn.Linear(d_inp, d_model, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embed(x)\n",
    "\n",
    "\n",
    "class DataEmbedding(nn.Module):\n",
    "    def __init__(self, c_in, d_model, embed_type='fixed', freq='h', dropout=0.1):\n",
    "        super(DataEmbedding, self).__init__()\n",
    "\n",
    "        self.value_embedding = TokenEmbedding(c_in=c_in, d_model=d_model)\n",
    "        self.position_embedding = PositionalEmbedding(d_model=d_model)\n",
    "        self.temporal_embedding = TemporalEmbedding(d_model=d_model, embed_type=embed_type,\n",
    "                                                    freq=freq) if embed_type != 'timeF' else TimeFeatureEmbedding(\n",
    "            d_model=d_model, embed_type=embed_type, freq=freq)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, x_mark):\n",
    "        if x_mark is None:\n",
    "            x = self.value_embedding(x) + self.position_embedding(x)\n",
    "        else:\n",
    "            x = self.value_embedding(\n",
    "                x) + self.temporal_embedding(x_mark) + self.position_embedding(x)\n",
    "        return self.dropout(x)\n",
    "    \n",
    "    \n",
    "# 编码器和解码器\n",
    "class Transform:\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def transform(self, x):\n",
    "        return self.jitter(self.shift(self.scale(x)))\n",
    "\n",
    "    def jitter(self, x):\n",
    "        return x + (torch.randn(x.shape).to(x.device) * self.sigma)\n",
    "\n",
    "    def scale(self, x):\n",
    "        return x * (torch.randn(x.size(-1)).to(x.device) * self.sigma + 1)\n",
    "\n",
    "    def shift(self, x):\n",
    "        return x + (torch.randn(x.size(-1)).to(x.device) * self.sigma)\n",
    "\n",
    "\n",
    "def conv1d_fft(f, g, dim=-1):\n",
    "    N = f.size(dim)\n",
    "    M = g.size(dim)\n",
    "\n",
    "    fast_len = next_fast_len(N + M - 1)\n",
    "\n",
    "    F_f = fft.rfft(f, fast_len, dim=dim)\n",
    "    F_g = fft.rfft(g, fast_len, dim=dim)\n",
    "\n",
    "    F_fg = F_f * F_g.conj()\n",
    "    out = fft.irfft(F_fg, fast_len, dim=dim)\n",
    "    out = out.roll((-1,), dims=(dim,))\n",
    "    idx = torch.as_tensor(range(fast_len - N, fast_len)).to(out.device)\n",
    "    out = out.index_select(dim, idx)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "class ExponentialSmoothing(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, nhead, dropout=0.1, aux=False):\n",
    "        super().__init__()\n",
    "        self._smoothing_weight = nn.Parameter(torch.randn(nhead, 1))\n",
    "        self.v0 = nn.Parameter(torch.randn(1, 1, nhead, dim))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        if aux:\n",
    "            self.aux_dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, values, aux_values=None):\n",
    "        b, t, h, d = values.shape\n",
    "\n",
    "        init_weight, weight = self.get_exponential_weight(t)\n",
    "        output = conv1d_fft(self.dropout(values), weight, dim=1)\n",
    "        output = init_weight * self.v0 + output\n",
    "\n",
    "        if aux_values is not None:\n",
    "            aux_weight = weight / (1 - self.weight) * self.weight\n",
    "            aux_output = conv1d_fft(self.aux_dropout(aux_values), aux_weight)\n",
    "            output = output + aux_output\n",
    "\n",
    "        return output\n",
    "\n",
    "    def get_exponential_weight(self, T):\n",
    "        # Generate array [0, 1, ..., T-1]\n",
    "        powers = torch.arange(T, dtype=torch.float, device=self.weight.device)\n",
    "\n",
    "        # (1 - \\alpha) * \\alpha^t, for all t = T-1, T-2, ..., 0]\n",
    "        weight = (1 - self.weight) * (self.weight ** torch.flip(powers, dims=(0,)))\n",
    "\n",
    "        # \\alpha^t for all t = 1, 2, ..., T\n",
    "        init_weight = self.weight ** (powers + 1)\n",
    "\n",
    "        return rearrange(init_weight, 'h t -> 1 t h 1'), \\\n",
    "               rearrange(weight, 'h t -> 1 t h 1')\n",
    "\n",
    "    @property\n",
    "    def weight(self):\n",
    "        return torch.sigmoid(self._smoothing_weight)\n",
    "\n",
    "\n",
    "class Feedforward(nn.Module):\n",
    "    def __init__(self, d_model, dim_feedforward, dropout=0.1, activation='sigmoid'):\n",
    "        # Implementation of Feedforward model\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward, bias=False)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model, bias=False)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.activation = getattr(F, activation)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear2(self.dropout1(self.activation(self.linear1(x))))\n",
    "        return self.dropout2(x)\n",
    "\n",
    "\n",
    "class GrowthLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, nhead, d_head=None, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_head = d_head or (d_model // nhead)\n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "\n",
    "        self.z0 = nn.Parameter(torch.randn(self.nhead, self.d_head))\n",
    "        self.in_proj = nn.Linear(self.d_model, self.d_head * self.nhead)\n",
    "        self.es = ExponentialSmoothing(self.d_head, self.nhead, dropout=dropout)\n",
    "        self.out_proj = nn.Linear(self.d_head * self.nhead, self.d_model)\n",
    "\n",
    "        assert self.d_head * self.nhead == self.d_model, \"d_model must be divisible by nhead\"\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        :param inputs: shape: (batch, seq_len, dim)\n",
    "        :return: shape: (batch, seq_len, dim)\n",
    "        \"\"\"\n",
    "        b, t, d = inputs.shape\n",
    "        values = self.in_proj(inputs).view(b, t, self.nhead, -1)\n",
    "        values = torch.cat([repeat(self.z0, 'h d -> b 1 h d', b=b), values], dim=1)\n",
    "        values = values[:, 1:] - values[:, :-1]\n",
    "        out = self.es(values)\n",
    "        out = torch.cat([repeat(self.es.v0, '1 1 h d -> b 1 h d', b=b), out], dim=1)\n",
    "        out = rearrange(out, 'b t h d -> b t (h d)')\n",
    "        return self.out_proj(out)\n",
    "\n",
    "\n",
    "class FourierLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, pred_len, k=None, low_freq=1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.pred_len = pred_len\n",
    "        self.k = k\n",
    "        self.low_freq = low_freq\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"x: (b, t, d)\"\"\"\n",
    "        b, t, d = x.shape\n",
    "        x_freq = fft.rfft(x, dim=1)\n",
    "\n",
    "        if t % 2 == 0:\n",
    "            x_freq = x_freq[:, self.low_freq:-1]\n",
    "            f = fft.rfftfreq(t)[self.low_freq:-1]\n",
    "        else:\n",
    "            x_freq = x_freq[:, self.low_freq:]\n",
    "            f = fft.rfftfreq(t)[self.low_freq:]\n",
    "\n",
    "        x_freq, index_tuple = self.topk_freq(x_freq)\n",
    "        f = repeat(f, 'f -> b f d', b=x_freq.size(0), d=x_freq.size(2)).to(x_freq.device)\n",
    "        f = rearrange(f[index_tuple], 'b f d -> b f () d').to(x_freq.device)\n",
    "\n",
    "        return self.extrapolate(x_freq, f, t)\n",
    "\n",
    "    def extrapolate(self, x_freq, f, t):\n",
    "        x_freq = torch.cat([x_freq, x_freq.conj()], dim=1)\n",
    "        f = torch.cat([f, -f], dim=1)\n",
    "        t_val = rearrange(torch.arange(t + self.pred_len, dtype=torch.float),\n",
    "                          't -> () () t ()').to(x_freq.device)\n",
    "\n",
    "        amp = rearrange(x_freq.abs() / t, 'b f d -> b f () d')\n",
    "        phase = rearrange(x_freq.angle(), 'b f d -> b f () d')\n",
    "\n",
    "        x_time = amp * torch.cos(2 * math.pi * f * t_val + phase)\n",
    "\n",
    "        return reduce(x_time, 'b f t d -> b t d', 'sum')\n",
    "\n",
    "    def topk_freq(self, x_freq):\n",
    "        values, indices = torch.topk(x_freq.abs(), self.k, dim=1, largest=True, sorted=True)\n",
    "        mesh_a, mesh_b = torch.meshgrid(torch.arange(x_freq.size(0)), torch.arange(x_freq.size(2)))\n",
    "        index_tuple = (mesh_a.unsqueeze(1), indices, mesh_b.unsqueeze(1))\n",
    "        x_freq = x_freq[index_tuple]\n",
    "\n",
    "        return x_freq, index_tuple\n",
    "\n",
    "\n",
    "class LevelLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, c_out, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.c_out = c_out\n",
    "\n",
    "        self.es = ExponentialSmoothing(1, self.c_out, dropout=dropout, aux=True)\n",
    "        self.growth_pred = nn.Linear(self.d_model, self.c_out)\n",
    "        self.season_pred = nn.Linear(self.d_model, self.c_out)\n",
    "\n",
    "    def forward(self, level, growth, season):\n",
    "        b, t, _ = level.shape\n",
    "        growth = self.growth_pred(growth).view(b, t, self.c_out, 1)\n",
    "        season = self.season_pred(season).view(b, t, self.c_out, 1)\n",
    "        growth = growth.view(b, t, self.c_out, 1)\n",
    "        season = season.view(b, t, self.c_out, 1)\n",
    "        level = level.view(b, t, self.c_out, 1)\n",
    "        out = self.es(level - season, aux_values=growth)\n",
    "        out = rearrange(out, 'b t h d -> b t (h d)')\n",
    "        return out\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, nhead, c_out, seq_len, pred_len, k, dim_feedforward=None, dropout=0.1,\n",
    "                 activation='sigmoid', layer_norm_eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "        self.c_out = c_out\n",
    "        self.seq_len = seq_len\n",
    "        self.pred_len = pred_len\n",
    "        dim_feedforward = dim_feedforward or 4 * d_model\n",
    "        self.dim_feedforward = dim_feedforward\n",
    "\n",
    "        self.growth_layer = GrowthLayer(d_model, nhead, dropout=dropout)\n",
    "        self.seasonal_layer = FourierLayer(d_model, pred_len, k=k)\n",
    "        self.level_layer = LevelLayer(d_model, c_out, dropout=dropout)\n",
    "\n",
    "        # Implementation of Feedforward model\n",
    "        self.ff = Feedforward(d_model, dim_feedforward, dropout=dropout, activation=activation)\n",
    "        self.norm1 = nn.LayerNorm(d_model, eps=layer_norm_eps)\n",
    "        self.norm2 = nn.LayerNorm(d_model, eps=layer_norm_eps)\n",
    "\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, res, level, attn_mask=None):\n",
    "        season = self._season_block(res)\n",
    "        res = res - season[:, :-self.pred_len]\n",
    "        growth = self._growth_block(res)\n",
    "        res = self.norm1(res - growth[:, 1:])\n",
    "        res = self.norm2(res + self.ff(res))\n",
    "\n",
    "        level = self.level_layer(level, growth[:, :-1], season[:, :-self.pred_len])\n",
    "        return res, level, growth, season\n",
    "\n",
    "    def _growth_block(self, x):\n",
    "        x = self.growth_layer(x)\n",
    "        return self.dropout1(x)\n",
    "\n",
    "    def _season_block(self, x):\n",
    "        x = self.seasonal_layer(x)\n",
    "        return self.dropout2(x)\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, layers):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, res, level, attn_mask=None):\n",
    "        growths = []\n",
    "        seasons = []\n",
    "        for layer in self.layers:\n",
    "            res, level, growth, season = layer(res, level, attn_mask=None)\n",
    "            growths.append(growth)\n",
    "            seasons.append(season)\n",
    "\n",
    "        return level, growths, seasons\n",
    "\n",
    "\n",
    "class DampingLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, pred_len, nhead, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.pred_len = pred_len\n",
    "        self.nhead = nhead\n",
    "        self._damping_factor = nn.Parameter(torch.randn(1, nhead))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = repeat(x, 'b 1 d -> b t d', t=self.pred_len)\n",
    "        b, t, d = x.shape\n",
    "\n",
    "        powers = torch.arange(self.pred_len).to(self._damping_factor.device) + 1\n",
    "        powers = powers.view(self.pred_len, 1)\n",
    "        damping_factors = self.damping_factor ** powers\n",
    "        damping_factors = damping_factors.cumsum(dim=0)\n",
    "        x = x.view(b, t, self.nhead, -1)\n",
    "        x = self.dropout(x) * damping_factors.unsqueeze(-1)\n",
    "        return x.view(b, t, d)\n",
    "\n",
    "    @property\n",
    "    def damping_factor(self):\n",
    "        return torch.sigmoid(self._damping_factor)\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, nhead, c_out, pred_len, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.nhead = nhead\n",
    "        self.c_out = c_out\n",
    "        self.pred_len = pred_len\n",
    "\n",
    "        self.growth_damping = DampingLayer(pred_len, nhead, dropout=dropout)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, growth, season):\n",
    "        growth_horizon = self.growth_damping(growth[:, -1:])\n",
    "        growth_horizon = self.dropout1(growth_horizon)\n",
    "\n",
    "        seasonal_horizon = season[:, -self.pred_len:]\n",
    "        return growth_horizon, seasonal_horizon\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, layers):\n",
    "        super().__init__()\n",
    "        self.d_model = layers[0].d_model\n",
    "        self.c_out = layers[0].c_out\n",
    "        self.pred_len = layers[0].pred_len\n",
    "        self.nhead = layers[0].nhead\n",
    "\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.pred = nn.Linear(self.d_model, self.c_out)\n",
    "\n",
    "    def forward(self, growths, seasons):\n",
    "        growth_repr = []\n",
    "        season_repr = []\n",
    "\n",
    "        for idx, layer in enumerate(self.layers):\n",
    "            growth_horizon, season_horizon = layer(growths[idx], seasons[idx])\n",
    "            growth_repr.append(growth_horizon)\n",
    "            season_repr.append(season_horizon)\n",
    "        growth_repr = sum(growth_repr)\n",
    "        season_repr = sum(season_repr)\n",
    "        return self.pred(growth_repr), self.pred(season_repr)\n",
    "\n",
    "    \n",
    "# ETSformer模型\n",
    "class ETSformer(nn.Module):\n",
    "    def __init__(self, seq_len, pred_len, label_len, e_layers, d_layers, enc_in, d_model, \n",
    "                 dropout, n_heads, top_k, d_ff, c_out, embed, freq):\n",
    "        super(ETSformer, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.pred_len = pred_len\n",
    "\n",
    "        assert e_layers == d_layers, \"Encoder and decoder layers must be equal\"\n",
    "\n",
    "        # Embedding\n",
    "        self.enc_embedding = DataEmbedding(enc_in, d_model, embed, freq, dropout)\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = Encoder(\n",
    "            [\n",
    "                EncoderLayer(\n",
    "                    d_model, n_heads, enc_in, seq_len, pred_len, top_k,\n",
    "                    dim_feedforward=d_ff,\n",
    "                    dropout=dropout,\n",
    "                    activation='relu',\n",
    "                ) for _ in range(e_layers)\n",
    "            ]\n",
    "        )\n",
    "        # Decoder\n",
    "        self.decoder = Decoder(\n",
    "            [\n",
    "                DecoderLayer(\n",
    "                    d_model, n_heads, c_out, pred_len,\n",
    "                    dropout=dropout,\n",
    "                ) for _ in range(d_layers)\n",
    "            ],\n",
    "        )\n",
    "        self.transform = Transform(sigma=0.2)\n",
    "\n",
    "    def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec):\n",
    "        with torch.no_grad():\n",
    "            if self.training:\n",
    "                x_enc = self.transform.transform(x_enc)\n",
    "        res = self.enc_embedding(x_enc, x_mark_enc)\n",
    "        level, growths, seasons = self.encoder(res, x_enc, attn_mask=None)\n",
    "\n",
    "        growth, season = self.decoder(growths, seasons)\n",
    "        dec_out = level[:, -1:] + growth + season\n",
    "        \n",
    "        output = dec_out[:, -self.pred_len:, :]  # [B, L, D]\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e272698",
   "metadata": {},
   "source": [
    "### 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "829705a4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:20:06.029678Z",
     "start_time": "2024-04-14T13:20:05.986678Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(task_args, train_args, model_args):\n",
    "    # 参数配置\n",
    "    columns = task_args['columns'] # 模型全部特征\n",
    "    target = task_args['target'] # 模型预测特征\n",
    "    features = task_args['features'] # 模型预测模式\n",
    "    model_name = train_args['model_name'] # 模型名称\n",
    "    train_loader = train_args['train_loader'] # 训练集\n",
    "    valid_loader = train_args['valid_loader'] # 验证集\n",
    "    n_epochs = train_args['n_epochs'] # 训练次数\n",
    "    learning_rate = train_args['learning_rate'] # 学习率\n",
    "    loss = train_args['loss'] # 损失函数\n",
    "    patience = train_args['patience'] # 最大早停次数阈值，超过就会早停\n",
    "    lradj = train_args['lradj'] # 学习率函数\n",
    "    path = train_args['path'] # 模型保存路径\n",
    "    verbose = train_args['verbose'] # 打印训练过程\n",
    "    plots = train_args['plots'] # 绘制损失图\n",
    "    pred_len = model_args['pred_len'] # 预测长度\n",
    "    label_len = model_args['label_len']\n",
    "    \n",
    "    #检查是否可用GPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # 创建模型和优化器\n",
    "    model = model_name(**model_args).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_fn = loss\n",
    "    \n",
    "    # 损失函数值\n",
    "    train_losses, val_losses = [], []\n",
    "    \n",
    "    # 调整学习率\n",
    "    def adjust_learning_rate(optimizer, epoch, lradj, learning_rate, train_epochs):\n",
    "        # lr = learning_rate * (0.2 ** (epoch // 2))\n",
    "        if lradj == 'type1':\n",
    "            lr_adjust = {epoch: learning_rate * (0.5 ** ((epoch - 1) // 1))}\n",
    "        elif lradj == 'type2':\n",
    "            lr_adjust = {\n",
    "                2: 5e-5, 4: 1e-5, 6: 5e-6, 8: 1e-6,\n",
    "                10: 5e-7, 15: 1e-7, 20: 5e-8\n",
    "            }\n",
    "        elif lradj == \"cosine\":\n",
    "            lr_adjust = {epoch: learning_rate /2 * (1 + math.cos(epoch / train_epochs * math.pi))}\n",
    "        if epoch in lr_adjust.keys():\n",
    "            lr = lr_adjust[epoch]\n",
    "            # 参数组(param_groups)是用来指定不同的参数组以便对它们进行不同的优化设置，比如'lr'\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "            print('Updating learning rate to {}'.format(lr))\n",
    "    \n",
    "    # 设置早停\n",
    "    class EarlyStopping():\n",
    "        def __init__(self, patience=7, verbose=False, delta=0):\n",
    "            self.patience = patience # 连续超限次数，如果满足条件，则早停\n",
    "            self.verbose = verbose\n",
    "            self.counter = 0\n",
    "            self.best_score = None\n",
    "            self.early_stop = False\n",
    "            self.val_loss_min = np.Inf\n",
    "            self.delta = delta\n",
    "\n",
    "        def __call__(self, val_loss, model, path):\n",
    "            score = -val_loss\n",
    "            if self.best_score is None:\n",
    "                self.best_score = score\n",
    "                self.save_checkpoint(val_loss, model, path)\n",
    "            elif score < self.best_score + self.delta:\n",
    "                self.counter += 1\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "                if self.counter >= self.patience:\n",
    "                    self.early_stop = True\n",
    "            else:\n",
    "                self.best_score = score\n",
    "                self.save_checkpoint(val_loss, model, path)\n",
    "                self.counter = 0\n",
    "\n",
    "        def save_checkpoint(self, val_loss, model, path):\n",
    "            if self.verbose:\n",
    "                print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "            torch.save(model.state_dict(), path + '/' + 'checkpoint.pth')\n",
    "            self.val_loss_min = val_loss\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=verbose)\n",
    "    \n",
    "    # 训练任务\n",
    "    def forecasting_task(columns, target, features):\n",
    "        \"\"\"\n",
    "        features: [M, S, MS]; \n",
    "            M:multivariate predict multivariate, \n",
    "            S:univariate predict univariate, \n",
    "            MS:multivariate predict univariate'\n",
    "        \"\"\"\n",
    "        # 字典索引生成\n",
    "        col_dict = {}\n",
    "        for i,j in enumerate(columns):\n",
    "            col_dict[j] = i\n",
    "\n",
    "        if features == 'MS':\n",
    "            target = target[0]\n",
    "            f_dim = col_dict[target]\n",
    "        elif features == 'S':\n",
    "            f_dim = 0\n",
    "        else:\n",
    "            f_dim = 0\n",
    "        return f_dim\n",
    "    f_dim = forecasting_task(columns, target, features)\n",
    "    \n",
    "    # 设置保存模型路径\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    # 模型训练和验证\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for batch_x, batch_y, batch_x_mark, batch_y_mark in train_loader:\n",
    "            #将数据移至 GPU\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            batch_x_mark = batch_x_mark.to(device)\n",
    "            batch_y_mark = batch_y_mark.to(device)\n",
    "            # 清空梯度\n",
    "            optimizer.zero_grad()\n",
    "            # decoder输入\n",
    "            dec_inp = torch.zeros_like(batch_y[:, -pred_len:, :]).float()\n",
    "            dec_inp = torch.cat([batch_y[:, :label_len, :], dec_inp], dim=1).float().to(device)\n",
    "            outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "            if features == 'MS':\n",
    "                outputs = outputs[:, :, f_dim: f_dim+1] \n",
    "            else:\n",
    "                outputs = outputs[:, :, f_dim:]\n",
    "            train_loss = loss_fn(outputs, batch_y)\n",
    "            # 反向传播计算得到每个参数的梯度值\n",
    "            train_loss.backward()\n",
    "            # 通过梯度下降执行一步参数更新\n",
    "            optimizer.step()\n",
    "            #每个batch的loss和\n",
    "            total_train_loss += train_loss.item() # .item()表示只包含一个元素的tensor中提取值\n",
    "                \n",
    "        #每个epoch的损失平均\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        \n",
    "        #所有epoch的loss\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        #评估模型\n",
    "        model.eval()\n",
    "        #关闭自动求导功能，只使用训练好的模型进行预测或评估，不需要进行梯度计算和参数更新\n",
    "        with torch.no_grad():\n",
    "            total_val_loss = 0\n",
    "            for batch_x, batch_y, batch_x_mark, batch_y_mark in valid_loader:\n",
    "                #将数据移至 GPU\n",
    "                batch_x = batch_x.to(device)\n",
    "                batch_y = batch_y.to(device)\n",
    "                batch_x_mark = batch_x_mark.to(device)\n",
    "                batch_y_mark = batch_y_mark.to(device)\n",
    "                # decoder输入\n",
    "                dec_inp = torch.zeros_like(batch_y[:, -pred_len:, :]).float()\n",
    "                dec_inp = torch.cat([batch_y[:, :label_len, :], dec_inp], dim=1).float().to(device)\n",
    "                val_outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "                if features == 'MS':\n",
    "                    val_outputs = val_outputs[:, :, f_dim: f_dim+1] \n",
    "                else:\n",
    "                    val_outputs = val_outputs[:, :, f_dim:]\n",
    "                val_loss = loss_fn(val_outputs, batch_y)\n",
    "                #每个batch的loss和\n",
    "                total_val_loss += val_loss.item()\n",
    "                \n",
    "        #每个epoch的损失平均\n",
    "        avg_val_loss = total_val_loss / len(valid_loader)\n",
    "        \n",
    "        #所有epoch的loss\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        #打印训练过程\n",
    "        if verbose:\n",
    "            print(f'Epoch [{epoch+1}/{n_epochs}], Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}')\n",
    "            \n",
    "        # 设置早停\n",
    "        early_stopping(avg_val_loss, model, path)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping!\")\n",
    "            break\n",
    "            \n",
    "        # 调整学习率\n",
    "        adjust_learning_rate(optimizer, epoch+1, lradj, learning_rate, n_epochs)\n",
    "        \n",
    "    # 加载最佳模型\n",
    "    best_model_path = path + '/' + 'checkpoint.pth'\n",
    "    model.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "    #绘制损失函数图\n",
    "    def plot_loss(train_losses, val_losses):\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.style.use('seaborn-v0_8-paper') #绘制背景色\n",
    "        plt.grid(axis = 'y',linewidth=0.35) #绘制网格\n",
    "        plt.plot(val_losses, linestyle='-',color = '#11b3b6')\n",
    "        plt.plot(train_losses, linestyle='-',color = '#f14643')\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Training and Validation Progress\")\n",
    "        plt.legend([\"Validation\", \"Training\"])\n",
    "        plt.show()\n",
    "    if plots:\n",
    "        plot_loss(train_losses, val_losses)\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b0801446",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:20:41.409481Z",
     "start_time": "2024-04-14T13:20:07.539496Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|████▏                                                                              | 1/20 [00:13<04:07, 13.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Training Loss: 0.0900, Validation Loss: 0.0318\n",
      "Validation loss decreased (inf --> 0.031839).  Saving model ...\n",
      "Updating learning rate to 0.0009938441702975688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|████████▎                                                                          | 2/20 [00:27<04:06, 13.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/20], Training Loss: 0.0374, Validation Loss: 0.0262\n",
      "Validation loss decreased (0.031839 --> 0.026249).  Saving model ...\n",
      "Updating learning rate to 0.0009755282581475768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|████████▎                                                                          | 2/20 [00:32<04:53, 16.28s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 38\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 构造参数字典\u001b[39;00m\n\u001b[0;32m      2\u001b[0m params3 \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtask_args\u001b[39m\u001b[38;5;124m\"\u001b[39m:{\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mload\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemp\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     36\u001b[0m     },\n\u001b[0;32m     37\u001b[0m }\n\u001b[1;32m---> 38\u001b[0m model \u001b[38;5;241m=\u001b[39m train(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams3)\n",
      "Cell \u001b[1;32mIn[33], line 134\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(task_args, train_args, model_args)\u001b[0m\n\u001b[0;32m    132\u001b[0m train_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    133\u001b[0m \u001b[38;5;66;03m# 通过梯度下降执行一步参数更新\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;66;03m#每个batch的loss和\u001b[39;00m\n\u001b[0;32m    136\u001b[0m total_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m train_loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;66;03m# .item()表示只包含一个元素的tensor中提取值\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\optim\\optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    277\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m                                \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 280\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    283\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\optim\\optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 33\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\optim\\adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    130\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    133\u001b[0m         group,\n\u001b[0;32m    134\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    138\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    139\u001b[0m         state_steps)\n\u001b[1;32m--> 141\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    145\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    146\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    150\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    152\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    153\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    155\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    156\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\optim\\adam.py:281\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    279\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 281\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    282\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    283\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    284\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    285\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    286\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    287\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    288\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    289\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    296\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    297\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\optim\\adam.py:434\u001b[0m, in \u001b[0;36m_multi_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;66;03m# Handle complex parameters\u001b[39;00m\n\u001b[0;32m    433\u001b[0m device_grads \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mview_as_real(x) \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_complex(x) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m device_grads]\n\u001b[1;32m--> 434\u001b[0m device_exp_avgs \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mview_as_real(x) \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_complex(x) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m device_exp_avgs]\n\u001b[0;32m    435\u001b[0m device_exp_avg_sqs \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mview_as_real(x) \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_complex(x) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m device_exp_avg_sqs]\n\u001b[0;32m    436\u001b[0m params_ \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mview_as_real(x) \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_complex(x) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m device_params]\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\optim\\adam.py:434\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;66;03m# Handle complex parameters\u001b[39;00m\n\u001b[0;32m    433\u001b[0m device_grads \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mview_as_real(x) \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_complex(x) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m device_grads]\n\u001b[1;32m--> 434\u001b[0m device_exp_avgs \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mview_as_real(x) \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m device_exp_avgs]\n\u001b[0;32m    435\u001b[0m device_exp_avg_sqs \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mview_as_real(x) \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_complex(x) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m device_exp_avg_sqs]\n\u001b[0;32m    436\u001b[0m params_ \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mview_as_real(x) \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_complex(x) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m device_params]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 构造参数字典\n",
    "params3 = {\n",
    "    \"task_args\":{\n",
    "        \"columns\": ['load', 'temp'],\n",
    "        \"target\": ['load'],\n",
    "        \"features\": 'MS'\n",
    "    },\n",
    "    \"train_args\": {\n",
    "        \"model_name\": ETSformer,\n",
    "        \"train_loader\": train_loader,\n",
    "        \"valid_loader\": valid_loader,\n",
    "        \"n_epochs\": 20,\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"loss\": nn.MSELoss(),\n",
    "        \"patience\": 7,\n",
    "        \"lradj\": 'cosine',\n",
    "        \"path\": \"../models/test\",\n",
    "        \"verbose\": True,\n",
    "        \"plots\": True,\n",
    "    },\n",
    "    \"model_args\": {\n",
    "        'enc_in': 2, \n",
    "        'seq_len': 6,\n",
    "        \"label_len\": 0,\n",
    "        'pred_len': 3,\n",
    "        'top_k': 2,\n",
    "        'd_model': 128,\n",
    "        'n_heads': 8,\n",
    "        'd_ff': 128,\n",
    "        'dropout': 0.1,\n",
    "        'e_layers': 1,\n",
    "        'd_layers': 1,\n",
    "        'embed': 'fixed',\n",
    "        'freq': 'h',\n",
    "        'c_out': 1\n",
    "    },\n",
    "}\n",
    "model = train(**params3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e78031",
   "metadata": {},
   "source": [
    "# 基于FEDformer的时间序列预测"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d59e8f4",
   "metadata": {},
   "source": [
    "## 多变量多步预测"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19dd9834",
   "metadata": {},
   "source": [
    "### 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e3077c42",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:20:46.135368Z",
     "start_time": "2024-04-14T13:20:46.118925Z"
    }
   },
   "outputs": [],
   "source": [
    "# 读取数据\n",
    "def get_dataset(path, time_col=None):\n",
    "    data = pd.read_csv(path)\n",
    "    rows = data.shape[0]\n",
    "    now = datetime.now()\n",
    "    newtime = now.replace(microsecond=0)\n",
    "    if time_col == None:\n",
    "        # 如果没有时间列，生成时间戳范围\n",
    "        time_index = pd.date_range(start=datetime.now() -\n",
    "                                   timedelta(seconds=rows - 1),\n",
    "                                   end=datetime.now(),\n",
    "                                   freq='S')\n",
    "        full_data = pd.DataFrame(data=data.values,\n",
    "                                 index=pd.to_datetime(time_index, unit='s'),\n",
    "                                 columns=data.columns)\n",
    "    else:\n",
    "        columns = [i for i in data.columns if i != time_col] # 去除时间列\n",
    "        full_data = pd.DataFrame(data=data.drop([time_col], axis=1).values,\n",
    "                                 index=pd.to_datetime(data[time_col].values),\n",
    "                                 columns=columns)\n",
    "    return full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "19668802",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:20:47.031487Z",
     "start_time": "2024-04-14T13:20:46.953764Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>load</th>\n",
       "      <th>temp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-01-01 00:00:00</th>\n",
       "      <td>2698.0</td>\n",
       "      <td>32.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-01 01:00:00</th>\n",
       "      <td>2558.0</td>\n",
       "      <td>32.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-01 02:00:00</th>\n",
       "      <td>2444.0</td>\n",
       "      <td>30.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-01 03:00:00</th>\n",
       "      <td>2402.0</td>\n",
       "      <td>31.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-01 04:00:00</th>\n",
       "      <td>2403.0</td>\n",
       "      <td>32.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-12-31 19:00:00</th>\n",
       "      <td>4012.0</td>\n",
       "      <td>18.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-12-31 20:00:00</th>\n",
       "      <td>3856.0</td>\n",
       "      <td>16.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-12-31 21:00:00</th>\n",
       "      <td>3671.0</td>\n",
       "      <td>17.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-12-31 22:00:00</th>\n",
       "      <td>3499.0</td>\n",
       "      <td>15.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-12-31 23:00:00</th>\n",
       "      <td>3345.0</td>\n",
       "      <td>15.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26304 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       load   temp\n",
       "2012-01-01 00:00:00  2698.0  32.00\n",
       "2012-01-01 01:00:00  2558.0  32.67\n",
       "2012-01-01 02:00:00  2444.0  30.00\n",
       "2012-01-01 03:00:00  2402.0  31.00\n",
       "2012-01-01 04:00:00  2403.0  32.00\n",
       "...                     ...    ...\n",
       "2014-12-31 19:00:00  4012.0  18.00\n",
       "2014-12-31 20:00:00  3856.0  16.67\n",
       "2014-12-31 21:00:00  3671.0  17.00\n",
       "2014-12-31 22:00:00  3499.0  15.33\n",
       "2014-12-31 23:00:00  3345.0  15.33\n",
       "\n",
       "[26304 rows x 2 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"../data/energy.csv\"\n",
    "ts_data = get_dataset(path, time_col='time')\n",
    "ts_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2aba21a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:20:48.821900Z",
     "start_time": "2024-04-14T13:20:48.791069Z"
    }
   },
   "outputs": [],
   "source": [
    "# 时间格式编码\n",
    "def time_features_from_frequency_str(freq_str: str):\n",
    "    \"\"\"\n",
    "    Returns a list of time features that will be appropriate for the given frequency string.\n",
    "    Parameters\n",
    "    ----------\n",
    "    freq_str\n",
    "        Frequency string of the form [multiple][granularity] such as \"12H\", \"5min\", \"1D\" etc.\n",
    "    \"\"\"\n",
    "\n",
    "    class TimeFeature:\n",
    "        def __init__(self):\n",
    "            pass\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            pass\n",
    "\n",
    "        def __repr__(self):\n",
    "            return self.__class__.__name__ + \"()\"\n",
    "\n",
    "\n",
    "    class SecondOfMinute(TimeFeature):\n",
    "        \"\"\"Minute of hour encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return index.second / 59.0 - 0.5\n",
    "\n",
    "\n",
    "    class MinuteOfHour(TimeFeature):\n",
    "        \"\"\"Minute of hour encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return index.minute / 59.0 - 0.5\n",
    "\n",
    "\n",
    "    class HourOfDay(TimeFeature):\n",
    "        \"\"\"Hour of day encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return index.hour / 23.0 - 0.5\n",
    "\n",
    "\n",
    "    class DayOfWeek(TimeFeature):\n",
    "        \"\"\"Hour of day encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return index.dayofweek / 6.0 - 0.5\n",
    "\n",
    "\n",
    "    class DayOfMonth(TimeFeature):\n",
    "        \"\"\"Day of month encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return (index.day - 1) / 30.0 - 0.5\n",
    "\n",
    "\n",
    "    class DayOfYear(TimeFeature):\n",
    "        \"\"\"Day of year encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return (index.dayofyear - 1) / 365.0 - 0.5\n",
    "\n",
    "\n",
    "    class MonthOfYear(TimeFeature):\n",
    "        \"\"\"Month of year encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return (index.month - 1) / 11.0 - 0.5\n",
    "\n",
    "\n",
    "    class WeekOfYear(TimeFeature):\n",
    "        \"\"\"Week of year encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return (index.isocalendar().week - 1) / 52.0 - 0.5\n",
    "\n",
    "    \n",
    "    features_by_offsets = {\n",
    "        offsets.YearEnd: [],\n",
    "        offsets.QuarterEnd: [MonthOfYear],\n",
    "        offsets.MonthEnd: [MonthOfYear],\n",
    "        offsets.Week: [DayOfMonth, WeekOfYear],\n",
    "        offsets.Day: [DayOfWeek, DayOfMonth, DayOfYear],\n",
    "        offsets.BusinessDay: [DayOfWeek, DayOfMonth, DayOfYear],\n",
    "        offsets.Hour: [HourOfDay, DayOfWeek, DayOfMonth, DayOfYear],\n",
    "        offsets.Minute: [\n",
    "            MinuteOfHour,\n",
    "            HourOfDay,\n",
    "            DayOfWeek,\n",
    "            DayOfMonth,\n",
    "            DayOfYear,\n",
    "        ],\n",
    "        offsets.Second: [\n",
    "            SecondOfMinute,\n",
    "            MinuteOfHour,\n",
    "            HourOfDay,\n",
    "            DayOfWeek,\n",
    "            DayOfMonth,\n",
    "            DayOfYear,\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    offset = to_offset(freq_str)\n",
    "\n",
    "    for offset_type, feature_classes in features_by_offsets.items():\n",
    "        if isinstance(offset, offset_type):\n",
    "            return [cls() for cls in feature_classes]\n",
    "\n",
    "    supported_freq_msg = f\"\"\"\n",
    "    Unsupported frequency {freq_str}\n",
    "    The following frequencies are supported:\n",
    "        Y   - yearly\n",
    "            alias: A\n",
    "        M   - monthly\n",
    "        W   - weekly\n",
    "        D   - daily\n",
    "        B   - business days\n",
    "        H   - hourly\n",
    "        T   - minutely\n",
    "            alias: min\n",
    "        S   - secondly\n",
    "    \"\"\"\n",
    "    raise RuntimeError(supported_freq_msg)\n",
    "    \n",
    "def time_features(dates, freq='h'):\n",
    "    return np.vstack([feat(dates) for feat in time_features_from_frequency_str(freq)])\n",
    "\n",
    "# 包含时间维度的数据集划分\n",
    "def divide_dataset(df, valid_date, test_date, x_feature_list, y_feature_list, freq):\n",
    "    #归一化\n",
    "    x_scaler = MinMaxScaler()\n",
    "    y_scaler = MinMaxScaler()\n",
    "\n",
    "    #测试集\n",
    "    train = df.copy()[df.index < valid_date][x_feature_list]\n",
    "    train_stamp = time_features(pd.to_datetime(train.index), freq=freq)\n",
    "    train_stamp = train_stamp.transpose(1, 0)\n",
    "    train[x_feature_list] = x_scaler.fit_transform(train)\n",
    "    xtr = train.values.astype('float32')\n",
    "\n",
    "    #验证集\n",
    "    valid = df.copy()[(df.index >= valid_date) & (df.index < test_date)][x_feature_list]\n",
    "    valid_stamp = time_features(pd.to_datetime(valid.index), freq=freq)\n",
    "    valid_stamp = valid_stamp.transpose(1, 0)\n",
    "    valid[x_feature_list] = x_scaler.fit_transform(valid)\n",
    "    xva = valid.values.astype('float32')\n",
    "\n",
    "    #测试集\n",
    "    test = df.copy()[test_date:][x_feature_list]\n",
    "    test_stamp = time_features(pd.to_datetime(test.index), freq=freq)\n",
    "    test_stamp = test_stamp.transpose(1, 0)\n",
    "    test[x_feature_list] = x_scaler.fit_transform(test)\n",
    "    xte = test.values.astype('float32')\n",
    "\n",
    "    #标签\n",
    "    ytr = df.copy()[df.index < valid_date][y_feature_list]\n",
    "    ytr[y_feature_list] = y_scaler.fit_transform(ytr)\n",
    "    ytr = ytr.values.astype('float32')\n",
    "\n",
    "    yva = df.copy()[(df.index >= valid_date) & (df.index < test_date)][y_feature_list]\n",
    "    yva[y_feature_list] = y_scaler.fit_transform(yva)\n",
    "    yva = yva.values.astype('float32')\n",
    "\n",
    "    yte = df.copy()[test_date:][y_feature_list]\n",
    "    yte[y_feature_list] = y_scaler.fit_transform(yte)\n",
    "    yte = yte.values.astype('float32')\n",
    "    \n",
    "    # 数据合并，[训练集，测试集，时间戳]\n",
    "    train = [xtr, ytr, train_stamp]\n",
    "    valid = [xva, yva, valid_stamp]\n",
    "    test = [xte, yte, test_stamp]\n",
    "    \n",
    "    return y_scaler, train, valid, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d2f92385",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:20:50.781521Z",
     "start_time": "2024-04-14T13:20:50.717651Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (23376, 2) y_train shape: (23376, 1) stamp_train shape: (23376, 4)\n",
      "x_valid shape: (1464, 2) y_valid shape: (1464, 1) stamp_valid shape: (1464, 4)\n",
      "x_test shape: (1464, 2) y_test shape: (1464, 1) stamp_test shape: (1464, 4)\n"
     ]
    }
   ],
   "source": [
    "# 构造参数字典\n",
    "params1 = {\n",
    "    \"df\": ts_data,\n",
    "    \"valid_date\": \"2014-09-01 00:00:00\",\n",
    "    \"test_date\": \"2014-11-01 00:00:00\",\n",
    "    \"x_feature_list\": [\"load\", \"temp\"],\n",
    "    \"y_feature_list\": [\"load\"],\n",
    "    \"freq\": 'h'\n",
    "}\n",
    "\n",
    "#函数传参\n",
    "scaler, train, valid, test = divide_dataset(**params1)\n",
    "print(\"x_train shape: {0} y_train shape: {1} stamp_train shape: {2}\".format(train[0].shape, train[1].shape, train[2].shape))\n",
    "print(\"x_valid shape: {0} y_valid shape: {1} stamp_valid shape: {2}\".format(valid[0].shape, valid[1].shape, valid[2].shape))\n",
    "print(\"x_test shape: {0} y_test shape: {1} stamp_test shape: {2}\".format(test[0].shape, test[1].shape, test[2].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "08234d45",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:20:52.858303Z",
     "start_time": "2024-04-14T13:20:52.841204Z"
    }
   },
   "outputs": [],
   "source": [
    "#利用前seq_len个数据，预测下pred_len个数据\n",
    "def create_dataset(data_list, seq_len, pred_len, label_len, batch_size):\n",
    "    \"\"\"\n",
    "    @参数设置：\n",
    "    data_list：特征，目标，时间戳\n",
    "    seq_len：输入数据包含过去多少个时间步\n",
    "    pred_len：目标应该在未来多少个时间步之后\n",
    "    label_len：先验时间步\n",
    "    \"\"\"\n",
    "    # 获取数据\n",
    "    feature = data_list[0] # 特征\n",
    "    target = data_list[1] # 目标\n",
    "    stamp = data_list[2] # 时间戳\n",
    "    \n",
    "    # 循环生成数据\n",
    "    X, y = [], []\n",
    "    X_stamp, y_stamp = [], []\n",
    "    seq_len = seq_len - 1 # 包含当前时间点\n",
    "    for i in range(seq_len, len(feature) - pred_len):\n",
    "        # 数据维度\n",
    "        feat = feature[i - seq_len:i + 1]\n",
    "        tar = target[i + 1:i + 1 + pred_len]\n",
    "        X.append(feat)\n",
    "        y.append(tar)\n",
    "        \n",
    "        # 时间维度\n",
    "        xs = stamp[i - seq_len:i + 1]\n",
    "        ys = stamp[i + 1 - label_len:i + 1 + pred_len]\n",
    "        X_stamp.append(xs)\n",
    "        y_stamp.append(ys)\n",
    "        \n",
    "    # 转为张量，数据维度\n",
    "    X = torch.as_tensor(X).float()\n",
    "    y = torch.as_tensor(y).float()\n",
    "    \n",
    "    # 转为张量，时间维度\n",
    "    X_stamp = torch.as_tensor(X_stamp).float()\n",
    "    y_stamp = torch.as_tensor(y_stamp).float()\n",
    "    \n",
    "    # 创建dataloader，[特征，目标，特征时间编码，目标时间编码]\n",
    "    data_loader = DataLoader(TensorDataset(X, y, X_stamp, y_stamp), shuffle=True, batch_size=batch_size)\n",
    "    \n",
    "    return X, y, data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5e299e35",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:20:55.182210Z",
     "start_time": "2024-04-14T13:20:54.210340Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_size: torch.Size([23368, 6, 2]),y_size: torch.Size([23368, 3, 1]),loader_len: 731\n",
      "X_size: torch.Size([1456, 6, 2]),y_size: torch.Size([1456, 3, 1]),loader_len: 46\n",
      "X_size: torch.Size([1456, 6, 2]),y_size: torch.Size([1456, 3, 1]),loader_len: 46\n"
     ]
    }
   ],
   "source": [
    "# 构造参数字典\n",
    "params2 = {\n",
    "    \"seq_len\": 6,\n",
    "    \"pred_len\": 3,\n",
    "    \"label_len\": 3,\n",
    "    \"batch_size\": 32,\n",
    "}\n",
    "X_train, y_train, train_loader = create_dataset(train, **params2)\n",
    "X_valid, y_valid, valid_loader = create_dataset(valid, **params2)\n",
    "X_test, y_test, test_loader = create_dataset(test, **params2)\n",
    "print(\"X_size: {0},y_size: {1},loader_len: {2}\".format(X_train.shape, y_train.shape, len(train_loader)))\n",
    "print(\"X_size: {0},y_size: {1},loader_len: {2}\".format(X_valid.shape, y_valid.shape, len(valid_loader)))\n",
    "print(\"X_size: {0},y_size: {1},loader_len: {2}\".format(X_test.shape, y_test.shape, len(test_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920fcf1e",
   "metadata": {},
   "source": [
    "### 模型定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "404f30a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:20:59.043287Z",
     "start_time": "2024-04-14T13:20:58.592404Z"
    }
   },
   "outputs": [],
   "source": [
    "# DataEmbedding编码类\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model).float()\n",
    "        pe.require_grad = False\n",
    "\n",
    "        position = torch.arange(0, max_len).float().unsqueeze(1)\n",
    "        div_term = (torch.arange(0, d_model, 2).float()\n",
    "                    * -(math.log(10000.0) / d_model)).exp()\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pe[:, :x.size(1)]\n",
    "\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, c_in, d_model):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        padding = 1 if torch.__version__ >= '1.5.0' else 2\n",
    "        self.tokenConv = nn.Conv1d(in_channels=c_in, out_channels=d_model,\n",
    "                                   kernel_size=3, padding=padding, padding_mode='circular', bias=False)\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                nn.init.kaiming_normal_(\n",
    "                    m.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.tokenConv(x.permute(0, 2, 1)).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class FixedEmbedding(nn.Module):\n",
    "    def __init__(self, c_in, d_model):\n",
    "        super(FixedEmbedding, self).__init__()\n",
    "\n",
    "        w = torch.zeros(c_in, d_model).float()\n",
    "        w.require_grad = False\n",
    "\n",
    "        position = torch.arange(0, c_in).float().unsqueeze(1)\n",
    "        div_term = (torch.arange(0, d_model, 2).float()\n",
    "                    * -(math.log(10000.0) / d_model)).exp()\n",
    "\n",
    "        w[:, 0::2] = torch.sin(position * div_term)\n",
    "        w[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        self.emb = nn.Embedding(c_in, d_model)\n",
    "        self.emb.weight = nn.Parameter(w, requires_grad=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.emb(x).detach()\n",
    "\n",
    "\n",
    "class TemporalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, embed_type='fixed', freq='h'):\n",
    "        super(TemporalEmbedding, self).__init__()\n",
    "\n",
    "        minute_size = 4\n",
    "        hour_size = 24\n",
    "        weekday_size = 7\n",
    "        day_size = 32\n",
    "        month_size = 13\n",
    "\n",
    "        Embed = FixedEmbedding if embed_type == 'fixed' else nn.Embedding\n",
    "        if freq == 't':\n",
    "            self.minute_embed = Embed(minute_size, d_model)\n",
    "        self.hour_embed = Embed(hour_size, d_model)\n",
    "        self.weekday_embed = Embed(weekday_size, d_model)\n",
    "        self.day_embed = Embed(day_size, d_model)\n",
    "        self.month_embed = Embed(month_size, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.long()\n",
    "        minute_x = self.minute_embed(x[:, :, 4]) if hasattr(\n",
    "            self, 'minute_embed') else 0.\n",
    "        hour_x = self.hour_embed(x[:, :, 3])\n",
    "        weekday_x = self.weekday_embed(x[:, :, 2])\n",
    "        day_x = self.day_embed(x[:, :, 1])\n",
    "        month_x = self.month_embed(x[:, :, 0])\n",
    "\n",
    "        return hour_x + weekday_x + day_x + month_x + minute_x\n",
    "\n",
    "\n",
    "class TimeFeatureEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, embed_type='timeF', freq='h'):\n",
    "        super(TimeFeatureEmbedding, self).__init__()\n",
    "\n",
    "        freq_map = {'h': 4, 't': 5, 's': 6,\n",
    "                    'm': 1, 'a': 1, 'w': 2, 'd': 3, 'b': 3}\n",
    "        d_inp = freq_map[freq]\n",
    "        self.embed = nn.Linear(d_inp, d_model, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embed(x)\n",
    "\n",
    "\n",
    "class DataEmbedding(nn.Module):\n",
    "    def __init__(self, c_in, d_model, embed_type='fixed', freq='h', dropout=0.1):\n",
    "        super(DataEmbedding, self).__init__()\n",
    "\n",
    "        self.value_embedding = TokenEmbedding(c_in=c_in, d_model=d_model)\n",
    "        self.position_embedding = PositionalEmbedding(d_model=d_model)\n",
    "        self.temporal_embedding = TemporalEmbedding(d_model=d_model, embed_type=embed_type,\n",
    "                                                    freq=freq) if embed_type != 'timeF' else TimeFeatureEmbedding(\n",
    "            d_model=d_model, embed_type=embed_type, freq=freq)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, x_mark):\n",
    "        if x_mark is None:\n",
    "            x = self.value_embedding(x) + self.position_embedding(x)\n",
    "        else:\n",
    "            x = self.value_embedding(\n",
    "                x) + self.temporal_embedding(x_mark) + self.position_embedding(x)\n",
    "        return self.dropout(x)\n",
    "    \n",
    "    \n",
    "# AutoCorrelationLayer类\n",
    "class AutoCorrelation(nn.Module):\n",
    "    \"\"\"\n",
    "    AutoCorrelation Mechanism with the following two phases:\n",
    "    (1) period-based dependencies discovery\n",
    "    (2) time delay aggregation\n",
    "    This block can replace the self-attention family mechanism seamlessly.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mask_flag=True, factor=1, scale=None, attention_dropout=0.1, output_attention=False):\n",
    "        super(AutoCorrelation, self).__init__()\n",
    "        self.factor = factor\n",
    "        self.scale = scale\n",
    "        self.mask_flag = mask_flag\n",
    "        self.output_attention = output_attention\n",
    "        self.dropout = nn.Dropout(attention_dropout)\n",
    "\n",
    "    def time_delay_agg_training(self, values, corr):\n",
    "        \"\"\"\n",
    "        SpeedUp version of Autocorrelation (a batch-normalization style design)\n",
    "        This is for the training phase.\n",
    "        \"\"\"\n",
    "        head = values.shape[1]\n",
    "        channel = values.shape[2]\n",
    "        length = values.shape[3]\n",
    "        # find top k\n",
    "        top_k = int(self.factor * math.log(length))\n",
    "        mean_value = torch.mean(torch.mean(corr, dim=1), dim=1)\n",
    "        index = torch.topk(torch.mean(mean_value, dim=0), top_k, dim=-1)[1]\n",
    "        weights = torch.stack([mean_value[:, index[i]] for i in range(top_k)], dim=-1)\n",
    "        # update corr\n",
    "        tmp_corr = torch.softmax(weights, dim=-1)\n",
    "        # aggregation\n",
    "        tmp_values = values\n",
    "        delays_agg = torch.zeros_like(values).float()\n",
    "        for i in range(top_k):\n",
    "            pattern = torch.roll(tmp_values, -int(index[i]), -1)\n",
    "            delays_agg = delays_agg + pattern * \\\n",
    "                         (tmp_corr[:, i].unsqueeze(1).unsqueeze(1).unsqueeze(1).repeat(1, head, channel, length))\n",
    "        return delays_agg\n",
    "\n",
    "    def time_delay_agg_inference(self, values, corr):\n",
    "        \"\"\"\n",
    "        SpeedUp version of Autocorrelation (a batch-normalization style design)\n",
    "        This is for the inference phase.\n",
    "        \"\"\"\n",
    "        batch = values.shape[0]\n",
    "        head = values.shape[1]\n",
    "        channel = values.shape[2]\n",
    "        length = values.shape[3]\n",
    "        # index init\n",
    "        init_index = torch.arange(length).unsqueeze(0).unsqueeze(0).unsqueeze(0).repeat(batch, head, channel, 1).cuda()\n",
    "        # find top k\n",
    "        top_k = int(self.factor * math.log(length))\n",
    "        mean_value = torch.mean(torch.mean(corr, dim=1), dim=1)\n",
    "        weights, delay = torch.topk(mean_value, top_k, dim=-1)\n",
    "        # update corr\n",
    "        tmp_corr = torch.softmax(weights, dim=-1)\n",
    "        # aggregation\n",
    "        tmp_values = values.repeat(1, 1, 1, 2)\n",
    "        delays_agg = torch.zeros_like(values).float()\n",
    "        for i in range(top_k):\n",
    "            tmp_delay = init_index + delay[:, i].unsqueeze(1).unsqueeze(1).unsqueeze(1).repeat(1, head, channel, length)\n",
    "            pattern = torch.gather(tmp_values, dim=-1, index=tmp_delay)\n",
    "            delays_agg = delays_agg + pattern * \\\n",
    "                         (tmp_corr[:, i].unsqueeze(1).unsqueeze(1).unsqueeze(1).repeat(1, head, channel, length))\n",
    "        return delays_agg\n",
    "\n",
    "    def time_delay_agg_full(self, values, corr):\n",
    "        \"\"\"\n",
    "        Standard version of Autocorrelation\n",
    "        \"\"\"\n",
    "        batch = values.shape[0]\n",
    "        head = values.shape[1]\n",
    "        channel = values.shape[2]\n",
    "        length = values.shape[3]\n",
    "        # index init\n",
    "        init_index = torch.arange(length).unsqueeze(0).unsqueeze(0).unsqueeze(0).repeat(batch, head, channel, 1).cuda()\n",
    "        # find top k\n",
    "        top_k = int(self.factor * math.log(length))\n",
    "        weights, delay = torch.topk(corr, top_k, dim=-1)\n",
    "        # update corr\n",
    "        tmp_corr = torch.softmax(weights, dim=-1)\n",
    "        # aggregation\n",
    "        tmp_values = values.repeat(1, 1, 1, 2)\n",
    "        delays_agg = torch.zeros_like(values).float()\n",
    "        for i in range(top_k):\n",
    "            tmp_delay = init_index + delay[..., i].unsqueeze(-1)\n",
    "            pattern = torch.gather(tmp_values, dim=-1, index=tmp_delay)\n",
    "            delays_agg = delays_agg + pattern * (tmp_corr[..., i].unsqueeze(-1))\n",
    "        return delays_agg\n",
    "\n",
    "    def forward(self, queries, keys, values, attn_mask):\n",
    "        B, L, H, E = queries.shape\n",
    "        _, S, _, D = values.shape\n",
    "        if L > S:\n",
    "            zeros = torch.zeros_like(queries[:, :(L - S), :]).float()\n",
    "            values = torch.cat([values, zeros], dim=1)\n",
    "            keys = torch.cat([keys, zeros], dim=1)\n",
    "        else:\n",
    "            values = values[:, :L, :, :]\n",
    "            keys = keys[:, :L, :, :]\n",
    "\n",
    "        # period-based dependencies\n",
    "        q_fft = torch.fft.rfft(queries.permute(0, 2, 3, 1).contiguous(), dim=-1)\n",
    "        k_fft = torch.fft.rfft(keys.permute(0, 2, 3, 1).contiguous(), dim=-1)\n",
    "        res = q_fft * torch.conj(k_fft)\n",
    "        corr = torch.fft.irfft(res, dim=-1)\n",
    "\n",
    "        # time delay agg\n",
    "        if self.training:\n",
    "            V = self.time_delay_agg_training(values.permute(0, 2, 3, 1).contiguous(), corr).permute(0, 3, 1, 2)\n",
    "        else:\n",
    "            V = self.time_delay_agg_inference(values.permute(0, 2, 3, 1).contiguous(), corr).permute(0, 3, 1, 2)\n",
    "\n",
    "        if self.output_attention:\n",
    "            return (V.contiguous(), corr.permute(0, 3, 1, 2))\n",
    "        else:\n",
    "            return (V.contiguous(), None)\n",
    "\n",
    "\n",
    "class AutoCorrelationLayer(nn.Module):\n",
    "    def __init__(self, correlation, d_model, n_heads, d_keys=None,\n",
    "                 d_values=None):\n",
    "        super(AutoCorrelationLayer, self).__init__()\n",
    "\n",
    "        d_keys = d_keys or (d_model // n_heads)\n",
    "        d_values = d_values or (d_model // n_heads)\n",
    "\n",
    "        self.inner_correlation = correlation\n",
    "        self.query_projection = nn.Linear(d_model, d_keys * n_heads)\n",
    "        self.key_projection = nn.Linear(d_model, d_keys * n_heads)\n",
    "        self.value_projection = nn.Linear(d_model, d_values * n_heads)\n",
    "        self.out_projection = nn.Linear(d_values * n_heads, d_model)\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "    def forward(self, queries, keys, values, attn_mask):\n",
    "        B, L, _ = queries.shape\n",
    "        _, S, _ = keys.shape\n",
    "        H = self.n_heads\n",
    "\n",
    "        queries = self.query_projection(queries).view(B, L, H, -1)\n",
    "        keys = self.key_projection(keys).view(B, S, H, -1)\n",
    "        values = self.value_projection(values).view(B, S, H, -1)\n",
    "\n",
    "        out, attn = self.inner_correlation(\n",
    "            queries,\n",
    "            keys,\n",
    "            values,\n",
    "            attn_mask\n",
    "        )\n",
    "        out = out.view(B, L, -1)\n",
    "\n",
    "        return self.out_projection(out), attn\n",
    "    \n",
    "    \n",
    "# 傅里叶层\n",
    "def get_frequency_modes(seq_len, modes=64, mode_select_method='random'):\n",
    "    \"\"\"\n",
    "    get modes on frequency domain:\n",
    "    'random' means sampling randomly;\n",
    "    'else' means sampling the lowest modes;\n",
    "    \"\"\"\n",
    "    modes = min(modes, seq_len // 2)\n",
    "    if mode_select_method == 'random':\n",
    "        index = list(range(0, seq_len // 2))\n",
    "        np.random.shuffle(index)\n",
    "        index = index[:modes]\n",
    "    else:\n",
    "        index = list(range(0, modes))\n",
    "    index.sort()\n",
    "    return index\n",
    "\n",
    "\n",
    "# ########## fourier layer #############\n",
    "class FourierBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, seq_len, modes=0, mode_select_method='random'):\n",
    "        super(FourierBlock, self).__init__()\n",
    "        print('fourier enhanced block used!')\n",
    "        \"\"\"\n",
    "        1D Fourier block. It performs representation learning on frequency domain, \n",
    "        it does FFT, linear transform, and Inverse FFT.    \n",
    "        \"\"\"\n",
    "        # get modes on frequency domain\n",
    "        self.index = get_frequency_modes(seq_len, modes=modes, mode_select_method=mode_select_method)\n",
    "        print('modes={}, index={}'.format(modes, self.index))\n",
    "\n",
    "        self.scale = (1 / (in_channels * out_channels))\n",
    "        self.weights1 = nn.Parameter(\n",
    "            self.scale * torch.rand(8, in_channels // 8, out_channels // 8, len(self.index), dtype=torch.float))\n",
    "        self.weights2 = nn.Parameter(\n",
    "            self.scale * torch.rand(8, in_channels // 8, out_channels // 8, len(self.index), dtype=torch.float))\n",
    "\n",
    "    # Complex multiplication\n",
    "    def compl_mul1d(self, order, x, weights):\n",
    "        x_flag = True\n",
    "        w_flag = True\n",
    "        if not torch.is_complex(x):\n",
    "            x_flag = False\n",
    "            x = torch.complex(x, torch.zeros_like(x).to(x.device))\n",
    "        if not torch.is_complex(weights):\n",
    "            w_flag = False\n",
    "            weights = torch.complex(weights, torch.zeros_like(weights).to(weights.device))\n",
    "        if x_flag or w_flag:\n",
    "            return torch.complex(torch.einsum(order, x.real, weights.real) - torch.einsum(order, x.imag, weights.imag),\n",
    "                                 torch.einsum(order, x.real, weights.imag) + torch.einsum(order, x.imag, weights.real))\n",
    "        else:\n",
    "            return torch.einsum(order, x.real, weights.real)\n",
    "\n",
    "    def forward(self, q, k, v, mask):\n",
    "        # size = [B, L, H, E]\n",
    "        B, L, H, E = q.shape\n",
    "        x = q.permute(0, 2, 3, 1)\n",
    "        # Compute Fourier coefficients\n",
    "        x_ft = torch.fft.rfft(x, dim=-1)\n",
    "        # Perform Fourier neural operations\n",
    "        out_ft = torch.zeros(B, H, E, L // 2 + 1, device=x.device, dtype=torch.cfloat)\n",
    "        for wi, i in enumerate(self.index):\n",
    "            if i >= x_ft.shape[3] or wi >= out_ft.shape[3]:\n",
    "                continue\n",
    "            out_ft[:, :, :, wi] = self.compl_mul1d(\"bhi,hio->bho\", x_ft[:, :, :, i],\n",
    "                                                   torch.complex(self.weights1, self.weights2)[:, :, :, wi])\n",
    "        # Return to time domain\n",
    "        x = torch.fft.irfft(out_ft, n=x.size(-1))\n",
    "        return (x, None)\n",
    "\n",
    "\n",
    "# ########## Fourier Cross Former ####################\n",
    "class FourierCrossAttention(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, seq_len_q, seq_len_kv, modes=64, mode_select_method='random',\n",
    "                 activation='tanh', policy=0, num_heads=8):\n",
    "        super(FourierCrossAttention, self).__init__()\n",
    "        print('fourier enhanced cross attention used!')\n",
    "        \"\"\"\n",
    "        1D Fourier Cross Attention layer. It does FFT, linear transform, attention mechanism and Inverse FFT.    \n",
    "        \"\"\"\n",
    "        self.activation = activation\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        # get modes for queries and keys (& values) on frequency domain\n",
    "        self.index_q = get_frequency_modes(seq_len_q, modes=modes, mode_select_method=mode_select_method)\n",
    "        self.index_kv = get_frequency_modes(seq_len_kv, modes=modes, mode_select_method=mode_select_method)\n",
    "\n",
    "        print('modes_q={}, index_q={}'.format(len(self.index_q), self.index_q))\n",
    "        print('modes_kv={}, index_kv={}'.format(len(self.index_kv), self.index_kv))\n",
    "\n",
    "        self.scale = (1 / (in_channels * out_channels))\n",
    "        self.weights1 = nn.Parameter(\n",
    "            self.scale * torch.rand(num_heads, in_channels // num_heads, out_channels // num_heads, len(self.index_q), dtype=torch.float))\n",
    "        self.weights2 = nn.Parameter(\n",
    "            self.scale * torch.rand(num_heads, in_channels // num_heads, out_channels // num_heads, len(self.index_q), dtype=torch.float))\n",
    "\n",
    "    # Complex multiplication\n",
    "    def compl_mul1d(self, order, x, weights):\n",
    "        x_flag = True\n",
    "        w_flag = True\n",
    "        if not torch.is_complex(x):\n",
    "            x_flag = False\n",
    "            x = torch.complex(x, torch.zeros_like(x).to(x.device))\n",
    "        if not torch.is_complex(weights):\n",
    "            w_flag = False\n",
    "            weights = torch.complex(weights, torch.zeros_like(weights).to(weights.device))\n",
    "        if x_flag or w_flag:\n",
    "            return torch.complex(torch.einsum(order, x.real, weights.real) - torch.einsum(order, x.imag, weights.imag),\n",
    "                                 torch.einsum(order, x.real, weights.imag) + torch.einsum(order, x.imag, weights.real))\n",
    "        else:\n",
    "            return torch.einsum(order, x.real, weights.real)\n",
    "\n",
    "    def forward(self, q, k, v, mask):\n",
    "        # size = [B, L, H, E]\n",
    "        B, L, H, E = q.shape\n",
    "        xq = q.permute(0, 2, 3, 1)  # size = [B, H, E, L]\n",
    "        xk = k.permute(0, 2, 3, 1)\n",
    "        xv = v.permute(0, 2, 3, 1)\n",
    "\n",
    "        # Compute Fourier coefficients\n",
    "        xq_ft_ = torch.zeros(B, H, E, len(self.index_q), device=xq.device, dtype=torch.cfloat)\n",
    "        xq_ft = torch.fft.rfft(xq, dim=-1)\n",
    "        for i, j in enumerate(self.index_q):\n",
    "            if j >= xq_ft.shape[3]:\n",
    "                continue\n",
    "            xq_ft_[:, :, :, i] = xq_ft[:, :, :, j]\n",
    "        xk_ft_ = torch.zeros(B, H, E, len(self.index_kv), device=xq.device, dtype=torch.cfloat)\n",
    "        xk_ft = torch.fft.rfft(xk, dim=-1)\n",
    "        for i, j in enumerate(self.index_kv):\n",
    "            if j >= xk_ft.shape[3]:\n",
    "                continue\n",
    "            xk_ft_[:, :, :, i] = xk_ft[:, :, :, j]\n",
    "\n",
    "        # perform attention mechanism on frequency domain\n",
    "        xqk_ft = (self.compl_mul1d(\"bhex,bhey->bhxy\", xq_ft_, xk_ft_))\n",
    "        if self.activation == 'tanh':\n",
    "            xqk_ft = torch.complex(xqk_ft.real.tanh(), xqk_ft.imag.tanh())\n",
    "        elif self.activation == 'softmax':\n",
    "            xqk_ft = torch.softmax(abs(xqk_ft), dim=-1)\n",
    "            xqk_ft = torch.complex(xqk_ft, torch.zeros_like(xqk_ft))\n",
    "        else:\n",
    "            raise Exception('{} actiation function is not implemented'.format(self.activation))\n",
    "        xqkv_ft = self.compl_mul1d(\"bhxy,bhey->bhex\", xqk_ft, xk_ft_)\n",
    "        xqkvw = self.compl_mul1d(\"bhex,heox->bhox\", xqkv_ft, torch.complex(self.weights1, self.weights2))\n",
    "        out_ft = torch.zeros(B, H, E, L // 2 + 1, device=xq.device, dtype=torch.cfloat)\n",
    "        for i, j in enumerate(self.index_q):\n",
    "            if i >= xqkvw.shape[3] or j >= out_ft.shape[3]:\n",
    "                continue\n",
    "            out_ft[:, :, :, j] = xqkvw[:, :, :, i]\n",
    "        # Return to time domain\n",
    "        out = torch.fft.irfft(out_ft / self.in_channels / self.out_channels, n=xq.size(-1))\n",
    "        return (out, None)\n",
    "    \n",
    "\n",
    "# MultiWaveletCorrelation类\n",
    "def legendreDer(k, x):\n",
    "    def _legendre(k, x):\n",
    "        return (2 * k + 1) * eval_legendre(k, x)\n",
    "\n",
    "    out = 0\n",
    "    for i in np.arange(k - 1, -1, -2):\n",
    "        out += _legendre(i, x)\n",
    "    return out\n",
    "\n",
    "\n",
    "def phi_(phi_c, x, lb=0, ub=1):\n",
    "    mask = np.logical_or(x < lb, x > ub) * 1.0\n",
    "    return np.polynomial.polynomial.Polynomial(phi_c)(x) * (1 - mask)\n",
    "\n",
    "\n",
    "def get_phi_psi(k, base):\n",
    "    x = Symbol('x')\n",
    "    phi_coeff = np.zeros((k, k))\n",
    "    phi_2x_coeff = np.zeros((k, k))\n",
    "    if base == 'legendre':\n",
    "        for ki in range(k):\n",
    "            coeff_ = Poly(legendre(ki, 2 * x - 1), x).all_coeffs()\n",
    "            phi_coeff[ki, :ki + 1] = np.flip(np.sqrt(2 * ki + 1) * np.array(coeff_).astype(np.float64))\n",
    "            coeff_ = Poly(legendre(ki, 4 * x - 1), x).all_coeffs()\n",
    "            phi_2x_coeff[ki, :ki + 1] = np.flip(np.sqrt(2) * np.sqrt(2 * ki + 1) * np.array(coeff_).astype(np.float64))\n",
    "\n",
    "        psi1_coeff = np.zeros((k, k))\n",
    "        psi2_coeff = np.zeros((k, k))\n",
    "        for ki in range(k):\n",
    "            psi1_coeff[ki, :] = phi_2x_coeff[ki, :]\n",
    "            for i in range(k):\n",
    "                a = phi_2x_coeff[ki, :ki + 1]\n",
    "                b = phi_coeff[i, :i + 1]\n",
    "                prod_ = np.convolve(a, b)\n",
    "                prod_[np.abs(prod_) < 1e-8] = 0\n",
    "                proj_ = (prod_ * 1 / (np.arange(len(prod_)) + 1) * np.power(0.5, 1 + np.arange(len(prod_)))).sum()\n",
    "                psi1_coeff[ki, :] -= proj_ * phi_coeff[i, :]\n",
    "                psi2_coeff[ki, :] -= proj_ * phi_coeff[i, :]\n",
    "            for j in range(ki):\n",
    "                a = phi_2x_coeff[ki, :ki + 1]\n",
    "                b = psi1_coeff[j, :]\n",
    "                prod_ = np.convolve(a, b)\n",
    "                prod_[np.abs(prod_) < 1e-8] = 0\n",
    "                proj_ = (prod_ * 1 / (np.arange(len(prod_)) + 1) * np.power(0.5, 1 + np.arange(len(prod_)))).sum()\n",
    "                psi1_coeff[ki, :] -= proj_ * psi1_coeff[j, :]\n",
    "                psi2_coeff[ki, :] -= proj_ * psi2_coeff[j, :]\n",
    "\n",
    "            a = psi1_coeff[ki, :]\n",
    "            prod_ = np.convolve(a, a)\n",
    "            prod_[np.abs(prod_) < 1e-8] = 0\n",
    "            norm1 = (prod_ * 1 / (np.arange(len(prod_)) + 1) * np.power(0.5, 1 + np.arange(len(prod_)))).sum()\n",
    "\n",
    "            a = psi2_coeff[ki, :]\n",
    "            prod_ = np.convolve(a, a)\n",
    "            prod_[np.abs(prod_) < 1e-8] = 0\n",
    "            norm2 = (prod_ * 1 / (np.arange(len(prod_)) + 1) * (1 - np.power(0.5, 1 + np.arange(len(prod_))))).sum()\n",
    "            norm_ = np.sqrt(norm1 + norm2)\n",
    "            psi1_coeff[ki, :] /= norm_\n",
    "            psi2_coeff[ki, :] /= norm_\n",
    "            psi1_coeff[np.abs(psi1_coeff) < 1e-8] = 0\n",
    "            psi2_coeff[np.abs(psi2_coeff) < 1e-8] = 0\n",
    "\n",
    "        phi = [np.poly1d(np.flip(phi_coeff[i, :])) for i in range(k)]\n",
    "        psi1 = [np.poly1d(np.flip(psi1_coeff[i, :])) for i in range(k)]\n",
    "        psi2 = [np.poly1d(np.flip(psi2_coeff[i, :])) for i in range(k)]\n",
    "\n",
    "    elif base == 'chebyshev':\n",
    "        for ki in range(k):\n",
    "            if ki == 0:\n",
    "                phi_coeff[ki, :ki + 1] = np.sqrt(2 / np.pi)\n",
    "                phi_2x_coeff[ki, :ki + 1] = np.sqrt(2 / np.pi) * np.sqrt(2)\n",
    "            else:\n",
    "                coeff_ = Poly(chebyshevt(ki, 2 * x - 1), x).all_coeffs()\n",
    "                phi_coeff[ki, :ki + 1] = np.flip(2 / np.sqrt(np.pi) * np.array(coeff_).astype(np.float64))\n",
    "                coeff_ = Poly(chebyshevt(ki, 4 * x - 1), x).all_coeffs()\n",
    "                phi_2x_coeff[ki, :ki + 1] = np.flip(\n",
    "                    np.sqrt(2) * 2 / np.sqrt(np.pi) * np.array(coeff_).astype(np.float64))\n",
    "\n",
    "        phi = [partial(phi_, phi_coeff[i, :]) for i in range(k)]\n",
    "\n",
    "        x = Symbol('x')\n",
    "        kUse = 2 * k\n",
    "        roots = Poly(chebyshevt(kUse, 2 * x - 1)).all_roots()\n",
    "        x_m = np.array([rt.evalf(20) for rt in roots]).astype(np.float64)\n",
    "        # x_m[x_m==0.5] = 0.5 + 1e-8 # add small noise to avoid the case of 0.5 belonging to both phi(2x) and phi(2x-1)\n",
    "        # not needed for our purpose here, we use even k always to avoid\n",
    "        wm = np.pi / kUse / 2\n",
    "\n",
    "        psi1_coeff = np.zeros((k, k))\n",
    "        psi2_coeff = np.zeros((k, k))\n",
    "\n",
    "        psi1 = [[] for _ in range(k)]\n",
    "        psi2 = [[] for _ in range(k)]\n",
    "\n",
    "        for ki in range(k):\n",
    "            psi1_coeff[ki, :] = phi_2x_coeff[ki, :]\n",
    "            for i in range(k):\n",
    "                proj_ = (wm * phi[i](x_m) * np.sqrt(2) * phi[ki](2 * x_m)).sum()\n",
    "                psi1_coeff[ki, :] -= proj_ * phi_coeff[i, :]\n",
    "                psi2_coeff[ki, :] -= proj_ * phi_coeff[i, :]\n",
    "\n",
    "            for j in range(ki):\n",
    "                proj_ = (wm * psi1[j](x_m) * np.sqrt(2) * phi[ki](2 * x_m)).sum()\n",
    "                psi1_coeff[ki, :] -= proj_ * psi1_coeff[j, :]\n",
    "                psi2_coeff[ki, :] -= proj_ * psi2_coeff[j, :]\n",
    "\n",
    "            psi1[ki] = partial(phi_, psi1_coeff[ki, :], lb=0, ub=0.5)\n",
    "            psi2[ki] = partial(phi_, psi2_coeff[ki, :], lb=0.5, ub=1)\n",
    "\n",
    "            norm1 = (wm * psi1[ki](x_m) * psi1[ki](x_m)).sum()\n",
    "            norm2 = (wm * psi2[ki](x_m) * psi2[ki](x_m)).sum()\n",
    "\n",
    "            norm_ = np.sqrt(norm1 + norm2)\n",
    "            psi1_coeff[ki, :] /= norm_\n",
    "            psi2_coeff[ki, :] /= norm_\n",
    "            psi1_coeff[np.abs(psi1_coeff) < 1e-8] = 0\n",
    "            psi2_coeff[np.abs(psi2_coeff) < 1e-8] = 0\n",
    "\n",
    "            psi1[ki] = partial(phi_, psi1_coeff[ki, :], lb=0, ub=0.5 + 1e-16)\n",
    "            psi2[ki] = partial(phi_, psi2_coeff[ki, :], lb=0.5 + 1e-16, ub=1)\n",
    "\n",
    "    return phi, psi1, psi2\n",
    "\n",
    "\n",
    "def get_filter(base, k):\n",
    "    def psi(psi1, psi2, i, inp):\n",
    "        mask = (inp <= 0.5) * 1.0\n",
    "        return psi1[i](inp) * mask + psi2[i](inp) * (1 - mask)\n",
    "\n",
    "    if base not in ['legendre', 'chebyshev']:\n",
    "        raise Exception('Base not supported')\n",
    "\n",
    "    x = Symbol('x')\n",
    "    H0 = np.zeros((k, k))\n",
    "    H1 = np.zeros((k, k))\n",
    "    G0 = np.zeros((k, k))\n",
    "    G1 = np.zeros((k, k))\n",
    "    PHI0 = np.zeros((k, k))\n",
    "    PHI1 = np.zeros((k, k))\n",
    "    phi, psi1, psi2 = get_phi_psi(k, base)\n",
    "    if base == 'legendre':\n",
    "        roots = Poly(legendre(k, 2 * x - 1)).all_roots()\n",
    "        x_m = np.array([rt.evalf(20) for rt in roots]).astype(np.float64)\n",
    "        wm = 1 / k / legendreDer(k, 2 * x_m - 1) / eval_legendre(k - 1, 2 * x_m - 1)\n",
    "\n",
    "        for ki in range(k):\n",
    "            for kpi in range(k):\n",
    "                H0[ki, kpi] = 1 / np.sqrt(2) * (wm * phi[ki](x_m / 2) * phi[kpi](x_m)).sum()\n",
    "                G0[ki, kpi] = 1 / np.sqrt(2) * (wm * psi(psi1, psi2, ki, x_m / 2) * phi[kpi](x_m)).sum()\n",
    "                H1[ki, kpi] = 1 / np.sqrt(2) * (wm * phi[ki]((x_m + 1) / 2) * phi[kpi](x_m)).sum()\n",
    "                G1[ki, kpi] = 1 / np.sqrt(2) * (wm * psi(psi1, psi2, ki, (x_m + 1) / 2) * phi[kpi](x_m)).sum()\n",
    "\n",
    "        PHI0 = np.eye(k)\n",
    "        PHI1 = np.eye(k)\n",
    "\n",
    "    elif base == 'chebyshev':\n",
    "        x = Symbol('x')\n",
    "        kUse = 2 * k\n",
    "        roots = Poly(chebyshevt(kUse, 2 * x - 1)).all_roots()\n",
    "        x_m = np.array([rt.evalf(20) for rt in roots]).astype(np.float64)\n",
    "        # x_m[x_m==0.5] = 0.5 + 1e-8 # add small noise to avoid the case of 0.5 belonging to both phi(2x) and phi(2x-1)\n",
    "        # not needed for our purpose here, we use even k always to avoid\n",
    "        wm = np.pi / kUse / 2\n",
    "\n",
    "        for ki in range(k):\n",
    "            for kpi in range(k):\n",
    "                H0[ki, kpi] = 1 / np.sqrt(2) * (wm * phi[ki](x_m / 2) * phi[kpi](x_m)).sum()\n",
    "                G0[ki, kpi] = 1 / np.sqrt(2) * (wm * psi(psi1, psi2, ki, x_m / 2) * phi[kpi](x_m)).sum()\n",
    "                H1[ki, kpi] = 1 / np.sqrt(2) * (wm * phi[ki]((x_m + 1) / 2) * phi[kpi](x_m)).sum()\n",
    "                G1[ki, kpi] = 1 / np.sqrt(2) * (wm * psi(psi1, psi2, ki, (x_m + 1) / 2) * phi[kpi](x_m)).sum()\n",
    "\n",
    "                PHI0[ki, kpi] = (wm * phi[ki](2 * x_m) * phi[kpi](2 * x_m)).sum() * 2\n",
    "                PHI1[ki, kpi] = (wm * phi[ki](2 * x_m - 1) * phi[kpi](2 * x_m - 1)).sum() * 2\n",
    "\n",
    "        PHI0[np.abs(PHI0) < 1e-8] = 0\n",
    "        PHI1[np.abs(PHI1) < 1e-8] = 0\n",
    "\n",
    "    H0[np.abs(H0) < 1e-8] = 0\n",
    "    H1[np.abs(H1) < 1e-8] = 0\n",
    "    G0[np.abs(G0) < 1e-8] = 0\n",
    "    G1[np.abs(G1) < 1e-8] = 0\n",
    "\n",
    "    return H0, H1, G0, G1, PHI0, PHI1\n",
    "\n",
    "\n",
    "class MultiWaveletTransform(nn.Module):\n",
    "    \"\"\"\n",
    "    1D multiwavelet block.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ich=1, k=8, alpha=16, c=128,\n",
    "                 nCZ=1, L=0, base='legendre', attention_dropout=0.1):\n",
    "        super(MultiWaveletTransform, self).__init__()\n",
    "        print('base', base)\n",
    "        self.k = k\n",
    "        self.c = c\n",
    "        self.L = L\n",
    "        self.nCZ = nCZ\n",
    "        self.Lk0 = nn.Linear(ich, c * k)\n",
    "        self.Lk1 = nn.Linear(c * k, ich)\n",
    "        self.ich = ich\n",
    "        self.MWT_CZ = nn.ModuleList(MWT_CZ1d(k, alpha, L, c, base) for i in range(nCZ))\n",
    "\n",
    "    def forward(self, queries, keys, values, attn_mask):\n",
    "        B, L, H, E = queries.shape\n",
    "        _, S, _, D = values.shape\n",
    "        if L > S:\n",
    "            zeros = torch.zeros_like(queries[:, :(L - S), :]).float()\n",
    "            values = torch.cat([values, zeros], dim=1)\n",
    "            keys = torch.cat([keys, zeros], dim=1)\n",
    "        else:\n",
    "            values = values[:, :L, :, :]\n",
    "            keys = keys[:, :L, :, :]\n",
    "        values = values.view(B, L, -1)\n",
    "\n",
    "        V = self.Lk0(values).view(B, L, self.c, -1)\n",
    "        for i in range(self.nCZ):\n",
    "            V = self.MWT_CZ[i](V)\n",
    "            if i < self.nCZ - 1:\n",
    "                V = F.relu(V)\n",
    "\n",
    "        V = self.Lk1(V.view(B, L, -1))\n",
    "        V = V.view(B, L, -1, D)\n",
    "        return (V.contiguous(), None)\n",
    "\n",
    "\n",
    "class MultiWaveletCross(nn.Module):\n",
    "    \"\"\"\n",
    "    1D Multiwavelet Cross Attention layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, seq_len_q, seq_len_kv, modes, c=64,\n",
    "                 k=8, ich=512,\n",
    "                 L=0,\n",
    "                 base='legendre',\n",
    "                 mode_select_method='random',\n",
    "                 initializer=None, activation='tanh',\n",
    "                 **kwargs):\n",
    "        super(MultiWaveletCross, self).__init__()\n",
    "        print('base', base)\n",
    "\n",
    "        self.c = c\n",
    "        self.k = k\n",
    "        self.L = L\n",
    "        H0, H1, G0, G1, PHI0, PHI1 = get_filter(base, k)\n",
    "        H0r = H0 @ PHI0\n",
    "        G0r = G0 @ PHI0\n",
    "        H1r = H1 @ PHI1\n",
    "        G1r = G1 @ PHI1\n",
    "\n",
    "        H0r[np.abs(H0r) < 1e-8] = 0\n",
    "        H1r[np.abs(H1r) < 1e-8] = 0\n",
    "        G0r[np.abs(G0r) < 1e-8] = 0\n",
    "        G1r[np.abs(G1r) < 1e-8] = 0\n",
    "        self.max_item = 3\n",
    "\n",
    "        self.attn1 = FourierCrossAttentionW(in_channels=in_channels, out_channels=out_channels, seq_len_q=seq_len_q,\n",
    "                                            seq_len_kv=seq_len_kv, modes=modes, activation=activation,\n",
    "                                            mode_select_method=mode_select_method)\n",
    "        self.attn2 = FourierCrossAttentionW(in_channels=in_channels, out_channels=out_channels, seq_len_q=seq_len_q,\n",
    "                                            seq_len_kv=seq_len_kv, modes=modes, activation=activation,\n",
    "                                            mode_select_method=mode_select_method)\n",
    "        self.attn3 = FourierCrossAttentionW(in_channels=in_channels, out_channels=out_channels, seq_len_q=seq_len_q,\n",
    "                                            seq_len_kv=seq_len_kv, modes=modes, activation=activation,\n",
    "                                            mode_select_method=mode_select_method)\n",
    "        self.attn4 = FourierCrossAttentionW(in_channels=in_channels, out_channels=out_channels, seq_len_q=seq_len_q,\n",
    "                                            seq_len_kv=seq_len_kv, modes=modes, activation=activation,\n",
    "                                            mode_select_method=mode_select_method)\n",
    "        self.T0 = nn.Linear(k, k)\n",
    "        self.register_buffer('ec_s', torch.Tensor(\n",
    "            np.concatenate((H0.T, H1.T), axis=0)))\n",
    "        self.register_buffer('ec_d', torch.Tensor(\n",
    "            np.concatenate((G0.T, G1.T), axis=0)))\n",
    "\n",
    "        self.register_buffer('rc_e', torch.Tensor(\n",
    "            np.concatenate((H0r, G0r), axis=0)))\n",
    "        self.register_buffer('rc_o', torch.Tensor(\n",
    "            np.concatenate((H1r, G1r), axis=0)))\n",
    "\n",
    "        self.Lk = nn.Linear(ich, c * k)\n",
    "        self.Lq = nn.Linear(ich, c * k)\n",
    "        self.Lv = nn.Linear(ich, c * k)\n",
    "        self.out = nn.Linear(c * k, ich)\n",
    "        self.modes1 = modes\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        B, N, H, E = q.shape  # (B, N, H, E) torch.Size([3, 768, 8, 2])\n",
    "        _, S, _, _ = k.shape  # (B, S, H, E) torch.Size([3, 96, 8, 2])\n",
    "\n",
    "        q = q.view(q.shape[0], q.shape[1], -1)\n",
    "        k = k.view(k.shape[0], k.shape[1], -1)\n",
    "        v = v.view(v.shape[0], v.shape[1], -1)\n",
    "        q = self.Lq(q)\n",
    "        q = q.view(q.shape[0], q.shape[1], self.c, self.k)\n",
    "        k = self.Lk(k)\n",
    "        k = k.view(k.shape[0], k.shape[1], self.c, self.k)\n",
    "        v = self.Lv(v)\n",
    "        v = v.view(v.shape[0], v.shape[1], self.c, self.k)\n",
    "\n",
    "        if N > S:\n",
    "            zeros = torch.zeros_like(q[:, :(N - S), :]).float()\n",
    "            v = torch.cat([v, zeros], dim=1)\n",
    "            k = torch.cat([k, zeros], dim=1)\n",
    "        else:\n",
    "            v = v[:, :N, :, :]\n",
    "            k = k[:, :N, :, :]\n",
    "\n",
    "        ns = math.floor(np.log2(N))\n",
    "        nl = pow(2, math.ceil(np.log2(N)))\n",
    "        extra_q = q[:, 0:nl - N, :, :]\n",
    "        extra_k = k[:, 0:nl - N, :, :]\n",
    "        extra_v = v[:, 0:nl - N, :, :]\n",
    "        q = torch.cat([q, extra_q], 1)\n",
    "        k = torch.cat([k, extra_k], 1)\n",
    "        v = torch.cat([v, extra_v], 1)\n",
    "\n",
    "        Ud_q = torch.jit.annotate(List[Tuple[Tensor]], [])\n",
    "        Ud_k = torch.jit.annotate(List[Tuple[Tensor]], [])\n",
    "        Ud_v = torch.jit.annotate(List[Tuple[Tensor]], [])\n",
    "\n",
    "        Us_q = torch.jit.annotate(List[Tensor], [])\n",
    "        Us_k = torch.jit.annotate(List[Tensor], [])\n",
    "        Us_v = torch.jit.annotate(List[Tensor], [])\n",
    "\n",
    "        Ud = torch.jit.annotate(List[Tensor], [])\n",
    "        Us = torch.jit.annotate(List[Tensor], [])\n",
    "\n",
    "        # decompose\n",
    "        for i in range(ns - self.L):\n",
    "            # print('q shape',q.shape)\n",
    "            d, q = self.wavelet_transform(q)\n",
    "            Ud_q += [tuple([d, q])]\n",
    "            Us_q += [d]\n",
    "        for i in range(ns - self.L):\n",
    "            d, k = self.wavelet_transform(k)\n",
    "            Ud_k += [tuple([d, k])]\n",
    "            Us_k += [d]\n",
    "        for i in range(ns - self.L):\n",
    "            d, v = self.wavelet_transform(v)\n",
    "            Ud_v += [tuple([d, v])]\n",
    "            Us_v += [d]\n",
    "        for i in range(ns - self.L):\n",
    "            dk, sk = Ud_k[i], Us_k[i]\n",
    "            dq, sq = Ud_q[i], Us_q[i]\n",
    "            dv, sv = Ud_v[i], Us_v[i]\n",
    "            Ud += [self.attn1(dq[0], dk[0], dv[0], mask)[0] + self.attn2(dq[1], dk[1], dv[1], mask)[0]]\n",
    "            Us += [self.attn3(sq, sk, sv, mask)[0]]\n",
    "        v = self.attn4(q, k, v, mask)[0]\n",
    "\n",
    "        # reconstruct\n",
    "        for i in range(ns - 1 - self.L, -1, -1):\n",
    "            v = v + Us[i]\n",
    "            v = torch.cat((v, Ud[i]), -1)\n",
    "            v = self.evenOdd(v)\n",
    "        v = self.out(v[:, :N, :, :].contiguous().view(B, N, -1))\n",
    "        return (v.contiguous(), None)\n",
    "\n",
    "    def wavelet_transform(self, x):\n",
    "        xa = torch.cat([x[:, ::2, :, :],\n",
    "                        x[:, 1::2, :, :],\n",
    "                        ], -1)\n",
    "        d = torch.matmul(xa, self.ec_d)\n",
    "        s = torch.matmul(xa, self.ec_s)\n",
    "        return d, s\n",
    "\n",
    "    def evenOdd(self, x):\n",
    "        B, N, c, ich = x.shape  # (B, N, c, k)\n",
    "        assert ich == 2 * self.k\n",
    "        x_e = torch.matmul(x, self.rc_e)\n",
    "        x_o = torch.matmul(x, self.rc_o)\n",
    "\n",
    "        x = torch.zeros(B, N * 2, c, self.k,\n",
    "                        device=x.device)\n",
    "        x[..., ::2, :, :] = x_e\n",
    "        x[..., 1::2, :, :] = x_o\n",
    "        return x\n",
    "\n",
    "\n",
    "class FourierCrossAttentionW(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, seq_len_q, seq_len_kv, modes=16, activation='tanh',\n",
    "                 mode_select_method='random'):\n",
    "        super(FourierCrossAttentionW, self).__init__()\n",
    "        print('corss fourier correlation used!')\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.modes1 = modes\n",
    "        self.activation = activation\n",
    "\n",
    "    def compl_mul1d(self, order, x, weights):\n",
    "        x_flag = True\n",
    "        w_flag = True\n",
    "        if not torch.is_complex(x):\n",
    "            x_flag = False\n",
    "            x = torch.complex(x, torch.zeros_like(x).to(x.device))\n",
    "        if not torch.is_complex(weights):\n",
    "            w_flag = False\n",
    "            weights = torch.complex(weights, torch.zeros_like(weights).to(weights.device))\n",
    "        if x_flag or w_flag:\n",
    "            return torch.complex(torch.einsum(order, x.real, weights.real) - torch.einsum(order, x.imag, weights.imag),\n",
    "                                 torch.einsum(order, x.real, weights.imag) + torch.einsum(order, x.imag, weights.real))\n",
    "        else:\n",
    "            return torch.einsum(order, x.real, weights.real)\n",
    "\n",
    "    def forward(self, q, k, v, mask):\n",
    "        B, L, E, H = q.shape\n",
    "\n",
    "        xq = q.permute(0, 3, 2, 1)  # size = [B, H, E, L] torch.Size([3, 8, 64, 512])\n",
    "        xk = k.permute(0, 3, 2, 1)\n",
    "        xv = v.permute(0, 3, 2, 1)\n",
    "        self.index_q = list(range(0, min(int(L // 2), self.modes1)))\n",
    "        self.index_k_v = list(range(0, min(int(xv.shape[3] // 2), self.modes1)))\n",
    "\n",
    "        # Compute Fourier coefficients\n",
    "        xq_ft_ = torch.zeros(B, H, E, len(self.index_q), device=xq.device, dtype=torch.cfloat)\n",
    "        xq_ft = torch.fft.rfft(xq, dim=-1)\n",
    "        for i, j in enumerate(self.index_q):\n",
    "            xq_ft_[:, :, :, i] = xq_ft[:, :, :, j]\n",
    "\n",
    "        xk_ft_ = torch.zeros(B, H, E, len(self.index_k_v), device=xq.device, dtype=torch.cfloat)\n",
    "        xk_ft = torch.fft.rfft(xk, dim=-1)\n",
    "        for i, j in enumerate(self.index_k_v):\n",
    "            xk_ft_[:, :, :, i] = xk_ft[:, :, :, j]\n",
    "        xqk_ft = (self.compl_mul1d(\"bhex,bhey->bhxy\", xq_ft_, xk_ft_))\n",
    "        if self.activation == 'tanh':\n",
    "            xqk_ft = torch.complex(xqk_ft.real.tanh(), xqk_ft.imag.tanh())\n",
    "        elif self.activation == 'softmax':\n",
    "            xqk_ft = torch.softmax(abs(xqk_ft), dim=-1)\n",
    "            xqk_ft = torch.complex(xqk_ft, torch.zeros_like(xqk_ft))\n",
    "        else:\n",
    "            raise Exception('{} actiation function is not implemented'.format(self.activation))\n",
    "        xqkv_ft = self.compl_mul1d(\"bhxy,bhey->bhex\", xqk_ft, xk_ft_)\n",
    "\n",
    "        xqkvw = xqkv_ft\n",
    "        out_ft = torch.zeros(B, H, E, L // 2 + 1, device=xq.device, dtype=torch.cfloat)\n",
    "        for i, j in enumerate(self.index_q):\n",
    "            out_ft[:, :, :, j] = xqkvw[:, :, :, i]\n",
    "\n",
    "        out = torch.fft.irfft(out_ft / self.in_channels / self.out_channels, n=xq.size(-1)).permute(0, 3, 2, 1)\n",
    "        # size = [B, L, H, E]\n",
    "        return (out, None)\n",
    "\n",
    "\n",
    "class sparseKernelFT1d(nn.Module):\n",
    "    def __init__(self,\n",
    "                 k, alpha, c=1,\n",
    "                 nl=1,\n",
    "                 initializer=None,\n",
    "                 **kwargs):\n",
    "        super(sparseKernelFT1d, self).__init__()\n",
    "\n",
    "        self.modes1 = alpha\n",
    "        self.scale = (1 / (c * k * c * k))\n",
    "        self.weights1 = nn.Parameter(self.scale * torch.rand(c * k, c * k, self.modes1, dtype=torch.float))\n",
    "        self.weights2 = nn.Parameter(self.scale * torch.rand(c * k, c * k, self.modes1, dtype=torch.float))\n",
    "        self.weights1.requires_grad = True\n",
    "        self.weights2.requires_grad = True\n",
    "        self.k = k\n",
    "\n",
    "    def compl_mul1d(self, order, x, weights):\n",
    "        x_flag = True\n",
    "        w_flag = True\n",
    "        if not torch.is_complex(x):\n",
    "            x_flag = False\n",
    "            x = torch.complex(x, torch.zeros_like(x).to(x.device))\n",
    "        if not torch.is_complex(weights):\n",
    "            w_flag = False\n",
    "            weights = torch.complex(weights, torch.zeros_like(weights).to(weights.device))\n",
    "        if x_flag or w_flag:\n",
    "            return torch.complex(torch.einsum(order, x.real, weights.real) - torch.einsum(order, x.imag, weights.imag),\n",
    "                                 torch.einsum(order, x.real, weights.imag) + torch.einsum(order, x.imag, weights.real))\n",
    "        else:\n",
    "            return torch.einsum(order, x.real, weights.real)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, c, k = x.shape  # (B, N, c, k)\n",
    "\n",
    "        x = x.view(B, N, -1)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x_fft = torch.fft.rfft(x)\n",
    "        # Multiply relevant Fourier modes\n",
    "        l = min(self.modes1, N // 2 + 1)\n",
    "        out_ft = torch.zeros(B, c * k, N // 2 + 1, device=x.device, dtype=torch.cfloat)\n",
    "        out_ft[:, :, :l] = self.compl_mul1d(\"bix,iox->box\", x_fft[:, :, :l],\n",
    "                                            torch.complex(self.weights1, self.weights2)[:, :, :l])\n",
    "        x = torch.fft.irfft(out_ft, n=N)\n",
    "        x = x.permute(0, 2, 1).view(B, N, c, k)\n",
    "        return x\n",
    "\n",
    "\n",
    "# ##\n",
    "class MWT_CZ1d(nn.Module):\n",
    "    def __init__(self,\n",
    "                 k=3, alpha=64,\n",
    "                 L=0, c=1,\n",
    "                 base='legendre',\n",
    "                 initializer=None,\n",
    "                 **kwargs):\n",
    "        super(MWT_CZ1d, self).__init__()\n",
    "\n",
    "        self.k = k\n",
    "        self.L = L\n",
    "        H0, H1, G0, G1, PHI0, PHI1 = get_filter(base, k)\n",
    "        H0r = H0 @ PHI0\n",
    "        G0r = G0 @ PHI0\n",
    "        H1r = H1 @ PHI1\n",
    "        G1r = G1 @ PHI1\n",
    "\n",
    "        H0r[np.abs(H0r) < 1e-8] = 0\n",
    "        H1r[np.abs(H1r) < 1e-8] = 0\n",
    "        G0r[np.abs(G0r) < 1e-8] = 0\n",
    "        G1r[np.abs(G1r) < 1e-8] = 0\n",
    "        self.max_item = 3\n",
    "\n",
    "        self.A = sparseKernelFT1d(k, alpha, c)\n",
    "        self.B = sparseKernelFT1d(k, alpha, c)\n",
    "        self.C = sparseKernelFT1d(k, alpha, c)\n",
    "\n",
    "        self.T0 = nn.Linear(k, k)\n",
    "\n",
    "        self.register_buffer('ec_s', torch.Tensor(\n",
    "            np.concatenate((H0.T, H1.T), axis=0)))\n",
    "        self.register_buffer('ec_d', torch.Tensor(\n",
    "            np.concatenate((G0.T, G1.T), axis=0)))\n",
    "\n",
    "        self.register_buffer('rc_e', torch.Tensor(\n",
    "            np.concatenate((H0r, G0r), axis=0)))\n",
    "        self.register_buffer('rc_o', torch.Tensor(\n",
    "            np.concatenate((H1r, G1r), axis=0)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, c, k = x.shape  # (B, N, k)\n",
    "        ns = math.floor(np.log2(N))\n",
    "        nl = pow(2, math.ceil(np.log2(N)))\n",
    "        extra_x = x[:, 0:nl - N, :, :]\n",
    "        x = torch.cat([x, extra_x], 1)\n",
    "        Ud = torch.jit.annotate(List[Tensor], [])\n",
    "        Us = torch.jit.annotate(List[Tensor], [])\n",
    "        for i in range(ns - self.L):\n",
    "            d, x = self.wavelet_transform(x)\n",
    "            Ud += [self.A(d) + self.B(x)]\n",
    "            Us += [self.C(d)]\n",
    "        x = self.T0(x)  # coarsest scale transform\n",
    "\n",
    "        #        reconstruct\n",
    "        for i in range(ns - 1 - self.L, -1, -1):\n",
    "            x = x + Us[i]\n",
    "            x = torch.cat((x, Ud[i]), -1)\n",
    "            x = self.evenOdd(x)\n",
    "        x = x[:, :N, :, :]\n",
    "\n",
    "        return x\n",
    "\n",
    "    def wavelet_transform(self, x):\n",
    "        xa = torch.cat([x[:, ::2, :, :],\n",
    "                        x[:, 1::2, :, :],\n",
    "                        ], -1)\n",
    "        d = torch.matmul(xa, self.ec_d)\n",
    "        s = torch.matmul(xa, self.ec_s)\n",
    "        return d, s\n",
    "\n",
    "    def evenOdd(self, x):\n",
    "\n",
    "        B, N, c, ich = x.shape  # (B, N, c, k)\n",
    "        assert ich == 2 * self.k\n",
    "        x_e = torch.matmul(x, self.rc_e)\n",
    "        x_o = torch.matmul(x, self.rc_o)\n",
    "\n",
    "        x = torch.zeros(B, N * 2, c, self.k,\n",
    "                        device=x.device)\n",
    "        x[..., ::2, :, :] = x_e\n",
    "        x[..., 1::2, :, :] = x_o\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    \n",
    "# Autoformer_EncDec类\n",
    "class my_Layernorm(nn.Module):\n",
    "    \"\"\"\n",
    "    Special designed layernorm for the seasonal part\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, channels):\n",
    "        super(my_Layernorm, self).__init__()\n",
    "        self.layernorm = nn.LayerNorm(channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_hat = self.layernorm(x)\n",
    "        bias = torch.mean(x_hat, dim=1).unsqueeze(1).repeat(1, x.shape[1], 1)\n",
    "        return x_hat - bias\n",
    "\n",
    "\n",
    "class moving_avg(nn.Module):\n",
    "    \"\"\"\n",
    "    Moving average block to highlight the trend of time series\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, kernel_size, stride):\n",
    "        super(moving_avg, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # padding on the both ends of time series\n",
    "        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        x = torch.cat([front, x, end], dim=1)\n",
    "        x = self.avg(x.permute(0, 2, 1))\n",
    "        x = x.permute(0, 2, 1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class series_decomp(nn.Module):\n",
    "    \"\"\"\n",
    "    Series decomposition block\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, kernel_size):\n",
    "        super(series_decomp, self).__init__()\n",
    "        self.moving_avg = moving_avg(kernel_size, stride=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        moving_mean = self.moving_avg(x)\n",
    "        res = x - moving_mean\n",
    "        return res, moving_mean\n",
    "\n",
    "\n",
    "class series_decomp_multi(nn.Module):\n",
    "    \"\"\"\n",
    "    Multiple Series decomposition block from FEDformer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, kernel_size):\n",
    "        super(series_decomp_multi, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.series_decomp = [series_decomp(kernel) for kernel in kernel_size]\n",
    "\n",
    "    def forward(self, x):\n",
    "        moving_mean = []\n",
    "        res = []\n",
    "        for func in self.series_decomp:\n",
    "            sea, moving_avg = func(x)\n",
    "            moving_mean.append(moving_avg)\n",
    "            res.append(sea)\n",
    "\n",
    "        sea = sum(res) / len(res)\n",
    "        moving_mean = sum(moving_mean) / len(moving_mean)\n",
    "        return sea, moving_mean\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Autoformer encoder layer with the progressive decomposition architecture\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, attention, d_model, d_ff=None, moving_avg=25, dropout=0.1, activation=\"relu\"):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        d_ff = d_ff or 4 * d_model\n",
    "        self.attention = attention\n",
    "        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1, bias=False)\n",
    "        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1, bias=False)\n",
    "        self.decomp1 = series_decomp(moving_avg)\n",
    "        self.decomp2 = series_decomp(moving_avg)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = F.relu if activation == \"relu\" else F.gelu\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        new_x, attn = self.attention(\n",
    "            x, x, x,\n",
    "            attn_mask=attn_mask\n",
    "        )\n",
    "        x = x + self.dropout(new_x)\n",
    "        x, _ = self.decomp1(x)\n",
    "        y = x\n",
    "        y = self.dropout(self.activation(self.conv1(y.transpose(-1, 1))))\n",
    "        y = self.dropout(self.conv2(y).transpose(-1, 1))\n",
    "        res, _ = self.decomp2(x + y)\n",
    "        return res, attn\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Autoformer encoder\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, attn_layers, conv_layers=None, norm_layer=None):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.attn_layers = nn.ModuleList(attn_layers)\n",
    "        self.conv_layers = nn.ModuleList(conv_layers) if conv_layers is not None else None\n",
    "        self.norm = norm_layer\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        attns = []\n",
    "        if self.conv_layers is not None:\n",
    "            for attn_layer, conv_layer in zip(self.attn_layers, self.conv_layers):\n",
    "                x, attn = attn_layer(x, attn_mask=attn_mask)\n",
    "                x = conv_layer(x)\n",
    "                attns.append(attn)\n",
    "            x, attn = self.attn_layers[-1](x)\n",
    "            attns.append(attn)\n",
    "        else:\n",
    "            for attn_layer in self.attn_layers:\n",
    "                x, attn = attn_layer(x, attn_mask=attn_mask)\n",
    "                attns.append(attn)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "\n",
    "        return x, attns\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Autoformer decoder layer with the progressive decomposition architecture\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, self_attention, cross_attention, d_model, c_out, d_ff=None,\n",
    "                 moving_avg=25, dropout=0.1, activation=\"relu\"):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        d_ff = d_ff or 4 * d_model\n",
    "        self.self_attention = self_attention\n",
    "        self.cross_attention = cross_attention\n",
    "        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1, bias=False)\n",
    "        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1, bias=False)\n",
    "        self.decomp1 = series_decomp(moving_avg)\n",
    "        self.decomp2 = series_decomp(moving_avg)\n",
    "        self.decomp3 = series_decomp(moving_avg)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.projection = nn.Conv1d(in_channels=d_model, out_channels=c_out, kernel_size=3, stride=1, padding=1,\n",
    "                                    padding_mode='circular', bias=False)\n",
    "        self.activation = F.relu if activation == \"relu\" else F.gelu\n",
    "\n",
    "    def forward(self, x, cross, x_mask=None, cross_mask=None):\n",
    "        x = x + self.dropout(self.self_attention(\n",
    "            x, x, x,\n",
    "            attn_mask=x_mask\n",
    "        )[0])\n",
    "        x, trend1 = self.decomp1(x)\n",
    "        x = x + self.dropout(self.cross_attention(\n",
    "            x, cross, cross,\n",
    "            attn_mask=cross_mask\n",
    "        )[0])\n",
    "        x, trend2 = self.decomp2(x)\n",
    "        y = x\n",
    "        y = self.dropout(self.activation(self.conv1(y.transpose(-1, 1))))\n",
    "        y = self.dropout(self.conv2(y).transpose(-1, 1))\n",
    "        x, trend3 = self.decomp3(x + y)\n",
    "\n",
    "        residual_trend = trend1 + trend2 + trend3\n",
    "        residual_trend = self.projection(residual_trend.permute(0, 2, 1)).transpose(1, 2)\n",
    "        return x, residual_trend\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Autoformer encoder\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, layers, norm_layer=None, projection=None):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.norm = norm_layer\n",
    "        self.projection = projection\n",
    "\n",
    "    def forward(self, x, cross, x_mask=None, cross_mask=None, trend=None):\n",
    "        for layer in self.layers:\n",
    "            x, residual_trend = layer(x, cross, x_mask=x_mask, cross_mask=cross_mask)\n",
    "            trend = trend + residual_trend\n",
    "\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "\n",
    "        if self.projection is not None:\n",
    "            x = self.projection(x)\n",
    "        return x, trend\n",
    "    \n",
    "    \n",
    "# FEDformer模块\n",
    "class FEDformer(nn.Module):\n",
    "    def __init__(self, seq_len, label_len, pred_len, moving_avg, enc_in, dec_in, d_model, dropout, n_heads, d_ff, \n",
    "                 e_layers, c_out, d_layers, embed, freq, version='fourier', mode_select='random', modes=32):\n",
    "        \"\"\"\n",
    "        version: str, for FEDformer, there are two versions to choose, options: [Fourier, Wavelets].\n",
    "        mode_select: str, for FEDformer, there are two mode selection method, options: [random, low].\n",
    "        modes: int, modes to be selected.\n",
    "        \"\"\"\n",
    "        super(FEDformer, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.label_len = label_len\n",
    "        self.pred_len = pred_len\n",
    "        self.version = version\n",
    "        self.mode_select = mode_select\n",
    "        self.modes = modes\n",
    "\n",
    "        # Decomp\n",
    "        self.decomp = series_decomp(moving_avg)\n",
    "        self.enc_embedding = DataEmbedding(enc_in, d_model, embed, freq, dropout)\n",
    "        self.dec_embedding = DataEmbedding(dec_in, d_model, embed, freq, dropout)\n",
    "\n",
    "        if self.version == 'Wavelets':\n",
    "            encoder_self_att = MultiWaveletTransform(ich=d_model, L=1, base='legendre')\n",
    "            decoder_self_att = MultiWaveletTransform(ich=d_model, L=1, base='legendre')\n",
    "            decoder_cross_att = MultiWaveletCross(in_channels=d_model,\n",
    "                                                  out_channels=d_model,\n",
    "                                                  seq_len_q=self.seq_len // 2 + self.pred_len,\n",
    "                                                  seq_len_kv=self.seq_len,\n",
    "                                                  modes=self.modes,\n",
    "                                                  ich=d_model,\n",
    "                                                  base='legendre',\n",
    "                                                  activation='tanh')\n",
    "        else:\n",
    "            encoder_self_att = FourierBlock(in_channels=d_model,\n",
    "                                            out_channels=d_model,\n",
    "                                            seq_len=self.seq_len,\n",
    "                                            modes=self.modes,\n",
    "                                            mode_select_method=self.mode_select)\n",
    "            decoder_self_att = FourierBlock(in_channels=d_model,\n",
    "                                            out_channels=d_model,\n",
    "                                            seq_len=self.seq_len // 2 + self.pred_len,\n",
    "                                            modes=self.modes,\n",
    "                                            mode_select_method=self.mode_select)\n",
    "            decoder_cross_att = FourierCrossAttention(in_channels=d_model,\n",
    "                                                      out_channels=d_model,\n",
    "                                                      seq_len_q=self.seq_len // 2 + self.pred_len,\n",
    "                                                      seq_len_kv=self.seq_len,\n",
    "                                                      modes=self.modes,\n",
    "                                                      mode_select_method=self.mode_select,\n",
    "                                                      num_heads=n_heads)\n",
    "        # Encoder\n",
    "        self.encoder = Encoder(\n",
    "            [\n",
    "                EncoderLayer(\n",
    "                    AutoCorrelationLayer(\n",
    "                        encoder_self_att,  # instead of multi-head attention in transformer\n",
    "                        d_model, n_heads),\n",
    "                    d_model,\n",
    "                    d_ff,\n",
    "                    moving_avg=moving_avg,\n",
    "                    dropout=dropout,\n",
    "                    activation='relu'\n",
    "                ) for l in range(e_layers)\n",
    "            ],\n",
    "            norm_layer=my_Layernorm(d_model)\n",
    "        )\n",
    "        # Decoder\n",
    "        self.decoder = Decoder(\n",
    "            [\n",
    "                DecoderLayer(\n",
    "                    AutoCorrelationLayer(\n",
    "                        decoder_self_att, d_model, n_heads),\n",
    "                    AutoCorrelationLayer(\n",
    "                        decoder_cross_att, d_model, n_heads),\n",
    "                    d_model,\n",
    "                    c_out,\n",
    "                    d_ff,\n",
    "                    moving_avg=moving_avg,\n",
    "                    dropout=dropout,\n",
    "                    activation='relu',\n",
    "                )\n",
    "                for l in range(d_layers)\n",
    "            ],\n",
    "            norm_layer=my_Layernorm(d_model),\n",
    "            projection=nn.Linear(d_model, c_out, bias=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec):\n",
    "        # decomp init\n",
    "        mean = torch.mean(x_enc, dim=1).unsqueeze(1).repeat(1, self.pred_len, 1)\n",
    "        seasonal_init, trend_init = self.decomp(x_enc)  # x - moving_avg, moving_avg\n",
    "        # decoder input\n",
    "        trend_init = torch.cat([trend_init[:, -self.label_len:, :], mean], dim=1)\n",
    "        seasonal_init = F.pad(seasonal_init[:, -self.label_len:, :], (0, 0, 0, self.pred_len))\n",
    "        # enc\n",
    "        enc_out = self.enc_embedding(x_enc, x_mark_enc)\n",
    "        dec_out = self.dec_embedding(seasonal_init, x_mark_dec)\n",
    "        enc_out, attns = self.encoder(enc_out, attn_mask=None)\n",
    "        # dec\n",
    "        seasonal_part, trend_part = self.decoder(dec_out, enc_out, x_mask=None, cross_mask=None, trend=trend_init)\n",
    "        # final\n",
    "        dec_out = trend_part + seasonal_part\n",
    "        \n",
    "        output = dec_out[:, -self.pred_len:, :]  # [B, L, D]\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666fe96f",
   "metadata": {},
   "source": [
    "### 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7b184f28",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:21:08.232878Z",
     "start_time": "2024-04-14T13:21:08.195758Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(task_args, train_args, model_args):\n",
    "    # 参数配置\n",
    "    columns = task_args['columns'] # 模型全部特征\n",
    "    target = task_args['target'] # 模型预测特征\n",
    "    features = task_args['features'] # 模型预测模式\n",
    "    model_name = train_args['model_name'] # 模型名称\n",
    "    train_loader = train_args['train_loader'] # 训练集\n",
    "    valid_loader = train_args['valid_loader'] # 验证集\n",
    "    n_epochs = train_args['n_epochs'] # 训练次数\n",
    "    learning_rate = train_args['learning_rate'] # 学习率\n",
    "    loss = train_args['loss'] # 损失函数\n",
    "    patience = train_args['patience'] # 最大早停次数阈值，超过就会早停\n",
    "    lradj = train_args['lradj'] # 学习率函数\n",
    "    path = train_args['path'] # 模型保存路径\n",
    "    verbose = train_args['verbose'] # 打印训练过程\n",
    "    plots = train_args['plots'] # 绘制损失图\n",
    "    pred_len = model_args['pred_len'] # 预测长度\n",
    "    label_len = model_args['label_len']\n",
    "    \n",
    "    #检查是否可用GPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # 创建模型和优化器\n",
    "    model = model_name(**model_args).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_fn = loss\n",
    "    \n",
    "    # 损失函数值\n",
    "    train_losses, val_losses = [], []\n",
    "    \n",
    "    # 调整学习率\n",
    "    def adjust_learning_rate(optimizer, epoch, lradj, learning_rate, train_epochs):\n",
    "        # lr = learning_rate * (0.2 ** (epoch // 2))\n",
    "        if lradj == 'type1':\n",
    "            lr_adjust = {epoch: learning_rate * (0.5 ** ((epoch - 1) // 1))}\n",
    "        elif lradj == 'type2':\n",
    "            lr_adjust = {\n",
    "                2: 5e-5, 4: 1e-5, 6: 5e-6, 8: 1e-6,\n",
    "                10: 5e-7, 15: 1e-7, 20: 5e-8\n",
    "            }\n",
    "        elif lradj == \"cosine\":\n",
    "            lr_adjust = {epoch: learning_rate /2 * (1 + math.cos(epoch / train_epochs * math.pi))}\n",
    "        if epoch in lr_adjust.keys():\n",
    "            lr = lr_adjust[epoch]\n",
    "            # 参数组(param_groups)是用来指定不同的参数组以便对它们进行不同的优化设置，比如'lr'\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "            print('Updating learning rate to {}'.format(lr))\n",
    "    \n",
    "    # 设置早停\n",
    "    class EarlyStopping():\n",
    "        def __init__(self, patience=7, verbose=False, delta=0):\n",
    "            self.patience = patience # 连续超限次数，如果满足条件，则早停\n",
    "            self.verbose = verbose\n",
    "            self.counter = 0\n",
    "            self.best_score = None\n",
    "            self.early_stop = False\n",
    "            self.val_loss_min = np.Inf\n",
    "            self.delta = delta\n",
    "\n",
    "        def __call__(self, val_loss, model, path):\n",
    "            score = -val_loss\n",
    "            if self.best_score is None:\n",
    "                self.best_score = score\n",
    "                self.save_checkpoint(val_loss, model, path)\n",
    "            elif score < self.best_score + self.delta:\n",
    "                self.counter += 1\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "                if self.counter >= self.patience:\n",
    "                    self.early_stop = True\n",
    "            else:\n",
    "                self.best_score = score\n",
    "                self.save_checkpoint(val_loss, model, path)\n",
    "                self.counter = 0\n",
    "\n",
    "        def save_checkpoint(self, val_loss, model, path):\n",
    "            if self.verbose:\n",
    "                print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "            torch.save(model.state_dict(), path + '/' + 'checkpoint.pth')\n",
    "            self.val_loss_min = val_loss\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=verbose)\n",
    "    \n",
    "    # 训练任务\n",
    "    def forecasting_task(columns, target, features):\n",
    "        \"\"\"\n",
    "        features: [M, S, MS]; \n",
    "            M:multivariate predict multivariate, \n",
    "            S:univariate predict univariate, \n",
    "            MS:multivariate predict univariate'\n",
    "        \"\"\"\n",
    "        # 字典索引生成\n",
    "        col_dict = {}\n",
    "        for i,j in enumerate(columns):\n",
    "            col_dict[j] = i\n",
    "\n",
    "        if features == 'MS':\n",
    "            target = target[0]\n",
    "            f_dim = col_dict[target]\n",
    "        elif features == 'S':\n",
    "            f_dim = 0\n",
    "        else:\n",
    "            f_dim = 0\n",
    "        return f_dim\n",
    "    f_dim = forecasting_task(columns, target, features)\n",
    "    \n",
    "    # 设置保存模型路径\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    # 模型训练和验证\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for batch_x, batch_y, batch_x_mark, batch_y_mark in train_loader:\n",
    "            #将数据移至 GPU\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            batch_x_mark = batch_x_mark.to(device)\n",
    "            batch_y_mark = batch_y_mark.to(device)\n",
    "            # 清空梯度\n",
    "            optimizer.zero_grad()\n",
    "            # decoder输入\n",
    "            dec_inp = torch.zeros_like(batch_y[:, -pred_len:, :]).float()\n",
    "            dec_inp = torch.cat([batch_y[:, :label_len, :], dec_inp], dim=1).float().to(device)\n",
    "            outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "            if features == 'MS':\n",
    "                outputs = outputs[:, :, f_dim: f_dim+1] \n",
    "            else:\n",
    "                outputs = outputs[:, :, f_dim:]\n",
    "            train_loss = loss_fn(outputs, batch_y)\n",
    "            # 反向传播计算得到每个参数的梯度值\n",
    "            train_loss.backward()\n",
    "            # 通过梯度下降执行一步参数更新\n",
    "            optimizer.step()\n",
    "            #每个batch的loss和\n",
    "            total_train_loss += train_loss.item() # .item()表示只包含一个元素的tensor中提取值\n",
    "                \n",
    "        #每个epoch的损失平均\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        \n",
    "        #所有epoch的loss\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        #评估模型\n",
    "        model.eval()\n",
    "        #关闭自动求导功能，只使用训练好的模型进行预测或评估，不需要进行梯度计算和参数更新\n",
    "        with torch.no_grad():\n",
    "            total_val_loss = 0\n",
    "            for batch_x, batch_y, batch_x_mark, batch_y_mark in valid_loader:\n",
    "                #将数据移至 GPU\n",
    "                batch_x = batch_x.to(device)\n",
    "                batch_y = batch_y.to(device)\n",
    "                batch_x_mark = batch_x_mark.to(device)\n",
    "                batch_y_mark = batch_y_mark.to(device)\n",
    "                # decoder输入\n",
    "                dec_inp = torch.zeros_like(batch_y[:, -pred_len:, :]).float()\n",
    "                dec_inp = torch.cat([batch_y[:, :label_len, :], dec_inp], dim=1).float().to(device)\n",
    "                val_outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "                if features == 'MS':\n",
    "                    val_outputs = val_outputs[:, :, f_dim: f_dim+1] \n",
    "                else:\n",
    "                    val_outputs = val_outputs[:, :, f_dim:]\n",
    "                val_loss = loss_fn(val_outputs, batch_y)\n",
    "                #每个batch的loss和\n",
    "                total_val_loss += val_loss.item()\n",
    "                \n",
    "        #每个epoch的损失平均\n",
    "        avg_val_loss = total_val_loss / len(valid_loader)\n",
    "        \n",
    "        #所有epoch的loss\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        #打印训练过程\n",
    "        if verbose:\n",
    "            print(f'Epoch [{epoch+1}/{n_epochs}], Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}')\n",
    "            \n",
    "        # 设置早停\n",
    "        early_stopping(avg_val_loss, model, path)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping!\")\n",
    "            break\n",
    "            \n",
    "        # 调整学习率\n",
    "        adjust_learning_rate(optimizer, epoch+1, lradj, learning_rate, n_epochs)\n",
    "        \n",
    "    # 加载最佳模型\n",
    "    best_model_path = path + '/' + 'checkpoint.pth'\n",
    "    model.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "    #绘制损失函数图\n",
    "    def plot_loss(train_losses, val_losses):\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.style.use('seaborn-v0_8-paper') #绘制背景色\n",
    "        plt.grid(axis = 'y',linewidth=0.35) #绘制网格\n",
    "        plt.plot(val_losses, linestyle='-',color = '#11b3b6')\n",
    "        plt.plot(train_losses, linestyle='-',color = '#f14643')\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Training and Validation Progress\")\n",
    "        plt.legend([\"Validation\", \"Training\"])\n",
    "        plt.show()\n",
    "    if plots:\n",
    "        plot_loss(train_losses, val_losses)\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b868730a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:22:25.611510Z",
     "start_time": "2024-04-14T13:21:16.072579Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fourier enhanced block used!\n",
      "modes=32, index=[0, 1, 2]\n",
      "fourier enhanced block used!\n",
      "modes=32, index=[0, 1, 2]\n",
      "fourier enhanced cross attention used!\n",
      "modes_q=3, index_q=[0, 1, 2]\n",
      "modes_kv=3, index_kv=[0, 1, 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|████▏                                                                              | 1/20 [00:29<09:21, 29.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Training Loss: 0.0792, Validation Loss: 0.9712\n",
      "Validation loss decreased (inf --> 0.971155).  Saving model ...\n",
      "Updating learning rate to 0.0009938441702975688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|████████▎                                                                          | 2/20 [01:04<09:47, 32.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/20], Training Loss: 0.0134, Validation Loss: 0.1032\n",
      "Validation loss decreased (0.971155 --> 0.103182).  Saving model ...\n",
      "Updating learning rate to 0.0009755282581475768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|████████▎                                                                          | 2/20 [01:08<10:13, 34.08s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 39\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 构造参数字典\u001b[39;00m\n\u001b[0;32m      2\u001b[0m params3 \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtask_args\u001b[39m\u001b[38;5;124m\"\u001b[39m:{\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mload\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemp\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m     },\n\u001b[0;32m     38\u001b[0m }\n\u001b[1;32m---> 39\u001b[0m model \u001b[38;5;241m=\u001b[39m train(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams3)\n",
      "Cell \u001b[1;32mIn[42], line 132\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(task_args, train_args, model_args)\u001b[0m\n\u001b[0;32m    130\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs, batch_y)\n\u001b[0;32m    131\u001b[0m \u001b[38;5;66;03m# 反向传播计算得到每个参数的梯度值\u001b[39;00m\n\u001b[1;32m--> 132\u001b[0m \u001b[43mtrain_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;66;03m# 通过梯度下降执行一步参数更新\u001b[39;00m\n\u001b[0;32m    134\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 构造参数字典\n",
    "params3 = {\n",
    "    \"task_args\":{\n",
    "        \"columns\": ['load', 'temp'],\n",
    "        \"target\": ['load'],\n",
    "        \"features\": 'MS'\n",
    "    },\n",
    "    \"train_args\": {\n",
    "        \"model_name\": FEDformer,\n",
    "        \"train_loader\": train_loader,\n",
    "        \"valid_loader\": valid_loader,\n",
    "        \"n_epochs\": 20,\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"loss\": nn.MSELoss(),\n",
    "        \"patience\": 7,\n",
    "        \"lradj\": 'cosine',\n",
    "        \"path\": \"../models/test\",\n",
    "        \"verbose\": True,\n",
    "        \"plots\": True,\n",
    "    },\n",
    "    \"model_args\": {\n",
    "        'enc_in': 2, \n",
    "        'dec_in': 2,\n",
    "        'seq_len': 6,\n",
    "        'pred_len': 3,\n",
    "        'label_len': 3,\n",
    "        'moving_avg': 3,\n",
    "        'd_model': 128,\n",
    "        'n_heads': 8,\n",
    "        'd_ff': 128,\n",
    "        'embed': 'fixed',\n",
    "        'freq': 'h',\n",
    "        'dropout': 0.1,\n",
    "        'e_layers': 1,\n",
    "        'd_layers': 1,\n",
    "        'c_out': 2\n",
    "    },\n",
    "}\n",
    "model = train(**params3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cefa5b",
   "metadata": {},
   "source": [
    "# 基于FiLM的时间序列预测"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf72af68",
   "metadata": {},
   "source": [
    "## 多变量多步预测"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c613a21c",
   "metadata": {},
   "source": [
    "### 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d7e7047a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:22:30.484029Z",
     "start_time": "2024-04-14T13:22:30.472983Z"
    }
   },
   "outputs": [],
   "source": [
    "# 读取数据\n",
    "def get_dataset(path, time_col=None):\n",
    "    data = pd.read_csv(path)\n",
    "    rows = data.shape[0]\n",
    "    now = datetime.now()\n",
    "    newtime = now.replace(microsecond=0)\n",
    "    if time_col == None:\n",
    "        # 如果没有时间列，生成时间戳范围\n",
    "        time_index = pd.date_range(start=datetime.now() -\n",
    "                                   timedelta(seconds=rows - 1),\n",
    "                                   end=datetime.now(),\n",
    "                                   freq='S')\n",
    "        full_data = pd.DataFrame(data=data.values,\n",
    "                                 index=pd.to_datetime(time_index, unit='s'),\n",
    "                                 columns=data.columns)\n",
    "    else:\n",
    "        columns = [i for i in data.columns if i != time_col] # 去除时间列\n",
    "        full_data = pd.DataFrame(data=data.drop([time_col], axis=1).values,\n",
    "                                 index=pd.to_datetime(data[time_col].values),\n",
    "                                 columns=columns)\n",
    "    return full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e0502843",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:22:31.772677Z",
     "start_time": "2024-04-14T13:22:31.693119Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>load</th>\n",
       "      <th>temp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-01-01 00:00:00</th>\n",
       "      <td>2698.0</td>\n",
       "      <td>32.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-01 01:00:00</th>\n",
       "      <td>2558.0</td>\n",
       "      <td>32.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-01 02:00:00</th>\n",
       "      <td>2444.0</td>\n",
       "      <td>30.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-01 03:00:00</th>\n",
       "      <td>2402.0</td>\n",
       "      <td>31.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-01 04:00:00</th>\n",
       "      <td>2403.0</td>\n",
       "      <td>32.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-12-31 19:00:00</th>\n",
       "      <td>4012.0</td>\n",
       "      <td>18.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-12-31 20:00:00</th>\n",
       "      <td>3856.0</td>\n",
       "      <td>16.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-12-31 21:00:00</th>\n",
       "      <td>3671.0</td>\n",
       "      <td>17.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-12-31 22:00:00</th>\n",
       "      <td>3499.0</td>\n",
       "      <td>15.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-12-31 23:00:00</th>\n",
       "      <td>3345.0</td>\n",
       "      <td>15.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26304 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       load   temp\n",
       "2012-01-01 00:00:00  2698.0  32.00\n",
       "2012-01-01 01:00:00  2558.0  32.67\n",
       "2012-01-01 02:00:00  2444.0  30.00\n",
       "2012-01-01 03:00:00  2402.0  31.00\n",
       "2012-01-01 04:00:00  2403.0  32.00\n",
       "...                     ...    ...\n",
       "2014-12-31 19:00:00  4012.0  18.00\n",
       "2014-12-31 20:00:00  3856.0  16.67\n",
       "2014-12-31 21:00:00  3671.0  17.00\n",
       "2014-12-31 22:00:00  3499.0  15.33\n",
       "2014-12-31 23:00:00  3345.0  15.33\n",
       "\n",
       "[26304 rows x 2 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"../data/energy.csv\"\n",
    "ts_data = get_dataset(path, time_col='time')\n",
    "ts_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f5f03197",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:22:33.359465Z",
     "start_time": "2024-04-14T13:22:33.324053Z"
    }
   },
   "outputs": [],
   "source": [
    "# 时间格式编码\n",
    "def time_features_from_frequency_str(freq_str: str):\n",
    "    \"\"\"\n",
    "    Returns a list of time features that will be appropriate for the given frequency string.\n",
    "    Parameters\n",
    "    ----------\n",
    "    freq_str\n",
    "        Frequency string of the form [multiple][granularity] such as \"12H\", \"5min\", \"1D\" etc.\n",
    "    \"\"\"\n",
    "\n",
    "    class TimeFeature:\n",
    "        def __init__(self):\n",
    "            pass\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            pass\n",
    "\n",
    "        def __repr__(self):\n",
    "            return self.__class__.__name__ + \"()\"\n",
    "\n",
    "\n",
    "    class SecondOfMinute(TimeFeature):\n",
    "        \"\"\"Minute of hour encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return index.second / 59.0 - 0.5\n",
    "\n",
    "\n",
    "    class MinuteOfHour(TimeFeature):\n",
    "        \"\"\"Minute of hour encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return index.minute / 59.0 - 0.5\n",
    "\n",
    "\n",
    "    class HourOfDay(TimeFeature):\n",
    "        \"\"\"Hour of day encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return index.hour / 23.0 - 0.5\n",
    "\n",
    "\n",
    "    class DayOfWeek(TimeFeature):\n",
    "        \"\"\"Hour of day encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return index.dayofweek / 6.0 - 0.5\n",
    "\n",
    "\n",
    "    class DayOfMonth(TimeFeature):\n",
    "        \"\"\"Day of month encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return (index.day - 1) / 30.0 - 0.5\n",
    "\n",
    "\n",
    "    class DayOfYear(TimeFeature):\n",
    "        \"\"\"Day of year encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return (index.dayofyear - 1) / 365.0 - 0.5\n",
    "\n",
    "\n",
    "    class MonthOfYear(TimeFeature):\n",
    "        \"\"\"Month of year encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return (index.month - 1) / 11.0 - 0.5\n",
    "\n",
    "\n",
    "    class WeekOfYear(TimeFeature):\n",
    "        \"\"\"Week of year encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return (index.isocalendar().week - 1) / 52.0 - 0.5\n",
    "\n",
    "    \n",
    "    features_by_offsets = {\n",
    "        offsets.YearEnd: [],\n",
    "        offsets.QuarterEnd: [MonthOfYear],\n",
    "        offsets.MonthEnd: [MonthOfYear],\n",
    "        offsets.Week: [DayOfMonth, WeekOfYear],\n",
    "        offsets.Day: [DayOfWeek, DayOfMonth, DayOfYear],\n",
    "        offsets.BusinessDay: [DayOfWeek, DayOfMonth, DayOfYear],\n",
    "        offsets.Hour: [HourOfDay, DayOfWeek, DayOfMonth, DayOfYear],\n",
    "        offsets.Minute: [\n",
    "            MinuteOfHour,\n",
    "            HourOfDay,\n",
    "            DayOfWeek,\n",
    "            DayOfMonth,\n",
    "            DayOfYear,\n",
    "        ],\n",
    "        offsets.Second: [\n",
    "            SecondOfMinute,\n",
    "            MinuteOfHour,\n",
    "            HourOfDay,\n",
    "            DayOfWeek,\n",
    "            DayOfMonth,\n",
    "            DayOfYear,\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    offset = to_offset(freq_str)\n",
    "\n",
    "    for offset_type, feature_classes in features_by_offsets.items():\n",
    "        if isinstance(offset, offset_type):\n",
    "            return [cls() for cls in feature_classes]\n",
    "\n",
    "    supported_freq_msg = f\"\"\"\n",
    "    Unsupported frequency {freq_str}\n",
    "    The following frequencies are supported:\n",
    "        Y   - yearly\n",
    "            alias: A\n",
    "        M   - monthly\n",
    "        W   - weekly\n",
    "        D   - daily\n",
    "        B   - business days\n",
    "        H   - hourly\n",
    "        T   - minutely\n",
    "            alias: min\n",
    "        S   - secondly\n",
    "    \"\"\"\n",
    "    raise RuntimeError(supported_freq_msg)\n",
    "    \n",
    "def time_features(dates, freq='h'):\n",
    "    return np.vstack([feat(dates) for feat in time_features_from_frequency_str(freq)])\n",
    "\n",
    "# 包含时间维度的数据集划分\n",
    "def divide_dataset(df, valid_date, test_date, x_feature_list, y_feature_list, freq):\n",
    "    #归一化\n",
    "    x_scaler = MinMaxScaler()\n",
    "    y_scaler = MinMaxScaler()\n",
    "\n",
    "    #测试集\n",
    "    train = df.copy()[df.index < valid_date][x_feature_list]\n",
    "    train_stamp = time_features(pd.to_datetime(train.index), freq=freq)\n",
    "    train_stamp = train_stamp.transpose(1, 0)\n",
    "    train[x_feature_list] = x_scaler.fit_transform(train)\n",
    "    xtr = train.values.astype('float32')\n",
    "\n",
    "    #验证集\n",
    "    valid = df.copy()[(df.index >= valid_date) & (df.index < test_date)][x_feature_list]\n",
    "    valid_stamp = time_features(pd.to_datetime(valid.index), freq=freq)\n",
    "    valid_stamp = valid_stamp.transpose(1, 0)\n",
    "    valid[x_feature_list] = x_scaler.fit_transform(valid)\n",
    "    xva = valid.values.astype('float32')\n",
    "\n",
    "    #测试集\n",
    "    test = df.copy()[test_date:][x_feature_list]\n",
    "    test_stamp = time_features(pd.to_datetime(test.index), freq=freq)\n",
    "    test_stamp = test_stamp.transpose(1, 0)\n",
    "    test[x_feature_list] = x_scaler.fit_transform(test)\n",
    "    xte = test.values.astype('float32')\n",
    "\n",
    "    #标签\n",
    "    ytr = df.copy()[df.index < valid_date][y_feature_list]\n",
    "    ytr[y_feature_list] = y_scaler.fit_transform(ytr)\n",
    "    ytr = ytr.values.astype('float32')\n",
    "\n",
    "    yva = df.copy()[(df.index >= valid_date) & (df.index < test_date)][y_feature_list]\n",
    "    yva[y_feature_list] = y_scaler.fit_transform(yva)\n",
    "    yva = yva.values.astype('float32')\n",
    "\n",
    "    yte = df.copy()[test_date:][y_feature_list]\n",
    "    yte[y_feature_list] = y_scaler.fit_transform(yte)\n",
    "    yte = yte.values.astype('float32')\n",
    "    \n",
    "    # 数据合并，[训练集，测试集，时间戳]\n",
    "    train = [xtr, ytr, train_stamp]\n",
    "    valid = [xva, yva, valid_stamp]\n",
    "    test = [xte, yte, test_stamp]\n",
    "    \n",
    "    return y_scaler, train, valid, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "94510130",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:22:34.445158Z",
     "start_time": "2024-04-14T13:22:34.371511Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (23376, 2) y_train shape: (23376, 1) stamp_train shape: (23376, 4)\n",
      "x_valid shape: (1464, 2) y_valid shape: (1464, 1) stamp_valid shape: (1464, 4)\n",
      "x_test shape: (1464, 2) y_test shape: (1464, 1) stamp_test shape: (1464, 4)\n"
     ]
    }
   ],
   "source": [
    "# 构造参数字典\n",
    "params1 = {\n",
    "    \"df\": ts_data,\n",
    "    \"valid_date\": \"2014-09-01 00:00:00\",\n",
    "    \"test_date\": \"2014-11-01 00:00:00\",\n",
    "    \"x_feature_list\": [\"load\", 'temp'],\n",
    "    \"y_feature_list\": [\"load\"],\n",
    "    \"freq\": 'h'\n",
    "}\n",
    "\n",
    "#函数传参\n",
    "scaler, train, valid, test = divide_dataset(**params1)\n",
    "print(\"x_train shape: {0} y_train shape: {1} stamp_train shape: {2}\".format(train[0].shape, train[1].shape, train[2].shape))\n",
    "print(\"x_valid shape: {0} y_valid shape: {1} stamp_valid shape: {2}\".format(valid[0].shape, valid[1].shape, valid[2].shape))\n",
    "print(\"x_test shape: {0} y_test shape: {1} stamp_test shape: {2}\".format(test[0].shape, test[1].shape, test[2].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "437dbe70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:22:36.009075Z",
     "start_time": "2024-04-14T13:22:35.991478Z"
    }
   },
   "outputs": [],
   "source": [
    "#利用前seq_len个数据，预测下pred_len个数据\n",
    "def create_dataset(data_list, seq_len, pred_len, label_len, batch_size):\n",
    "    \"\"\"\n",
    "    @参数设置：\n",
    "    data_list：特征，目标，时间戳\n",
    "    seq_len：输入数据包含过去多少个时间步\n",
    "    pred_len：目标应该在未来多少个时间步之后\n",
    "    label_len：先验时间步\n",
    "    \"\"\"\n",
    "    # 获取数据\n",
    "    feature = data_list[0] # 特征\n",
    "    target = data_list[1] # 目标\n",
    "    stamp = data_list[2] # 时间戳\n",
    "    \n",
    "    # 循环生成数据\n",
    "    X, y = [], []\n",
    "    X_stamp, y_stamp = [], []\n",
    "    seq_len = seq_len - 1 # 包含当前时间点\n",
    "    for i in range(seq_len, len(feature) - pred_len):\n",
    "        # 数据维度\n",
    "        feat = feature[i - seq_len:i + 1]\n",
    "        tar = target[i + 1:i + 1 + pred_len]\n",
    "        X.append(feat)\n",
    "        y.append(tar)\n",
    "        \n",
    "        # 时间维度\n",
    "        xs = stamp[i - seq_len:i + 1]\n",
    "        ys = stamp[i + 1 - label_len:i + 1 + pred_len]\n",
    "        X_stamp.append(xs)\n",
    "        y_stamp.append(ys)\n",
    "        \n",
    "    # 转为张量，数据维度\n",
    "    X = torch.as_tensor(X).float()\n",
    "    y = torch.as_tensor(y).float()\n",
    "    \n",
    "    # 转为张量，时间维度\n",
    "    X_stamp = torch.as_tensor(X_stamp).float()\n",
    "    y_stamp = torch.as_tensor(y_stamp).float()\n",
    "    \n",
    "    # 创建dataloader，[特征，目标，特征时间编码，目标时间编码]\n",
    "    data_loader = DataLoader(TensorDataset(X, y, X_stamp, y_stamp), shuffle=True, batch_size=batch_size)\n",
    "    \n",
    "    return X, y, data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "60657bb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:22:38.393111Z",
     "start_time": "2024-04-14T13:22:37.363411Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_size: torch.Size([23368, 6, 2]),y_size: torch.Size([23368, 3, 1]),loader_len: 731\n",
      "X_size: torch.Size([1456, 6, 2]),y_size: torch.Size([1456, 3, 1]),loader_len: 46\n",
      "X_size: torch.Size([1456, 6, 2]),y_size: torch.Size([1456, 3, 1]),loader_len: 46\n"
     ]
    }
   ],
   "source": [
    "# 构造参数字典\n",
    "params2 = {\n",
    "    \"seq_len\": 6,\n",
    "    \"pred_len\": 3,\n",
    "    \"label_len\": 3,\n",
    "    \"batch_size\": 32,\n",
    "}\n",
    "X_train, y_train, train_loader = create_dataset(train, **params2)\n",
    "X_valid, y_valid, valid_loader = create_dataset(valid, **params2)\n",
    "X_test, y_test, test_loader = create_dataset(test, **params2)\n",
    "print(\"X_size: {0},y_size: {1},loader_len: {2}\".format(X_train.shape, y_train.shape, len(train_loader)))\n",
    "print(\"X_size: {0},y_size: {1},loader_len: {2}\".format(X_valid.shape, y_valid.shape, len(valid_loader)))\n",
    "print(\"X_size: {0},y_size: {1},loader_len: {2}\".format(X_test.shape, y_test.shape, len(test_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc791901",
   "metadata": {},
   "source": [
    "### 模型定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bcbd4b41",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:22:41.066590Z",
     "start_time": "2024-04-14T13:22:41.026965Z"
    }
   },
   "outputs": [],
   "source": [
    "def transition(N):\n",
    "    Q = np.arange(N, dtype=np.float64)\n",
    "    R = (2 * Q + 1)[:, None]  # / theta\n",
    "    j, i = np.meshgrid(Q, Q)\n",
    "    A = np.where(i < j, -1, (-1.) ** (i - j + 1)) * R\n",
    "    B = (-1.) ** Q[:, None] * R\n",
    "    return A, B\n",
    "\n",
    "\n",
    "class HiPPO_LegT(nn.Module):\n",
    "    def __init__(self, N, dt=1.0, discretization='bilinear'):\n",
    "        \"\"\"\n",
    "        N: the order of the HiPPO projection\n",
    "        dt: discretization step size - should be roughly inverse to the length of the sequence\n",
    "        \"\"\"\n",
    "        super(HiPPO_LegT, self).__init__()\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.N = N\n",
    "        A, B = transition(N)\n",
    "        C = np.ones((1, N))\n",
    "        D = np.zeros((1,))\n",
    "        A, B, _, _, _ = signal.cont2discrete((A, B, C, D), dt=dt, method=discretization)\n",
    "\n",
    "        B = B.squeeze(-1)\n",
    "\n",
    "        self.register_buffer('A', torch.Tensor(A).to(self.device))\n",
    "        self.register_buffer('B', torch.Tensor(B).to(self.device))\n",
    "        vals = np.arange(0.0, 1.0, dt)\n",
    "        self.register_buffer('eval_matrix', torch.Tensor(eval_legendre(np.arange(N)[:, None], 1 - 2 * vals).T).to(self.device))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        inputs : (length, ...)\n",
    "        output : (length, ..., N) where N is the order of the HiPPO projection\n",
    "        \"\"\"\n",
    "        c = torch.zeros(inputs.shape[:-1] + tuple([self.N])).to(self.device)\n",
    "        cs = []\n",
    "        for f in inputs.permute([-1, 0, 1]):\n",
    "            f = f.unsqueeze(-1)\n",
    "            new = f @ self.B.unsqueeze(0)\n",
    "            c = F.linear(c, self.A) + new\n",
    "            cs.append(c)\n",
    "        return torch.stack(cs, dim=0)\n",
    "\n",
    "    def reconstruct(self, c):\n",
    "        return (self.eval_matrix @ c.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "\n",
    "class SpectralConv1d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, seq_len, ratio=0.5):\n",
    "        \"\"\"\n",
    "        1D Fourier layer. It does FFT, linear transform, and Inverse FFT.\n",
    "        \"\"\"\n",
    "        super(SpectralConv1d, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.ratio = ratio\n",
    "        self.modes = min(32, seq_len // 2)\n",
    "        self.index = list(range(0, self.modes))\n",
    "\n",
    "        self.scale = (1 / (in_channels * out_channels))\n",
    "        self.weights_real = nn.Parameter(\n",
    "            self.scale * torch.rand(in_channels, out_channels, len(self.index), dtype=torch.float))\n",
    "        self.weights_imag = nn.Parameter(\n",
    "            self.scale * torch.rand(in_channels, out_channels, len(self.index), dtype=torch.float))\n",
    "\n",
    "    def compl_mul1d(self, order, x, weights_real, weights_imag):\n",
    "        return torch.complex(torch.einsum(order, x.real, weights_real) - torch.einsum(order, x.imag, weights_imag),\n",
    "                                 torch.einsum(order, x.real, weights_imag) + torch.einsum(order, x.imag, weights_real))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, H, E, N = x.shape\n",
    "        x_ft = torch.fft.rfft(x)\n",
    "        out_ft = torch.zeros(B, H, self.out_channels, x.size(-1) // 2 + 1, device=x.device, dtype=torch.cfloat)\n",
    "        a = x_ft[:, :, :, :self.modes]\n",
    "        out_ft[:, :, :, :self.modes] = self.compl_mul1d(\"bjix,iox->bjox\", a, self.weights_real, self.weights_imag)\n",
    "        x = torch.fft.irfft(out_ft, n=x.size(-1))\n",
    "        return x\n",
    "\n",
    "# FiLM模型\n",
    "class FiLM(nn.Module):\n",
    "    def __init__(self, seq_len, label_len, pred_len, enc_in):\n",
    "        super(FiLM, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.label_len = label_len\n",
    "        self.pred_len = seq_len if pred_len == 0 else pred_len\n",
    "\n",
    "        self.seq_len_all = self.seq_len + self.label_len\n",
    "\n",
    "        # b, s, f means b, f\n",
    "        self.affine_weight = nn.Parameter(torch.ones(1, 1, enc_in))\n",
    "        self.affine_bias = nn.Parameter(torch.zeros(1, 1, enc_in))\n",
    "\n",
    "        self.multiscale = [1, 2, 4]\n",
    "        self.window_size = [256]\n",
    "        ratio = 0.5\n",
    "        self.legts = nn.ModuleList(\n",
    "            [HiPPO_LegT(N=n, dt=1. / self.pred_len / i) for n in self.window_size for i in self.multiscale])\n",
    "        self.spec_conv_1 = nn.ModuleList([SpectralConv1d(in_channels=n, out_channels=n,\n",
    "                                                         seq_len=min(self.pred_len, self.seq_len),\n",
    "                                                         ratio=ratio) for n in\n",
    "                                          self.window_size for _ in range(len(self.multiscale))])\n",
    "        self.mlp = nn.Linear(len(self.multiscale) * len(self.window_size), 1)\n",
    "\n",
    "    def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec):\n",
    "        # Normalization from Non-stationary Transformer\n",
    "        means = x_enc.mean(1, keepdim=True).detach()\n",
    "        x_enc = x_enc - means\n",
    "        stdev = torch.sqrt(torch.var(x_enc, dim=1, keepdim=True, unbiased=False) + 1e-5).detach()\n",
    "        x_enc /= stdev\n",
    "\n",
    "        x_enc = x_enc * self.affine_weight + self.affine_bias\n",
    "        x_decs = []\n",
    "        jump_dist = 0\n",
    "        for i in range(0, len(self.multiscale) * len(self.window_size)):\n",
    "            x_in_len = self.multiscale[i % len(self.multiscale)] * self.pred_len\n",
    "            x_in = x_enc[:, -x_in_len:]\n",
    "            legt = self.legts[i]\n",
    "            x_in_c = legt(x_in.transpose(1, 2)).permute([1, 2, 3, 0])[:, :, :, jump_dist:]\n",
    "            out1 = self.spec_conv_1[i](x_in_c)\n",
    "            if self.seq_len >= self.pred_len:\n",
    "                x_dec_c = out1.transpose(2, 3)[:, :, self.pred_len - 1 - jump_dist, :]\n",
    "            else:\n",
    "                x_dec_c = out1.transpose(2, 3)[:, :, -1, :]\n",
    "            x_dec = x_dec_c @ legt.eval_matrix[-self.pred_len:, :].T\n",
    "            x_decs.append(x_dec)\n",
    "        x_dec = torch.stack(x_decs, dim=-1)\n",
    "        x_dec = self.mlp(x_dec).squeeze(-1).permute(0, 2, 1)\n",
    "\n",
    "        # De-Normalization from Non-stationary Transformer\n",
    "        x_dec = x_dec - self.affine_bias\n",
    "        x_dec = x_dec / (self.affine_weight + 1e-10)\n",
    "        x_dec = x_dec * stdev\n",
    "        dec_out = x_dec + means\n",
    "        \n",
    "        output = dec_out[:, -self.pred_len:, :]  # [B, L, D]\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e8d572",
   "metadata": {},
   "source": [
    "### 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9274aa96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:22:48.448879Z",
     "start_time": "2024-04-14T13:22:48.407043Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(task_args, train_args, model_args):\n",
    "    # 参数配置\n",
    "    columns = task_args['columns'] # 模型全部特征\n",
    "    target = task_args['target'] # 模型预测特征\n",
    "    features = task_args['features'] # 模型预测模式\n",
    "    model_name = train_args['model_name'] # 模型名称\n",
    "    train_loader = train_args['train_loader'] # 训练集\n",
    "    valid_loader = train_args['valid_loader'] # 验证集\n",
    "    n_epochs = train_args['n_epochs'] # 训练次数\n",
    "    learning_rate = train_args['learning_rate'] # 学习率\n",
    "    loss = train_args['loss'] # 损失函数\n",
    "    patience = train_args['patience'] # 最大早停次数阈值，超过就会早停\n",
    "    lradj = train_args['lradj'] # 学习率函数\n",
    "    path = train_args['path'] # 模型保存路径\n",
    "    verbose = train_args['verbose'] # 打印训练过程\n",
    "    plots = train_args['plots'] # 绘制损失图\n",
    "    pred_len = model_args['pred_len'] # 预测长度\n",
    "    label_len = model_args['label_len']\n",
    "    \n",
    "    #检查是否可用GPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # 创建模型和优化器\n",
    "    model = model_name(**model_args).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_fn = loss\n",
    "    \n",
    "    # 损失函数值\n",
    "    train_losses, val_losses = [], []\n",
    "    \n",
    "    # 调整学习率\n",
    "    def adjust_learning_rate(optimizer, epoch, lradj, learning_rate, train_epochs):\n",
    "        # lr = learning_rate * (0.2 ** (epoch // 2))\n",
    "        if lradj == 'type1':\n",
    "            lr_adjust = {epoch: learning_rate * (0.5 ** ((epoch - 1) // 1))}\n",
    "        elif lradj == 'type2':\n",
    "            lr_adjust = {\n",
    "                2: 5e-5, 4: 1e-5, 6: 5e-6, 8: 1e-6,\n",
    "                10: 5e-7, 15: 1e-7, 20: 5e-8\n",
    "            }\n",
    "        elif lradj == \"cosine\":\n",
    "            lr_adjust = {epoch: learning_rate /2 * (1 + math.cos(epoch / train_epochs * math.pi))}\n",
    "        if epoch in lr_adjust.keys():\n",
    "            lr = lr_adjust[epoch]\n",
    "            # 参数组(param_groups)是用来指定不同的参数组以便对它们进行不同的优化设置，比如'lr'\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "            print('Updating learning rate to {}'.format(lr))\n",
    "    \n",
    "    # 设置早停\n",
    "    class EarlyStopping():\n",
    "        def __init__(self, patience=7, verbose=False, delta=0):\n",
    "            self.patience = patience # 连续超限次数，如果满足条件，则早停\n",
    "            self.verbose = verbose\n",
    "            self.counter = 0\n",
    "            self.best_score = None\n",
    "            self.early_stop = False\n",
    "            self.val_loss_min = np.Inf\n",
    "            self.delta = delta\n",
    "\n",
    "        def __call__(self, val_loss, model, path):\n",
    "            score = -val_loss\n",
    "            if self.best_score is None:\n",
    "                self.best_score = score\n",
    "                self.save_checkpoint(val_loss, model, path)\n",
    "            elif score < self.best_score + self.delta:\n",
    "                self.counter += 1\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "                if self.counter >= self.patience:\n",
    "                    self.early_stop = True\n",
    "            else:\n",
    "                self.best_score = score\n",
    "                self.save_checkpoint(val_loss, model, path)\n",
    "                self.counter = 0\n",
    "\n",
    "        def save_checkpoint(self, val_loss, model, path):\n",
    "            if self.verbose:\n",
    "                print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "            torch.save(model.state_dict(), path + '/' + 'checkpoint.pth')\n",
    "            self.val_loss_min = val_loss\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=verbose)\n",
    "    \n",
    "    # 训练任务\n",
    "    def forecasting_task(columns, target, features):\n",
    "        \"\"\"\n",
    "        features: [M, S, MS]; \n",
    "            M:multivariate predict multivariate, \n",
    "            S:univariate predict univariate, \n",
    "            MS:multivariate predict univariate'\n",
    "        \"\"\"\n",
    "        # 字典索引生成\n",
    "        col_dict = {}\n",
    "        for i,j in enumerate(columns):\n",
    "            col_dict[j] = i\n",
    "\n",
    "        if features == 'MS':\n",
    "            target = target[0]\n",
    "            f_dim = col_dict[target]\n",
    "        elif features == 'S':\n",
    "            f_dim = 0\n",
    "        else:\n",
    "            f_dim = 0\n",
    "        return f_dim\n",
    "    f_dim = forecasting_task(columns, target, features)\n",
    "    \n",
    "    # 设置保存模型路径\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    # 模型训练和验证\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for batch_x, batch_y, batch_x_mark, batch_y_mark in train_loader:\n",
    "            #将数据移至 GPU\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            batch_x_mark = batch_x_mark.to(device)\n",
    "            batch_y_mark = batch_y_mark.to(device)\n",
    "            # 清空梯度\n",
    "            optimizer.zero_grad()\n",
    "            # decoder输入\n",
    "            dec_inp = torch.zeros_like(batch_y[:, -pred_len:, :]).float()\n",
    "            dec_inp = torch.cat([batch_y[:, :label_len, :], dec_inp], dim=1).float().to(device)\n",
    "            outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "            if features == 'MS':\n",
    "                outputs = outputs[:, :, f_dim: f_dim+1] \n",
    "            else:\n",
    "                outputs = outputs[:, :, f_dim:]\n",
    "            train_loss = loss_fn(outputs, batch_y)\n",
    "            # 反向传播计算得到每个参数的梯度值\n",
    "            train_loss.backward()\n",
    "            # 通过梯度下降执行一步参数更新\n",
    "            optimizer.step()\n",
    "            #每个batch的loss和\n",
    "            total_train_loss += train_loss.item() # .item()表示只包含一个元素的tensor中提取值\n",
    "                \n",
    "        #每个epoch的损失平均\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        \n",
    "        #所有epoch的loss\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        #评估模型\n",
    "        model.eval()\n",
    "        #关闭自动求导功能，只使用训练好的模型进行预测或评估，不需要进行梯度计算和参数更新\n",
    "        with torch.no_grad():\n",
    "            total_val_loss = 0\n",
    "            for batch_x, batch_y, batch_x_mark, batch_y_mark in valid_loader:\n",
    "                #将数据移至 GPU\n",
    "                batch_x = batch_x.to(device)\n",
    "                batch_y = batch_y.to(device)\n",
    "                batch_x_mark = batch_x_mark.to(device)\n",
    "                batch_y_mark = batch_y_mark.to(device)\n",
    "                # decoder输入\n",
    "                dec_inp = torch.zeros_like(batch_y[:, -pred_len:, :]).float()\n",
    "                dec_inp = torch.cat([batch_y[:, :label_len, :], dec_inp], dim=1).float().to(device)\n",
    "                val_outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "                if features == 'MS':\n",
    "                    val_outputs = val_outputs[:, :, f_dim: f_dim+1] \n",
    "                else:\n",
    "                    val_outputs = val_outputs[:, :, f_dim:]\n",
    "                val_loss = loss_fn(val_outputs, batch_y)\n",
    "                #每个batch的loss和\n",
    "                total_val_loss += val_loss.item()\n",
    "                \n",
    "        #每个epoch的损失平均\n",
    "        avg_val_loss = total_val_loss / len(valid_loader)\n",
    "        \n",
    "        #所有epoch的loss\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        #打印训练过程\n",
    "        if verbose:\n",
    "            print(f'Epoch [{epoch+1}/{n_epochs}], Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}')\n",
    "            \n",
    "        # 设置早停\n",
    "        early_stopping(avg_val_loss, model, path)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping!\")\n",
    "            break\n",
    "            \n",
    "        # 调整学习率\n",
    "        adjust_learning_rate(optimizer, epoch+1, lradj, learning_rate, n_epochs)\n",
    "        \n",
    "    # 加载最佳模型\n",
    "    best_model_path = path + '/' + 'checkpoint.pth'\n",
    "    model.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "    #绘制损失函数图\n",
    "    def plot_loss(train_losses, val_losses):\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.style.use('seaborn-v0_8-paper') #绘制背景色\n",
    "        plt.grid(axis = 'y',linewidth=0.35) #绘制网格\n",
    "        plt.plot(val_losses, linestyle='-',color = '#11b3b6')\n",
    "        plt.plot(train_losses, linestyle='-',color = '#f14643')\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Training and Validation Progress\")\n",
    "        plt.legend([\"Validation\", \"Training\"])\n",
    "        plt.show()\n",
    "    if plots:\n",
    "        plot_loss(train_losses, val_losses)\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7ff66ae6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:23:11.811352Z",
     "start_time": "2024-04-14T13:22:51.836012Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|████▏                                                                              | 1/20 [00:15<05:03, 15.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Training Loss: 0.0091, Validation Loss: 0.0115\n",
      "Validation loss decreased (inf --> 0.011456).  Saving model ...\n",
      "Updating learning rate to 0.0009938441702975688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|████▏                                                                              | 1/20 [00:18<05:54, 18.68s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 28\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 构造参数字典\u001b[39;00m\n\u001b[0;32m      2\u001b[0m params3 \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtask_args\u001b[39m\u001b[38;5;124m\"\u001b[39m:{\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mload\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemp\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     26\u001b[0m     },\n\u001b[0;32m     27\u001b[0m }\n\u001b[1;32m---> 28\u001b[0m model \u001b[38;5;241m=\u001b[39m train(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams3)\n",
      "Cell \u001b[1;32mIn[51], line 132\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(task_args, train_args, model_args)\u001b[0m\n\u001b[0;32m    130\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs, batch_y)\n\u001b[0;32m    131\u001b[0m \u001b[38;5;66;03m# 反向传播计算得到每个参数的梯度值\u001b[39;00m\n\u001b[1;32m--> 132\u001b[0m \u001b[43mtrain_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;66;03m# 通过梯度下降执行一步参数更新\u001b[39;00m\n\u001b[0;32m    134\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 构造参数字典\n",
    "params3 = {\n",
    "    \"task_args\":{\n",
    "        \"columns\": ['load', 'temp'],\n",
    "        \"target\": ['load'],\n",
    "        \"features\": 'MS'\n",
    "    },\n",
    "    \"train_args\": {\n",
    "        \"model_name\": FiLM,\n",
    "        \"train_loader\": train_loader,\n",
    "        \"valid_loader\": valid_loader,\n",
    "        \"n_epochs\": 20,\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"loss\": nn.MSELoss(),\n",
    "        \"patience\": 7,\n",
    "        \"lradj\": 'cosine',\n",
    "        \"path\": \"../models/test\",\n",
    "        \"verbose\": True,\n",
    "        \"plots\": True,\n",
    "    },\n",
    "    \"model_args\": {\n",
    "        'enc_in': 2, \n",
    "        'seq_len': 6,\n",
    "        'pred_len': 3,\n",
    "        'label_len': 3,\n",
    "    },\n",
    "}\n",
    "model = train(**params3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945d8b27",
   "metadata": {},
   "source": [
    "# 基于Koopa的时间序列预测"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bf2ba6",
   "metadata": {},
   "source": [
    "## 多变量多步预测"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf13a7b5",
   "metadata": {},
   "source": [
    "### 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "affd3f52",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:23:16.055582Z",
     "start_time": "2024-04-14T13:23:16.044932Z"
    }
   },
   "outputs": [],
   "source": [
    "# 读取数据\n",
    "def get_dataset(path, time_col=None):\n",
    "    data = pd.read_csv(path)\n",
    "    rows = data.shape[0]\n",
    "    now = datetime.now()\n",
    "    newtime = now.replace(microsecond=0)\n",
    "    if time_col == None:\n",
    "        # 如果没有时间列，生成时间戳范围\n",
    "        time_index = pd.date_range(start=datetime.now() -\n",
    "                                   timedelta(seconds=rows - 1),\n",
    "                                   end=datetime.now(),\n",
    "                                   freq='S')\n",
    "        full_data = pd.DataFrame(data=data.values,\n",
    "                                 index=pd.to_datetime(time_index, unit='s'),\n",
    "                                 columns=data.columns)\n",
    "    else:\n",
    "        columns = [i for i in data.columns if i != time_col] # 去除时间列\n",
    "        full_data = pd.DataFrame(data=data.drop([time_col], axis=1).values,\n",
    "                                 index=pd.to_datetime(data[time_col].values),\n",
    "                                 columns=columns)\n",
    "    return full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "60b2b13f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:23:17.474971Z",
     "start_time": "2024-04-14T13:23:17.390728Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>load</th>\n",
       "      <th>temp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-01-01 00:00:00</th>\n",
       "      <td>2698.0</td>\n",
       "      <td>32.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-01 01:00:00</th>\n",
       "      <td>2558.0</td>\n",
       "      <td>32.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-01 02:00:00</th>\n",
       "      <td>2444.0</td>\n",
       "      <td>30.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-01 03:00:00</th>\n",
       "      <td>2402.0</td>\n",
       "      <td>31.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-01 04:00:00</th>\n",
       "      <td>2403.0</td>\n",
       "      <td>32.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-12-31 19:00:00</th>\n",
       "      <td>4012.0</td>\n",
       "      <td>18.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-12-31 20:00:00</th>\n",
       "      <td>3856.0</td>\n",
       "      <td>16.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-12-31 21:00:00</th>\n",
       "      <td>3671.0</td>\n",
       "      <td>17.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-12-31 22:00:00</th>\n",
       "      <td>3499.0</td>\n",
       "      <td>15.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-12-31 23:00:00</th>\n",
       "      <td>3345.0</td>\n",
       "      <td>15.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26304 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       load   temp\n",
       "2012-01-01 00:00:00  2698.0  32.00\n",
       "2012-01-01 01:00:00  2558.0  32.67\n",
       "2012-01-01 02:00:00  2444.0  30.00\n",
       "2012-01-01 03:00:00  2402.0  31.00\n",
       "2012-01-01 04:00:00  2403.0  32.00\n",
       "...                     ...    ...\n",
       "2014-12-31 19:00:00  4012.0  18.00\n",
       "2014-12-31 20:00:00  3856.0  16.67\n",
       "2014-12-31 21:00:00  3671.0  17.00\n",
       "2014-12-31 22:00:00  3499.0  15.33\n",
       "2014-12-31 23:00:00  3345.0  15.33\n",
       "\n",
       "[26304 rows x 2 columns]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"../data/energy.csv\"\n",
    "ts_data = get_dataset(path, time_col='time')\n",
    "ts_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5f7bb731",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:23:19.334838Z",
     "start_time": "2024-04-14T13:23:19.292152Z"
    }
   },
   "outputs": [],
   "source": [
    "# 时间格式编码\n",
    "def time_features_from_frequency_str(freq_str: str):\n",
    "    \"\"\"\n",
    "    Returns a list of time features that will be appropriate for the given frequency string.\n",
    "    Parameters\n",
    "    ----------\n",
    "    freq_str\n",
    "        Frequency string of the form [multiple][granularity] such as \"12H\", \"5min\", \"1D\" etc.\n",
    "    \"\"\"\n",
    "\n",
    "    class TimeFeature:\n",
    "        def __init__(self):\n",
    "            pass\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            pass\n",
    "\n",
    "        def __repr__(self):\n",
    "            return self.__class__.__name__ + \"()\"\n",
    "\n",
    "\n",
    "    class SecondOfMinute(TimeFeature):\n",
    "        \"\"\"Minute of hour encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return index.second / 59.0 - 0.5\n",
    "\n",
    "\n",
    "    class MinuteOfHour(TimeFeature):\n",
    "        \"\"\"Minute of hour encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return index.minute / 59.0 - 0.5\n",
    "\n",
    "\n",
    "    class HourOfDay(TimeFeature):\n",
    "        \"\"\"Hour of day encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return index.hour / 23.0 - 0.5\n",
    "\n",
    "\n",
    "    class DayOfWeek(TimeFeature):\n",
    "        \"\"\"Hour of day encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return index.dayofweek / 6.0 - 0.5\n",
    "\n",
    "\n",
    "    class DayOfMonth(TimeFeature):\n",
    "        \"\"\"Day of month encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return (index.day - 1) / 30.0 - 0.5\n",
    "\n",
    "\n",
    "    class DayOfYear(TimeFeature):\n",
    "        \"\"\"Day of year encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return (index.dayofyear - 1) / 365.0 - 0.5\n",
    "\n",
    "\n",
    "    class MonthOfYear(TimeFeature):\n",
    "        \"\"\"Month of year encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return (index.month - 1) / 11.0 - 0.5\n",
    "\n",
    "\n",
    "    class WeekOfYear(TimeFeature):\n",
    "        \"\"\"Week of year encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return (index.isocalendar().week - 1) / 52.0 - 0.5\n",
    "\n",
    "    \n",
    "    features_by_offsets = {\n",
    "        offsets.YearEnd: [],\n",
    "        offsets.QuarterEnd: [MonthOfYear],\n",
    "        offsets.MonthEnd: [MonthOfYear],\n",
    "        offsets.Week: [DayOfMonth, WeekOfYear],\n",
    "        offsets.Day: [DayOfWeek, DayOfMonth, DayOfYear],\n",
    "        offsets.BusinessDay: [DayOfWeek, DayOfMonth, DayOfYear],\n",
    "        offsets.Hour: [HourOfDay, DayOfWeek, DayOfMonth, DayOfYear],\n",
    "        offsets.Minute: [\n",
    "            MinuteOfHour,\n",
    "            HourOfDay,\n",
    "            DayOfWeek,\n",
    "            DayOfMonth,\n",
    "            DayOfYear,\n",
    "        ],\n",
    "        offsets.Second: [\n",
    "            SecondOfMinute,\n",
    "            MinuteOfHour,\n",
    "            HourOfDay,\n",
    "            DayOfWeek,\n",
    "            DayOfMonth,\n",
    "            DayOfYear,\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    offset = to_offset(freq_str)\n",
    "\n",
    "    for offset_type, feature_classes in features_by_offsets.items():\n",
    "        if isinstance(offset, offset_type):\n",
    "            return [cls() for cls in feature_classes]\n",
    "\n",
    "    supported_freq_msg = f\"\"\"\n",
    "    Unsupported frequency {freq_str}\n",
    "    The following frequencies are supported:\n",
    "        Y   - yearly\n",
    "            alias: A\n",
    "        M   - monthly\n",
    "        W   - weekly\n",
    "        D   - daily\n",
    "        B   - business days\n",
    "        H   - hourly\n",
    "        T   - minutely\n",
    "            alias: min\n",
    "        S   - secondly\n",
    "    \"\"\"\n",
    "    raise RuntimeError(supported_freq_msg)\n",
    "    \n",
    "def time_features(dates, freq='h'):\n",
    "    return np.vstack([feat(dates) for feat in time_features_from_frequency_str(freq)])\n",
    "\n",
    "# 包含时间维度的数据集划分\n",
    "def divide_dataset(df, valid_date, test_date, x_feature_list, y_feature_list, freq):\n",
    "    #归一化\n",
    "    x_scaler = MinMaxScaler()\n",
    "    y_scaler = MinMaxScaler()\n",
    "\n",
    "    #测试集\n",
    "    train = df.copy()[df.index < valid_date][x_feature_list]\n",
    "    train_stamp = time_features(pd.to_datetime(train.index), freq=freq)\n",
    "    train_stamp = train_stamp.transpose(1, 0)\n",
    "    train[x_feature_list] = x_scaler.fit_transform(train)\n",
    "    xtr = train.values.astype('float32')\n",
    "\n",
    "    #验证集\n",
    "    valid = df.copy()[(df.index >= valid_date) & (df.index < test_date)][x_feature_list]\n",
    "    valid_stamp = time_features(pd.to_datetime(valid.index), freq=freq)\n",
    "    valid_stamp = valid_stamp.transpose(1, 0)\n",
    "    valid[x_feature_list] = x_scaler.fit_transform(valid)\n",
    "    xva = valid.values.astype('float32')\n",
    "\n",
    "    #测试集\n",
    "    test = df.copy()[test_date:][x_feature_list]\n",
    "    test_stamp = time_features(pd.to_datetime(test.index), freq=freq)\n",
    "    test_stamp = test_stamp.transpose(1, 0)\n",
    "    test[x_feature_list] = x_scaler.fit_transform(test)\n",
    "    xte = test.values.astype('float32')\n",
    "\n",
    "    #标签\n",
    "    ytr = df.copy()[df.index < valid_date][y_feature_list]\n",
    "    ytr[y_feature_list] = y_scaler.fit_transform(ytr)\n",
    "    ytr = ytr.values.astype('float32')\n",
    "\n",
    "    yva = df.copy()[(df.index >= valid_date) & (df.index < test_date)][y_feature_list]\n",
    "    yva[y_feature_list] = y_scaler.fit_transform(yva)\n",
    "    yva = yva.values.astype('float32')\n",
    "\n",
    "    yte = df.copy()[test_date:][y_feature_list]\n",
    "    yte[y_feature_list] = y_scaler.fit_transform(yte)\n",
    "    yte = yte.values.astype('float32')\n",
    "    \n",
    "    # 数据合并，[训练集，测试集，时间戳]\n",
    "    train = [xtr, ytr, train_stamp]\n",
    "    valid = [xva, yva, valid_stamp]\n",
    "    test = [xte, yte, test_stamp]\n",
    "    \n",
    "    return y_scaler, train, valid, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f686ff4a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:23:21.118072Z",
     "start_time": "2024-04-14T13:23:21.049820Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (23376, 2) y_train shape: (23376, 1) stamp_train shape: (23376, 4)\n",
      "x_valid shape: (1464, 2) y_valid shape: (1464, 1) stamp_valid shape: (1464, 4)\n",
      "x_test shape: (1464, 2) y_test shape: (1464, 1) stamp_test shape: (1464, 4)\n"
     ]
    }
   ],
   "source": [
    "# 构造参数字典\n",
    "params1 = {\n",
    "    \"df\": ts_data,\n",
    "    \"valid_date\": \"2014-09-01 00:00:00\",\n",
    "    \"test_date\": \"2014-11-01 00:00:00\",\n",
    "    \"x_feature_list\": [\"load\", 'temp'],\n",
    "    \"y_feature_list\": [\"load\"],\n",
    "    \"freq\": 'h'\n",
    "}\n",
    "\n",
    "#函数传参\n",
    "scaler, train, valid, test = divide_dataset(**params1)\n",
    "print(\"x_train shape: {0} y_train shape: {1} stamp_train shape: {2}\".format(train[0].shape, train[1].shape, train[2].shape))\n",
    "print(\"x_valid shape: {0} y_valid shape: {1} stamp_valid shape: {2}\".format(valid[0].shape, valid[1].shape, valid[2].shape))\n",
    "print(\"x_test shape: {0} y_test shape: {1} stamp_test shape: {2}\".format(test[0].shape, test[1].shape, test[2].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "10123319",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:23:23.105082Z",
     "start_time": "2024-04-14T13:23:23.092650Z"
    }
   },
   "outputs": [],
   "source": [
    "#利用前seq_len个数据，预测下pred_len个数据\n",
    "def create_dataset(data_list, seq_len, pred_len, label_len, batch_size):\n",
    "    \"\"\"\n",
    "    @参数设置：\n",
    "    data_list：特征，目标，时间戳\n",
    "    seq_len：输入数据包含过去多少个时间步\n",
    "    pred_len：目标应该在未来多少个时间步之后\n",
    "    label_len：先验时间步\n",
    "    \"\"\"\n",
    "    # 获取数据\n",
    "    feature = data_list[0] # 特征\n",
    "    target = data_list[1] # 目标\n",
    "    stamp = data_list[2] # 时间戳\n",
    "    \n",
    "    # 循环生成数据\n",
    "    X, y = [], []\n",
    "    X_stamp, y_stamp = [], []\n",
    "    seq_len = seq_len - 1 # 包含当前时间点\n",
    "    for i in range(seq_len, len(feature) - pred_len):\n",
    "        # 数据维度\n",
    "        feat = feature[i - seq_len:i + 1]\n",
    "        tar = target[i + 1:i + 1 + pred_len]\n",
    "        X.append(feat)\n",
    "        y.append(tar)\n",
    "        \n",
    "        # 时间维度\n",
    "        xs = stamp[i - seq_len:i + 1]\n",
    "        ys = stamp[i + 1 - label_len:i + 1 + pred_len]\n",
    "        X_stamp.append(xs)\n",
    "        y_stamp.append(ys)\n",
    "        \n",
    "    # 转为张量，数据维度\n",
    "    X = torch.as_tensor(X).float()\n",
    "    y = torch.as_tensor(y).float()\n",
    "    \n",
    "    # 转为张量，时间维度\n",
    "    X_stamp = torch.as_tensor(X_stamp).float()\n",
    "    y_stamp = torch.as_tensor(y_stamp).float()\n",
    "    \n",
    "    # 创建dataloader，[特征，目标，特征时间编码，目标时间编码]\n",
    "    data_loader = DataLoader(TensorDataset(X, y, X_stamp, y_stamp), shuffle=True, batch_size=batch_size)\n",
    "    \n",
    "    return X, y, data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b725980e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:23:25.838113Z",
     "start_time": "2024-04-14T13:23:24.942023Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_size: torch.Size([23368, 6, 2]),y_size: torch.Size([23368, 3, 1]),loader_len: 731\n",
      "X_size: torch.Size([1456, 6, 2]),y_size: torch.Size([1456, 3, 1]),loader_len: 46\n",
      "X_size: torch.Size([1456, 6, 2]),y_size: torch.Size([1456, 3, 1]),loader_len: 46\n"
     ]
    }
   ],
   "source": [
    "# 构造参数字典\n",
    "params2 = {\n",
    "    \"seq_len\": 6,\n",
    "    \"pred_len\": 3,\n",
    "    \"label_len\": 0,\n",
    "    \"batch_size\": 32,\n",
    "}\n",
    "X_train, y_train, train_loader = create_dataset(train, **params2)\n",
    "X_valid, y_valid, valid_loader = create_dataset(valid, **params2)\n",
    "X_test, y_test, test_loader = create_dataset(test, **params2)\n",
    "print(\"X_size: {0},y_size: {1},loader_len: {2}\".format(X_train.shape, y_train.shape, len(train_loader)))\n",
    "print(\"X_size: {0},y_size: {1},loader_len: {2}\".format(X_valid.shape, y_valid.shape, len(valid_loader)))\n",
    "print(\"X_size: {0},y_size: {1},loader_len: {2}\".format(X_test.shape, y_test.shape, len(test_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda04ab3",
   "metadata": {},
   "source": [
    "### 模型定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7a085a4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:24:20.331208Z",
     "start_time": "2024-04-14T13:24:20.267323Z"
    }
   },
   "outputs": [],
   "source": [
    "class FourierFilter(nn.Module):\n",
    "    \"\"\"\n",
    "    Fourier Filter: to time-variant and time-invariant term\n",
    "    \"\"\"\n",
    "    def __init__(self, mask_spectrum):\n",
    "        super(FourierFilter, self).__init__()\n",
    "        self.mask_spectrum = mask_spectrum\n",
    "        \n",
    "    def forward(self, x):\n",
    "        xf = torch.fft.rfft(x, dim=1)\n",
    "        mask = torch.ones_like(xf)\n",
    "        mask[:, self.mask_spectrum, :] = 0\n",
    "        x_var = torch.fft.irfft(xf*mask, dim=1)\n",
    "        x_inv = x - x_var\n",
    "        \n",
    "        return x_var, x_inv\n",
    "    \n",
    "\n",
    "class MLP(nn.Module):\n",
    "    '''\n",
    "    Multilayer perceptron to encode/decode high dimension representation of sequential data\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 f_in, \n",
    "                 f_out, \n",
    "                 hidden_dim=128, \n",
    "                 hidden_layers=2, \n",
    "                 dropout=0.05,\n",
    "                 activation='tanh'): \n",
    "        super(MLP, self).__init__()\n",
    "        self.f_in = f_in\n",
    "        self.f_out = f_out\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.dropout = dropout\n",
    "        if activation == 'relu':\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation == 'tanh':\n",
    "            self.activation = nn.Tanh()\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        \n",
    "        layers = [nn.Linear(self.f_in, self.hidden_dim), \n",
    "                  self.activation, nn.Dropout(self.dropout)]\n",
    "        for i in range(self.hidden_layers-2):\n",
    "            layers += [nn.Linear(self.hidden_dim, self.hidden_dim),\n",
    "                       self.activation, nn.Dropout(dropout)]\n",
    "        \n",
    "        layers += [nn.Linear(hidden_dim, f_out)]\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x:     B x S x f_in\n",
    "        # y:     B x S x f_out\n",
    "        y = self.layers(x)\n",
    "        return y\n",
    "    \n",
    "\n",
    "class KPLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Find koopman transition of linear system by DMD with one step approximation\n",
    "    \"\"\"\n",
    "    def __init__(self): \n",
    "        super(KPLayer, self).__init__()\n",
    "        \n",
    "        self.K = None # B E E\n",
    "\n",
    "    def one_step_forward(self, z, return_rec=False, return_K=False):\n",
    "        B, input_len, E = z.shape\n",
    "        assert input_len > 1, 'snapshots number should be larger than 1'\n",
    "        x, y = z[:, :-1], z[:, 1:]\n",
    "\n",
    "        # solve linear system\n",
    "        self.K = torch.linalg.lstsq(x, y).solution # B E E\n",
    "        if torch.isnan(self.K).any():\n",
    "            print('Encounter K with nan, replace K by identity matrix')\n",
    "            self.K = torch.eye(self.K.shape[1]).to(self.K.device).unsqueeze(0).repeat(B, 1, 1)\n",
    "\n",
    "        z_pred = torch.bmm(z[:, -1:], self.K)\n",
    "        if return_rec:\n",
    "            z_rec = torch.cat((z[:, :1], torch.bmm(x, self.K)), dim=1)\n",
    "            return z_rec, z_pred\n",
    "\n",
    "        return z_pred\n",
    "    \n",
    "    def forward(self, z, pred_len=1):\n",
    "        z_rec, z_pred= self.one_step_forward(z, return_rec=True)\n",
    "        z_preds = []\n",
    "        for i in range(pred_len):\n",
    "            z_pred = torch.bmm(z_pred, self.K)\n",
    "            z_preds.append(z_pred)\n",
    "        z_preds = torch.cat(z_preds, dim=1)\n",
    "        return z_rec, z_preds\n",
    "\n",
    "\n",
    "class KPLayerApprox(nn.Module):\n",
    "    \"\"\"\n",
    "    Find koopman transition of linear system by DMD with multistep K approximation\n",
    "    \"\"\"\n",
    "    def __init__(self): \n",
    "        super(KPLayerApprox, self).__init__()\n",
    "        \n",
    "        self.K = None # B E E\n",
    "        self.K_step = None # B E E\n",
    "\n",
    "    def forward(self, z, pred_len=1):\n",
    "        # z:       B L E, koopman invariance space representation\n",
    "        # z_rec:   B L E, reconstructed representation\n",
    "        # z_pred:  B S E, forecasting representation\n",
    "        B, input_len, E = z.shape\n",
    "        assert input_len > 1, 'snapshots number should be larger than 1'\n",
    "        x, y = z[:, :-1], z[:, 1:]\n",
    "\n",
    "        # solve linear system\n",
    "        self.K = torch.linalg.lstsq(x, y).solution # B E E\n",
    "\n",
    "        if torch.isnan(self.K).any():\n",
    "            print('Encounter K with nan, replace K by identity matrix')\n",
    "            self.K = torch.eye(self.K.shape[1]).to(self.K.device).unsqueeze(0).repeat(B, 1, 1)\n",
    "\n",
    "        z_rec = torch.cat((z[:, :1], torch.bmm(x, self.K)), dim=1) # B L E\n",
    "        \n",
    "        if pred_len <= input_len:\n",
    "            self.K_step = torch.linalg.matrix_power(self.K, pred_len)\n",
    "            if torch.isnan(self.K_step).any():\n",
    "                print('Encounter multistep K with nan, replace it by identity matrix')\n",
    "                self.K_step = torch.eye(self.K_step.shape[1]).to(self.K_step.device).unsqueeze(0).repeat(B, 1, 1)\n",
    "            z_pred = torch.bmm(z[:, -pred_len:, :], self.K_step)\n",
    "        else:\n",
    "            self.K_step = torch.linalg.matrix_power(self.K, input_len)\n",
    "            if torch.isnan(self.K_step).any():\n",
    "                print('Encounter multistep K with nan, replace it by identity matrix')\n",
    "                self.K_step = torch.eye(self.K_step.shape[1]).to(self.K_step.device).unsqueeze(0).repeat(B, 1, 1)\n",
    "            temp_z_pred, all_pred = z, []\n",
    "            for _ in range(math.ceil(pred_len / input_len)):\n",
    "                temp_z_pred = torch.bmm(temp_z_pred, self.K_step)\n",
    "                all_pred.append(temp_z_pred)\n",
    "            z_pred = torch.cat(all_pred, dim=1)[:, :pred_len, :]\n",
    "\n",
    "        return z_rec, z_pred\n",
    "    \n",
    "\n",
    "class TimeVarKP(nn.Module):\n",
    "    \"\"\"\n",
    "    Koopman Predictor with DMD (analysitical solution of Koopman operator)\n",
    "    Utilize local variations within individual sliding window to predict the future of time-variant term\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 enc_in=8,\n",
    "                 input_len=96,\n",
    "                 pred_len=96,\n",
    "                 seg_len=24,\n",
    "                 dynamic_dim=128,\n",
    "                 encoder=None,\n",
    "                 decoder=None,\n",
    "                 multistep=False,\n",
    "                ):\n",
    "        super(TimeVarKP, self).__init__()\n",
    "        self.input_len = input_len\n",
    "        self.pred_len = pred_len\n",
    "        self.enc_in = enc_in\n",
    "        self.seg_len = seg_len\n",
    "        self.dynamic_dim = dynamic_dim\n",
    "        self.multistep = multistep\n",
    "        self.encoder, self.decoder = encoder, decoder            \n",
    "        self.freq = math.ceil(self.input_len / self.seg_len)  # segment number of input\n",
    "        self.step = math.ceil(self.pred_len / self.seg_len)   # segment number of output\n",
    "        self.padding_len = self.seg_len * self.freq - self.input_len\n",
    "        # Approximate mulitstep K by KPLayerApprox when pred_len is large\n",
    "        self.dynamics = KPLayerApprox() if self.multistep else KPLayer() \n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: B L C\n",
    "        B, L, C = x.shape\n",
    "\n",
    "        res = torch.cat((x[:, L-self.padding_len:, :], x) ,dim=1)\n",
    "\n",
    "        res = res.chunk(self.freq, dim=1)     # F x B P C, P means seg_len\n",
    "        res = torch.stack(res, dim=1).reshape(B, self.freq, -1)   # B F PC\n",
    "\n",
    "        res = self.encoder(res) # B F H\n",
    "        x_rec, x_pred = self.dynamics(res, self.step) # B F H, B S H\n",
    "\n",
    "        x_rec = self.decoder(x_rec) # B F PC\n",
    "        x_rec = x_rec.reshape(B, self.freq, self.seg_len, self.enc_in)\n",
    "        x_rec = x_rec.reshape(B, -1, self.enc_in)[:, :self.input_len, :]  # B L C\n",
    "        \n",
    "        x_pred = self.decoder(x_pred)     # B S PC\n",
    "        x_pred = x_pred.reshape(B, self.step, self.seg_len, self.enc_in)\n",
    "        x_pred = x_pred.reshape(B, -1, self.enc_in)[:, :self.pred_len, :] # B S C\n",
    "\n",
    "        return x_rec, x_pred\n",
    "\n",
    "\n",
    "class TimeInvKP(nn.Module):\n",
    "    \"\"\"\n",
    "    Koopman Predictor with learnable Koopman operator\n",
    "    Utilize lookback and forecast window snapshots to predict the future of time-invariant term\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 input_len=96,\n",
    "                 pred_len=96,\n",
    "                 dynamic_dim=128,\n",
    "                 encoder=None,\n",
    "                 decoder=None):\n",
    "        super(TimeInvKP, self).__init__()\n",
    "        self.dynamic_dim = dynamic_dim\n",
    "        self.input_len = input_len\n",
    "        self.pred_len = pred_len\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "        K_init = torch.randn(self.dynamic_dim, self.dynamic_dim)\n",
    "        U, _, V = torch.svd(K_init) # stable initialization\n",
    "        self.K = nn.Linear(self.dynamic_dim, self.dynamic_dim, bias=False)\n",
    "        self.K.weight.data = torch.mm(U, V.t())\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: B L C\n",
    "        res = x.transpose(1, 2) # B C L\n",
    "        res = self.encoder(res) # B C H\n",
    "        res = self.K(res) # B C H\n",
    "        res = self.decoder(res) # B C S\n",
    "        res = res.transpose(1, 2) # B S C\n",
    "\n",
    "        return res\n",
    "\n",
    "\n",
    "# Koopa模型\n",
    "class Koopa(nn.Module):\n",
    "    def __init__(self, enc_in, seq_len, pred_len, label_len, train_loader, dynamic_dim=128, hidden_dim=64, \n",
    "                 hidden_layers=2, num_blocks=3, multistep=False):\n",
    "        \"\"\"\n",
    "        mask_spectrum: list, shared frequency spectrums\n",
    "        seg_len: int, segment length of time series\n",
    "        dynamic_dim: int, latent dimension of koopman embedding\n",
    "        hidden_dim: int, hidden dimension of en/decoder\n",
    "        hidden_layers: int, number of hidden layers of en/decoder\n",
    "        num_blocks: int, number of Koopa blocks\n",
    "        multistep: bool, whether to use approximation for multistep K\n",
    "        alpha: float, spectrum filter ratio\n",
    "        \"\"\"\n",
    "        super(Koopa, self).__init__()\n",
    "        self.enc_in = enc_in\n",
    "        self.input_len = seq_len\n",
    "        self.pred_len = pred_len\n",
    "\n",
    "        self.seg_len = self.pred_len\n",
    "        self.num_blocks = num_blocks\n",
    "        self.dynamic_dim = dynamic_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.multistep = multistep\n",
    "        self.alpha = 0.2\n",
    "        self.mask_spectrum = self._get_mask_spectrum(train_loader)\n",
    "\n",
    "        self.disentanglement = FourierFilter(self.mask_spectrum)\n",
    "\n",
    "        # shared encoder/decoder to make koopman embedding consistent\n",
    "        self.time_inv_encoder = MLP(f_in=self.input_len, f_out=self.dynamic_dim, activation='relu',\n",
    "                    hidden_dim=self.hidden_dim, hidden_layers=self.hidden_layers)\n",
    "        self.time_inv_decoder = MLP(f_in=self.dynamic_dim, f_out=self.pred_len, activation='relu',\n",
    "                           hidden_dim=self.hidden_dim, hidden_layers=self.hidden_layers)\n",
    "        self.time_inv_kps = self.time_var_kps = nn.ModuleList([\n",
    "                                TimeInvKP(input_len=self.input_len,\n",
    "                                    pred_len=self.pred_len, \n",
    "                                    dynamic_dim=self.dynamic_dim,\n",
    "                                    encoder=self.time_inv_encoder, \n",
    "                                    decoder=self.time_inv_decoder)\n",
    "                                for _ in range(self.num_blocks)])\n",
    "\n",
    "        # shared encoder/decoder to make koopman embedding consistent\n",
    "        self.time_var_encoder = MLP(f_in=self.seg_len*self.enc_in, f_out=self.dynamic_dim, activation='tanh',\n",
    "                           hidden_dim=self.hidden_dim, hidden_layers=self.hidden_layers)\n",
    "        self.time_var_decoder = MLP(f_in=self.dynamic_dim, f_out=self.seg_len*self.enc_in, activation='tanh',\n",
    "                           hidden_dim=self.hidden_dim, hidden_layers=self.hidden_layers)\n",
    "        self.time_var_kps = nn.ModuleList([\n",
    "                    TimeVarKP(enc_in=enc_in,\n",
    "                        input_len=self.input_len,\n",
    "                        pred_len=self.pred_len,\n",
    "                        seg_len=self.seg_len,\n",
    "                        dynamic_dim=self.dynamic_dim,\n",
    "                        encoder=self.time_var_encoder,\n",
    "                        decoder=self.time_var_decoder,\n",
    "                        multistep=self.multistep)\n",
    "                    for _ in range(self.num_blocks)])\n",
    "\n",
    "    def _get_mask_spectrum(self, train_loader):\n",
    "        \"\"\"\n",
    "        get shared frequency spectrums\n",
    "        \"\"\"\n",
    "#         train_data, train_loader = data_provider(configs, 'train')\n",
    "        amps = 0.0\n",
    "        for data in train_loader:\n",
    "            lookback_window = data[0]\n",
    "            amps += abs(torch.fft.rfft(lookback_window, dim=1)).mean(dim=0).mean(dim=1)\n",
    "        mask_spectrum = amps.topk(int(amps.shape[0]*self.alpha)).indices\n",
    "        return mask_spectrum # as the spectrums of time-invariant component\n",
    "    \n",
    "    def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec):\n",
    "        # Series Stationarization adopted from NSformer\n",
    "        mean_enc = x_enc.mean(1, keepdim=True).detach() # B x 1 x E\n",
    "        x_enc = x_enc - mean_enc\n",
    "        std_enc = torch.sqrt(torch.var(x_enc, dim=1, keepdim=True, unbiased=False) + 1e-5).detach()\n",
    "        x_enc = x_enc / std_enc\n",
    "\n",
    "        # Koopman Forecasting\n",
    "        residual, forecast = x_enc, None\n",
    "        for i in range(self.num_blocks):\n",
    "            time_var_input, time_inv_input = self.disentanglement(residual)\n",
    "            time_inv_output = self.time_inv_kps[i](time_inv_input)\n",
    "            time_var_backcast, time_var_output = self.time_var_kps[i](time_var_input)\n",
    "            residual = residual - time_var_backcast\n",
    "            if forecast is None:\n",
    "                forecast = (time_inv_output + time_var_output)\n",
    "            else:\n",
    "                forecast += (time_inv_output + time_var_output)\n",
    "\n",
    "        # Series Stationarization adopted from NSformer\n",
    "        dec_out = forecast * std_enc + mean_enc\n",
    "\n",
    "        output = dec_out[:, -self.pred_len:, :] # [B, L, D]\n",
    "        return output        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7bbdc4",
   "metadata": {},
   "source": [
    "### 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5952dbbd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:24:22.807326Z",
     "start_time": "2024-04-14T13:24:22.765773Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(task_args, train_args, model_args):\n",
    "    # 参数配置\n",
    "    columns = task_args['columns'] # 模型全部特征\n",
    "    target = task_args['target'] # 模型预测特征\n",
    "    features = task_args['features'] # 模型预测模式\n",
    "    model_name = train_args['model_name'] # 模型名称\n",
    "    train_loader = train_args['train_loader'] # 训练集\n",
    "    valid_loader = train_args['valid_loader'] # 验证集\n",
    "    n_epochs = train_args['n_epochs'] # 训练次数\n",
    "    learning_rate = train_args['learning_rate'] # 学习率\n",
    "    loss = train_args['loss'] # 损失函数\n",
    "    patience = train_args['patience'] # 最大早停次数阈值，超过就会早停\n",
    "    lradj = train_args['lradj'] # 学习率函数\n",
    "    path = train_args['path'] # 模型保存路径\n",
    "    verbose = train_args['verbose'] # 打印训练过程\n",
    "    plots = train_args['plots'] # 绘制损失图\n",
    "    pred_len = model_args['pred_len'] # 预测长度\n",
    "    label_len = model_args['label_len']\n",
    "    \n",
    "    #检查是否可用GPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # 创建模型和优化器\n",
    "    model = model_name(**model_args).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_fn = loss\n",
    "    \n",
    "    # 损失函数值\n",
    "    train_losses, val_losses = [], []\n",
    "    \n",
    "    # 调整学习率\n",
    "    def adjust_learning_rate(optimizer, epoch, lradj, learning_rate, train_epochs):\n",
    "        # lr = learning_rate * (0.2 ** (epoch // 2))\n",
    "        if lradj == 'type1':\n",
    "            lr_adjust = {epoch: learning_rate * (0.5 ** ((epoch - 1) // 1))}\n",
    "        elif lradj == 'type2':\n",
    "            lr_adjust = {\n",
    "                2: 5e-5, 4: 1e-5, 6: 5e-6, 8: 1e-6,\n",
    "                10: 5e-7, 15: 1e-7, 20: 5e-8\n",
    "            }\n",
    "        elif lradj == \"cosine\":\n",
    "            lr_adjust = {epoch: learning_rate /2 * (1 + math.cos(epoch / train_epochs * math.pi))}\n",
    "        if epoch in lr_adjust.keys():\n",
    "            lr = lr_adjust[epoch]\n",
    "            # 参数组(param_groups)是用来指定不同的参数组以便对它们进行不同的优化设置，比如'lr'\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "            print('Updating learning rate to {}'.format(lr))\n",
    "    \n",
    "    # 设置早停\n",
    "    class EarlyStopping():\n",
    "        def __init__(self, patience=7, verbose=False, delta=0):\n",
    "            self.patience = patience # 连续超限次数，如果满足条件，则早停\n",
    "            self.verbose = verbose\n",
    "            self.counter = 0\n",
    "            self.best_score = None\n",
    "            self.early_stop = False\n",
    "            self.val_loss_min = np.Inf\n",
    "            self.delta = delta\n",
    "\n",
    "        def __call__(self, val_loss, model, path):\n",
    "            score = -val_loss\n",
    "            if self.best_score is None:\n",
    "                self.best_score = score\n",
    "                self.save_checkpoint(val_loss, model, path)\n",
    "            elif score < self.best_score + self.delta:\n",
    "                self.counter += 1\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "                if self.counter >= self.patience:\n",
    "                    self.early_stop = True\n",
    "            else:\n",
    "                self.best_score = score\n",
    "                self.save_checkpoint(val_loss, model, path)\n",
    "                self.counter = 0\n",
    "\n",
    "        def save_checkpoint(self, val_loss, model, path):\n",
    "            if self.verbose:\n",
    "                print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "            torch.save(model.state_dict(), path + '/' + 'checkpoint.pth')\n",
    "            self.val_loss_min = val_loss\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=verbose)\n",
    "    \n",
    "    # 训练任务\n",
    "    def forecasting_task(columns, target, features):\n",
    "        \"\"\"\n",
    "        features: [M, S, MS]; \n",
    "            M:multivariate predict multivariate, \n",
    "            S:univariate predict univariate, \n",
    "            MS:multivariate predict univariate'\n",
    "        \"\"\"\n",
    "        # 字典索引生成\n",
    "        col_dict = {}\n",
    "        for i,j in enumerate(columns):\n",
    "            col_dict[j] = i\n",
    "\n",
    "        if features == 'MS':\n",
    "            target = target[0]\n",
    "            f_dim = col_dict[target]\n",
    "        elif features == 'S':\n",
    "            f_dim = 0\n",
    "        else:\n",
    "            f_dim = 0\n",
    "        return f_dim\n",
    "    f_dim = forecasting_task(columns, target, features)\n",
    "    \n",
    "    # 设置保存模型路径\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    # 模型训练和验证\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for batch_x, batch_y, batch_x_mark, batch_y_mark in train_loader:\n",
    "            #将数据移至 GPU\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            batch_x_mark = batch_x_mark.to(device)\n",
    "            batch_y_mark = batch_y_mark.to(device)\n",
    "            # 清空梯度\n",
    "            optimizer.zero_grad()\n",
    "            # decoder输入\n",
    "            dec_inp = torch.zeros_like(batch_y[:, -pred_len:, :]).float()\n",
    "            dec_inp = torch.cat([batch_y[:, :label_len, :], dec_inp], dim=1).float().to(device)\n",
    "            outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "            if features == 'MS':\n",
    "                outputs = outputs[:, :, f_dim: f_dim+1] \n",
    "            else:\n",
    "                outputs = outputs[:, :, f_dim:]\n",
    "            train_loss = loss_fn(outputs, batch_y)\n",
    "            # 反向传播计算得到每个参数的梯度值\n",
    "            train_loss.backward()\n",
    "            # 通过梯度下降执行一步参数更新\n",
    "            optimizer.step()\n",
    "            #每个batch的loss和\n",
    "            total_train_loss += train_loss.item() # .item()表示只包含一个元素的tensor中提取值\n",
    "                \n",
    "        #每个epoch的损失平均\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        \n",
    "        #所有epoch的loss\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        #评估模型\n",
    "        model.eval()\n",
    "        #关闭自动求导功能，只使用训练好的模型进行预测或评估，不需要进行梯度计算和参数更新\n",
    "        with torch.no_grad():\n",
    "            total_val_loss = 0\n",
    "            for batch_x, batch_y, batch_x_mark, batch_y_mark in valid_loader:\n",
    "                #将数据移至 GPU\n",
    "                batch_x = batch_x.to(device)\n",
    "                batch_y = batch_y.to(device)\n",
    "                batch_x_mark = batch_x_mark.to(device)\n",
    "                batch_y_mark = batch_y_mark.to(device)\n",
    "                # decoder输入\n",
    "                dec_inp = torch.zeros_like(batch_y[:, -pred_len:, :]).float()\n",
    "                dec_inp = torch.cat([batch_y[:, :label_len, :], dec_inp], dim=1).float().to(device)\n",
    "                val_outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "                if features == 'MS':\n",
    "                    val_outputs = val_outputs[:, :, f_dim: f_dim+1] \n",
    "                else:\n",
    "                    val_outputs = val_outputs[:, :, f_dim:]\n",
    "                val_loss = loss_fn(val_outputs, batch_y)\n",
    "                #每个batch的loss和\n",
    "                total_val_loss += val_loss.item()\n",
    "                \n",
    "        #每个epoch的损失平均\n",
    "        avg_val_loss = total_val_loss / len(valid_loader)\n",
    "        \n",
    "        #所有epoch的loss\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        #打印训练过程\n",
    "        if verbose:\n",
    "            print(f'Epoch [{epoch+1}/{n_epochs}], Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}')\n",
    "            \n",
    "        # 设置早停\n",
    "        early_stopping(avg_val_loss, model, path)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping!\")\n",
    "            break\n",
    "            \n",
    "        # 调整学习率\n",
    "        adjust_learning_rate(optimizer, epoch+1, lradj, learning_rate, n_epochs)\n",
    "        \n",
    "    # 加载最佳模型\n",
    "    best_model_path = path + '/' + 'checkpoint.pth'\n",
    "    model.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "    #绘制损失函数图\n",
    "    def plot_loss(train_losses, val_losses):\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.style.use('seaborn-v0_8-paper') #绘制背景色\n",
    "        plt.grid(axis = 'y',linewidth=0.35) #绘制网格\n",
    "        plt.plot(val_losses, linestyle='-',color = '#11b3b6')\n",
    "        plt.plot(train_losses, linestyle='-',color = '#f14643')\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Training and Validation Progress\")\n",
    "        plt.legend([\"Validation\", \"Training\"])\n",
    "        plt.show()\n",
    "    if plots:\n",
    "        plot_loss(train_losses, val_losses)\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "319a795a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:25:02.470137Z",
     "start_time": "2024-04-14T13:24:24.371032Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|████▏                                                                              | 1/20 [00:33<10:31, 33.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Training Loss: 0.0050, Validation Loss: 0.0067\n",
      "Validation loss decreased (inf --> 0.006670).  Saving model ...\n",
      "Updating learning rate to 0.0009938441702975688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|████▏                                                                              | 1/20 [00:36<11:27, 36.18s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[64], line 29\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 构造参数字典\u001b[39;00m\n\u001b[0;32m      2\u001b[0m params3 \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtask_args\u001b[39m\u001b[38;5;124m\"\u001b[39m:{\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mload\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemp\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     27\u001b[0m     },\n\u001b[0;32m     28\u001b[0m }\n\u001b[1;32m---> 29\u001b[0m model \u001b[38;5;241m=\u001b[39m train(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams3)\n",
      "Cell \u001b[1;32mIn[63], line 125\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(task_args, train_args, model_args)\u001b[0m\n\u001b[0;32m    123\u001b[0m dec_inp \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(batch_y[:, \u001b[38;5;241m-\u001b[39mpred_len:, :])\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m    124\u001b[0m dec_inp \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([batch_y[:, :label_len, :], dec_inp], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m--> 125\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_x_mark\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdec_inp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_y_mark\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m features \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMS\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    127\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m outputs[:, :, f_dim: f_dim\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m] \n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[62], line 312\u001b[0m, in \u001b[0;36mKoopa.forward\u001b[1;34m(self, x_enc, x_mark_enc, x_dec, x_mark_dec)\u001b[0m\n\u001b[0;32m    310\u001b[0m time_var_input, time_inv_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisentanglement(residual)\n\u001b[0;32m    311\u001b[0m time_inv_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_inv_kps[i](time_inv_input)\n\u001b[1;32m--> 312\u001b[0m time_var_backcast, time_var_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtime_var_kps\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtime_var_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    313\u001b[0m residual \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m-\u001b[39m time_var_backcast\n\u001b[0;32m    314\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m forecast \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[62], line 182\u001b[0m, in \u001b[0;36mTimeVarKP.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    179\u001b[0m res \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(res, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(B, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfreq, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)   \u001b[38;5;66;03m# B F PC\u001b[39;00m\n\u001b[0;32m    181\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(res) \u001b[38;5;66;03m# B F H\u001b[39;00m\n\u001b[1;32m--> 182\u001b[0m x_rec, x_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdynamics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# B F H, B S H\u001b[39;00m\n\u001b[0;32m    184\u001b[0m x_rec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(x_rec) \u001b[38;5;66;03m# B F PC\u001b[39;00m\n\u001b[0;32m    185\u001b[0m x_rec \u001b[38;5;241m=\u001b[39m x_rec\u001b[38;5;241m.\u001b[39mreshape(B, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfreq, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseg_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menc_in)\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[62], line 87\u001b[0m, in \u001b[0;36mKPLayer.forward\u001b[1;34m(self, z, pred_len)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, z, pred_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m---> 87\u001b[0m     z_rec, z_pred\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mone_step_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_rec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     88\u001b[0m     z_preds \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(pred_len):\n",
      "Cell \u001b[1;32mIn[62], line 74\u001b[0m, in \u001b[0;36mKPLayer.one_step_forward\u001b[1;34m(self, z, return_rec, return_K)\u001b[0m\n\u001b[0;32m     71\u001b[0m x, y \u001b[38;5;241m=\u001b[39m z[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], z[:, \u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# solve linear system\u001b[39;00m\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mK \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstsq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msolution \u001b[38;5;66;03m# B E E\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misnan(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mK)\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m     76\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEncounter K with nan, replace K by identity matrix\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 构造参数字典\n",
    "params3 = {\n",
    "    \"task_args\":{\n",
    "        \"columns\": ['load', 'temp'],\n",
    "        \"target\": ['load'],\n",
    "        \"features\": 'MS'\n",
    "    },\n",
    "    \"train_args\": {\n",
    "        \"model_name\": Koopa,\n",
    "        \"train_loader\": train_loader,\n",
    "        \"valid_loader\": valid_loader,\n",
    "        \"n_epochs\": 20,\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"loss\": nn.MSELoss(),\n",
    "        \"patience\": 3,\n",
    "        \"lradj\": 'cosine',\n",
    "        \"path\": \"../models/test\",\n",
    "        \"verbose\": True,\n",
    "        \"plots\": True,\n",
    "    },\n",
    "    \"model_args\": {\n",
    "        'enc_in': 2, \n",
    "        'seq_len': 6,\n",
    "        'pred_len': 3,\n",
    "        \"label_len\": 0,\n",
    "        'train_loader': train_loader,\n",
    "    },\n",
    "}\n",
    "model = train(**params3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bd0701",
   "metadata": {},
   "source": [
    "# 基于LightTS的时间序列预测"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7b98b6",
   "metadata": {},
   "source": [
    "## 多变量多步预测"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3773c5f3",
   "metadata": {},
   "source": [
    "### 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3774513a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:25:05.808373Z",
     "start_time": "2024-04-14T13:25:05.794653Z"
    }
   },
   "outputs": [],
   "source": [
    "# 读取数据\n",
    "def get_dataset(path, time_col=None):\n",
    "    data = pd.read_csv(path)\n",
    "    rows = data.shape[0]\n",
    "    now = datetime.now()\n",
    "    newtime = now.replace(microsecond=0)\n",
    "    if time_col == None:\n",
    "        # 如果没有时间列，生成时间戳范围\n",
    "        time_index = pd.date_range(start=datetime.now() -\n",
    "                                   timedelta(seconds=rows - 1),\n",
    "                                   end=datetime.now(),\n",
    "                                   freq='S')\n",
    "        full_data = pd.DataFrame(data=data.values,\n",
    "                                 index=pd.to_datetime(time_index, unit='s'),\n",
    "                                 columns=data.columns)\n",
    "    else:\n",
    "        columns = [i for i in data.columns if i != time_col] # 去除时间列\n",
    "        full_data = pd.DataFrame(data=data.drop([time_col], axis=1).values,\n",
    "                                 index=pd.to_datetime(data[time_col].values),\n",
    "                                 columns=columns)\n",
    "    return full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "01289bd3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:25:06.815598Z",
     "start_time": "2024-04-14T13:25:06.733431Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>load</th>\n",
       "      <th>temp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-01-01 00:00:00</th>\n",
       "      <td>2698.0</td>\n",
       "      <td>32.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-01 01:00:00</th>\n",
       "      <td>2558.0</td>\n",
       "      <td>32.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-01 02:00:00</th>\n",
       "      <td>2444.0</td>\n",
       "      <td>30.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-01 03:00:00</th>\n",
       "      <td>2402.0</td>\n",
       "      <td>31.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-01 04:00:00</th>\n",
       "      <td>2403.0</td>\n",
       "      <td>32.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-12-31 19:00:00</th>\n",
       "      <td>4012.0</td>\n",
       "      <td>18.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-12-31 20:00:00</th>\n",
       "      <td>3856.0</td>\n",
       "      <td>16.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-12-31 21:00:00</th>\n",
       "      <td>3671.0</td>\n",
       "      <td>17.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-12-31 22:00:00</th>\n",
       "      <td>3499.0</td>\n",
       "      <td>15.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-12-31 23:00:00</th>\n",
       "      <td>3345.0</td>\n",
       "      <td>15.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26304 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       load   temp\n",
       "2012-01-01 00:00:00  2698.0  32.00\n",
       "2012-01-01 01:00:00  2558.0  32.67\n",
       "2012-01-01 02:00:00  2444.0  30.00\n",
       "2012-01-01 03:00:00  2402.0  31.00\n",
       "2012-01-01 04:00:00  2403.0  32.00\n",
       "...                     ...    ...\n",
       "2014-12-31 19:00:00  4012.0  18.00\n",
       "2014-12-31 20:00:00  3856.0  16.67\n",
       "2014-12-31 21:00:00  3671.0  17.00\n",
       "2014-12-31 22:00:00  3499.0  15.33\n",
       "2014-12-31 23:00:00  3345.0  15.33\n",
       "\n",
       "[26304 rows x 2 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"../data/energy.csv\"\n",
    "ts_data = get_dataset(path, time_col='time')\n",
    "ts_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0cf04d97",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:25:08.099507Z",
     "start_time": "2024-04-14T13:25:08.065344Z"
    }
   },
   "outputs": [],
   "source": [
    "# 时间格式编码\n",
    "def time_features_from_frequency_str(freq_str: str):\n",
    "    \"\"\"\n",
    "    Returns a list of time features that will be appropriate for the given frequency string.\n",
    "    Parameters\n",
    "    ----------\n",
    "    freq_str\n",
    "        Frequency string of the form [multiple][granularity] such as \"12H\", \"5min\", \"1D\" etc.\n",
    "    \"\"\"\n",
    "\n",
    "    class TimeFeature:\n",
    "        def __init__(self):\n",
    "            pass\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            pass\n",
    "\n",
    "        def __repr__(self):\n",
    "            return self.__class__.__name__ + \"()\"\n",
    "\n",
    "\n",
    "    class SecondOfMinute(TimeFeature):\n",
    "        \"\"\"Minute of hour encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return index.second / 59.0 - 0.5\n",
    "\n",
    "\n",
    "    class MinuteOfHour(TimeFeature):\n",
    "        \"\"\"Minute of hour encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return index.minute / 59.0 - 0.5\n",
    "\n",
    "\n",
    "    class HourOfDay(TimeFeature):\n",
    "        \"\"\"Hour of day encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return index.hour / 23.0 - 0.5\n",
    "\n",
    "\n",
    "    class DayOfWeek(TimeFeature):\n",
    "        \"\"\"Hour of day encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return index.dayofweek / 6.0 - 0.5\n",
    "\n",
    "\n",
    "    class DayOfMonth(TimeFeature):\n",
    "        \"\"\"Day of month encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return (index.day - 1) / 30.0 - 0.5\n",
    "\n",
    "\n",
    "    class DayOfYear(TimeFeature):\n",
    "        \"\"\"Day of year encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return (index.dayofyear - 1) / 365.0 - 0.5\n",
    "\n",
    "\n",
    "    class MonthOfYear(TimeFeature):\n",
    "        \"\"\"Month of year encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return (index.month - 1) / 11.0 - 0.5\n",
    "\n",
    "\n",
    "    class WeekOfYear(TimeFeature):\n",
    "        \"\"\"Week of year encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return (index.isocalendar().week - 1) / 52.0 - 0.5\n",
    "\n",
    "    \n",
    "    features_by_offsets = {\n",
    "        offsets.YearEnd: [],\n",
    "        offsets.QuarterEnd: [MonthOfYear],\n",
    "        offsets.MonthEnd: [MonthOfYear],\n",
    "        offsets.Week: [DayOfMonth, WeekOfYear],\n",
    "        offsets.Day: [DayOfWeek, DayOfMonth, DayOfYear],\n",
    "        offsets.BusinessDay: [DayOfWeek, DayOfMonth, DayOfYear],\n",
    "        offsets.Hour: [HourOfDay, DayOfWeek, DayOfMonth, DayOfYear],\n",
    "        offsets.Minute: [\n",
    "            MinuteOfHour,\n",
    "            HourOfDay,\n",
    "            DayOfWeek,\n",
    "            DayOfMonth,\n",
    "            DayOfYear,\n",
    "        ],\n",
    "        offsets.Second: [\n",
    "            SecondOfMinute,\n",
    "            MinuteOfHour,\n",
    "            HourOfDay,\n",
    "            DayOfWeek,\n",
    "            DayOfMonth,\n",
    "            DayOfYear,\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    offset = to_offset(freq_str)\n",
    "\n",
    "    for offset_type, feature_classes in features_by_offsets.items():\n",
    "        if isinstance(offset, offset_type):\n",
    "            return [cls() for cls in feature_classes]\n",
    "\n",
    "    supported_freq_msg = f\"\"\"\n",
    "    Unsupported frequency {freq_str}\n",
    "    The following frequencies are supported:\n",
    "        Y   - yearly\n",
    "            alias: A\n",
    "        M   - monthly\n",
    "        W   - weekly\n",
    "        D   - daily\n",
    "        B   - business days\n",
    "        H   - hourly\n",
    "        T   - minutely\n",
    "            alias: min\n",
    "        S   - secondly\n",
    "    \"\"\"\n",
    "    raise RuntimeError(supported_freq_msg)\n",
    "    \n",
    "def time_features(dates, freq='h'):\n",
    "    return np.vstack([feat(dates) for feat in time_features_from_frequency_str(freq)])\n",
    "\n",
    "# 包含时间维度的数据集划分\n",
    "def divide_dataset(df, valid_date, test_date, x_feature_list, y_feature_list, freq):\n",
    "    #归一化\n",
    "    x_scaler = MinMaxScaler()\n",
    "    y_scaler = MinMaxScaler()\n",
    "\n",
    "    #测试集\n",
    "    train = df.copy()[df.index < valid_date][x_feature_list]\n",
    "    train_stamp = time_features(pd.to_datetime(train.index), freq=freq)\n",
    "    train_stamp = train_stamp.transpose(1, 0)\n",
    "    train[x_feature_list] = x_scaler.fit_transform(train)\n",
    "    xtr = train.values.astype('float32')\n",
    "\n",
    "    #验证集\n",
    "    valid = df.copy()[(df.index >= valid_date) & (df.index < test_date)][x_feature_list]\n",
    "    valid_stamp = time_features(pd.to_datetime(valid.index), freq=freq)\n",
    "    valid_stamp = valid_stamp.transpose(1, 0)\n",
    "    valid[x_feature_list] = x_scaler.fit_transform(valid)\n",
    "    xva = valid.values.astype('float32')\n",
    "\n",
    "    #测试集\n",
    "    test = df.copy()[test_date:][x_feature_list]\n",
    "    test_stamp = time_features(pd.to_datetime(test.index), freq=freq)\n",
    "    test_stamp = test_stamp.transpose(1, 0)\n",
    "    test[x_feature_list] = x_scaler.fit_transform(test)\n",
    "    xte = test.values.astype('float32')\n",
    "\n",
    "    #标签\n",
    "    ytr = df.copy()[df.index < valid_date][y_feature_list]\n",
    "    ytr[y_feature_list] = y_scaler.fit_transform(ytr)\n",
    "    ytr = ytr.values.astype('float32')\n",
    "\n",
    "    yva = df.copy()[(df.index >= valid_date) & (df.index < test_date)][y_feature_list]\n",
    "    yva[y_feature_list] = y_scaler.fit_transform(yva)\n",
    "    yva = yva.values.astype('float32')\n",
    "\n",
    "    yte = df.copy()[test_date:][y_feature_list]\n",
    "    yte[y_feature_list] = y_scaler.fit_transform(yte)\n",
    "    yte = yte.values.astype('float32')\n",
    "    \n",
    "    # 数据合并，[训练集，测试集，时间戳]\n",
    "    train = [xtr, ytr, train_stamp]\n",
    "    valid = [xva, yva, valid_stamp]\n",
    "    test = [xte, yte, test_stamp]\n",
    "    \n",
    "    return y_scaler, train, valid, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "00c2f379",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:25:09.198121Z",
     "start_time": "2024-04-14T13:25:09.127706Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (23376, 2) y_train shape: (23376, 1) stamp_train shape: (23376, 4)\n",
      "x_valid shape: (1464, 2) y_valid shape: (1464, 1) stamp_valid shape: (1464, 4)\n",
      "x_test shape: (1464, 2) y_test shape: (1464, 1) stamp_test shape: (1464, 4)\n"
     ]
    }
   ],
   "source": [
    "# 构造参数字典\n",
    "params1 = {\n",
    "    \"df\": ts_data,\n",
    "    \"valid_date\": \"2014-09-01 00:00:00\",\n",
    "    \"test_date\": \"2014-11-01 00:00:00\",\n",
    "    \"x_feature_list\": [\"load\", 'temp'],\n",
    "    \"y_feature_list\": [\"load\"],\n",
    "    \"freq\": 'h'\n",
    "}\n",
    "\n",
    "#函数传参\n",
    "scaler, train, valid, test = divide_dataset(**params1)\n",
    "print(\"x_train shape: {0} y_train shape: {1} stamp_train shape: {2}\".format(train[0].shape, train[1].shape, train[2].shape))\n",
    "print(\"x_valid shape: {0} y_valid shape: {1} stamp_valid shape: {2}\".format(valid[0].shape, valid[1].shape, valid[2].shape))\n",
    "print(\"x_test shape: {0} y_test shape: {1} stamp_test shape: {2}\".format(test[0].shape, test[1].shape, test[2].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f0482dd1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:25:10.556901Z",
     "start_time": "2024-04-14T13:25:10.536851Z"
    }
   },
   "outputs": [],
   "source": [
    "#利用前seq_len个数据，预测下pred_len个数据\n",
    "def create_dataset(data_list, seq_len, pred_len, label_len, batch_size):\n",
    "    \"\"\"\n",
    "    @参数设置：\n",
    "    data_list：特征，目标，时间戳\n",
    "    seq_len：输入数据包含过去多少个时间步\n",
    "    pred_len：目标应该在未来多少个时间步之后\n",
    "    label_len：先验时间步\n",
    "    \"\"\"\n",
    "    # 获取数据\n",
    "    feature = data_list[0] # 特征\n",
    "    target = data_list[1] # 目标\n",
    "    stamp = data_list[2] # 时间戳\n",
    "    \n",
    "    # 循环生成数据\n",
    "    X, y = [], []\n",
    "    X_stamp, y_stamp = [], []\n",
    "    seq_len = seq_len - 1 # 包含当前时间点\n",
    "    for i in range(seq_len, len(feature) - pred_len):\n",
    "        # 数据维度\n",
    "        feat = feature[i - seq_len:i + 1]\n",
    "        tar = target[i + 1:i + 1 + pred_len]\n",
    "        X.append(feat)\n",
    "        y.append(tar)\n",
    "        \n",
    "        # 时间维度\n",
    "        xs = stamp[i - seq_len:i + 1]\n",
    "        ys = stamp[i + 1 - label_len:i + 1 + pred_len]\n",
    "        X_stamp.append(xs)\n",
    "        y_stamp.append(ys)\n",
    "        \n",
    "    # 转为张量，数据维度\n",
    "    X = torch.as_tensor(X).float()\n",
    "    y = torch.as_tensor(y).float()\n",
    "    \n",
    "    # 转为张量，时间维度\n",
    "    X_stamp = torch.as_tensor(X_stamp).float()\n",
    "    y_stamp = torch.as_tensor(y_stamp).float()\n",
    "    \n",
    "    # 创建dataloader，[特征，目标，特征时间编码，目标时间编码]\n",
    "    data_loader = DataLoader(TensorDataset(X, y, X_stamp, y_stamp), shuffle=True, batch_size=batch_size)\n",
    "    \n",
    "    return X, y, data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "892e7676",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:25:12.647496Z",
     "start_time": "2024-04-14T13:25:11.786489Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_size: torch.Size([23368, 6, 2]),y_size: torch.Size([23368, 3, 1]),loader_len: 731\n",
      "X_size: torch.Size([1456, 6, 2]),y_size: torch.Size([1456, 3, 1]),loader_len: 46\n",
      "X_size: torch.Size([1456, 6, 2]),y_size: torch.Size([1456, 3, 1]),loader_len: 46\n"
     ]
    }
   ],
   "source": [
    "# 构造参数字典\n",
    "params2 = {\n",
    "    \"seq_len\": 6,\n",
    "    \"pred_len\": 3,\n",
    "    \"label_len\": 0,\n",
    "    \"batch_size\": 32,\n",
    "}\n",
    "X_train, y_train, train_loader = create_dataset(train, **params2)\n",
    "X_valid, y_valid, valid_loader = create_dataset(valid, **params2)\n",
    "X_test, y_test, test_loader = create_dataset(test, **params2)\n",
    "print(\"X_size: {0},y_size: {1},loader_len: {2}\".format(X_train.shape, y_train.shape, len(train_loader)))\n",
    "print(\"X_size: {0},y_size: {1},loader_len: {2}\".format(X_valid.shape, y_valid.shape, len(valid_loader)))\n",
    "print(\"X_size: {0},y_size: {1},loader_len: {2}\".format(X_test.shape, y_test.shape, len(test_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc0aa3e",
   "metadata": {},
   "source": [
    "### 模型定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2c194077",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:25:15.135398Z",
     "start_time": "2024-04-14T13:25:15.107363Z"
    }
   },
   "outputs": [],
   "source": [
    "class IEBlock(nn.Module):\n",
    "    def __init__(self, input_dim, hid_dim, output_dim, num_node):\n",
    "        super(IEBlock, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_node = num_node\n",
    "\n",
    "        self._build()\n",
    "\n",
    "    def _build(self):\n",
    "        self.spatial_proj = nn.Sequential(\n",
    "            nn.Linear(self.input_dim, self.hid_dim),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(self.hid_dim, self.hid_dim // 4)\n",
    "        )\n",
    "\n",
    "        self.channel_proj = nn.Linear(self.num_node, self.num_node)\n",
    "        torch.nn.init.eye_(self.channel_proj.weight)\n",
    "\n",
    "        self.output_proj = nn.Linear(self.hid_dim // 4, self.output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.spatial_proj(x.permute(0, 2, 1))\n",
    "        x = x.permute(0, 2, 1) + self.channel_proj(x.permute(0, 2, 1))\n",
    "        x = self.output_proj(x.permute(0, 2, 1))\n",
    "\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# LightTS模型\n",
    "class LightTS(nn.Module):\n",
    "    def __init__(self, seq_len, pred_len, label_len, chunk_size, d_model, enc_in, dropout):\n",
    "        \"\"\"\n",
    "        chunk_size: int, reshape T into [num_chunks, chunk_size]\n",
    "        \"\"\"\n",
    "        super(LightTS, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.pred_len = pred_len\n",
    "\n",
    "        self.chunk_size = min(pred_len, seq_len, chunk_size)\n",
    "        assert (self.seq_len % self.chunk_size == 0)\n",
    "        self.num_chunks = self.seq_len // self.chunk_size\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.enc_in = enc_in\n",
    "        self.dropout = dropout\n",
    "        self._build()\n",
    "\n",
    "    def _build(self):\n",
    "        self.layer_1 = IEBlock(\n",
    "            input_dim=self.chunk_size,\n",
    "            hid_dim=self.d_model // 4,\n",
    "            output_dim=self.d_model // 4,\n",
    "            num_node=self.num_chunks\n",
    "        )\n",
    "\n",
    "        self.chunk_proj_1 = nn.Linear(self.num_chunks, 1)\n",
    "\n",
    "        self.layer_2 = IEBlock(\n",
    "            input_dim=self.chunk_size,\n",
    "            hid_dim=self.d_model // 4,\n",
    "            output_dim=self.d_model // 4,\n",
    "            num_node=self.num_chunks\n",
    "        )\n",
    "\n",
    "        self.chunk_proj_2 = nn.Linear(self.num_chunks, 1)\n",
    "\n",
    "        self.layer_3 = IEBlock(\n",
    "            input_dim=self.d_model // 2,\n",
    "            hid_dim=self.d_model // 2,\n",
    "            output_dim=self.pred_len,\n",
    "            num_node=self.enc_in\n",
    "        )\n",
    "\n",
    "        self.ar = nn.Linear(self.seq_len, self.pred_len)\n",
    "\n",
    "    def encoder(self, x):\n",
    "        B, T, N = x.size()\n",
    "\n",
    "        highway = self.ar(x.permute(0, 2, 1))\n",
    "        highway = highway.permute(0, 2, 1)\n",
    "\n",
    "        # continuous sampling\n",
    "        x1 = x.reshape(B, self.num_chunks, self.chunk_size, N)\n",
    "        x1 = x1.permute(0, 3, 2, 1)\n",
    "        x1 = x1.reshape(-1, self.chunk_size, self.num_chunks)\n",
    "        x1 = self.layer_1(x1)\n",
    "        x1 = self.chunk_proj_1(x1).squeeze(dim=-1)\n",
    "\n",
    "        # interval sampling\n",
    "        x2 = x.reshape(B, self.chunk_size, self.num_chunks, N)\n",
    "        x2 = x2.permute(0, 3, 1, 2)\n",
    "        x2 = x2.reshape(-1, self.chunk_size, self.num_chunks)\n",
    "        x2 = self.layer_2(x2)\n",
    "        x2 = self.chunk_proj_2(x2).squeeze(dim=-1)\n",
    "\n",
    "        x3 = torch.cat([x1, x2], dim=-1)\n",
    "\n",
    "        x3 = x3.reshape(B, N, -1)\n",
    "        x3 = x3.permute(0, 2, 1)\n",
    "\n",
    "        out = self.layer_3(x3)\n",
    "\n",
    "        out = out + highway\n",
    "        return out\n",
    "\n",
    "    def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec):\n",
    "        dec_out = self.encoder(x_enc)\n",
    "        \n",
    "        output = dec_out[:, -self.pred_len:, :]  # [B, L, D]\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c4dda3",
   "metadata": {},
   "source": [
    "### 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "2dcad24b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:25:21.848673Z",
     "start_time": "2024-04-14T13:25:21.806887Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(task_args, train_args, model_args):\n",
    "    # 参数配置\n",
    "    columns = task_args['columns'] # 模型全部特征\n",
    "    target = task_args['target'] # 模型预测特征\n",
    "    features = task_args['features'] # 模型预测模式\n",
    "    model_name = train_args['model_name'] # 模型名称\n",
    "    train_loader = train_args['train_loader'] # 训练集\n",
    "    valid_loader = train_args['valid_loader'] # 验证集\n",
    "    n_epochs = train_args['n_epochs'] # 训练次数\n",
    "    learning_rate = train_args['learning_rate'] # 学习率\n",
    "    loss = train_args['loss'] # 损失函数\n",
    "    patience = train_args['patience'] # 最大早停次数阈值，超过就会早停\n",
    "    lradj = train_args['lradj'] # 学习率函数\n",
    "    path = train_args['path'] # 模型保存路径\n",
    "    verbose = train_args['verbose'] # 打印训练过程\n",
    "    plots = train_args['plots'] # 绘制损失图\n",
    "    pred_len = model_args['pred_len'] # 预测长度\n",
    "    label_len = model_args['label_len']\n",
    "    \n",
    "    #检查是否可用GPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # 创建模型和优化器\n",
    "    model = model_name(**model_args).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_fn = loss\n",
    "    \n",
    "    # 损失函数值\n",
    "    train_losses, val_losses = [], []\n",
    "    \n",
    "    # 调整学习率\n",
    "    def adjust_learning_rate(optimizer, epoch, lradj, learning_rate, train_epochs):\n",
    "        # lr = learning_rate * (0.2 ** (epoch // 2))\n",
    "        if lradj == 'type1':\n",
    "            lr_adjust = {epoch: learning_rate * (0.5 ** ((epoch - 1) // 1))}\n",
    "        elif lradj == 'type2':\n",
    "            lr_adjust = {\n",
    "                2: 5e-5, 4: 1e-5, 6: 5e-6, 8: 1e-6,\n",
    "                10: 5e-7, 15: 1e-7, 20: 5e-8\n",
    "            }\n",
    "        elif lradj == \"cosine\":\n",
    "            lr_adjust = {epoch: learning_rate /2 * (1 + math.cos(epoch / train_epochs * math.pi))}\n",
    "        if epoch in lr_adjust.keys():\n",
    "            lr = lr_adjust[epoch]\n",
    "            # 参数组(param_groups)是用来指定不同的参数组以便对它们进行不同的优化设置，比如'lr'\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "            print('Updating learning rate to {}'.format(lr))\n",
    "    \n",
    "    # 设置早停\n",
    "    class EarlyStopping():\n",
    "        def __init__(self, patience=7, verbose=False, delta=0):\n",
    "            self.patience = patience # 连续超限次数，如果满足条件，则早停\n",
    "            self.verbose = verbose\n",
    "            self.counter = 0\n",
    "            self.best_score = None\n",
    "            self.early_stop = False\n",
    "            self.val_loss_min = np.Inf\n",
    "            self.delta = delta\n",
    "\n",
    "        def __call__(self, val_loss, model, path):\n",
    "            score = -val_loss\n",
    "            if self.best_score is None:\n",
    "                self.best_score = score\n",
    "                self.save_checkpoint(val_loss, model, path)\n",
    "            elif score < self.best_score + self.delta:\n",
    "                self.counter += 1\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "                if self.counter >= self.patience:\n",
    "                    self.early_stop = True\n",
    "            else:\n",
    "                self.best_score = score\n",
    "                self.save_checkpoint(val_loss, model, path)\n",
    "                self.counter = 0\n",
    "\n",
    "        def save_checkpoint(self, val_loss, model, path):\n",
    "            if self.verbose:\n",
    "                print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "            torch.save(model.state_dict(), path + '/' + 'checkpoint.pth')\n",
    "            self.val_loss_min = val_loss\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=verbose)\n",
    "    \n",
    "    # 训练任务\n",
    "    def forecasting_task(columns, target, features):\n",
    "        \"\"\"\n",
    "        features: [M, S, MS]; \n",
    "            M:multivariate predict multivariate, \n",
    "            S:univariate predict univariate, \n",
    "            MS:multivariate predict univariate'\n",
    "        \"\"\"\n",
    "        # 字典索引生成\n",
    "        col_dict = {}\n",
    "        for i,j in enumerate(columns):\n",
    "            col_dict[j] = i\n",
    "\n",
    "        if features == 'MS':\n",
    "            target = target[0]\n",
    "            f_dim = col_dict[target]\n",
    "        elif features == 'S':\n",
    "            f_dim = 0\n",
    "        else:\n",
    "            f_dim = 0\n",
    "        return f_dim\n",
    "    f_dim = forecasting_task(columns, target, features)\n",
    "    \n",
    "    # 设置保存模型路径\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    # 模型训练和验证\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for batch_x, batch_y, batch_x_mark, batch_y_mark in train_loader:\n",
    "            #将数据移至 GPU\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            batch_x_mark = batch_x_mark.to(device)\n",
    "            batch_y_mark = batch_y_mark.to(device)\n",
    "            # 清空梯度\n",
    "            optimizer.zero_grad()\n",
    "            # decoder输入\n",
    "            dec_inp = torch.zeros_like(batch_y[:, -pred_len:, :]).float()\n",
    "            dec_inp = torch.cat([batch_y[:, :label_len, :], dec_inp], dim=1).float().to(device)\n",
    "            outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "            if features == 'MS':\n",
    "                outputs = outputs[:, :, f_dim: f_dim+1] \n",
    "            else:\n",
    "                outputs = outputs[:, :, f_dim:]\n",
    "            train_loss = loss_fn(outputs, batch_y)\n",
    "            # 反向传播计算得到每个参数的梯度值\n",
    "            train_loss.backward()\n",
    "            # 通过梯度下降执行一步参数更新\n",
    "            optimizer.step()\n",
    "            #每个batch的loss和\n",
    "            total_train_loss += train_loss.item() # .item()表示只包含一个元素的tensor中提取值\n",
    "                \n",
    "        #每个epoch的损失平均\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        \n",
    "        #所有epoch的loss\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        #评估模型\n",
    "        model.eval()\n",
    "        #关闭自动求导功能，只使用训练好的模型进行预测或评估，不需要进行梯度计算和参数更新\n",
    "        with torch.no_grad():\n",
    "            total_val_loss = 0\n",
    "            for batch_x, batch_y, batch_x_mark, batch_y_mark in valid_loader:\n",
    "                #将数据移至 GPU\n",
    "                batch_x = batch_x.to(device)\n",
    "                batch_y = batch_y.to(device)\n",
    "                batch_x_mark = batch_x_mark.to(device)\n",
    "                batch_y_mark = batch_y_mark.to(device)\n",
    "                # decoder输入\n",
    "                dec_inp = torch.zeros_like(batch_y[:, -pred_len:, :]).float()\n",
    "                dec_inp = torch.cat([batch_y[:, :label_len, :], dec_inp], dim=1).float().to(device)\n",
    "                val_outputs = model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
    "                if features == 'MS':\n",
    "                    val_outputs = val_outputs[:, :, f_dim: f_dim+1] \n",
    "                else:\n",
    "                    val_outputs = val_outputs[:, :, f_dim:]\n",
    "                val_loss = loss_fn(val_outputs, batch_y)\n",
    "                #每个batch的loss和\n",
    "                total_val_loss += val_loss.item()\n",
    "                \n",
    "        #每个epoch的损失平均\n",
    "        avg_val_loss = total_val_loss / len(valid_loader)\n",
    "        \n",
    "        #所有epoch的loss\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        #打印训练过程\n",
    "        if verbose:\n",
    "            print(f'Epoch [{epoch+1}/{n_epochs}], Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}')\n",
    "            \n",
    "        # 设置早停\n",
    "        early_stopping(avg_val_loss, model, path)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping!\")\n",
    "            break\n",
    "            \n",
    "        # 调整学习率\n",
    "        adjust_learning_rate(optimizer, epoch+1, lradj, learning_rate, n_epochs)\n",
    "        \n",
    "    # 加载最佳模型\n",
    "    best_model_path = path + '/' + 'checkpoint.pth'\n",
    "    model.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "    #绘制损失函数图\n",
    "    def plot_loss(train_losses, val_losses):\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.style.use('seaborn-v0_8-paper') #绘制背景色\n",
    "        plt.grid(axis = 'y',linewidth=0.35) #绘制网格\n",
    "        plt.plot(val_losses, linestyle='-',color = '#11b3b6')\n",
    "        plt.plot(train_losses, linestyle='-',color = '#f14643')\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Training and Validation Progress\")\n",
    "        plt.legend([\"Validation\", \"Training\"])\n",
    "        plt.show()\n",
    "    if plots:\n",
    "        plot_loss(train_losses, val_losses)\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "52e7c02a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:25:47.356643Z",
     "start_time": "2024-04-14T13:25:26.159293Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|████▏                                                                              | 1/20 [00:04<01:28,  4.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Training Loss: 0.0081, Validation Loss: 0.0067\n",
      "Validation loss decreased (inf --> 0.006748).  Saving model ...\n",
      "Updating learning rate to 0.0009938441702975688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|████████▎                                                                          | 2/20 [00:09<01:23,  4.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/20], Training Loss: 0.0038, Validation Loss: 0.0071\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Updating learning rate to 0.0009755282581475768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 15%|████████████▍                                                                      | 3/20 [00:13<01:17,  4.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/20], Training Loss: 0.0032, Validation Loss: 0.0045\n",
      "Validation loss decreased (0.006748 --> 0.004522).  Saving model ...\n",
      "Updating learning rate to 0.0009455032620941839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|████████████████▌                                                                  | 4/20 [00:18<01:13,  4.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/20], Training Loss: 0.0029, Validation Loss: 0.0042\n",
      "Validation loss decreased (0.004522 --> 0.004229).  Saving model ...\n",
      "Updating learning rate to 0.0009045084971874737\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████▌                                                                  | 4/20 [00:20<01:20,  5.02s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[73], line 31\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 构造参数字典\u001b[39;00m\n\u001b[0;32m      2\u001b[0m params3 \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtask_args\u001b[39m\u001b[38;5;124m\"\u001b[39m:{\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mload\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemp\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     29\u001b[0m     },\n\u001b[0;32m     30\u001b[0m }\n\u001b[1;32m---> 31\u001b[0m model \u001b[38;5;241m=\u001b[39m train(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams3)\n",
      "Cell \u001b[1;32mIn[72], line 123\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(task_args, train_args, model_args)\u001b[0m\n\u001b[0;32m    121\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# decoder输入\u001b[39;00m\n\u001b[1;32m--> 123\u001b[0m dec_inp \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_y\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mpred_len\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m    124\u001b[0m dec_inp \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([batch_y[:, :label_len, :], dec_inp], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    125\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(batch_x, batch_x_mark, dec_inp, batch_y_mark)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 构造参数字典\n",
    "params3 = {\n",
    "    \"task_args\":{\n",
    "        \"columns\": ['load', 'temp'],\n",
    "        \"target\": ['load'],\n",
    "        \"features\": 'MS'\n",
    "    },\n",
    "    \"train_args\": {\n",
    "        \"model_name\": LightTS,\n",
    "        \"train_loader\": train_loader,\n",
    "        \"valid_loader\": valid_loader,\n",
    "        \"n_epochs\": 20,\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"loss\": nn.MSELoss(),\n",
    "        \"patience\": 3,\n",
    "        \"lradj\": 'cosine',\n",
    "        \"path\": \"../models/test\",\n",
    "        \"verbose\": True,\n",
    "        \"plots\": True,\n",
    "    },\n",
    "    \"model_args\": {\n",
    "        'enc_in': 2, \n",
    "        'seq_len': 6,\n",
    "        'pred_len': 3,\n",
    "        \"label_len\": 0,\n",
    "        'd_model': 512,\n",
    "        'chunk_size': 3,\n",
    "        'dropout': 0.1\n",
    "    },\n",
    "}\n",
    "model = train(**params3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cebc78",
   "metadata": {},
   "source": [
    "# 基于MICN的时间序列预测"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4df734",
   "metadata": {},
   "source": [
    "## 多变量多步预测"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f233cf90",
   "metadata": {},
   "source": [
    "### 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "95abb636",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:25:51.988890Z",
     "start_time": "2024-04-14T13:25:51.976886Z"
    }
   },
   "outputs": [],
   "source": [
    "# 读取数据\n",
    "def get_dataset(path, time_col=None):\n",
    "    data = pd.read_csv(path)\n",
    "    rows = data.shape[0]\n",
    "    now = datetime.now()\n",
    "    newtime = now.replace(microsecond=0)\n",
    "    if time_col == None:\n",
    "        # 如果没有时间列，生成时间戳范围\n",
    "        time_index = pd.date_range(start=datetime.now() -\n",
    "                                   timedelta(seconds=rows - 1),\n",
    "                                   end=datetime.now(),\n",
    "                                   freq='S')\n",
    "        full_data = pd.DataFrame(data=data.values,\n",
    "                                 index=pd.to_datetime(time_index, unit='s'),\n",
    "                                 columns=data.columns)\n",
    "    else:\n",
    "        columns = [i for i in data.columns if i != time_col] # 去除时间列\n",
    "        full_data = pd.DataFrame(data=data.drop([time_col], axis=1).values,\n",
    "                                 index=pd.to_datetime(data[time_col].values),\n",
    "                                 columns=columns)\n",
    "    return full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b037cc4d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:25:53.099502Z",
     "start_time": "2024-04-14T13:25:53.027367Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>load</th>\n",
       "      <th>temp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-01-01 00:00:00</th>\n",
       "      <td>2698.0</td>\n",
       "      <td>32.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-01 01:00:00</th>\n",
       "      <td>2558.0</td>\n",
       "      <td>32.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-01 02:00:00</th>\n",
       "      <td>2444.0</td>\n",
       "      <td>30.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-01 03:00:00</th>\n",
       "      <td>2402.0</td>\n",
       "      <td>31.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-01 04:00:00</th>\n",
       "      <td>2403.0</td>\n",
       "      <td>32.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-12-31 19:00:00</th>\n",
       "      <td>4012.0</td>\n",
       "      <td>18.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-12-31 20:00:00</th>\n",
       "      <td>3856.0</td>\n",
       "      <td>16.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-12-31 21:00:00</th>\n",
       "      <td>3671.0</td>\n",
       "      <td>17.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-12-31 22:00:00</th>\n",
       "      <td>3499.0</td>\n",
       "      <td>15.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-12-31 23:00:00</th>\n",
       "      <td>3345.0</td>\n",
       "      <td>15.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26304 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       load   temp\n",
       "2012-01-01 00:00:00  2698.0  32.00\n",
       "2012-01-01 01:00:00  2558.0  32.67\n",
       "2012-01-01 02:00:00  2444.0  30.00\n",
       "2012-01-01 03:00:00  2402.0  31.00\n",
       "2012-01-01 04:00:00  2403.0  32.00\n",
       "...                     ...    ...\n",
       "2014-12-31 19:00:00  4012.0  18.00\n",
       "2014-12-31 20:00:00  3856.0  16.67\n",
       "2014-12-31 21:00:00  3671.0  17.00\n",
       "2014-12-31 22:00:00  3499.0  15.33\n",
       "2014-12-31 23:00:00  3345.0  15.33\n",
       "\n",
       "[26304 rows x 2 columns]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"../data/energy.csv\"\n",
    "ts_data = get_dataset(path, time_col='time')\n",
    "ts_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bf2145bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:25:54.473736Z",
     "start_time": "2024-04-14T13:25:54.438760Z"
    }
   },
   "outputs": [],
   "source": [
    "# 时间格式编码\n",
    "def time_features_from_frequency_str(freq_str: str):\n",
    "    \"\"\"\n",
    "    Returns a list of time features that will be appropriate for the given frequency string.\n",
    "    Parameters\n",
    "    ----------\n",
    "    freq_str\n",
    "        Frequency string of the form [multiple][granularity] such as \"12H\", \"5min\", \"1D\" etc.\n",
    "    \"\"\"\n",
    "\n",
    "    class TimeFeature:\n",
    "        def __init__(self):\n",
    "            pass\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            pass\n",
    "\n",
    "        def __repr__(self):\n",
    "            return self.__class__.__name__ + \"()\"\n",
    "\n",
    "\n",
    "    class SecondOfMinute(TimeFeature):\n",
    "        \"\"\"Minute of hour encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return index.second / 59.0 - 0.5\n",
    "\n",
    "\n",
    "    class MinuteOfHour(TimeFeature):\n",
    "        \"\"\"Minute of hour encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return index.minute / 59.0 - 0.5\n",
    "\n",
    "\n",
    "    class HourOfDay(TimeFeature):\n",
    "        \"\"\"Hour of day encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return index.hour / 23.0 - 0.5\n",
    "\n",
    "\n",
    "    class DayOfWeek(TimeFeature):\n",
    "        \"\"\"Hour of day encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return index.dayofweek / 6.0 - 0.5\n",
    "\n",
    "\n",
    "    class DayOfMonth(TimeFeature):\n",
    "        \"\"\"Day of month encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return (index.day - 1) / 30.0 - 0.5\n",
    "\n",
    "\n",
    "    class DayOfYear(TimeFeature):\n",
    "        \"\"\"Day of year encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return (index.dayofyear - 1) / 365.0 - 0.5\n",
    "\n",
    "\n",
    "    class MonthOfYear(TimeFeature):\n",
    "        \"\"\"Month of year encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return (index.month - 1) / 11.0 - 0.5\n",
    "\n",
    "\n",
    "    class WeekOfYear(TimeFeature):\n",
    "        \"\"\"Week of year encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return (index.isocalendar().week - 1) / 52.0 - 0.5\n",
    "\n",
    "    \n",
    "    features_by_offsets = {\n",
    "        offsets.YearEnd: [],\n",
    "        offsets.QuarterEnd: [MonthOfYear],\n",
    "        offsets.MonthEnd: [MonthOfYear],\n",
    "        offsets.Week: [DayOfMonth, WeekOfYear],\n",
    "        offsets.Day: [DayOfWeek, DayOfMonth, DayOfYear],\n",
    "        offsets.BusinessDay: [DayOfWeek, DayOfMonth, DayOfYear],\n",
    "        offsets.Hour: [HourOfDay, DayOfWeek, DayOfMonth, DayOfYear],\n",
    "        offsets.Minute: [\n",
    "            MinuteOfHour,\n",
    "            HourOfDay,\n",
    "            DayOfWeek,\n",
    "            DayOfMonth,\n",
    "            DayOfYear,\n",
    "        ],\n",
    "        offsets.Second: [\n",
    "            SecondOfMinute,\n",
    "            MinuteOfHour,\n",
    "            HourOfDay,\n",
    "            DayOfWeek,\n",
    "            DayOfMonth,\n",
    "            DayOfYear,\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    offset = to_offset(freq_str)\n",
    "\n",
    "    for offset_type, feature_classes in features_by_offsets.items():\n",
    "        if isinstance(offset, offset_type):\n",
    "            return [cls() for cls in feature_classes]\n",
    "\n",
    "    supported_freq_msg = f\"\"\"\n",
    "    Unsupported frequency {freq_str}\n",
    "    The following frequencies are supported:\n",
    "        Y   - yearly\n",
    "            alias: A\n",
    "        M   - monthly\n",
    "        W   - weekly\n",
    "        D   - daily\n",
    "        B   - business days\n",
    "        H   - hourly\n",
    "        T   - minutely\n",
    "            alias: min\n",
    "        S   - secondly\n",
    "    \"\"\"\n",
    "    raise RuntimeError(supported_freq_msg)\n",
    "    \n",
    "def time_features(dates, freq='h'):\n",
    "    return np.vstack([feat(dates) for feat in time_features_from_frequency_str(freq)])\n",
    "\n",
    "# 包含时间维度的数据集划分\n",
    "def divide_dataset(df, valid_date, test_date, x_feature_list, y_feature_list, freq):\n",
    "    #归一化\n",
    "    x_scaler = MinMaxScaler()\n",
    "    y_scaler = MinMaxScaler()\n",
    "\n",
    "    #测试集\n",
    "    train = df.copy()[df.index < valid_date][x_feature_list]\n",
    "    train_stamp = time_features(pd.to_datetime(train.index), freq=freq)\n",
    "    train_stamp = train_stamp.transpose(1, 0)\n",
    "    train[x_feature_list] = x_scaler.fit_transform(train)\n",
    "    xtr = train.values.astype('float32')\n",
    "\n",
    "    #验证集\n",
    "    valid = df.copy()[(df.index >= valid_date) & (df.index < test_date)][x_feature_list]\n",
    "    valid_stamp = time_features(pd.to_datetime(valid.index), freq=freq)\n",
    "    valid_stamp = valid_stamp.transpose(1, 0)\n",
    "    valid[x_feature_list] = x_scaler.fit_transform(valid)\n",
    "    xva = valid.values.astype('float32')\n",
    "\n",
    "    #测试集\n",
    "    test = df.copy()[test_date:][x_feature_list]\n",
    "    test_stamp = time_features(pd.to_datetime(test.index), freq=freq)\n",
    "    test_stamp = test_stamp.transpose(1, 0)\n",
    "    test[x_feature_list] = x_scaler.fit_transform(test)\n",
    "    xte = test.values.astype('float32')\n",
    "\n",
    "    #标签\n",
    "    ytr = df.copy()[df.index < valid_date][y_feature_list]\n",
    "    ytr[y_feature_list] = y_scaler.fit_transform(ytr)\n",
    "    ytr = ytr.values.astype('float32')\n",
    "\n",
    "    yva = df.copy()[(df.index >= valid_date) & (df.index < test_date)][y_feature_list]\n",
    "    yva[y_feature_list] = y_scaler.fit_transform(yva)\n",
    "    yva = yva.values.astype('float32')\n",
    "\n",
    "    yte = df.copy()[test_date:][y_feature_list]\n",
    "    yte[y_feature_list] = y_scaler.fit_transform(yte)\n",
    "    yte = yte.values.astype('float32')\n",
    "    \n",
    "    # 数据合并，[训练集，测试集，时间戳]\n",
    "    train = [xtr, ytr, train_stamp]\n",
    "    valid = [xva, yva, valid_stamp]\n",
    "    test = [xte, yte, test_stamp]\n",
    "    \n",
    "    return y_scaler, train, valid, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6a7671ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:25:55.608801Z",
     "start_time": "2024-04-14T13:25:55.539375Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (23376, 2) y_train shape: (23376, 2) stamp_train shape: (23376, 4)\n",
      "x_valid shape: (1464, 2) y_valid shape: (1464, 2) stamp_valid shape: (1464, 4)\n",
      "x_test shape: (1464, 2) y_test shape: (1464, 2) stamp_test shape: (1464, 4)\n"
     ]
    }
   ],
   "source": [
    "# 构造参数字典\n",
    "params1 = {\n",
    "    \"df\": ts_data,\n",
    "    \"valid_date\": \"2014-09-01 00:00:00\",\n",
    "    \"test_date\": \"2014-11-01 00:00:00\",\n",
    "    \"x_feature_list\": [\"load\", 'temp'],\n",
    "    \"y_feature_list\": [\"load\", 'temp'],\n",
    "    \"freq\": 'h'\n",
    "}\n",
    "\n",
    "#函数传参\n",
    "scaler, train, valid, test = divide_dataset(**params1)\n",
    "print(\"x_train shape: {0} y_train shape: {1} stamp_train shape: {2}\".format(train[0].shape, train[1].shape, train[2].shape))\n",
    "print(\"x_valid shape: {0} y_valid shape: {1} stamp_valid shape: {2}\".format(valid[0].shape, valid[1].shape, valid[2].shape))\n",
    "print(\"x_test shape: {0} y_test shape: {1} stamp_test shape: {2}\".format(test[0].shape, test[1].shape, test[2].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9735e4c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:25:57.038268Z",
     "start_time": "2024-04-14T13:25:57.013552Z"
    }
   },
   "outputs": [],
   "source": [
    "#利用前seq_len个数据，预测下pred_len个数据\n",
    "def create_dataset(data_list, seq_len, pred_len, label_len, batch_size):\n",
    "    \"\"\"\n",
    "    @参数设置：\n",
    "    data_list：特征，目标，时间戳\n",
    "    seq_len：输入数据包含过去多少个时间步\n",
    "    pred_len：目标应该在未来多少个时间步之后\n",
    "    label_len：先验时间步\n",
    "    \"\"\"\n",
    "    # 获取数据\n",
    "    feature = data_list[0] # 特征\n",
    "    target = data_list[1] # 目标\n",
    "    stamp = data_list[2] # 时间戳\n",
    "    \n",
    "    # 循环生成数据\n",
    "    X, y = [], []\n",
    "    X_stamp, y_stamp = [], []\n",
    "    seq_len = seq_len - 1 # 包含当前时间点\n",
    "    for i in range(seq_len, len(feature) - pred_len):\n",
    "        # 数据维度\n",
    "        feat = feature[i - seq_len:i + 1]\n",
    "        tar = target[i + 1:i + 1 + pred_len]\n",
    "        X.append(feat)\n",
    "        y.append(tar)\n",
    "        \n",
    "        # 时间维度\n",
    "        xs = stamp[i - seq_len:i + 1]\n",
    "        ys = stamp[i + 1 - label_len:i + 1 + pred_len]\n",
    "        X_stamp.append(xs)\n",
    "        y_stamp.append(ys)\n",
    "        \n",
    "    # 转为张量，数据维度\n",
    "    X = torch.as_tensor(X).float()\n",
    "    y = torch.as_tensor(y).float()\n",
    "    \n",
    "    # 转为张量，时间维度\n",
    "    X_stamp = torch.as_tensor(X_stamp).float()\n",
    "    y_stamp = torch.as_tensor(y_stamp).float()\n",
    "    \n",
    "    # 创建dataloader，[特征，目标，特征时间编码，目标时间编码]\n",
    "    data_loader = DataLoader(TensorDataset(X, y, X_stamp, y_stamp), shuffle=True, batch_size=batch_size)\n",
    "    \n",
    "    return X, y, data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b26f9013",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:25:58.995467Z",
     "start_time": "2024-04-14T13:25:58.164627Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_size: torch.Size([23368, 6, 2]),y_size: torch.Size([23368, 3, 2]),loader_len: 731\n",
      "X_size: torch.Size([1456, 6, 2]),y_size: torch.Size([1456, 3, 2]),loader_len: 46\n",
      "X_size: torch.Size([1456, 6, 2]),y_size: torch.Size([1456, 3, 2]),loader_len: 46\n"
     ]
    }
   ],
   "source": [
    "# 构造参数字典\n",
    "params2 = {\n",
    "    \"seq_len\": 6,\n",
    "    \"pred_len\": 3,\n",
    "    \"label_len\": 0,\n",
    "    \"batch_size\": 32,\n",
    "}\n",
    "X_train, y_train, train_loader = create_dataset(train, **params2)\n",
    "X_valid, y_valid, valid_loader = create_dataset(valid, **params2)\n",
    "X_test, y_test, test_loader = create_dataset(test, **params2)\n",
    "print(\"X_size: {0},y_size: {1},loader_len: {2}\".format(X_train.shape, y_train.shape, len(train_loader)))\n",
    "print(\"X_size: {0},y_size: {1},loader_len: {2}\".format(X_valid.shape, y_valid.shape, len(valid_loader)))\n",
    "print(\"X_size: {0},y_size: {1},loader_len: {2}\".format(X_test.shape, y_test.shape, len(test_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd312df",
   "metadata": {},
   "source": [
    "### 模型定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c1da8a8d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:26:01.313037Z",
     "start_time": "2024-04-14T13:26:01.251127Z"
    }
   },
   "outputs": [],
   "source": [
    "# DataEmbedding编码类\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model).float()\n",
    "        pe.require_grad = False\n",
    "\n",
    "        position = torch.arange(0, max_len).float().unsqueeze(1)\n",
    "        div_term = (torch.arange(0, d_model, 2).float()\n",
    "                    * -(math.log(10000.0) / d_model)).exp()\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pe[:, :x.size(1)]\n",
    "\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, c_in, d_model):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        padding = 1 if torch.__version__ >= '1.5.0' else 2\n",
    "        self.tokenConv = nn.Conv1d(in_channels=c_in, out_channels=d_model,\n",
    "                                   kernel_size=3, padding=padding, padding_mode='circular', bias=False)\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                nn.init.kaiming_normal_(\n",
    "                    m.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.tokenConv(x.permute(0, 2, 1)).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class FixedEmbedding(nn.Module):\n",
    "    def __init__(self, c_in, d_model):\n",
    "        super(FixedEmbedding, self).__init__()\n",
    "\n",
    "        w = torch.zeros(c_in, d_model).float()\n",
    "        w.require_grad = False\n",
    "\n",
    "        position = torch.arange(0, c_in).float().unsqueeze(1)\n",
    "        div_term = (torch.arange(0, d_model, 2).float()\n",
    "                    * -(math.log(10000.0) / d_model)).exp()\n",
    "\n",
    "        w[:, 0::2] = torch.sin(position * div_term)\n",
    "        w[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        self.emb = nn.Embedding(c_in, d_model)\n",
    "        self.emb.weight = nn.Parameter(w, requires_grad=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.emb(x).detach()\n",
    "\n",
    "\n",
    "class TemporalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, embed_type='fixed', freq='h'):\n",
    "        super(TemporalEmbedding, self).__init__()\n",
    "\n",
    "        minute_size = 4\n",
    "        hour_size = 24\n",
    "        weekday_size = 7\n",
    "        day_size = 32\n",
    "        month_size = 13\n",
    "\n",
    "        Embed = FixedEmbedding if embed_type == 'fixed' else nn.Embedding\n",
    "        if freq == 't':\n",
    "            self.minute_embed = Embed(minute_size, d_model)\n",
    "        self.hour_embed = Embed(hour_size, d_model)\n",
    "        self.weekday_embed = Embed(weekday_size, d_model)\n",
    "        self.day_embed = Embed(day_size, d_model)\n",
    "        self.month_embed = Embed(month_size, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.long()\n",
    "        minute_x = self.minute_embed(x[:, :, 4]) if hasattr(\n",
    "            self, 'minute_embed') else 0.\n",
    "        hour_x = self.hour_embed(x[:, :, 3])\n",
    "        weekday_x = self.weekday_embed(x[:, :, 2])\n",
    "        day_x = self.day_embed(x[:, :, 1])\n",
    "        month_x = self.month_embed(x[:, :, 0])\n",
    "\n",
    "        return hour_x + weekday_x + day_x + month_x + minute_x\n",
    "\n",
    "\n",
    "class TimeFeatureEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, embed_type='timeF', freq='h'):\n",
    "        super(TimeFeatureEmbedding, self).__init__()\n",
    "\n",
    "        freq_map = {'h': 4, 't': 5, 's': 6,\n",
    "                    'm': 1, 'a': 1, 'w': 2, 'd': 3, 'b': 3}\n",
    "        d_inp = freq_map[freq]\n",
    "        self.embed = nn.Linear(d_inp, d_model, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embed(x)\n",
    "\n",
    "\n",
    "class DataEmbedding(nn.Module):\n",
    "    def __init__(self, c_in, d_model, embed_type='fixed', freq='h', dropout=0.1):\n",
    "        super(DataEmbedding, self).__init__()\n",
    "\n",
    "        self.value_embedding = TokenEmbedding(c_in=c_in, d_model=d_model)\n",
    "        self.position_embedding = PositionalEmbedding(d_model=d_model)\n",
    "        self.temporal_embedding = TemporalEmbedding(d_model=d_model, embed_type=embed_type,\n",
    "                                                    freq=freq) if embed_type != 'timeF' else TimeFeatureEmbedding(\n",
    "            d_model=d_model, embed_type=embed_type, freq=freq)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, x_mark):\n",
    "        if x_mark is None:\n",
    "            x = self.value_embedding(x) + self.position_embedding(x)\n",
    "        else:\n",
    "            x = self.value_embedding(\n",
    "                x) + self.temporal_embedding(x_mark) + self.position_embedding(x)\n",
    "        return self.dropout(x)\n",
    "    \n",
    "    \n",
    "# 编码器和解码器\n",
    "class moving_avg(nn.Module):\n",
    "    \"\"\"\n",
    "    Moving average block to highlight the trend of time series\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, kernel_size, stride):\n",
    "        super(moving_avg, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.avg = nn.AvgPool1d(kernel_size=kernel_size, stride=stride, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # padding on the both ends of time series\n",
    "        front = x[:, 0:1, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        end = x[:, -1:, :].repeat(1, (self.kernel_size - 1) // 2, 1)\n",
    "        x = torch.cat([front, x, end], dim=1)\n",
    "        x = self.avg(x.permute(0, 2, 1))\n",
    "        x = x.permute(0, 2, 1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class series_decomp(nn.Module):\n",
    "    \"\"\"\n",
    "    Series decomposition block\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, kernel_size):\n",
    "        super(series_decomp, self).__init__()\n",
    "        self.moving_avg = moving_avg(kernel_size, stride=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        moving_mean = self.moving_avg(x)\n",
    "        res = x - moving_mean\n",
    "        return res, moving_mean\n",
    "\n",
    "\n",
    "class series_decomp_multi(nn.Module):\n",
    "    \"\"\"\n",
    "    Multiple Series decomposition block from FEDformer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, kernel_size):\n",
    "        super(series_decomp_multi, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.series_decomp = [series_decomp(kernel) for kernel in kernel_size]\n",
    "\n",
    "    def forward(self, x):\n",
    "        moving_mean = []\n",
    "        res = []\n",
    "        for func in self.series_decomp:\n",
    "            sea, moving_avg = func(x)\n",
    "            moving_mean.append(moving_avg)\n",
    "            res.append(sea)\n",
    "\n",
    "        sea = sum(res) / len(res)\n",
    "        moving_mean = sum(moving_mean) / len(moving_mean)\n",
    "        return sea, moving_mean\n",
    "    \n",
    "    \n",
    "class MIC(nn.Module):\n",
    "    \"\"\"\n",
    "    MIC layer to extract local and global features\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, feature_size=512, n_heads=8, dropout=0.05, decomp_kernel=[32], conv_kernel=[24],\n",
    "                 isometric_kernel=[18, 6]):\n",
    "        super(MIC, self).__init__()\n",
    "        self.conv_kernel = conv_kernel\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # isometric convolution\n",
    "        self.isometric_conv = nn.ModuleList([nn.Conv1d(in_channels=feature_size, out_channels=feature_size,\n",
    "                                                       kernel_size=i, padding=0, stride=1)\n",
    "                                             for i in isometric_kernel])\n",
    "\n",
    "        # downsampling convolution: padding=i//2, stride=i\n",
    "        self.conv = nn.ModuleList([nn.Conv1d(in_channels=feature_size, out_channels=feature_size,\n",
    "                                             kernel_size=i, padding=i // 2, stride=i)\n",
    "                                   for i in conv_kernel])\n",
    "\n",
    "        # upsampling convolution\n",
    "        self.conv_trans = nn.ModuleList([nn.ConvTranspose1d(in_channels=feature_size, out_channels=feature_size,\n",
    "                                                            kernel_size=i, padding=0, stride=i)\n",
    "                                         for i in conv_kernel])\n",
    "\n",
    "        self.decomp = nn.ModuleList([series_decomp(k) for k in decomp_kernel])\n",
    "        self.merge = torch.nn.Conv2d(in_channels=feature_size, out_channels=feature_size,\n",
    "                                     kernel_size=(len(self.conv_kernel), 1))\n",
    "\n",
    "        # feedforward network\n",
    "        self.conv1 = nn.Conv1d(in_channels=feature_size, out_channels=feature_size * 4, kernel_size=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=feature_size * 4, out_channels=feature_size, kernel_size=1)\n",
    "        self.norm1 = nn.LayerNorm(feature_size)\n",
    "        self.norm2 = nn.LayerNorm(feature_size)\n",
    "\n",
    "        self.norm = torch.nn.LayerNorm(feature_size)\n",
    "        self.act = torch.nn.Tanh()\n",
    "        self.drop = torch.nn.Dropout(0.05)\n",
    "\n",
    "    def conv_trans_conv(self, input, conv1d, conv1d_trans, isometric):\n",
    "        batch, seq_len, channel = input.shape\n",
    "        x = input.permute(0, 2, 1)\n",
    "\n",
    "        # downsampling convolution\n",
    "        x1 = self.drop(self.act(conv1d(x)))\n",
    "        x = x1\n",
    "\n",
    "        # isometric convolution\n",
    "        zeros = torch.zeros((x.shape[0], x.shape[1], x.shape[2] - 1), device=self.device)\n",
    "        x = torch.cat((zeros, x), dim=-1)\n",
    "        x = self.drop(self.act(isometric(x)))\n",
    "        x = self.norm((x + x1).permute(0, 2, 1)).permute(0, 2, 1)\n",
    "\n",
    "        # upsampling convolution\n",
    "        x = self.drop(self.act(conv1d_trans(x)))\n",
    "        x = x[:, :, :seq_len]  # truncate\n",
    "\n",
    "        x = self.norm(x.permute(0, 2, 1) + input)\n",
    "        return x\n",
    "\n",
    "    def forward(self, src):\n",
    "        # multi-scale\n",
    "        multi = []\n",
    "        for i in range(len(self.conv_kernel)):\n",
    "            src_out, trend1 = self.decomp[i](src)\n",
    "            src_out = self.conv_trans_conv(src_out, self.conv[i], self.conv_trans[i], self.isometric_conv[i])\n",
    "            multi.append(src_out)\n",
    "\n",
    "            # merge\n",
    "        mg = torch.tensor([], device=self.device)\n",
    "        for i in range(len(self.conv_kernel)):\n",
    "            mg = torch.cat((mg, multi[i].unsqueeze(1)), dim=1)\n",
    "        mg = self.merge(mg.permute(0, 3, 1, 2)).squeeze(-2).permute(0, 2, 1)\n",
    "\n",
    "        y = self.norm1(mg)\n",
    "        y = self.conv2(self.conv1(y.transpose(-1, 1))).transpose(-1, 1)\n",
    "\n",
    "        return self.norm2(mg + y)\n",
    "\n",
    "\n",
    "class SeasonalPrediction(nn.Module):\n",
    "    def __init__(self, embedding_size=512, n_heads=8, dropout=0.05, d_layers=1, decomp_kernel=[32], c_out=1,\n",
    "                 conv_kernel=[2, 4], isometric_kernel=[18, 6]):\n",
    "        super(SeasonalPrediction, self).__init__()\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.mic = nn.ModuleList([MIC(feature_size=embedding_size, n_heads=n_heads,\n",
    "                                      decomp_kernel=decomp_kernel, conv_kernel=conv_kernel,\n",
    "                                      isometric_kernel=isometric_kernel)\n",
    "                                  for i in range(d_layers)])\n",
    "\n",
    "        self.projection = nn.Linear(embedding_size, c_out)\n",
    "\n",
    "    def forward(self, dec):\n",
    "        for mic_layer in self.mic:\n",
    "            dec = mic_layer(dec)\n",
    "        return self.projection(dec)\n",
    "\n",
    "\n",
    "# MICN模型\n",
    "class MICN(nn.Module):\n",
    "    def __init__(self, seq_len, pred_len, label_len, enc_in, d_model, dropout, n_heads, d_layers, c_out, embed, freq, \n",
    "                 conv_kernel=[12, 16]):\n",
    "        \"\"\"\n",
    "        conv_kernel: downsampling and upsampling convolution kernel_size\n",
    "        \"\"\"\n",
    "        super(MICN, self).__init__()\n",
    "\n",
    "        decomp_kernel = []  # kernel of decomposition operation\n",
    "        isometric_kernel = []  # kernel of isometric convolution\n",
    "        for ii in conv_kernel:\n",
    "            if ii % 2 == 0:  # the kernel of decomposition operation must be odd\n",
    "                decomp_kernel.append(ii + 1)\n",
    "                isometric_kernel.append((seq_len + pred_len + ii) // ii)\n",
    "            else:\n",
    "                decomp_kernel.append(ii)\n",
    "                isometric_kernel.append((seq_len + pred_len + ii - 1) // ii)\n",
    "\n",
    "        self.pred_len = pred_len\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        # Multiple Series decomposition block from FEDformer\n",
    "        self.decomp_multi = series_decomp_multi(decomp_kernel)\n",
    "\n",
    "        # embedding\n",
    "        self.dec_embedding = DataEmbedding(enc_in, d_model, embed, freq, dropout)\n",
    "\n",
    "        self.conv_trans = SeasonalPrediction(embedding_size=d_model, n_heads=n_heads,\n",
    "                                             dropout=dropout, d_layers=d_layers, decomp_kernel=decomp_kernel,\n",
    "                                             c_out=c_out, conv_kernel=conv_kernel,\n",
    "                                             isometric_kernel=isometric_kernel)\n",
    "        # refer to DLinear\n",
    "        self.regression = nn.Linear(seq_len, pred_len)\n",
    "        self.regression.weight = nn.Parameter((1 / pred_len) * torch.ones([pred_len, seq_len]), requires_grad=True)\n",
    "\n",
    "    def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec):\n",
    "        # Multi-scale Hybrid Decomposition\n",
    "        seasonal_init_enc, trend = self.decomp_multi(x_enc)\n",
    "        trend = self.regression(trend.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "\n",
    "        # embedding\n",
    "        zeros = torch.zeros([x_dec.shape[0], self.pred_len, x_dec.shape[2]], device=x_enc.device)\n",
    "        seasonal_init_dec = torch.cat([seasonal_init_enc[:, -self.seq_len:, :], zeros], dim=1)\n",
    "        dec_out = self.dec_embedding(seasonal_init_dec, x_mark_dec)\n",
    "        dec_out = self.conv_trans(dec_out)\n",
    "        dec_out = dec_out[:, -self.pred_len:, :] + trend[:, -self.pred_len:, :]\n",
    "        \n",
    "        output = dec_out[:, -self.pred_len:, :]  # [B, L, D]\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c658e3ff",
   "metadata": {},
   "source": [
    "### 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2dd094f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:27:13.997132Z",
     "start_time": "2024-04-14T13:27:13.955809Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(task_args, train_args, model_args):\n",
    "    # 参数配置\n",
    "    columns = task_args['columns'] # 模型全部特征\n",
    "    target = task_args['target'] # 模型预测特征\n",
    "    features = task_args['features'] # 模型预测模式\n",
    "    model_name = train_args['model_name'] # 模型名称\n",
    "    train_loader = train_args['train_loader'] # 训练集\n",
    "    valid_loader = train_args['valid_loader'] # 验证集\n",
    "    n_epochs = train_args['n_epochs'] # 训练次数\n",
    "    learning_rate = train_args['learning_rate'] # 学习率\n",
    "    loss = train_args['loss'] # 损失函数\n",
    "    patience = train_args['patience'] # 最大早停次数阈值，超过就会早停\n",
    "    lradj = train_args['lradj'] # 学习率函数\n",
    "    path = train_args['path'] # 模型保存路径\n",
    "    verbose = train_args['verbose'] # 打印训练过程\n",
    "    plots = train_args['plots'] # 绘制损失图\n",
    "    pred_len = model_args['pred_len'] # 预测长度\n",
    "    label_len = model_args['label_len']\n",
    "    \n",
    "    #检查是否可用GPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # 创建模型和优化器\n",
    "    model = model_name(**model_args).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_fn = loss\n",
    "    \n",
    "    # 损失函数值\n",
    "    train_losses, val_losses = [], []\n",
    "    \n",
    "    # 调整学习率\n",
    "    def adjust_learning_rate(optimizer, epoch, lradj, learning_rate, train_epochs):\n",
    "        # lr = learning_rate * (0.2 ** (epoch // 2))\n",
    "        if lradj == 'type1':\n",
    "            lr_adjust = {epoch: learning_rate * (0.5 ** ((epoch - 1) // 1))}\n",
    "        elif lradj == 'type2':\n",
    "            lr_adjust = {\n",
    "                2: 5e-5, 4: 1e-5, 6: 5e-6, 8: 1e-6,\n",
    "                10: 5e-7, 15: 1e-7, 20: 5e-8\n",
    "            }\n",
    "        elif lradj == \"cosine\":\n",
    "            lr_adjust = {epoch: learning_rate /2 * (1 + math.cos(epoch / train_epochs * math.pi))}\n",
    "        if epoch in lr_adjust.keys():\n",
    "            lr = lr_adjust[epoch]\n",
    "            # 参数组(param_groups)是用来指定不同的参数组以便对它们进行不同的优化设置，比如'lr'\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "            print('Updating learning rate to {}'.format(lr))\n",
    "    \n",
    "    # 设置早停\n",
    "    class EarlyStopping():\n",
    "        def __init__(self, patience=7, verbose=False, delta=0):\n",
    "            self.patience = patience # 连续超限次数，如果满足条件，则早停\n",
    "            self.verbose = verbose\n",
    "            self.counter = 0\n",
    "            self.best_score = None\n",
    "            self.early_stop = False\n",
    "            self.val_loss_min = np.Inf\n",
    "            self.delta = delta\n",
    "\n",
    "        def __call__(self, val_loss, model, path):\n",
    "            score = -val_loss\n",
    "            if self.best_score is None:\n",
    "                self.best_score = score\n",
    "                self.save_checkpoint(val_loss, model, path)\n",
    "            elif score < self.best_score + self.delta:\n",
    "                self.counter += 1\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "                if self.counter >= self.patience:\n",
    "                    self.early_stop = True\n",
    "            else:\n",
    "                self.best_score = score\n",
    "                self.save_checkpoint(val_loss, model, path)\n",
    "                self.counter = 0\n",
    "\n",
    "        def save_checkpoint(self, val_loss, model, path):\n",
    "            if self.verbose:\n",
    "                print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "            torch.save(model.state_dict(), path + '/' + 'checkpoint.pth')\n",
    "            self.val_loss_min = val_loss\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=verbose)\n",
    "    \n",
    "    # 训练任务\n",
    "    def forecasting_task(columns, target, features):\n",
    "        \"\"\"\n",
    "        features: [M, S, MS]; \n",
    "            M:multivariate predict multivariate, \n",
    "            S:univariate predict univariate, \n",
    "            MS:multivariate predict univariate'\n",
    "        \"\"\"\n",
    "        # 字典索引生成\n",
    "        col_dict = {}\n",
    "        for i,j in enumerate(columns):\n",
    "            col_dict[j] = i\n",
    "\n",
    "        if features == 'MS':\n",
    "            target = target[0]\n",
    "            f_dim = col_dict[target]\n",
    "        elif features == 'S':\n",
    "            f_dim = 0\n",
    "        else:\n",
    "            f_dim = 0\n",
    "        return f_dim\n",
    "    f_dim = forecasting_task(columns, target, features)\n",
    "    \n",
    "    # 设置保存模型路径\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    # 模型训练和验证\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for batch_x, batch_y, batch_x_mark, batch_y_mark in train_loader:\n",
    "            #将数据移至 GPU\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            batch_x_mark = batch_x_mark.to(device)\n",
    "            batch_y_mark = batch_y_mark.to(device)\n",
    "            # 清空梯度\n",
    "            optimizer.zero_grad()\n",
    "            # decoder输入\n",
    "            dec_inp = torch.zeros_like(batch_y[:, -pred_len:, :]).float()\n",
    "            dec_inp = torch.cat([batch_y[:, :label_len, :], dec_inp], dim=1).float().to(device)\n",
    "            outputs = model(batch_x, batch_x_mark, dec_inp, None)\n",
    "            if features == 'MS':\n",
    "                outputs = outputs[:, :, f_dim: f_dim+1] \n",
    "            else:\n",
    "                outputs = outputs[:, :, f_dim:]\n",
    "            train_loss = loss_fn(outputs, batch_y)\n",
    "            # 反向传播计算得到每个参数的梯度值\n",
    "            train_loss.backward()\n",
    "            # 通过梯度下降执行一步参数更新\n",
    "            optimizer.step()\n",
    "            #每个batch的loss和\n",
    "            total_train_loss += train_loss.item() # .item()表示只包含一个元素的tensor中提取值\n",
    "                \n",
    "        #每个epoch的损失平均\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        \n",
    "        #所有epoch的loss\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        #评估模型\n",
    "        model.eval()\n",
    "        #关闭自动求导功能，只使用训练好的模型进行预测或评估，不需要进行梯度计算和参数更新\n",
    "        with torch.no_grad():\n",
    "            total_val_loss = 0\n",
    "            for batch_x, batch_y, batch_x_mark, batch_y_mark in valid_loader:\n",
    "                #将数据移至 GPU\n",
    "                batch_x = batch_x.to(device)\n",
    "                batch_y = batch_y.to(device)\n",
    "                batch_x_mark = batch_x_mark.to(device)\n",
    "                batch_y_mark = batch_y_mark.to(device)\n",
    "                # decoder输入\n",
    "                dec_inp = torch.zeros_like(batch_y[:, -pred_len:, :]).float()\n",
    "                dec_inp = torch.cat([batch_y[:, :label_len, :], dec_inp], dim=1).float().to(device)\n",
    "                val_outputs = model(batch_x, batch_x_mark, dec_inp, None)\n",
    "                if features == 'MS':\n",
    "                    val_outputs = val_outputs[:, :, f_dim: f_dim+1] \n",
    "                else:\n",
    "                    val_outputs = val_outputs[:, :, f_dim:]\n",
    "                val_loss = loss_fn(val_outputs, batch_y)\n",
    "                #每个batch的loss和\n",
    "                total_val_loss += val_loss.item()\n",
    "                \n",
    "        #每个epoch的损失平均\n",
    "        avg_val_loss = total_val_loss / len(valid_loader)\n",
    "        \n",
    "        #所有epoch的loss\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        #打印训练过程\n",
    "        if verbose:\n",
    "            print(f'Epoch [{epoch+1}/{n_epochs}], Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}')\n",
    "            \n",
    "        # 设置早停\n",
    "        early_stopping(avg_val_loss, model, path)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping!\")\n",
    "            break\n",
    "            \n",
    "        # 调整学习率\n",
    "        adjust_learning_rate(optimizer, epoch+1, lradj, learning_rate, n_epochs)\n",
    "        \n",
    "    # 加载最佳模型\n",
    "    best_model_path = path + '/' + 'checkpoint.pth'\n",
    "    model.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "    #绘制损失函数图\n",
    "    def plot_loss(train_losses, val_losses):\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.style.use('seaborn-v0_8-paper') #绘制背景色\n",
    "        plt.grid(axis = 'y',linewidth=0.35) #绘制网格\n",
    "        plt.plot(val_losses, linestyle='-',color = '#11b3b6')\n",
    "        plt.plot(train_losses, linestyle='-',color = '#f14643')\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Training and Validation Progress\")\n",
    "        plt.legend([\"Validation\", \"Training\"])\n",
    "        plt.show()\n",
    "    if plots:\n",
    "        plot_loss(train_losses, val_losses)\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "c6a01af3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:27:38.556896Z",
     "start_time": "2024-04-14T13:27:15.636846Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|████▏                                                                              | 1/20 [00:18<05:46, 18.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Training Loss: 0.0378, Validation Loss: 0.0147\n",
      "Validation loss decreased (inf --> 0.014746).  Saving model ...\n",
      "Updating learning rate to 0.0009938441702975688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|████▏                                                                              | 1/20 [00:21<06:51, 21.68s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[84], line 35\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 构造参数字典\u001b[39;00m\n\u001b[0;32m      2\u001b[0m params3 \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtask_args\u001b[39m\u001b[38;5;124m\"\u001b[39m:{\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mload\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     33\u001b[0m     },\n\u001b[0;32m     34\u001b[0m }\n\u001b[1;32m---> 35\u001b[0m model \u001b[38;5;241m=\u001b[39m train(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams3)\n",
      "Cell \u001b[1;32mIn[83], line 125\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(task_args, train_args, model_args)\u001b[0m\n\u001b[0;32m    123\u001b[0m dec_inp \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(batch_y[:, \u001b[38;5;241m-\u001b[39mpred_len:, :])\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m    124\u001b[0m dec_inp \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([batch_y[:, :label_len, :], dec_inp], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m--> 125\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_x_mark\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdec_inp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m features \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMS\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    127\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m outputs[:, :, f_dim: f_dim\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m] \n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[80], line 328\u001b[0m, in \u001b[0;36mMICN.forward\u001b[1;34m(self, x_enc, x_mark_enc, x_dec, x_mark_dec)\u001b[0m\n\u001b[0;32m    326\u001b[0m dec_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdec_embedding(seasonal_init_dec, x_mark_dec)\n\u001b[0;32m    327\u001b[0m dec_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_trans(dec_out)\n\u001b[1;32m--> 328\u001b[0m dec_out \u001b[38;5;241m=\u001b[39m \u001b[43mdec_out\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpred_len\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrend\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpred_len\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    330\u001b[0m output \u001b[38;5;241m=\u001b[39m dec_out[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpred_len:, :]  \u001b[38;5;66;03m# [B, L, D]\u001b[39;00m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 构造参数字典\n",
    "params3 = {\n",
    "    \"task_args\":{\n",
    "        \"columns\": ['load'],\n",
    "        \"target\": ['load'],\n",
    "        \"features\": 'S'\n",
    "    },\n",
    "    \"train_args\": {\n",
    "        \"model_name\": MICN,\n",
    "        \"train_loader\": train_loader,\n",
    "        \"valid_loader\": valid_loader,\n",
    "        \"n_epochs\": 20,\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"loss\": nn.MSELoss(),\n",
    "        \"patience\": 3,\n",
    "        \"lradj\": 'cosine',\n",
    "        \"path\": \"../models/test\",\n",
    "        \"verbose\": True,\n",
    "        \"plots\": True,\n",
    "    },\n",
    "    \"model_args\": {\n",
    "        'enc_in': 2, \n",
    "        'seq_len': 6,\n",
    "        'pred_len': 3,\n",
    "        \"label_len\": 0,\n",
    "        'd_model': 128,\n",
    "        'n_heads': 8,\n",
    "        'dropout': 0.1,\n",
    "        'd_layers': 1,\n",
    "        'c_out': 1,\n",
    "        'embed': 'fixed',\n",
    "        'freq': 'h',\n",
    "    },\n",
    "}\n",
    "model = train(**params3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebaf496",
   "metadata": {},
   "source": [
    "# 基于Nonstationary_Transformer的时间序列预测"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e47cb66",
   "metadata": {},
   "source": [
    "非平稳时间序列预测"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1363432",
   "metadata": {},
   "source": [
    "## 多变量多步预测"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ad9466",
   "metadata": {},
   "source": [
    "### 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "18fea494",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:27:42.605171Z",
     "start_time": "2024-04-14T13:27:42.577451Z"
    }
   },
   "outputs": [],
   "source": [
    "# 读取数据\n",
    "def get_dataset(path, time_col=None):\n",
    "    data = pd.read_csv(path)\n",
    "    rows = data.shape[0]\n",
    "    now = datetime.now()\n",
    "    newtime = now.replace(microsecond=0)\n",
    "    if time_col == None:\n",
    "        # 如果没有时间列，生成时间戳范围\n",
    "        time_index = pd.date_range(start=datetime.now() -\n",
    "                                   timedelta(seconds=rows - 1),\n",
    "                                   end=datetime.now(),\n",
    "                                   freq='S')\n",
    "        full_data = pd.DataFrame(data=data.values,\n",
    "                                 index=pd.to_datetime(time_index, unit='s'),\n",
    "                                 columns=data.columns)\n",
    "    else:\n",
    "        columns = [i for i in data.columns if i != time_col] # 去除时间列\n",
    "        full_data = pd.DataFrame(data=data.drop([time_col], axis=1).values,\n",
    "                                 index=pd.to_datetime(data[time_col].values),\n",
    "                                 columns=columns)\n",
    "    return full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "9ec9a5ff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:27:43.559720Z",
     "start_time": "2024-04-14T13:27:43.479496Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>load</th>\n",
       "      <th>temp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-01-01 00:00:00</th>\n",
       "      <td>2698.0</td>\n",
       "      <td>32.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-01 01:00:00</th>\n",
       "      <td>2558.0</td>\n",
       "      <td>32.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-01 02:00:00</th>\n",
       "      <td>2444.0</td>\n",
       "      <td>30.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-01 03:00:00</th>\n",
       "      <td>2402.0</td>\n",
       "      <td>31.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-01 04:00:00</th>\n",
       "      <td>2403.0</td>\n",
       "      <td>32.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-12-31 19:00:00</th>\n",
       "      <td>4012.0</td>\n",
       "      <td>18.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-12-31 20:00:00</th>\n",
       "      <td>3856.0</td>\n",
       "      <td>16.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-12-31 21:00:00</th>\n",
       "      <td>3671.0</td>\n",
       "      <td>17.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-12-31 22:00:00</th>\n",
       "      <td>3499.0</td>\n",
       "      <td>15.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-12-31 23:00:00</th>\n",
       "      <td>3345.0</td>\n",
       "      <td>15.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26304 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       load   temp\n",
       "2012-01-01 00:00:00  2698.0  32.00\n",
       "2012-01-01 01:00:00  2558.0  32.67\n",
       "2012-01-01 02:00:00  2444.0  30.00\n",
       "2012-01-01 03:00:00  2402.0  31.00\n",
       "2012-01-01 04:00:00  2403.0  32.00\n",
       "...                     ...    ...\n",
       "2014-12-31 19:00:00  4012.0  18.00\n",
       "2014-12-31 20:00:00  3856.0  16.67\n",
       "2014-12-31 21:00:00  3671.0  17.00\n",
       "2014-12-31 22:00:00  3499.0  15.33\n",
       "2014-12-31 23:00:00  3345.0  15.33\n",
       "\n",
       "[26304 rows x 2 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"../data/energy.csv\"\n",
    "ts_data = get_dataset(path, time_col='time')\n",
    "ts_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "770724f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:27:44.781315Z",
     "start_time": "2024-04-14T13:27:44.742487Z"
    }
   },
   "outputs": [],
   "source": [
    "# 时间格式编码\n",
    "def time_features_from_frequency_str(freq_str: str):\n",
    "    \"\"\"\n",
    "    Returns a list of time features that will be appropriate for the given frequency string.\n",
    "    Parameters\n",
    "    ----------\n",
    "    freq_str\n",
    "        Frequency string of the form [multiple][granularity] such as \"12H\", \"5min\", \"1D\" etc.\n",
    "    \"\"\"\n",
    "\n",
    "    class TimeFeature:\n",
    "        def __init__(self):\n",
    "            pass\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            pass\n",
    "\n",
    "        def __repr__(self):\n",
    "            return self.__class__.__name__ + \"()\"\n",
    "\n",
    "\n",
    "    class SecondOfMinute(TimeFeature):\n",
    "        \"\"\"Minute of hour encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return index.second / 59.0 - 0.5\n",
    "\n",
    "\n",
    "    class MinuteOfHour(TimeFeature):\n",
    "        \"\"\"Minute of hour encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return index.minute / 59.0 - 0.5\n",
    "\n",
    "\n",
    "    class HourOfDay(TimeFeature):\n",
    "        \"\"\"Hour of day encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return index.hour / 23.0 - 0.5\n",
    "\n",
    "\n",
    "    class DayOfWeek(TimeFeature):\n",
    "        \"\"\"Hour of day encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return index.dayofweek / 6.0 - 0.5\n",
    "\n",
    "\n",
    "    class DayOfMonth(TimeFeature):\n",
    "        \"\"\"Day of month encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return (index.day - 1) / 30.0 - 0.5\n",
    "\n",
    "\n",
    "    class DayOfYear(TimeFeature):\n",
    "        \"\"\"Day of year encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return (index.dayofyear - 1) / 365.0 - 0.5\n",
    "\n",
    "\n",
    "    class MonthOfYear(TimeFeature):\n",
    "        \"\"\"Month of year encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return (index.month - 1) / 11.0 - 0.5\n",
    "\n",
    "\n",
    "    class WeekOfYear(TimeFeature):\n",
    "        \"\"\"Week of year encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return (index.isocalendar().week - 1) / 52.0 - 0.5\n",
    "\n",
    "    \n",
    "    features_by_offsets = {\n",
    "        offsets.YearEnd: [],\n",
    "        offsets.QuarterEnd: [MonthOfYear],\n",
    "        offsets.MonthEnd: [MonthOfYear],\n",
    "        offsets.Week: [DayOfMonth, WeekOfYear],\n",
    "        offsets.Day: [DayOfWeek, DayOfMonth, DayOfYear],\n",
    "        offsets.BusinessDay: [DayOfWeek, DayOfMonth, DayOfYear],\n",
    "        offsets.Hour: [HourOfDay, DayOfWeek, DayOfMonth, DayOfYear],\n",
    "        offsets.Minute: [\n",
    "            MinuteOfHour,\n",
    "            HourOfDay,\n",
    "            DayOfWeek,\n",
    "            DayOfMonth,\n",
    "            DayOfYear,\n",
    "        ],\n",
    "        offsets.Second: [\n",
    "            SecondOfMinute,\n",
    "            MinuteOfHour,\n",
    "            HourOfDay,\n",
    "            DayOfWeek,\n",
    "            DayOfMonth,\n",
    "            DayOfYear,\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    offset = to_offset(freq_str)\n",
    "\n",
    "    for offset_type, feature_classes in features_by_offsets.items():\n",
    "        if isinstance(offset, offset_type):\n",
    "            return [cls() for cls in feature_classes]\n",
    "\n",
    "    supported_freq_msg = f\"\"\"\n",
    "    Unsupported frequency {freq_str}\n",
    "    The following frequencies are supported:\n",
    "        Y   - yearly\n",
    "            alias: A\n",
    "        M   - monthly\n",
    "        W   - weekly\n",
    "        D   - daily\n",
    "        B   - business days\n",
    "        H   - hourly\n",
    "        T   - minutely\n",
    "            alias: min\n",
    "        S   - secondly\n",
    "    \"\"\"\n",
    "    raise RuntimeError(supported_freq_msg)\n",
    "    \n",
    "def time_features(dates, freq='h'):\n",
    "    return np.vstack([feat(dates) for feat in time_features_from_frequency_str(freq)])\n",
    "\n",
    "# 包含时间维度的数据集划分\n",
    "def divide_dataset(df, valid_date, test_date, x_feature_list, y_feature_list, freq):\n",
    "    #归一化\n",
    "    x_scaler = MinMaxScaler()\n",
    "    y_scaler = MinMaxScaler()\n",
    "\n",
    "    #测试集\n",
    "    train = df.copy()[df.index < valid_date][x_feature_list]\n",
    "    train_stamp = time_features(pd.to_datetime(train.index), freq=freq)\n",
    "    train_stamp = train_stamp.transpose(1, 0)\n",
    "    train[x_feature_list] = x_scaler.fit_transform(train)\n",
    "    xtr = train.values.astype('float32')\n",
    "\n",
    "    #验证集\n",
    "    valid = df.copy()[(df.index >= valid_date) & (df.index < test_date)][x_feature_list]\n",
    "    valid_stamp = time_features(pd.to_datetime(valid.index), freq=freq)\n",
    "    valid_stamp = valid_stamp.transpose(1, 0)\n",
    "    valid[x_feature_list] = x_scaler.fit_transform(valid)\n",
    "    xva = valid.values.astype('float32')\n",
    "\n",
    "    #测试集\n",
    "    test = df.copy()[test_date:][x_feature_list]\n",
    "    test_stamp = time_features(pd.to_datetime(test.index), freq=freq)\n",
    "    test_stamp = test_stamp.transpose(1, 0)\n",
    "    test[x_feature_list] = x_scaler.fit_transform(test)\n",
    "    xte = test.values.astype('float32')\n",
    "\n",
    "    #标签\n",
    "    ytr = df.copy()[df.index < valid_date][y_feature_list]\n",
    "    ytr[y_feature_list] = y_scaler.fit_transform(ytr)\n",
    "    ytr = ytr.values.astype('float32')\n",
    "\n",
    "    yva = df.copy()[(df.index >= valid_date) & (df.index < test_date)][y_feature_list]\n",
    "    yva[y_feature_list] = y_scaler.fit_transform(yva)\n",
    "    yva = yva.values.astype('float32')\n",
    "\n",
    "    yte = df.copy()[test_date:][y_feature_list]\n",
    "    yte[y_feature_list] = y_scaler.fit_transform(yte)\n",
    "    yte = yte.values.astype('float32')\n",
    "    \n",
    "    # 数据合并，[训练集，测试集，时间戳]\n",
    "    train = [xtr, ytr, train_stamp]\n",
    "    valid = [xva, yva, valid_stamp]\n",
    "    test = [xte, yte, test_stamp]\n",
    "    \n",
    "    return y_scaler, train, valid, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "88d19b82",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:27:45.869772Z",
     "start_time": "2024-04-14T13:27:45.796832Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (23376, 2) y_train shape: (23376, 2) stamp_train shape: (23376, 4)\n",
      "x_valid shape: (1464, 2) y_valid shape: (1464, 2) stamp_valid shape: (1464, 4)\n",
      "x_test shape: (1464, 2) y_test shape: (1464, 2) stamp_test shape: (1464, 4)\n"
     ]
    }
   ],
   "source": [
    "# 构造参数字典\n",
    "params1 = {\n",
    "    \"df\": ts_data,\n",
    "    \"valid_date\": \"2014-09-01 00:00:00\",\n",
    "    \"test_date\": \"2014-11-01 00:00:00\",\n",
    "    \"x_feature_list\": [\"load\", 'temp'],\n",
    "    \"y_feature_list\": [\"load\", 'temp'],\n",
    "    \"freq\": 'h'\n",
    "}\n",
    "\n",
    "#函数传参\n",
    "scaler, train, valid, test = divide_dataset(**params1)\n",
    "print(\"x_train shape: {0} y_train shape: {1} stamp_train shape: {2}\".format(train[0].shape, train[1].shape, train[2].shape))\n",
    "print(\"x_valid shape: {0} y_valid shape: {1} stamp_valid shape: {2}\".format(valid[0].shape, valid[1].shape, valid[2].shape))\n",
    "print(\"x_test shape: {0} y_test shape: {1} stamp_test shape: {2}\".format(test[0].shape, test[1].shape, test[2].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4f77dbfa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:27:47.351781Z",
     "start_time": "2024-04-14T13:27:47.329621Z"
    }
   },
   "outputs": [],
   "source": [
    "#利用前seq_len个数据，预测下pred_len个数据\n",
    "def create_dataset(data_list, seq_len, pred_len, label_len, batch_size):\n",
    "    \"\"\"\n",
    "    @参数设置：\n",
    "    data_list：特征，目标，时间戳\n",
    "    seq_len：输入数据包含过去多少个时间步\n",
    "    pred_len：目标应该在未来多少个时间步之后\n",
    "    label_len：先验时间步\n",
    "    \"\"\"\n",
    "    # 获取数据\n",
    "    feature = data_list[0] # 特征\n",
    "    target = data_list[1] # 目标\n",
    "    stamp = data_list[2] # 时间戳\n",
    "    \n",
    "    # 循环生成数据\n",
    "    X, y = [], []\n",
    "    X_stamp, y_stamp = [], []\n",
    "    seq_len = seq_len - 1 # 包含当前时间点\n",
    "    for i in range(seq_len, len(feature) - pred_len):\n",
    "        # 数据维度\n",
    "        feat = feature[i - seq_len:i + 1]\n",
    "        tar = target[i + 1:i + 1 + pred_len]\n",
    "        X.append(feat)\n",
    "        y.append(tar)\n",
    "        \n",
    "        # 时间维度\n",
    "        xs = stamp[i - seq_len:i + 1]\n",
    "        ys = stamp[i + 1 - label_len:i + 1 + pred_len]\n",
    "        X_stamp.append(xs)\n",
    "        y_stamp.append(ys)\n",
    "        \n",
    "    # 转为张量，数据维度\n",
    "    X = torch.as_tensor(X).float()\n",
    "    y = torch.as_tensor(y).float()\n",
    "    \n",
    "    # 转为张量，时间维度\n",
    "    X_stamp = torch.as_tensor(X_stamp).float()\n",
    "    y_stamp = torch.as_tensor(y_stamp).float()\n",
    "    \n",
    "    # 创建dataloader，[特征，目标，特征时间编码，目标时间编码]\n",
    "    data_loader = DataLoader(TensorDataset(X, y, X_stamp, y_stamp), shuffle=True, batch_size=batch_size)\n",
    "    \n",
    "    return X, y, data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "22a9a9d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:27:49.741551Z",
     "start_time": "2024-04-14T13:27:48.775651Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_size: torch.Size([23368, 6, 2]),y_size: torch.Size([23368, 3, 2]),loader_len: 731\n",
      "X_size: torch.Size([1456, 6, 2]),y_size: torch.Size([1456, 3, 2]),loader_len: 46\n",
      "X_size: torch.Size([1456, 6, 2]),y_size: torch.Size([1456, 3, 2]),loader_len: 46\n"
     ]
    }
   ],
   "source": [
    "# 构造参数字典\n",
    "params2 = {\n",
    "    \"seq_len\": 6,\n",
    "    \"pred_len\": 3,\n",
    "    \"label_len\": 3,\n",
    "    \"batch_size\": 32,\n",
    "}\n",
    "X_train, y_train, train_loader = create_dataset(train, **params2)\n",
    "X_valid, y_valid, valid_loader = create_dataset(valid, **params2)\n",
    "X_test, y_test, test_loader = create_dataset(test, **params2)\n",
    "print(\"X_size: {0},y_size: {1},loader_len: {2}\".format(X_train.shape, y_train.shape, len(train_loader)))\n",
    "print(\"X_size: {0},y_size: {1},loader_len: {2}\".format(X_valid.shape, y_valid.shape, len(valid_loader)))\n",
    "print(\"X_size: {0},y_size: {1},loader_len: {2}\".format(X_test.shape, y_test.shape, len(test_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20496d35",
   "metadata": {},
   "source": [
    "### 模型定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c17c68c0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:27:52.320725Z",
     "start_time": "2024-04-14T13:27:52.245312Z"
    }
   },
   "outputs": [],
   "source": [
    "# DataEmbedding编码类\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model).float()\n",
    "        pe.require_grad = False\n",
    "\n",
    "        position = torch.arange(0, max_len).float().unsqueeze(1)\n",
    "        div_term = (torch.arange(0, d_model, 2).float()\n",
    "                    * -(math.log(10000.0) / d_model)).exp()\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pe[:, :x.size(1)]\n",
    "\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, c_in, d_model):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        padding = 1 if torch.__version__ >= '1.5.0' else 2\n",
    "        self.tokenConv = nn.Conv1d(in_channels=c_in, out_channels=d_model,\n",
    "                                   kernel_size=3, padding=padding, padding_mode='circular', bias=False)\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                nn.init.kaiming_normal_(\n",
    "                    m.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.tokenConv(x.permute(0, 2, 1)).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class FixedEmbedding(nn.Module):\n",
    "    def __init__(self, c_in, d_model):\n",
    "        super(FixedEmbedding, self).__init__()\n",
    "\n",
    "        w = torch.zeros(c_in, d_model).float()\n",
    "        w.require_grad = False\n",
    "\n",
    "        position = torch.arange(0, c_in).float().unsqueeze(1)\n",
    "        div_term = (torch.arange(0, d_model, 2).float()\n",
    "                    * -(math.log(10000.0) / d_model)).exp()\n",
    "\n",
    "        w[:, 0::2] = torch.sin(position * div_term)\n",
    "        w[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        self.emb = nn.Embedding(c_in, d_model)\n",
    "        self.emb.weight = nn.Parameter(w, requires_grad=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.emb(x).detach()\n",
    "\n",
    "\n",
    "class TemporalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, embed_type='fixed', freq='h'):\n",
    "        super(TemporalEmbedding, self).__init__()\n",
    "\n",
    "        minute_size = 4\n",
    "        hour_size = 24\n",
    "        weekday_size = 7\n",
    "        day_size = 32\n",
    "        month_size = 13\n",
    "\n",
    "        Embed = FixedEmbedding if embed_type == 'fixed' else nn.Embedding\n",
    "        if freq == 't':\n",
    "            self.minute_embed = Embed(minute_size, d_model)\n",
    "        self.hour_embed = Embed(hour_size, d_model)\n",
    "        self.weekday_embed = Embed(weekday_size, d_model)\n",
    "        self.day_embed = Embed(day_size, d_model)\n",
    "        self.month_embed = Embed(month_size, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.long()\n",
    "        minute_x = self.minute_embed(x[:, :, 4]) if hasattr(\n",
    "            self, 'minute_embed') else 0.\n",
    "        hour_x = self.hour_embed(x[:, :, 3])\n",
    "        weekday_x = self.weekday_embed(x[:, :, 2])\n",
    "        day_x = self.day_embed(x[:, :, 1])\n",
    "        month_x = self.month_embed(x[:, :, 0])\n",
    "\n",
    "        return hour_x + weekday_x + day_x + month_x + minute_x\n",
    "\n",
    "\n",
    "class TimeFeatureEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, embed_type='timeF', freq='h'):\n",
    "        super(TimeFeatureEmbedding, self).__init__()\n",
    "\n",
    "        freq_map = {'h': 4, 't': 5, 's': 6,\n",
    "                    'm': 1, 'a': 1, 'w': 2, 'd': 3, 'b': 3}\n",
    "        d_inp = freq_map[freq]\n",
    "        self.embed = nn.Linear(d_inp, d_model, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embed(x)\n",
    "\n",
    "\n",
    "class DataEmbedding(nn.Module):\n",
    "    def __init__(self, c_in, d_model, embed_type='fixed', freq='h', dropout=0.1):\n",
    "        super(DataEmbedding, self).__init__()\n",
    "\n",
    "        self.value_embedding = TokenEmbedding(c_in=c_in, d_model=d_model)\n",
    "        self.position_embedding = PositionalEmbedding(d_model=d_model)\n",
    "        self.temporal_embedding = TemporalEmbedding(d_model=d_model, embed_type=embed_type,\n",
    "                                                    freq=freq) if embed_type != 'timeF' else TimeFeatureEmbedding(\n",
    "            d_model=d_model, embed_type=embed_type, freq=freq)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, x_mark):\n",
    "        if x_mark is None:\n",
    "            x = self.value_embedding(x) + self.position_embedding(x)\n",
    "        else:\n",
    "            x = self.value_embedding(x) + self.temporal_embedding(x_mark) + self.position_embedding(x)\n",
    "        return self.dropout(x)\n",
    "    \n",
    "# Transformer_EncDec类\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, attention, d_model, d_ff=None, dropout=0.1, activation=\"relu\"):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        d_ff = d_ff or 4 * d_model\n",
    "        self.attention = attention\n",
    "        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = F.relu if activation == \"relu\" else F.gelu\n",
    "\n",
    "    def forward(self, x, attn_mask=None, tau=None, delta=None):\n",
    "        new_x, attn = self.attention(\n",
    "            x, x, x,\n",
    "            attn_mask=attn_mask,\n",
    "            tau=tau, delta=delta\n",
    "        )\n",
    "        x = x + self.dropout(new_x)\n",
    "\n",
    "        y = x = self.norm1(x)\n",
    "        y = self.dropout(self.activation(self.conv1(y.transpose(-1, 1))))\n",
    "        y = self.dropout(self.conv2(y).transpose(-1, 1))\n",
    "\n",
    "        return self.norm2(x + y), attn\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, attn_layers, conv_layers=None, norm_layer=None):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.attn_layers = nn.ModuleList(attn_layers)\n",
    "        self.conv_layers = nn.ModuleList(conv_layers) if conv_layers is not None else None\n",
    "        self.norm = norm_layer\n",
    "\n",
    "    def forward(self, x, attn_mask=None, tau=None, delta=None):\n",
    "        # x [B, L, D]\n",
    "        attns = []\n",
    "        if self.conv_layers is not None:\n",
    "            for i, (attn_layer, conv_layer) in enumerate(zip(self.attn_layers, self.conv_layers)):\n",
    "                delta = delta if i == 0 else None\n",
    "                x, attn = attn_layer(x, attn_mask=attn_mask, tau=tau, delta=delta)\n",
    "                x = conv_layer(x)\n",
    "                attns.append(attn)\n",
    "            x, attn = self.attn_layers[-1](x, tau=tau, delta=None)\n",
    "            attns.append(attn)\n",
    "        else:\n",
    "            for attn_layer in self.attn_layers:\n",
    "                x, attn = attn_layer(x, attn_mask=attn_mask, tau=tau, delta=delta)\n",
    "                attns.append(attn)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "\n",
    "        return x, attns\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, self_attention, cross_attention, d_model, d_ff=None,\n",
    "                 dropout=0.1, activation=\"relu\"):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        d_ff = d_ff or 4 * d_model\n",
    "        self.self_attention = self_attention\n",
    "        self.cross_attention = cross_attention\n",
    "        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = F.relu if activation == \"relu\" else F.gelu\n",
    "\n",
    "    def forward(self, x, cross, x_mask=None, cross_mask=None, tau=None, delta=None):\n",
    "        x = x + self.dropout(self.self_attention(\n",
    "            x, x, x,\n",
    "            attn_mask=x_mask,\n",
    "            tau=tau, delta=None\n",
    "        )[0])\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        x = x + self.dropout(self.cross_attention(\n",
    "            x, cross, cross,\n",
    "            attn_mask=cross_mask,\n",
    "            tau=tau, delta=delta\n",
    "        )[0])\n",
    "\n",
    "        y = x = self.norm2(x)\n",
    "        y = self.dropout(self.activation(self.conv1(y.transpose(-1, 1))))\n",
    "        y = self.dropout(self.conv2(y).transpose(-1, 1))\n",
    "\n",
    "        return self.norm3(x + y)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, layers, norm_layer=None, projection=None):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.norm = norm_layer\n",
    "        self.projection = projection\n",
    "\n",
    "    def forward(self, x, cross, x_mask=None, cross_mask=None, tau=None, delta=None):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, cross, x_mask=x_mask, cross_mask=cross_mask, tau=tau, delta=delta)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "\n",
    "        if self.projection is not None:\n",
    "            x = self.projection(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "# 掩码层\n",
    "class TriangularCausalMask():\n",
    "    def __init__(self, B, L, device=\"cpu\"):\n",
    "        mask_shape = [B, 1, L, L]\n",
    "        with torch.no_grad():\n",
    "            self._mask = torch.triu(torch.ones(mask_shape, dtype=torch.bool), diagonal=1).to(device)\n",
    "\n",
    "    @property\n",
    "    def mask(self):\n",
    "        return self._mask    \n",
    "    \n",
    "    \n",
    "# 自注意力层\n",
    "class DSAttention(nn.Module):\n",
    "    '''De-stationary Attention'''\n",
    "\n",
    "    def __init__(self, mask_flag=True, factor=5, scale=None, attention_dropout=0.1, output_attention=False):\n",
    "        super(DSAttention, self).__init__()\n",
    "        self.scale = scale\n",
    "        self.mask_flag = mask_flag\n",
    "        self.output_attention = output_attention\n",
    "        self.dropout = nn.Dropout(attention_dropout)\n",
    "\n",
    "    def forward(self, queries, keys, values, attn_mask, tau=None, delta=None):\n",
    "        B, L, H, E = queries.shape\n",
    "        _, S, _, D = values.shape\n",
    "        scale = self.scale or 1. / math.sqrt(E)\n",
    "\n",
    "        tau = 1.0 if tau is None else tau.unsqueeze(\n",
    "            1).unsqueeze(1)  # B x 1 x 1 x 1\n",
    "        delta = 0.0 if delta is None else delta.unsqueeze(\n",
    "            1).unsqueeze(1)  # B x 1 x 1 x S\n",
    "\n",
    "        # De-stationary Attention, rescaling pre-softmax score with learned de-stationary factors\n",
    "        scores = torch.einsum(\"blhe,bshe->bhls\", queries, keys) * tau + delta\n",
    "\n",
    "        if self.mask_flag:\n",
    "            if attn_mask is None:\n",
    "                attn_mask = TriangularCausalMask(B, L, device=queries.device)\n",
    "\n",
    "            scores.masked_fill_(attn_mask.mask, -np.inf)\n",
    "\n",
    "        A = self.dropout(torch.softmax(scale * scores, dim=-1))\n",
    "        V = torch.einsum(\"bhls,bshd->blhd\", A, values)\n",
    "\n",
    "        if self.output_attention:\n",
    "            return V.contiguous(), A\n",
    "        else:\n",
    "            return V.contiguous(), None\n",
    "    \n",
    "    \n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, attention, d_model, n_heads, d_keys=None,\n",
    "                 d_values=None):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "\n",
    "        d_keys = d_keys or (d_model // n_heads)\n",
    "        d_values = d_values or (d_model // n_heads)\n",
    "\n",
    "        self.inner_attention = attention\n",
    "        self.query_projection = nn.Linear(d_model, d_keys * n_heads)\n",
    "        self.key_projection = nn.Linear(d_model, d_keys * n_heads)\n",
    "        self.value_projection = nn.Linear(d_model, d_values * n_heads)\n",
    "        self.out_projection = nn.Linear(d_values * n_heads, d_model)\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "    def forward(self, queries, keys, values, attn_mask, tau=None, delta=None):\n",
    "        B, L, _ = queries.shape\n",
    "        _, S, _ = keys.shape\n",
    "        H = self.n_heads\n",
    "\n",
    "        queries = self.query_projection(queries).view(B, L, H, -1)\n",
    "        keys = self.key_projection(keys).view(B, S, H, -1)\n",
    "        values = self.value_projection(values).view(B, S, H, -1)\n",
    "\n",
    "        out, attn = self.inner_attention(\n",
    "            queries,\n",
    "            keys,\n",
    "            values,\n",
    "            attn_mask,\n",
    "            tau=tau,\n",
    "            delta=delta\n",
    "        )\n",
    "        out = out.view(B, L, -1)\n",
    "\n",
    "        return self.out_projection(out), attn\n",
    "    \n",
    "    \n",
    "class Projector(nn.Module):\n",
    "    '''\n",
    "    MLP to learn the De-stationary factors\n",
    "    '''\n",
    "\n",
    "    def __init__(self, enc_in, seq_len, hidden_dims, hidden_layers, output_dim, kernel_size=3):\n",
    "        super(Projector, self).__init__()\n",
    "\n",
    "        padding = 1 if torch.__version__ >= '1.5.0' else 2\n",
    "        self.series_conv = nn.Conv1d(in_channels=seq_len, out_channels=1, kernel_size=kernel_size, padding=padding,\n",
    "                                     padding_mode='circular', bias=False)\n",
    "\n",
    "        layers = [nn.Linear(2 * enc_in, hidden_dims[0]), nn.ReLU()]\n",
    "        for i in range(hidden_layers - 1):\n",
    "            layers += [nn.Linear(hidden_dims[i], hidden_dims[i + 1]), nn.ReLU()]\n",
    "\n",
    "        layers += [nn.Linear(hidden_dims[-1], output_dim, bias=False)]\n",
    "        self.backbone = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x, stats):\n",
    "        # x:     B x S x E\n",
    "        # stats: B x 1 x E\n",
    "        # y:     B x O\n",
    "        batch_size = x.shape[0]\n",
    "        x = self.series_conv(x)  # B x 1 x E\n",
    "        x = torch.cat([x, stats], dim=1)  # B x 2 x E\n",
    "        x = x.view(batch_size, -1)  # B x 2E\n",
    "        y = self.backbone(x)  # B x O\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "# Nonstationary_Transformer模型\n",
    "class Nonstationary_Transformer(nn.Module):\n",
    "    def __init__(self, pred_len, seq_len, label_len, output_attention, enc_in, d_model, \n",
    "                 dropout, factor, n_heads, d_ff, e_layers, d_layers, dec_in, c_out, p_hidden_dims, \n",
    "                p_hidden_layers, embed, freq):\n",
    "        super(Nonstationary_Transformer, self).__init__()\n",
    "        self.pred_len = pred_len\n",
    "        self.seq_len = seq_len\n",
    "        self.label_len = label_len\n",
    "        self.output_attention = output_attention\n",
    "\n",
    "        # Embedding\n",
    "        self.enc_embedding = DataEmbedding(enc_in, d_model, embed, freq, dropout)\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = Encoder(\n",
    "            [\n",
    "                EncoderLayer(\n",
    "                    AttentionLayer(\n",
    "                        DSAttention(False, factor, attention_dropout=dropout,\n",
    "                                    output_attention=output_attention), d_model, n_heads),\n",
    "                    d_model,\n",
    "                    d_ff,\n",
    "                    dropout=dropout,\n",
    "                    activation='relu'\n",
    "                ) for l in range(e_layers)\n",
    "            ],\n",
    "            norm_layer=torch.nn.LayerNorm(d_model)\n",
    "        )\n",
    "        # Decoder\n",
    "        self.dec_embedding = DataEmbedding(dec_in, d_model, dropout)\n",
    "        self.decoder = Decoder(\n",
    "            [\n",
    "                DecoderLayer(\n",
    "                    AttentionLayer(\n",
    "                        DSAttention(True, factor, attention_dropout=dropout,\n",
    "                                    output_attention=False),\n",
    "                        d_model, n_heads),\n",
    "                    AttentionLayer(\n",
    "                        DSAttention(False, factor, attention_dropout=dropout,\n",
    "                                    output_attention=False),\n",
    "                        d_model, n_heads),\n",
    "                    d_model,\n",
    "                    d_ff,\n",
    "                    dropout=dropout,\n",
    "                    activation='relu',\n",
    "                )\n",
    "                for l in range(d_layers)\n",
    "            ],\n",
    "            norm_layer=torch.nn.LayerNorm(d_model),\n",
    "            projection=nn.Linear(d_model, c_out, bias=True)\n",
    "        )\n",
    "\n",
    "        self.tau_learner = Projector(enc_in=enc_in, seq_len=seq_len, hidden_dims=p_hidden_dims,\n",
    "                                     hidden_layers=p_hidden_layers, output_dim=1)\n",
    "        self.delta_learner = Projector(enc_in=enc_in, seq_len=seq_len,\n",
    "                                       hidden_dims=p_hidden_dims, hidden_layers=p_hidden_layers,\n",
    "                                       output_dim=seq_len)\n",
    "\n",
    "    def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec):\n",
    "        x_raw = x_enc.clone().detach()\n",
    "\n",
    "        # Normalization\n",
    "        mean_enc = x_enc.mean(1, keepdim=True).detach()  # B x 1 x E\n",
    "        x_enc = x_enc - mean_enc\n",
    "        std_enc = torch.sqrt(torch.var(x_enc, dim=1, keepdim=True, unbiased=False) + 1e-5).detach()  # B x 1 x E\n",
    "        x_enc = x_enc / std_enc\n",
    "        # B x S x E, B x 1 x E -> B x 1, positive scalar\n",
    "        tau = self.tau_learner(x_raw, std_enc).exp()\n",
    "        # B x S x E, B x 1 x E -> B x S\n",
    "        delta = self.delta_learner(x_raw, mean_enc)\n",
    "\n",
    "        x_dec_new = torch.cat([x_enc[:, -self.label_len:, :], torch.zeros_like(x_dec[:, -self.pred_len:, :])],\n",
    "                              dim=1).to(x_enc.device).clone()\n",
    "\n",
    "        enc_out = self.enc_embedding(x_enc, x_mark_enc)\n",
    "        enc_out, attns = self.encoder(enc_out, attn_mask=None, tau=tau, delta=delta)\n",
    "\n",
    "        dec_out = self.dec_embedding(x_dec_new, x_mark_dec)\n",
    "        dec_out = self.decoder(dec_out, enc_out, x_mask=None, cross_mask=None, tau=tau, delta=delta)\n",
    "        dec_out = dec_out * std_enc + mean_enc\n",
    "        \n",
    "        output = dec_out[:, -self.pred_len:, :]  # [B, L, D]\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f79d78",
   "metadata": {},
   "source": [
    "### 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7aa9f8b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:27:55.022609Z",
     "start_time": "2024-04-14T13:27:54.980567Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(task_args, train_args, model_args):\n",
    "    # 参数配置\n",
    "    columns = task_args['columns'] # 模型全部特征\n",
    "    target = task_args['target'] # 模型预测特征\n",
    "    features = task_args['features'] # 模型预测模式\n",
    "    model_name = train_args['model_name'] # 模型名称\n",
    "    train_loader = train_args['train_loader'] # 训练集\n",
    "    valid_loader = train_args['valid_loader'] # 验证集\n",
    "    n_epochs = train_args['n_epochs'] # 训练次数\n",
    "    learning_rate = train_args['learning_rate'] # 学习率\n",
    "    loss = train_args['loss'] # 损失函数\n",
    "    patience = train_args['patience'] # 最大早停次数阈值，超过就会早停\n",
    "    lradj = train_args['lradj'] # 学习率函数\n",
    "    path = train_args['path'] # 模型保存路径\n",
    "    verbose = train_args['verbose'] # 打印训练过程\n",
    "    plots = train_args['plots'] # 绘制损失图\n",
    "    \n",
    "    #检查是否可用GPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # 创建模型和优化器\n",
    "    model = model_name(**model_args).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_fn = loss\n",
    "    \n",
    "    # 损失函数值\n",
    "    train_losses, val_losses = [], []\n",
    "    \n",
    "    # 调整学习率\n",
    "    def adjust_learning_rate(optimizer, epoch, lradj, learning_rate, train_epochs):\n",
    "        # lr = learning_rate * (0.2 ** (epoch // 2))\n",
    "        if lradj == 'type1':\n",
    "            lr_adjust = {epoch: learning_rate * (0.5 ** ((epoch - 1) // 1))}\n",
    "        elif lradj == 'type2':\n",
    "            lr_adjust = {\n",
    "                2: 5e-5, 4: 1e-5, 6: 5e-6, 8: 1e-6,\n",
    "                10: 5e-7, 15: 1e-7, 20: 5e-8\n",
    "            }\n",
    "        elif lradj == \"cosine\":\n",
    "            lr_adjust = {epoch: learning_rate /2 * (1 + math.cos(epoch / train_epochs * math.pi))}\n",
    "        if epoch in lr_adjust.keys():\n",
    "            lr = lr_adjust[epoch]\n",
    "            # 参数组(param_groups)是用来指定不同的参数组以便对它们进行不同的优化设置，比如'lr'\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "            print('Updating learning rate to {}'.format(lr))\n",
    "    \n",
    "    # 设置早停\n",
    "    class EarlyStopping():\n",
    "        def __init__(self, patience=7, verbose=False, delta=0):\n",
    "            self.patience = patience # 连续超限次数，如果满足条件，则早停\n",
    "            self.verbose = verbose\n",
    "            self.counter = 0\n",
    "            self.best_score = None\n",
    "            self.early_stop = False\n",
    "            self.val_loss_min = np.Inf\n",
    "            self.delta = delta\n",
    "\n",
    "        def __call__(self, val_loss, model, path):\n",
    "            score = -val_loss\n",
    "            if self.best_score is None:\n",
    "                self.best_score = score\n",
    "                self.save_checkpoint(val_loss, model, path)\n",
    "            elif score < self.best_score + self.delta:\n",
    "                self.counter += 1\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "                if self.counter >= self.patience:\n",
    "                    self.early_stop = True\n",
    "            else:\n",
    "                self.best_score = score\n",
    "                self.save_checkpoint(val_loss, model, path)\n",
    "                self.counter = 0\n",
    "\n",
    "        def save_checkpoint(self, val_loss, model, path):\n",
    "            if self.verbose:\n",
    "                print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "            torch.save(model.state_dict(), path + '/' + 'checkpoint.pth')\n",
    "            self.val_loss_min = val_loss\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=verbose)\n",
    "    \n",
    "    # 训练任务\n",
    "    def forecasting_task(columns, target, features):\n",
    "        \"\"\"\n",
    "        features: [M, S, MS]; \n",
    "            M:multivariate predict multivariate, \n",
    "            S:univariate predict univariate, \n",
    "            MS:multivariate predict univariate'\n",
    "        \"\"\"\n",
    "        # 字典索引生成\n",
    "        col_dict = {}\n",
    "        for i,j in enumerate(columns):\n",
    "            col_dict[j] = i\n",
    "\n",
    "        if features == 'MS':\n",
    "            target = target[0]\n",
    "            f_dim = col_dict[target]\n",
    "        elif features == 'S':\n",
    "            f_dim = 0\n",
    "        else:\n",
    "            f_dim = 0\n",
    "        return f_dim\n",
    "    f_dim = forecasting_task(columns, target, features)\n",
    "    \n",
    "    # 设置保存模型路径\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    # 模型训练和验证\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for batch_x, batch_y, batch_x_mark, batch_y_mark in train_loader:\n",
    "            #将数据移至 GPU\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            batch_x_mark = batch_x_mark.to(device)\n",
    "            batch_y_mark = batch_y_mark.to(device)\n",
    "            # 清空梯度\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x, batch_x_mark, batch_y, batch_y_mark)\n",
    "            if features == 'MS':\n",
    "                outputs = outputs[:, :, f_dim: f_dim+1] \n",
    "            else:\n",
    "                outputs = outputs[:, :, f_dim:]\n",
    "            train_loss = loss_fn(outputs, batch_y)\n",
    "            # 反向传播计算得到每个参数的梯度值\n",
    "            train_loss.backward()\n",
    "            # 通过梯度下降执行一步参数更新\n",
    "            optimizer.step()\n",
    "            #每个batch的loss和\n",
    "            total_train_loss += train_loss.item() # .item()表示只包含一个元素的tensor中提取值\n",
    "                \n",
    "        #每个epoch的损失平均\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        \n",
    "        #所有epoch的loss\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        #评估模型\n",
    "        model.eval()\n",
    "        #关闭自动求导功能，只使用训练好的模型进行预测或评估，不需要进行梯度计算和参数更新\n",
    "        with torch.no_grad():\n",
    "            total_val_loss = 0\n",
    "            for batch_x, batch_y, batch_x_mark, batch_y_mark in valid_loader:\n",
    "                #将数据移至 GPU\n",
    "                batch_x = batch_x.to(device)\n",
    "                batch_y = batch_y.to(device)\n",
    "                batch_x_mark = batch_x_mark.to(device)\n",
    "                batch_y_mark = batch_y_mark.to(device)\n",
    "                val_outputs = model(batch_x, batch_x_mark, batch_y, batch_y_mark)\n",
    "                if features == 'MS':\n",
    "                    val_outputs = val_outputs[:, :, f_dim: f_dim+1] \n",
    "                else:\n",
    "                    val_outputs = val_outputs[:, :, f_dim:]\n",
    "                val_loss = loss_fn(val_outputs, batch_y)\n",
    "                #每个batch的loss和\n",
    "                total_val_loss += val_loss.item()\n",
    "                \n",
    "        #每个epoch的损失平均\n",
    "        avg_val_loss = total_val_loss / len(valid_loader)\n",
    "        \n",
    "        #所有epoch的loss\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        #打印训练过程\n",
    "        if verbose:\n",
    "            print(f'Epoch [{epoch+1}/{n_epochs}], Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}')\n",
    "            \n",
    "        # 设置早停\n",
    "        early_stopping(avg_val_loss, model, path)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping!\")\n",
    "            break\n",
    "            \n",
    "        # 调整学习率\n",
    "        adjust_learning_rate(optimizer, epoch+1, lradj, learning_rate, n_epochs)\n",
    "        \n",
    "    # 加载最佳模型\n",
    "    best_model_path = path + '/' + 'checkpoint.pth'\n",
    "    model.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "    #绘制损失函数图\n",
    "    def plot_loss(train_losses, val_losses):\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.style.use('seaborn-v0_8-paper') #绘制背景色\n",
    "        plt.grid(axis = 'y',linewidth=0.35) #绘制网格\n",
    "        plt.plot(val_losses, linestyle='-',color = '#11b3b6')\n",
    "        plt.plot(train_losses, linestyle='-',color = '#f14643')\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Training and Validation Progress\")\n",
    "        plt.legend([\"Validation\", \"Training\"])\n",
    "        plt.show()\n",
    "    if plots:\n",
    "        plot_loss(train_losses, val_losses)\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2878c71f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:28:16.326591Z",
     "start_time": "2024-04-14T13:27:58.155883Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|████▏                                                                              | 1/20 [00:12<03:55, 12.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Training Loss: 0.0030, Validation Loss: 0.0036\n",
      "Validation loss decreased (inf --> 0.003550).  Saving model ...\n",
      "Updating learning rate to 0.0009938441702975688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|████▏                                                                              | 1/20 [00:16<05:15, 16.60s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[93], line 42\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 构造参数字典\u001b[39;00m\n\u001b[0;32m      2\u001b[0m params3 \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtask_args\u001b[39m\u001b[38;5;124m\"\u001b[39m:{\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mload\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemp\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     40\u001b[0m     },\n\u001b[0;32m     41\u001b[0m }\n\u001b[1;32m---> 42\u001b[0m model \u001b[38;5;241m=\u001b[39m train(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams3)\n",
      "Cell \u001b[1;32mIn[92], line 112\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(task_args, train_args, model_args)\u001b[0m\n\u001b[0;32m    110\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m    111\u001b[0m total_train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 112\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_x, batch_y, batch_x_mark, batch_y_mark \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;66;03m#将数据移至 GPU\u001b[39;00m\n\u001b[0;32m    114\u001b[0m     batch_x \u001b[38;5;241m=\u001b[39m batch_x\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    115\u001b[0m     batch_y \u001b[38;5;241m=\u001b[39m batch_y\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    677\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 678\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    679\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    680\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:264\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    204\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;124;03m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;124;03m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;124;03m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 264\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:142\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    139\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:142\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    139\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:119\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 119\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    122\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:162\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    160\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    161\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m--> 162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 构造参数字典\n",
    "params3 = {\n",
    "    \"task_args\":{\n",
    "        \"columns\": ['load', 'temp'],\n",
    "        \"target\": ['load', 'temp'],\n",
    "        \"features\": 'M'\n",
    "    },\n",
    "    \"train_args\": {\n",
    "        \"model_name\": Nonstationary_Transformer,\n",
    "        \"train_loader\": train_loader,\n",
    "        \"valid_loader\": valid_loader,\n",
    "        \"n_epochs\": 20,\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"loss\": nn.MSELoss(),\n",
    "        \"patience\": 3,\n",
    "        \"lradj\": 'cosine',\n",
    "        \"path\": \"../models/test\",\n",
    "        \"verbose\": True,\n",
    "        \"plots\": True,\n",
    "    },\n",
    "    \"model_args\": {\n",
    "        'pred_len': 3, \n",
    "        'seq_len': 6,\n",
    "        'label_len': 3,\n",
    "        'd_model': 128,\n",
    "        'output_attention': False,\n",
    "        'enc_in': 2,\n",
    "        'dec_in': 2,\n",
    "        'dropout': 0.1,\n",
    "        'factor': 3, \n",
    "        'n_heads': 8,\n",
    "        'd_ff': 128,\n",
    "        'e_layers': 1,\n",
    "        'd_layers': 1,\n",
    "        'p_hidden_dims': [256, 256],\n",
    "        'p_hidden_layers': 1,\n",
    "        'embed': 'fixed',\n",
    "        'freq': 'h',\n",
    "        'c_out': 2,\n",
    "    },\n",
    "}\n",
    "model = train(**params3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b10d82f",
   "metadata": {},
   "source": [
    "# 基于Pyraformer的时间序列预测"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d27401",
   "metadata": {},
   "source": [
    "## 多变量多步预测"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84b5940",
   "metadata": {},
   "source": [
    "### 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "accdf4bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:28:19.409670Z",
     "start_time": "2024-04-14T13:28:19.398538Z"
    }
   },
   "outputs": [],
   "source": [
    "# 读取数据\n",
    "def get_dataset(path, time_col=None):\n",
    "    data = pd.read_csv(path)\n",
    "    rows = data.shape[0]\n",
    "    now = datetime.now()\n",
    "    newtime = now.replace(microsecond=0)\n",
    "    if time_col == None:\n",
    "        # 如果没有时间列，生成时间戳范围\n",
    "        time_index = pd.date_range(start=datetime.now() -\n",
    "                                   timedelta(seconds=rows - 1),\n",
    "                                   end=datetime.now(),\n",
    "                                   freq='S')\n",
    "        full_data = pd.DataFrame(data=data.values,\n",
    "                                 index=pd.to_datetime(time_index, unit='s'),\n",
    "                                 columns=data.columns)\n",
    "    else:\n",
    "        columns = [i for i in data.columns if i != time_col] # 去除时间列\n",
    "        full_data = pd.DataFrame(data=data.drop([time_col], axis=1).values,\n",
    "                                 index=pd.to_datetime(data[time_col].values),\n",
    "                                 columns=columns)\n",
    "    return full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ad782b3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:28:20.546377Z",
     "start_time": "2024-04-14T13:28:20.461764Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>load</th>\n",
       "      <th>temp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-01-01 00:00:00</th>\n",
       "      <td>2698.0</td>\n",
       "      <td>32.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-01 01:00:00</th>\n",
       "      <td>2558.0</td>\n",
       "      <td>32.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-01 02:00:00</th>\n",
       "      <td>2444.0</td>\n",
       "      <td>30.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-01 03:00:00</th>\n",
       "      <td>2402.0</td>\n",
       "      <td>31.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-01 04:00:00</th>\n",
       "      <td>2403.0</td>\n",
       "      <td>32.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-12-31 19:00:00</th>\n",
       "      <td>4012.0</td>\n",
       "      <td>18.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-12-31 20:00:00</th>\n",
       "      <td>3856.0</td>\n",
       "      <td>16.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-12-31 21:00:00</th>\n",
       "      <td>3671.0</td>\n",
       "      <td>17.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-12-31 22:00:00</th>\n",
       "      <td>3499.0</td>\n",
       "      <td>15.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-12-31 23:00:00</th>\n",
       "      <td>3345.0</td>\n",
       "      <td>15.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26304 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       load   temp\n",
       "2012-01-01 00:00:00  2698.0  32.00\n",
       "2012-01-01 01:00:00  2558.0  32.67\n",
       "2012-01-01 02:00:00  2444.0  30.00\n",
       "2012-01-01 03:00:00  2402.0  31.00\n",
       "2012-01-01 04:00:00  2403.0  32.00\n",
       "...                     ...    ...\n",
       "2014-12-31 19:00:00  4012.0  18.00\n",
       "2014-12-31 20:00:00  3856.0  16.67\n",
       "2014-12-31 21:00:00  3671.0  17.00\n",
       "2014-12-31 22:00:00  3499.0  15.33\n",
       "2014-12-31 23:00:00  3345.0  15.33\n",
       "\n",
       "[26304 rows x 2 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"../data/energy.csv\"\n",
    "ts_data = get_dataset(path, time_col='time')\n",
    "ts_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "0b154ca0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:28:21.995447Z",
     "start_time": "2024-04-14T13:28:21.957599Z"
    }
   },
   "outputs": [],
   "source": [
    "# 时间格式编码\n",
    "def time_features_from_frequency_str(freq_str: str):\n",
    "    \"\"\"\n",
    "    Returns a list of time features that will be appropriate for the given frequency string.\n",
    "    Parameters\n",
    "    ----------\n",
    "    freq_str\n",
    "        Frequency string of the form [multiple][granularity] such as \"12H\", \"5min\", \"1D\" etc.\n",
    "    \"\"\"\n",
    "\n",
    "    class TimeFeature:\n",
    "        def __init__(self):\n",
    "            pass\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            pass\n",
    "\n",
    "        def __repr__(self):\n",
    "            return self.__class__.__name__ + \"()\"\n",
    "\n",
    "\n",
    "    class SecondOfMinute(TimeFeature):\n",
    "        \"\"\"Minute of hour encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return index.second / 59.0 - 0.5\n",
    "\n",
    "\n",
    "    class MinuteOfHour(TimeFeature):\n",
    "        \"\"\"Minute of hour encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return index.minute / 59.0 - 0.5\n",
    "\n",
    "\n",
    "    class HourOfDay(TimeFeature):\n",
    "        \"\"\"Hour of day encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return index.hour / 23.0 - 0.5\n",
    "\n",
    "\n",
    "    class DayOfWeek(TimeFeature):\n",
    "        \"\"\"Hour of day encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return index.dayofweek / 6.0 - 0.5\n",
    "\n",
    "\n",
    "    class DayOfMonth(TimeFeature):\n",
    "        \"\"\"Day of month encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return (index.day - 1) / 30.0 - 0.5\n",
    "\n",
    "\n",
    "    class DayOfYear(TimeFeature):\n",
    "        \"\"\"Day of year encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return (index.dayofyear - 1) / 365.0 - 0.5\n",
    "\n",
    "\n",
    "    class MonthOfYear(TimeFeature):\n",
    "        \"\"\"Month of year encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return (index.month - 1) / 11.0 - 0.5\n",
    "\n",
    "\n",
    "    class WeekOfYear(TimeFeature):\n",
    "        \"\"\"Week of year encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return (index.isocalendar().week - 1) / 52.0 - 0.5\n",
    "\n",
    "    \n",
    "    features_by_offsets = {\n",
    "        offsets.YearEnd: [],\n",
    "        offsets.QuarterEnd: [MonthOfYear],\n",
    "        offsets.MonthEnd: [MonthOfYear],\n",
    "        offsets.Week: [DayOfMonth, WeekOfYear],\n",
    "        offsets.Day: [DayOfWeek, DayOfMonth, DayOfYear],\n",
    "        offsets.BusinessDay: [DayOfWeek, DayOfMonth, DayOfYear],\n",
    "        offsets.Hour: [HourOfDay, DayOfWeek, DayOfMonth, DayOfYear],\n",
    "        offsets.Minute: [\n",
    "            MinuteOfHour,\n",
    "            HourOfDay,\n",
    "            DayOfWeek,\n",
    "            DayOfMonth,\n",
    "            DayOfYear,\n",
    "        ],\n",
    "        offsets.Second: [\n",
    "            SecondOfMinute,\n",
    "            MinuteOfHour,\n",
    "            HourOfDay,\n",
    "            DayOfWeek,\n",
    "            DayOfMonth,\n",
    "            DayOfYear,\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    offset = to_offset(freq_str)\n",
    "\n",
    "    for offset_type, feature_classes in features_by_offsets.items():\n",
    "        if isinstance(offset, offset_type):\n",
    "            return [cls() for cls in feature_classes]\n",
    "\n",
    "    supported_freq_msg = f\"\"\"\n",
    "    Unsupported frequency {freq_str}\n",
    "    The following frequencies are supported:\n",
    "        Y   - yearly\n",
    "            alias: A\n",
    "        M   - monthly\n",
    "        W   - weekly\n",
    "        D   - daily\n",
    "        B   - business days\n",
    "        H   - hourly\n",
    "        T   - minutely\n",
    "            alias: min\n",
    "        S   - secondly\n",
    "    \"\"\"\n",
    "    raise RuntimeError(supported_freq_msg)\n",
    "    \n",
    "def time_features(dates, freq='h'):\n",
    "    return np.vstack([feat(dates) for feat in time_features_from_frequency_str(freq)])\n",
    "\n",
    "# 包含时间维度的数据集划分\n",
    "def divide_dataset(df, valid_date, test_date, x_feature_list, y_feature_list, freq):\n",
    "    #归一化\n",
    "    x_scaler = MinMaxScaler()\n",
    "    y_scaler = MinMaxScaler()\n",
    "\n",
    "    #测试集\n",
    "    train = df.copy()[df.index < valid_date][x_feature_list]\n",
    "    train_stamp = time_features(pd.to_datetime(train.index), freq=freq)\n",
    "    train_stamp = train_stamp.transpose(1, 0)\n",
    "    train[x_feature_list] = x_scaler.fit_transform(train)\n",
    "    xtr = train.values.astype('float32')\n",
    "\n",
    "    #验证集\n",
    "    valid = df.copy()[(df.index >= valid_date) & (df.index < test_date)][x_feature_list]\n",
    "    valid_stamp = time_features(pd.to_datetime(valid.index), freq=freq)\n",
    "    valid_stamp = valid_stamp.transpose(1, 0)\n",
    "    valid[x_feature_list] = x_scaler.fit_transform(valid)\n",
    "    xva = valid.values.astype('float32')\n",
    "\n",
    "    #测试集\n",
    "    test = df.copy()[test_date:][x_feature_list]\n",
    "    test_stamp = time_features(pd.to_datetime(test.index), freq=freq)\n",
    "    test_stamp = test_stamp.transpose(1, 0)\n",
    "    test[x_feature_list] = x_scaler.fit_transform(test)\n",
    "    xte = test.values.astype('float32')\n",
    "\n",
    "    #标签\n",
    "    ytr = df.copy()[df.index < valid_date][y_feature_list]\n",
    "    ytr[y_feature_list] = y_scaler.fit_transform(ytr)\n",
    "    ytr = ytr.values.astype('float32')\n",
    "\n",
    "    yva = df.copy()[(df.index >= valid_date) & (df.index < test_date)][y_feature_list]\n",
    "    yva[y_feature_list] = y_scaler.fit_transform(yva)\n",
    "    yva = yva.values.astype('float32')\n",
    "\n",
    "    yte = df.copy()[test_date:][y_feature_list]\n",
    "    yte[y_feature_list] = y_scaler.fit_transform(yte)\n",
    "    yte = yte.values.astype('float32')\n",
    "    \n",
    "    # 数据合并，[训练集，测试集，时间戳]\n",
    "    train = [xtr, ytr, train_stamp]\n",
    "    valid = [xva, yva, valid_stamp]\n",
    "    test = [xte, yte, test_stamp]\n",
    "    \n",
    "    return y_scaler, train, valid, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d8c8a495",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:28:23.061731Z",
     "start_time": "2024-04-14T13:28:22.975184Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (23376, 2) y_train shape: (23376, 1) stamp_train shape: (23376, 4)\n",
      "x_valid shape: (1464, 2) y_valid shape: (1464, 1) stamp_valid shape: (1464, 4)\n",
      "x_test shape: (1464, 2) y_test shape: (1464, 1) stamp_test shape: (1464, 4)\n"
     ]
    }
   ],
   "source": [
    "# 构造参数字典\n",
    "params1 = {\n",
    "    \"df\": ts_data,\n",
    "    \"valid_date\": \"2014-09-01 00:00:00\",\n",
    "    \"test_date\": \"2014-11-01 00:00:00\",\n",
    "    \"x_feature_list\": [\"load\", 'temp'],\n",
    "    \"y_feature_list\": [\"load\"],\n",
    "    \"freq\": 'h'\n",
    "}\n",
    "\n",
    "#函数传参\n",
    "scaler, train, valid, test = divide_dataset(**params1)\n",
    "print(\"x_train shape: {0} y_train shape: {1} stamp_train shape: {2}\".format(train[0].shape, train[1].shape, train[2].shape))\n",
    "print(\"x_valid shape: {0} y_valid shape: {1} stamp_valid shape: {2}\".format(valid[0].shape, valid[1].shape, valid[2].shape))\n",
    "print(\"x_test shape: {0} y_test shape: {1} stamp_test shape: {2}\".format(test[0].shape, test[1].shape, test[2].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "c115f8e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:28:24.439808Z",
     "start_time": "2024-04-14T13:28:24.416743Z"
    }
   },
   "outputs": [],
   "source": [
    "#利用前seq_len个数据，预测下pred_len个数据\n",
    "def create_dataset(data_list, seq_len, pred_len, label_len, batch_size):\n",
    "    \"\"\"\n",
    "    @参数设置：\n",
    "    data_list：特征，目标，时间戳\n",
    "    seq_len：输入数据包含过去多少个时间步\n",
    "    pred_len：目标应该在未来多少个时间步之后\n",
    "    label_len：先验时间步\n",
    "    \"\"\"\n",
    "    # 获取数据\n",
    "    feature = data_list[0] # 特征\n",
    "    target = data_list[1] # 目标\n",
    "    stamp = data_list[2] # 时间戳\n",
    "    \n",
    "    # 循环生成数据\n",
    "    X, y = [], []\n",
    "    X_stamp, y_stamp = [], []\n",
    "    seq_len = seq_len - 1 # 包含当前时间点\n",
    "    for i in range(seq_len, len(feature) - pred_len):\n",
    "        # 数据维度\n",
    "        feat = feature[i - seq_len:i + 1]\n",
    "        tar = target[i + 1:i + 1 + pred_len]\n",
    "        X.append(feat)\n",
    "        y.append(tar)\n",
    "        \n",
    "        # 时间维度\n",
    "        xs = stamp[i - seq_len:i + 1]\n",
    "        ys = stamp[i + 1 - label_len:i + 1 + pred_len]\n",
    "        X_stamp.append(xs)\n",
    "        y_stamp.append(ys)\n",
    "        \n",
    "    # 转为张量，数据维度\n",
    "    X = torch.as_tensor(X).float()\n",
    "    y = torch.as_tensor(y).float()\n",
    "    \n",
    "    # 转为张量，时间维度\n",
    "    X_stamp = torch.as_tensor(X_stamp).float()\n",
    "    y_stamp = torch.as_tensor(y_stamp).float()\n",
    "    \n",
    "    # 创建dataloader，[特征，目标，特征时间编码，目标时间编码]\n",
    "    data_loader = DataLoader(TensorDataset(X, y, X_stamp, y_stamp), shuffle=True, batch_size=batch_size)\n",
    "    \n",
    "    return X, y, data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "acee0507",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:28:26.559551Z",
     "start_time": "2024-04-14T13:28:25.656320Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_size: torch.Size([23368, 6, 2]),y_size: torch.Size([23368, 3, 1]),loader_len: 731\n",
      "X_size: torch.Size([1456, 6, 2]),y_size: torch.Size([1456, 3, 1]),loader_len: 46\n",
      "X_size: torch.Size([1456, 6, 2]),y_size: torch.Size([1456, 3, 1]),loader_len: 46\n"
     ]
    }
   ],
   "source": [
    "# 构造参数字典\n",
    "params2 = {\n",
    "    \"seq_len\": 6,\n",
    "    \"pred_len\": 3,\n",
    "    \"label_len\": 0,\n",
    "    \"batch_size\": 32,\n",
    "}\n",
    "X_train, y_train, train_loader = create_dataset(train, **params2)\n",
    "X_valid, y_valid, valid_loader = create_dataset(valid, **params2)\n",
    "X_test, y_test, test_loader = create_dataset(test, **params2)\n",
    "print(\"X_size: {0},y_size: {1},loader_len: {2}\".format(X_train.shape, y_train.shape, len(train_loader)))\n",
    "print(\"X_size: {0},y_size: {1},loader_len: {2}\".format(X_valid.shape, y_valid.shape, len(valid_loader)))\n",
    "print(\"X_size: {0},y_size: {1},loader_len: {2}\".format(X_test.shape, y_test.shape, len(test_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2885a8ec",
   "metadata": {},
   "source": [
    "### 模型定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "796c6adc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:28:29.663971Z",
     "start_time": "2024-04-14T13:28:29.577508Z"
    }
   },
   "outputs": [],
   "source": [
    "# DataEmbedding编码类\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model).float()\n",
    "        pe.require_grad = False\n",
    "\n",
    "        position = torch.arange(0, max_len).float().unsqueeze(1)\n",
    "        div_term = (torch.arange(0, d_model, 2).float()\n",
    "                    * -(math.log(10000.0) / d_model)).exp()\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pe[:, :x.size(1)]\n",
    "\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, c_in, d_model):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        padding = 1 if torch.__version__ >= '1.5.0' else 2\n",
    "        self.tokenConv = nn.Conv1d(in_channels=c_in, out_channels=d_model,\n",
    "                                   kernel_size=3, padding=padding, padding_mode='circular', bias=False)\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                nn.init.kaiming_normal_(\n",
    "                    m.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.tokenConv(x.permute(0, 2, 1)).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class FixedEmbedding(nn.Module):\n",
    "    def __init__(self, c_in, d_model):\n",
    "        super(FixedEmbedding, self).__init__()\n",
    "\n",
    "        w = torch.zeros(c_in, d_model).float()\n",
    "        w.require_grad = False\n",
    "\n",
    "        position = torch.arange(0, c_in).float().unsqueeze(1)\n",
    "        div_term = (torch.arange(0, d_model, 2).float()\n",
    "                    * -(math.log(10000.0) / d_model)).exp()\n",
    "\n",
    "        w[:, 0::2] = torch.sin(position * div_term)\n",
    "        w[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        self.emb = nn.Embedding(c_in, d_model)\n",
    "        self.emb.weight = nn.Parameter(w, requires_grad=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.emb(x).detach()\n",
    "\n",
    "\n",
    "class TemporalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, embed_type='fixed', freq='h'):\n",
    "        super(TemporalEmbedding, self).__init__()\n",
    "\n",
    "        minute_size = 4\n",
    "        hour_size = 24\n",
    "        weekday_size = 7\n",
    "        day_size = 32\n",
    "        month_size = 13\n",
    "\n",
    "        Embed = FixedEmbedding if embed_type == 'fixed' else nn.Embedding\n",
    "        if freq == 't':\n",
    "            self.minute_embed = Embed(minute_size, d_model)\n",
    "        self.hour_embed = Embed(hour_size, d_model)\n",
    "        self.weekday_embed = Embed(weekday_size, d_model)\n",
    "        self.day_embed = Embed(day_size, d_model)\n",
    "        self.month_embed = Embed(month_size, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.long()\n",
    "        minute_x = self.minute_embed(x[:, :, 4]) if hasattr(\n",
    "            self, 'minute_embed') else 0.\n",
    "        hour_x = self.hour_embed(x[:, :, 3])\n",
    "        weekday_x = self.weekday_embed(x[:, :, 2])\n",
    "        day_x = self.day_embed(x[:, :, 1])\n",
    "        month_x = self.month_embed(x[:, :, 0])\n",
    "\n",
    "        return hour_x + weekday_x + day_x + month_x + minute_x\n",
    "\n",
    "\n",
    "class TimeFeatureEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, embed_type='timeF', freq='h'):\n",
    "        super(TimeFeatureEmbedding, self).__init__()\n",
    "\n",
    "        freq_map = {'h': 4, 't': 5, 's': 6,\n",
    "                    'm': 1, 'a': 1, 'w': 2, 'd': 3, 'b': 3}\n",
    "        d_inp = freq_map[freq]\n",
    "        self.embed = nn.Linear(d_inp, d_model, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embed(x)\n",
    "\n",
    "\n",
    "class DataEmbedding(nn.Module):\n",
    "    def __init__(self, c_in, d_model, embed_type='fixed', freq='h', dropout=0.1):\n",
    "        super(DataEmbedding, self).__init__()\n",
    "\n",
    "        self.value_embedding = TokenEmbedding(c_in=c_in, d_model=d_model)\n",
    "        self.position_embedding = PositionalEmbedding(d_model=d_model)\n",
    "        self.temporal_embedding = TemporalEmbedding(d_model=d_model, embed_type=embed_type,\n",
    "                                                    freq=freq) if embed_type != 'timeF' else TimeFeatureEmbedding(\n",
    "            d_model=d_model, embed_type=embed_type, freq=freq)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, x_mark):\n",
    "        if x_mark is None:\n",
    "            x = self.value_embedding(x) + self.position_embedding(x)\n",
    "        else:\n",
    "            x = self.value_embedding(x) + self.temporal_embedding(x_mark) + self.position_embedding(x)\n",
    "        return self.dropout(x)\n",
    "    \n",
    "    \n",
    "# 自注意力模块\n",
    "class TriangularCausalMask():\n",
    "    def __init__(self, B, L, device=\"cpu\"):\n",
    "        mask_shape = [B, 1, L, L]\n",
    "        with torch.no_grad():\n",
    "            self._mask = torch.triu(torch.ones(mask_shape, dtype=torch.bool), diagonal=1).to(device)\n",
    "\n",
    "    @property\n",
    "    def mask(self):\n",
    "        return self._mask\n",
    "    \n",
    "    \n",
    "class FullAttention(nn.Module):\n",
    "    def __init__(self, mask_flag=True, factor=5, scale=None, attention_dropout=0.1, output_attention=False):\n",
    "        super(FullAttention, self).__init__()\n",
    "        self.scale = scale\n",
    "        self.mask_flag = mask_flag\n",
    "        self.output_attention = output_attention\n",
    "        self.dropout = nn.Dropout(attention_dropout)\n",
    "\n",
    "    def forward(self, queries, keys, values, attn_mask, tau=None, delta=None):\n",
    "        B, L, H, E = queries.shape\n",
    "        _, S, _, D = values.shape\n",
    "        scale = self.scale or 1. / math.sqrt(E)\n",
    "\n",
    "        scores = torch.einsum(\"blhe,bshe->bhls\", queries, keys)\n",
    "\n",
    "        if self.mask_flag:\n",
    "            if attn_mask is None:\n",
    "                attn_mask = TriangularCausalMask(B, L, device=queries.device)\n",
    "\n",
    "            scores.masked_fill_(attn_mask.mask, -np.inf)\n",
    "\n",
    "        A = self.dropout(torch.softmax(scale * scores, dim=-1))\n",
    "        V = torch.einsum(\"bhls,bshd->blhd\", A, values)\n",
    "\n",
    "        if self.output_attention:\n",
    "            return V.contiguous(), A\n",
    "        else:\n",
    "            return V.contiguous(), None\n",
    "        \n",
    "        \n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, attention, d_model, n_heads, d_keys=None,\n",
    "                 d_values=None):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "\n",
    "        d_keys = d_keys or (d_model // n_heads)\n",
    "        d_values = d_values or (d_model // n_heads)\n",
    "\n",
    "        self.inner_attention = attention\n",
    "        self.query_projection = nn.Linear(d_model, d_keys * n_heads)\n",
    "        self.key_projection = nn.Linear(d_model, d_keys * n_heads)\n",
    "        self.value_projection = nn.Linear(d_model, d_values * n_heads)\n",
    "        self.out_projection = nn.Linear(d_values * n_heads, d_model)\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "    def forward(self, queries, keys, values, attn_mask, tau=None, delta=None):\n",
    "        B, L, _ = queries.shape\n",
    "        _, S, _ = keys.shape\n",
    "        H = self.n_heads\n",
    "\n",
    "        queries = self.query_projection(queries).view(B, L, H, -1)\n",
    "        keys = self.key_projection(keys).view(B, S, H, -1)\n",
    "        values = self.value_projection(values).view(B, S, H, -1)\n",
    "\n",
    "        out, attn = self.inner_attention(\n",
    "            queries,\n",
    "            keys,\n",
    "            values,\n",
    "            attn_mask,\n",
    "            tau=tau,\n",
    "            delta=delta\n",
    "        )\n",
    "        out = out.view(B, L, -1)\n",
    "\n",
    "        return self.out_projection(out), attn\n",
    "    \n",
    "    \n",
    "# Pyraformer_EncDec模块\n",
    "def get_mask(input_size, window_size, inner_size):\n",
    "    \"\"\"Get the attention mask of PAM-Naive\"\"\"\n",
    "    # Get the size of all layers\n",
    "    all_size = []\n",
    "    all_size.append(input_size)\n",
    "    for i in range(len(window_size)):\n",
    "        layer_size = math.floor(all_size[i] / window_size[i])\n",
    "        all_size.append(layer_size)\n",
    "\n",
    "    seq_length = sum(all_size)\n",
    "    mask = torch.zeros(seq_length, seq_length)\n",
    "\n",
    "    # get intra-scale mask\n",
    "    inner_window = inner_size // 2\n",
    "    for layer_idx in range(len(all_size)):\n",
    "        start = sum(all_size[:layer_idx])\n",
    "        for i in range(start, start + all_size[layer_idx]):\n",
    "            left_side = max(i - inner_window, start)\n",
    "            right_side = min(i + inner_window + 1, start + all_size[layer_idx])\n",
    "            mask[i, left_side:right_side] = 1\n",
    "\n",
    "    # get inter-scale mask\n",
    "    for layer_idx in range(1, len(all_size)):\n",
    "        start = sum(all_size[:layer_idx])\n",
    "        for i in range(start, start + all_size[layer_idx]):\n",
    "            left_side = (start - all_size[layer_idx - 1]) + \\\n",
    "                (i - start) * window_size[layer_idx - 1]\n",
    "            if i == (start + all_size[layer_idx] - 1):\n",
    "                right_side = start\n",
    "            else:\n",
    "                right_side = (\n",
    "                    start - all_size[layer_idx - 1]) + (i - start + 1) * window_size[layer_idx - 1]\n",
    "            mask[i, left_side:right_side] = 1\n",
    "            mask[left_side:right_side, i] = 1\n",
    "\n",
    "    mask = (1 - mask).bool()\n",
    "\n",
    "    return mask, all_size\n",
    "\n",
    "\n",
    "def refer_points(all_sizes, window_size):\n",
    "    \"\"\"Gather features from PAM's pyramid sequences\"\"\"\n",
    "    input_size = all_sizes[0]\n",
    "    indexes = torch.zeros(input_size, len(all_sizes))\n",
    "\n",
    "    for i in range(input_size):\n",
    "        indexes[i][0] = i\n",
    "        former_index = i\n",
    "        for j in range(1, len(all_sizes)):\n",
    "            start = sum(all_sizes[:j])\n",
    "            inner_layer_idx = former_index - (start - all_sizes[j - 1])\n",
    "            former_index = start + \\\n",
    "                min(inner_layer_idx // window_size[j - 1], all_sizes[j] - 1)\n",
    "            indexes[i][j] = former_index\n",
    "\n",
    "    indexes = indexes.unsqueeze(0).unsqueeze(3)\n",
    "\n",
    "    return indexes.long()\n",
    "\n",
    "\n",
    "class RegularMask():\n",
    "    def __init__(self, mask):\n",
    "        self._mask = mask.unsqueeze(1)\n",
    "\n",
    "    @property\n",
    "    def mask(self):\n",
    "        return self._mask\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\" Compose with two layers \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, d_inner, n_head, dropout=0.1, normalize_before=True):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.slf_attn = AttentionLayer(\n",
    "            FullAttention(mask_flag=True, factor=0,\n",
    "                          attention_dropout=dropout, output_attention=False),\n",
    "            d_model, n_head)\n",
    "        self.pos_ffn = PositionwiseFeedForward(\n",
    "            d_model, d_inner, dropout=dropout, normalize_before=normalize_before)\n",
    "\n",
    "    def forward(self, enc_input, slf_attn_mask=None):\n",
    "        attn_mask = RegularMask(slf_attn_mask)\n",
    "        enc_output, _ = self.slf_attn(\n",
    "            enc_input, enc_input, enc_input, attn_mask=attn_mask)\n",
    "        enc_output = self.pos_ffn(enc_output)\n",
    "        return enc_output\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\" A encoder model with self attention mechanism. \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, seq_len, d_ff, n_heads, dropout, e_layers, enc_in, \n",
    "                 window_size, inner_size, embed, freq):\n",
    "        super().__init__()\n",
    "\n",
    "        d_bottleneck = d_model//4\n",
    "\n",
    "        self.mask, self.all_size = get_mask(seq_len, window_size, inner_size)\n",
    "        self.indexes = refer_points(self.all_size, window_size)\n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderLayer(d_model, d_ff, n_heads, dropout=dropout,\n",
    "                         normalize_before=False) for _ in range(e_layers)\n",
    "        ])  # naive pyramid attention\n",
    "\n",
    "        self.enc_embedding = DataEmbedding(enc_in, d_model, embed, freq, dropout)\n",
    "        self.conv_layers = Bottleneck_Construct(d_model, window_size, d_bottleneck)\n",
    "\n",
    "    def forward(self, x_enc, x_mark_enc=None):\n",
    "        seq_enc = self.enc_embedding(x_enc, x_mark_enc)\n",
    "\n",
    "        mask = self.mask.repeat(len(seq_enc), 1, 1).to(x_enc.device)\n",
    "        seq_enc = self.conv_layers(seq_enc)\n",
    "\n",
    "        for i in range(len(self.layers)):\n",
    "            seq_enc = self.layers[i](seq_enc, mask)\n",
    "\n",
    "        indexes = self.indexes.repeat(seq_enc.size(\n",
    "            0), 1, 1, seq_enc.size(2)).to(seq_enc.device)\n",
    "        indexes = indexes.view(seq_enc.size(0), -1, seq_enc.size(2))\n",
    "        all_enc = torch.gather(seq_enc, 1, indexes)\n",
    "        seq_enc = all_enc.view(seq_enc.size(0), self.all_size[0], -1)\n",
    "\n",
    "        return seq_enc\n",
    "\n",
    "\n",
    "class ConvLayer(nn.Module):\n",
    "    def __init__(self, c_in, window_size):\n",
    "        super(ConvLayer, self).__init__()\n",
    "        self.downConv = nn.Conv1d(in_channels=c_in,\n",
    "                                  out_channels=c_in,\n",
    "                                  kernel_size=window_size,\n",
    "                                  stride=window_size)\n",
    "        self.norm = nn.BatchNorm1d(c_in)\n",
    "        self.activation = nn.ELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.downConv(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Bottleneck_Construct(nn.Module):\n",
    "    \"\"\"Bottleneck convolution CSCM\"\"\"\n",
    "\n",
    "    def __init__(self, d_model, window_size, d_inner):\n",
    "        super(Bottleneck_Construct, self).__init__()\n",
    "        if not isinstance(window_size, list):\n",
    "            self.conv_layers = nn.ModuleList([\n",
    "                ConvLayer(d_inner, window_size),\n",
    "                ConvLayer(d_inner, window_size),\n",
    "                ConvLayer(d_inner, window_size)\n",
    "            ])\n",
    "        else:\n",
    "            self.conv_layers = []\n",
    "            for i in range(len(window_size)):\n",
    "                self.conv_layers.append(ConvLayer(d_inner, window_size[i]))\n",
    "            self.conv_layers = nn.ModuleList(self.conv_layers)\n",
    "        self.up = Linear(d_inner, d_model)\n",
    "        self.down = Linear(d_model, d_inner)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, enc_input):\n",
    "        temp_input = self.down(enc_input).permute(0, 2, 1)\n",
    "        all_inputs = []\n",
    "        for i in range(len(self.conv_layers)):\n",
    "            temp_input = self.conv_layers[i](temp_input)\n",
    "            all_inputs.append(temp_input)\n",
    "\n",
    "        all_inputs = torch.cat(all_inputs, dim=2).transpose(1, 2)\n",
    "        all_inputs = self.up(all_inputs)\n",
    "        all_inputs = torch.cat([enc_input, all_inputs], dim=1)\n",
    "\n",
    "        all_inputs = self.norm(all_inputs)\n",
    "        return all_inputs\n",
    "\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    \"\"\" Two-layer position-wise feed-forward neural network. \"\"\"\n",
    "\n",
    "    def __init__(self, d_in, d_hid, dropout=0.1, normalize_before=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.normalize_before = normalize_before\n",
    "\n",
    "        self.w_1 = nn.Linear(d_in, d_hid)\n",
    "        self.w_2 = nn.Linear(d_hid, d_in)\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(d_in, eps=1e-6)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        if self.normalize_before:\n",
    "            x = self.layer_norm(x)\n",
    "\n",
    "        x = F.gelu(self.w_1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.w_2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + residual\n",
    "\n",
    "        if not self.normalize_before:\n",
    "            x = self.layer_norm(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "# Pyraformer模型\n",
    "class Pyraformer(nn.Module):\n",
    "    \"\"\" \n",
    "    Pyraformer: Pyramidal attention to reduce complexity\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, task_name, pred_len, label_len, d_model, seq_len, d_ff, n_heads, dropout, e_layers, enc_in, embed, freq,\n",
    "                 window_size=[4,4], inner_size=5):\n",
    "        \"\"\"\n",
    "        window_size: list, the downsample window size in pyramidal attention.\n",
    "        inner_size: int, the size of neighbour attention\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.task_name = task_name\n",
    "        self.pred_len = pred_len\n",
    "        self.d_model = d_model\n",
    "\n",
    "        if self.task_name == 'short_term_forecast':\n",
    "            window_size = [2,2]\n",
    "        self.encoder = Encoder(d_model, seq_len, d_ff, n_heads, dropout, e_layers, enc_in, \n",
    "                 window_size, inner_size, embed, freq)\n",
    "\n",
    "        if self.task_name == 'long_term_forecast' or self.task_name == 'short_term_forecast':\n",
    "            self.projection = nn.Linear(\n",
    "                (len(window_size)+1)*self.d_model, self.pred_len * enc_in)\n",
    "\n",
    "    def long_forecast(self, x_enc, x_mark_enc):\n",
    "        enc_out = self.encoder(x_enc, x_mark_enc)[:, -1, :]\n",
    "        dec_out = self.projection(enc_out).view(\n",
    "            enc_out.size(0), self.pred_len, -1)\n",
    "        return dec_out\n",
    "    \n",
    "    def short_forecast(self, x_enc, x_mark_enc):\n",
    "        # Normalization\n",
    "        mean_enc = x_enc.mean(1, keepdim=True).detach()  # B x 1 x E\n",
    "        x_enc = x_enc - mean_enc\n",
    "        std_enc = torch.sqrt(torch.var(x_enc, dim=1, keepdim=True, unbiased=False) + 1e-5).detach()  # B x 1 x E\n",
    "        x_enc = x_enc / std_enc\n",
    "\n",
    "        enc_out = self.encoder(x_enc, x_mark_enc)[:, -1, :]\n",
    "        dec_out = self.projection(enc_out).view(\n",
    "            enc_out.size(0), self.pred_len, -1)\n",
    "        \n",
    "        dec_out = dec_out * std_enc + mean_enc\n",
    "        return dec_out\n",
    "\n",
    "    def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec):\n",
    "        if self.task_name == 'long_term_forecast':\n",
    "            dec_out = self.long_forecast(x_enc, x_mark_enc)\n",
    "            return dec_out[:, -self.pred_len:, :]  # [B, L, D]\n",
    "        if self.task_name == 'short_term_forecast':\n",
    "            dec_out = self.short_forecast(x_enc, x_mark_enc)\n",
    "            return dec_out[:, -self.pred_len:, :]  # [B, L, D]\n",
    "        \n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9112cc97",
   "metadata": {},
   "source": [
    "### 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a944cfbc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:28:32.195228Z",
     "start_time": "2024-04-14T13:28:32.154182Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(task_args, train_args, model_args):\n",
    "    # 参数配置\n",
    "    columns = task_args['columns'] # 模型全部特征\n",
    "    target = task_args['target'] # 模型预测特征\n",
    "    features = task_args['features'] # 模型预测模式\n",
    "    model_name = train_args['model_name'] # 模型名称\n",
    "    train_loader = train_args['train_loader'] # 训练集\n",
    "    valid_loader = train_args['valid_loader'] # 验证集\n",
    "    n_epochs = train_args['n_epochs'] # 训练次数\n",
    "    learning_rate = train_args['learning_rate'] # 学习率\n",
    "    loss = train_args['loss'] # 损失函数\n",
    "    patience = train_args['patience'] # 最大早停次数阈值，超过就会早停\n",
    "    lradj = train_args['lradj'] # 学习率函数\n",
    "    path = train_args['path'] # 模型保存路径\n",
    "    verbose = train_args['verbose'] # 打印训练过程\n",
    "    plots = train_args['plots'] # 绘制损失图\n",
    "    \n",
    "    #检查是否可用GPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # 创建模型和优化器\n",
    "    model = model_name(**model_args).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_fn = loss\n",
    "    \n",
    "    # 损失函数值\n",
    "    train_losses, val_losses = [], []\n",
    "    \n",
    "    # 调整学习率\n",
    "    def adjust_learning_rate(optimizer, epoch, lradj, learning_rate, train_epochs):\n",
    "        # lr = learning_rate * (0.2 ** (epoch // 2))\n",
    "        if lradj == 'type1':\n",
    "            lr_adjust = {epoch: learning_rate * (0.5 ** ((epoch - 1) // 1))}\n",
    "        elif lradj == 'type2':\n",
    "            lr_adjust = {\n",
    "                2: 5e-5, 4: 1e-5, 6: 5e-6, 8: 1e-6,\n",
    "                10: 5e-7, 15: 1e-7, 20: 5e-8\n",
    "            }\n",
    "        elif lradj == \"cosine\":\n",
    "            lr_adjust = {epoch: learning_rate /2 * (1 + math.cos(epoch / train_epochs * math.pi))}\n",
    "        if epoch in lr_adjust.keys():\n",
    "            lr = lr_adjust[epoch]\n",
    "            # 参数组(param_groups)是用来指定不同的参数组以便对它们进行不同的优化设置，比如'lr'\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "            print('Updating learning rate to {}'.format(lr))\n",
    "    \n",
    "    # 设置早停\n",
    "    class EarlyStopping():\n",
    "        def __init__(self, patience=7, verbose=False, delta=0):\n",
    "            self.patience = patience # 连续超限次数，如果满足条件，则早停\n",
    "            self.verbose = verbose\n",
    "            self.counter = 0\n",
    "            self.best_score = None\n",
    "            self.early_stop = False\n",
    "            self.val_loss_min = np.Inf\n",
    "            self.delta = delta\n",
    "\n",
    "        def __call__(self, val_loss, model, path):\n",
    "            score = -val_loss\n",
    "            if self.best_score is None:\n",
    "                self.best_score = score\n",
    "                self.save_checkpoint(val_loss, model, path)\n",
    "            elif score < self.best_score + self.delta:\n",
    "                self.counter += 1\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "                if self.counter >= self.patience:\n",
    "                    self.early_stop = True\n",
    "            else:\n",
    "                self.best_score = score\n",
    "                self.save_checkpoint(val_loss, model, path)\n",
    "                self.counter = 0\n",
    "\n",
    "        def save_checkpoint(self, val_loss, model, path):\n",
    "            if self.verbose:\n",
    "                print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "            torch.save(model.state_dict(), path + '/' + 'checkpoint.pth')\n",
    "            self.val_loss_min = val_loss\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=verbose)\n",
    "    \n",
    "    # 训练任务\n",
    "    def forecasting_task(columns, target, features):\n",
    "        \"\"\"\n",
    "        features: [M, S, MS]; \n",
    "            M:multivariate predict multivariate, \n",
    "            S:univariate predict univariate, \n",
    "            MS:multivariate predict univariate'\n",
    "        \"\"\"\n",
    "        # 字典索引生成\n",
    "        col_dict = {}\n",
    "        for i,j in enumerate(columns):\n",
    "            col_dict[j] = i\n",
    "\n",
    "        if features == 'MS':\n",
    "            target = target[0]\n",
    "            f_dim = col_dict[target]\n",
    "        elif features == 'S':\n",
    "            f_dim = 0\n",
    "        else:\n",
    "            f_dim = 0\n",
    "        return f_dim\n",
    "    f_dim = forecasting_task(columns, target, features)\n",
    "    \n",
    "    # 设置保存模型路径\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    # 模型训练和验证\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for batch_x, batch_y, batch_x_mark, batch_y_mark in train_loader:\n",
    "            #将数据移至 GPU\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            batch_x_mark = batch_x_mark.to(device)\n",
    "            batch_y_mark = batch_y_mark.to(device)\n",
    "            # 清空梯度\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x, batch_x_mark, batch_y, batch_y_mark)\n",
    "            if features == 'MS':\n",
    "                outputs = outputs[:, :, f_dim: f_dim+1] \n",
    "            else:\n",
    "                outputs = outputs[:, :, f_dim:]\n",
    "            train_loss = loss_fn(outputs, batch_y)\n",
    "            # 反向传播计算得到每个参数的梯度值\n",
    "            train_loss.backward()\n",
    "            # 通过梯度下降执行一步参数更新\n",
    "            optimizer.step()\n",
    "            #每个batch的loss和\n",
    "            total_train_loss += train_loss.item() # .item()表示只包含一个元素的tensor中提取值\n",
    "                \n",
    "        #每个epoch的损失平均\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        \n",
    "        #所有epoch的loss\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        #评估模型\n",
    "        model.eval()\n",
    "        #关闭自动求导功能，只使用训练好的模型进行预测或评估，不需要进行梯度计算和参数更新\n",
    "        with torch.no_grad():\n",
    "            total_val_loss = 0\n",
    "            for batch_x, batch_y, batch_x_mark, batch_y_mark in valid_loader:\n",
    "                #将数据移至 GPU\n",
    "                batch_x = batch_x.to(device)\n",
    "                batch_y = batch_y.to(device)\n",
    "                batch_x_mark = batch_x_mark.to(device)\n",
    "                batch_y_mark = batch_y_mark.to(device)\n",
    "                val_outputs = model(batch_x, batch_x_mark, batch_y, batch_y_mark)\n",
    "                if features == 'MS':\n",
    "                    val_outputs = val_outputs[:, :, f_dim: f_dim+1] \n",
    "                else:\n",
    "                    val_outputs = val_outputs[:, :, f_dim:]\n",
    "                val_loss = loss_fn(val_outputs, batch_y)\n",
    "                #每个batch的loss和\n",
    "                total_val_loss += val_loss.item()\n",
    "                \n",
    "        #每个epoch的损失平均\n",
    "        avg_val_loss = total_val_loss / len(valid_loader)\n",
    "        \n",
    "        #所有epoch的loss\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        #打印训练过程\n",
    "        if verbose:\n",
    "            print(f'Epoch [{epoch+1}/{n_epochs}], Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}')\n",
    "            \n",
    "        # 设置早停\n",
    "        early_stopping(avg_val_loss, model, path)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping!\")\n",
    "            break\n",
    "            \n",
    "        # 调整学习率\n",
    "        adjust_learning_rate(optimizer, epoch+1, lradj, learning_rate, n_epochs)\n",
    "        \n",
    "    # 加载最佳模型\n",
    "    best_model_path = path + '/' + 'checkpoint.pth'\n",
    "    model.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "    #绘制损失函数图\n",
    "    def plot_loss(train_losses, val_losses):\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.style.use('seaborn-v0_8-paper') #绘制背景色\n",
    "        plt.grid(axis = 'y',linewidth=0.35) #绘制网格\n",
    "        plt.plot(val_losses, linestyle='-',color = '#11b3b6')\n",
    "        plt.plot(train_losses, linestyle='-',color = '#f14643')\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Training and Validation Progress\")\n",
    "        plt.legend([\"Validation\", \"Training\"])\n",
    "        plt.show()\n",
    "    if plots:\n",
    "        plot_loss(train_losses, val_losses)\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "68f150b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:28:53.506113Z",
     "start_time": "2024-04-14T13:28:34.290845Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|████▏                                                                              | 1/20 [00:07<02:22,  7.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Training Loss: 0.0042, Validation Loss: 0.0047\n",
      "Validation loss decreased (inf --> 0.004659).  Saving model ...\n",
      "Updating learning rate to 0.0009938441702975688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 10%|████████▎                                                                          | 2/20 [00:14<02:07,  7.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/20], Training Loss: 0.0029, Validation Loss: 0.0045\n",
      "Validation loss decreased (0.004659 --> 0.004531).  Saving model ...\n",
      "Updating learning rate to 0.0009755282581475768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|████████▎                                                                          | 2/20 [00:17<02:40,  8.91s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[102], line 36\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 构造参数字典\u001b[39;00m\n\u001b[0;32m      2\u001b[0m params3 \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtask_args\u001b[39m\u001b[38;5;124m\"\u001b[39m:{\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mload\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemp\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     34\u001b[0m     },\n\u001b[0;32m     35\u001b[0m }\n\u001b[1;32m---> 36\u001b[0m model \u001b[38;5;241m=\u001b[39m train(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams3)\n",
      "Cell \u001b[1;32mIn[101], line 120\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(task_args, train_args, model_args)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;66;03m# 清空梯度\u001b[39;00m\n\u001b[0;32m    119\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 120\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_x_mark\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_y_mark\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m features \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMS\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    122\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m outputs[:, :, f_dim: f_dim\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m] \n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[100], line 462\u001b[0m, in \u001b[0;36mPyraformer.forward\u001b[1;34m(self, x_enc, x_mark_enc, x_dec, x_mark_dec)\u001b[0m\n\u001b[0;32m    460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dec_out[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpred_len:, :]  \u001b[38;5;66;03m# [B, L, D]\u001b[39;00m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshort_term_forecast\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 462\u001b[0m     dec_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshort_forecast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_enc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_mark_enc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dec_out[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpred_len:, :]  \u001b[38;5;66;03m# [B, L, D]\u001b[39;00m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[100], line 450\u001b[0m, in \u001b[0;36mPyraformer.short_forecast\u001b[1;34m(self, x_enc, x_mark_enc)\u001b[0m\n\u001b[0;32m    447\u001b[0m std_enc \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msqrt(torch\u001b[38;5;241m.\u001b[39mvar(x_enc, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, unbiased\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-5\u001b[39m)\u001b[38;5;241m.\u001b[39mdetach()  \u001b[38;5;66;03m# B x 1 x E\u001b[39;00m\n\u001b[0;32m    448\u001b[0m x_enc \u001b[38;5;241m=\u001b[39m x_enc \u001b[38;5;241m/\u001b[39m std_enc\n\u001b[1;32m--> 450\u001b[0m enc_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_enc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_mark_enc\u001b[49m\u001b[43m)\u001b[49m[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :]\n\u001b[0;32m    451\u001b[0m dec_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprojection(enc_out)\u001b[38;5;241m.\u001b[39mview(\n\u001b[0;32m    452\u001b[0m     enc_out\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpred_len, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    454\u001b[0m dec_out \u001b[38;5;241m=\u001b[39m dec_out \u001b[38;5;241m*\u001b[39m std_enc \u001b[38;5;241m+\u001b[39m mean_enc\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[100], line 320\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[1;34m(self, x_enc, x_mark_enc)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers)):\n\u001b[0;32m    318\u001b[0m     seq_enc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[i](seq_enc, mask)\n\u001b[1;32m--> 320\u001b[0m indexes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindexes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepeat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq_enc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_enc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(seq_enc\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    322\u001b[0m indexes \u001b[38;5;241m=\u001b[39m indexes\u001b[38;5;241m.\u001b[39mview(seq_enc\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, seq_enc\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m2\u001b[39m))\n\u001b[0;32m    323\u001b[0m all_enc \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mgather(seq_enc, \u001b[38;5;241m1\u001b[39m, indexes)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 构造参数字典\n",
    "params3 = {\n",
    "    \"task_args\":{\n",
    "        \"columns\": ['load', 'temp'],\n",
    "        \"target\": ['load'],\n",
    "        \"features\": 'MS'\n",
    "    },\n",
    "    \"train_args\": {\n",
    "        \"model_name\": Pyraformer,\n",
    "        \"train_loader\": train_loader,\n",
    "        \"valid_loader\": valid_loader,\n",
    "        \"n_epochs\": 20,\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"loss\": nn.MSELoss(),\n",
    "        \"patience\": 7,\n",
    "        \"lradj\": 'cosine',\n",
    "        \"path\": \"../models/test\",\n",
    "        \"verbose\": True,\n",
    "        \"plots\": True,\n",
    "    },\n",
    "    \"model_args\": {\n",
    "        'enc_in': 2, \n",
    "        'task_name': 'short_term_forecast',\n",
    "        'seq_len': 6,\n",
    "        'pred_len': 3,\n",
    "        \"label_len\": 0,\n",
    "        'd_model': 128,\n",
    "        'n_heads': 8,\n",
    "        'd_ff': 128,\n",
    "        'dropout': 0.1,\n",
    "        'e_layers': 1,\n",
    "        'embed': 'fixed',\n",
    "        'freq': 'h',\n",
    "    },\n",
    "}\n",
    "model = train(**params3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afffa2f",
   "metadata": {},
   "source": [
    "# 基于Reformer的时间序列预测"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14414277",
   "metadata": {},
   "source": [
    "## 多变量多步预测"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad49d3ef",
   "metadata": {},
   "source": [
    "### 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "8271a52a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:28:57.475554Z",
     "start_time": "2024-04-14T13:28:57.454518Z"
    }
   },
   "outputs": [],
   "source": [
    "# 读取数据\n",
    "def get_dataset(path, time_col=None):\n",
    "    data = pd.read_csv(path)\n",
    "    rows = data.shape[0]\n",
    "    now = datetime.now()\n",
    "    newtime = now.replace(microsecond=0)\n",
    "    if time_col == None:\n",
    "        # 如果没有时间列，生成时间戳范围\n",
    "        time_index = pd.date_range(start=datetime.now() -\n",
    "                                   timedelta(seconds=rows - 1),\n",
    "                                   end=datetime.now(),\n",
    "                                   freq='S')\n",
    "        full_data = pd.DataFrame(data=data.values,\n",
    "                                 index=pd.to_datetime(time_index, unit='s'),\n",
    "                                 columns=data.columns)\n",
    "    else:\n",
    "        columns = [i for i in data.columns if i != time_col] # 去除时间列\n",
    "        full_data = pd.DataFrame(data=data.drop([time_col], axis=1).values,\n",
    "                                 index=pd.to_datetime(data[time_col].values),\n",
    "                                 columns=columns)\n",
    "    return full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "599595a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:28:58.493325Z",
     "start_time": "2024-04-14T13:28:58.411911Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>load</th>\n",
       "      <th>temp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-01-01 00:00:00</th>\n",
       "      <td>2698.0</td>\n",
       "      <td>32.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-01 01:00:00</th>\n",
       "      <td>2558.0</td>\n",
       "      <td>32.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-01 02:00:00</th>\n",
       "      <td>2444.0</td>\n",
       "      <td>30.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-01 03:00:00</th>\n",
       "      <td>2402.0</td>\n",
       "      <td>31.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-01 04:00:00</th>\n",
       "      <td>2403.0</td>\n",
       "      <td>32.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-12-31 19:00:00</th>\n",
       "      <td>4012.0</td>\n",
       "      <td>18.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-12-31 20:00:00</th>\n",
       "      <td>3856.0</td>\n",
       "      <td>16.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-12-31 21:00:00</th>\n",
       "      <td>3671.0</td>\n",
       "      <td>17.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-12-31 22:00:00</th>\n",
       "      <td>3499.0</td>\n",
       "      <td>15.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-12-31 23:00:00</th>\n",
       "      <td>3345.0</td>\n",
       "      <td>15.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26304 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       load   temp\n",
       "2012-01-01 00:00:00  2698.0  32.00\n",
       "2012-01-01 01:00:00  2558.0  32.67\n",
       "2012-01-01 02:00:00  2444.0  30.00\n",
       "2012-01-01 03:00:00  2402.0  31.00\n",
       "2012-01-01 04:00:00  2403.0  32.00\n",
       "...                     ...    ...\n",
       "2014-12-31 19:00:00  4012.0  18.00\n",
       "2014-12-31 20:00:00  3856.0  16.67\n",
       "2014-12-31 21:00:00  3671.0  17.00\n",
       "2014-12-31 22:00:00  3499.0  15.33\n",
       "2014-12-31 23:00:00  3345.0  15.33\n",
       "\n",
       "[26304 rows x 2 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"../data/energy.csv\"\n",
    "ts_data = get_dataset(path, time_col='time')\n",
    "ts_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "6c2db148",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:28:59.768340Z",
     "start_time": "2024-04-14T13:28:59.728176Z"
    }
   },
   "outputs": [],
   "source": [
    "# 时间格式编码\n",
    "def time_features_from_frequency_str(freq_str: str):\n",
    "    \"\"\"\n",
    "    Returns a list of time features that will be appropriate for the given frequency string.\n",
    "    Parameters\n",
    "    ----------\n",
    "    freq_str\n",
    "        Frequency string of the form [multiple][granularity] such as \"12H\", \"5min\", \"1D\" etc.\n",
    "    \"\"\"\n",
    "\n",
    "    class TimeFeature:\n",
    "        def __init__(self):\n",
    "            pass\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            pass\n",
    "\n",
    "        def __repr__(self):\n",
    "            return self.__class__.__name__ + \"()\"\n",
    "\n",
    "\n",
    "    class SecondOfMinute(TimeFeature):\n",
    "        \"\"\"Minute of hour encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return index.second / 59.0 - 0.5\n",
    "\n",
    "\n",
    "    class MinuteOfHour(TimeFeature):\n",
    "        \"\"\"Minute of hour encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return index.minute / 59.0 - 0.5\n",
    "\n",
    "\n",
    "    class HourOfDay(TimeFeature):\n",
    "        \"\"\"Hour of day encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return index.hour / 23.0 - 0.5\n",
    "\n",
    "\n",
    "    class DayOfWeek(TimeFeature):\n",
    "        \"\"\"Hour of day encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return index.dayofweek / 6.0 - 0.5\n",
    "\n",
    "\n",
    "    class DayOfMonth(TimeFeature):\n",
    "        \"\"\"Day of month encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return (index.day - 1) / 30.0 - 0.5\n",
    "\n",
    "\n",
    "    class DayOfYear(TimeFeature):\n",
    "        \"\"\"Day of year encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return (index.dayofyear - 1) / 365.0 - 0.5\n",
    "\n",
    "\n",
    "    class MonthOfYear(TimeFeature):\n",
    "        \"\"\"Month of year encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return (index.month - 1) / 11.0 - 0.5\n",
    "\n",
    "\n",
    "    class WeekOfYear(TimeFeature):\n",
    "        \"\"\"Week of year encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return (index.isocalendar().week - 1) / 52.0 - 0.5\n",
    "\n",
    "    \n",
    "    features_by_offsets = {\n",
    "        offsets.YearEnd: [],\n",
    "        offsets.QuarterEnd: [MonthOfYear],\n",
    "        offsets.MonthEnd: [MonthOfYear],\n",
    "        offsets.Week: [DayOfMonth, WeekOfYear],\n",
    "        offsets.Day: [DayOfWeek, DayOfMonth, DayOfYear],\n",
    "        offsets.BusinessDay: [DayOfWeek, DayOfMonth, DayOfYear],\n",
    "        offsets.Hour: [HourOfDay, DayOfWeek, DayOfMonth, DayOfYear],\n",
    "        offsets.Minute: [\n",
    "            MinuteOfHour,\n",
    "            HourOfDay,\n",
    "            DayOfWeek,\n",
    "            DayOfMonth,\n",
    "            DayOfYear,\n",
    "        ],\n",
    "        offsets.Second: [\n",
    "            SecondOfMinute,\n",
    "            MinuteOfHour,\n",
    "            HourOfDay,\n",
    "            DayOfWeek,\n",
    "            DayOfMonth,\n",
    "            DayOfYear,\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    offset = to_offset(freq_str)\n",
    "\n",
    "    for offset_type, feature_classes in features_by_offsets.items():\n",
    "        if isinstance(offset, offset_type):\n",
    "            return [cls() for cls in feature_classes]\n",
    "\n",
    "    supported_freq_msg = f\"\"\"\n",
    "    Unsupported frequency {freq_str}\n",
    "    The following frequencies are supported:\n",
    "        Y   - yearly\n",
    "            alias: A\n",
    "        M   - monthly\n",
    "        W   - weekly\n",
    "        D   - daily\n",
    "        B   - business days\n",
    "        H   - hourly\n",
    "        T   - minutely\n",
    "            alias: min\n",
    "        S   - secondly\n",
    "    \"\"\"\n",
    "    raise RuntimeError(supported_freq_msg)\n",
    "    \n",
    "def time_features(dates, freq='h'):\n",
    "    return np.vstack([feat(dates) for feat in time_features_from_frequency_str(freq)])\n",
    "\n",
    "# 包含时间维度的数据集划分\n",
    "def divide_dataset(df, valid_date, test_date, x_feature_list, y_feature_list, freq):\n",
    "    #归一化\n",
    "    x_scaler = MinMaxScaler()\n",
    "    y_scaler = MinMaxScaler()\n",
    "\n",
    "    #测试集\n",
    "    train = df.copy()[df.index < valid_date][x_feature_list]\n",
    "    train_stamp = time_features(pd.to_datetime(train.index), freq=freq)\n",
    "    train_stamp = train_stamp.transpose(1, 0)\n",
    "    train[x_feature_list] = x_scaler.fit_transform(train)\n",
    "    xtr = train.values.astype('float32')\n",
    "\n",
    "    #验证集\n",
    "    valid = df.copy()[(df.index >= valid_date) & (df.index < test_date)][x_feature_list]\n",
    "    valid_stamp = time_features(pd.to_datetime(valid.index), freq=freq)\n",
    "    valid_stamp = valid_stamp.transpose(1, 0)\n",
    "    valid[x_feature_list] = x_scaler.fit_transform(valid)\n",
    "    xva = valid.values.astype('float32')\n",
    "\n",
    "    #测试集\n",
    "    test = df.copy()[test_date:][x_feature_list]\n",
    "    test_stamp = time_features(pd.to_datetime(test.index), freq=freq)\n",
    "    test_stamp = test_stamp.transpose(1, 0)\n",
    "    test[x_feature_list] = x_scaler.fit_transform(test)\n",
    "    xte = test.values.astype('float32')\n",
    "\n",
    "    #标签\n",
    "    ytr = df.copy()[df.index < valid_date][y_feature_list]\n",
    "    ytr[y_feature_list] = y_scaler.fit_transform(ytr)\n",
    "    ytr = ytr.values.astype('float32')\n",
    "\n",
    "    yva = df.copy()[(df.index >= valid_date) & (df.index < test_date)][y_feature_list]\n",
    "    yva[y_feature_list] = y_scaler.fit_transform(yva)\n",
    "    yva = yva.values.astype('float32')\n",
    "\n",
    "    yte = df.copy()[test_date:][y_feature_list]\n",
    "    yte[y_feature_list] = y_scaler.fit_transform(yte)\n",
    "    yte = yte.values.astype('float32')\n",
    "    \n",
    "    # 数据合并，[训练集，测试集，时间戳]\n",
    "    train = [xtr, ytr, train_stamp]\n",
    "    valid = [xva, yva, valid_stamp]\n",
    "    test = [xte, yte, test_stamp]\n",
    "    \n",
    "    return y_scaler, train, valid, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ccb51c6a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:29:00.898961Z",
     "start_time": "2024-04-14T13:29:00.829917Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (23376, 2) y_train shape: (23376, 2) stamp_train shape: (23376, 4)\n",
      "x_valid shape: (1464, 2) y_valid shape: (1464, 2) stamp_valid shape: (1464, 4)\n",
      "x_test shape: (1464, 2) y_test shape: (1464, 2) stamp_test shape: (1464, 4)\n"
     ]
    }
   ],
   "source": [
    "# 构造参数字典\n",
    "params1 = {\n",
    "    \"df\": ts_data,\n",
    "    \"valid_date\": \"2014-09-01 00:00:00\",\n",
    "    \"test_date\": \"2014-11-01 00:00:00\",\n",
    "    \"x_feature_list\": [\"load\", 'temp'],\n",
    "    \"y_feature_list\": [\"load\", 'temp'],\n",
    "    \"freq\": 'h'\n",
    "}\n",
    "\n",
    "#函数传参\n",
    "scaler, train, valid, test = divide_dataset(**params1)\n",
    "print(\"x_train shape: {0} y_train shape: {1} stamp_train shape: {2}\".format(train[0].shape, train[1].shape, train[2].shape))\n",
    "print(\"x_valid shape: {0} y_valid shape: {1} stamp_valid shape: {2}\".format(valid[0].shape, valid[1].shape, valid[2].shape))\n",
    "print(\"x_test shape: {0} y_test shape: {1} stamp_test shape: {2}\".format(test[0].shape, test[1].shape, test[2].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f6d84a81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:29:02.003708Z",
     "start_time": "2024-04-14T13:29:01.984850Z"
    }
   },
   "outputs": [],
   "source": [
    "#利用前seq_len个数据，预测下pred_len个数据\n",
    "def create_dataset(data_list, seq_len, pred_len, label_len, batch_size):\n",
    "    \"\"\"\n",
    "    @参数设置：\n",
    "    data_list：特征，目标，时间戳\n",
    "    seq_len：输入数据包含过去多少个时间步\n",
    "    pred_len：目标应该在未来多少个时间步之后\n",
    "    label_len：先验时间步\n",
    "    \"\"\"\n",
    "    # 获取数据\n",
    "    feature = data_list[0] # 特征\n",
    "    target = data_list[1] # 目标\n",
    "    stamp = data_list[2] # 时间戳\n",
    "    \n",
    "    # 循环生成数据\n",
    "    X, y = [], []\n",
    "    X_stamp, y_stamp = [], []\n",
    "    seq_len = seq_len - 1 # 包含当前时间点\n",
    "    for i in range(seq_len, len(feature) - pred_len):\n",
    "        # 数据维度\n",
    "        feat = feature[i - seq_len:i + 1]\n",
    "        tar = target[i + 1:i + 1 + pred_len]\n",
    "        X.append(feat)\n",
    "        y.append(tar)\n",
    "        \n",
    "        # 时间维度\n",
    "        xs = stamp[i - seq_len:i + 1]\n",
    "        ys = stamp[i + 1 - label_len:i + 1 + pred_len]\n",
    "        X_stamp.append(xs)\n",
    "        y_stamp.append(ys)\n",
    "        \n",
    "    # 转为张量，数据维度\n",
    "    X = torch.as_tensor(X).float()\n",
    "    y = torch.as_tensor(y).float()\n",
    "    \n",
    "    # 转为张量，时间维度\n",
    "    X_stamp = torch.as_tensor(X_stamp).float()\n",
    "    y_stamp = torch.as_tensor(y_stamp).float()\n",
    "    \n",
    "    # 创建dataloader，[特征，目标，特征时间编码，目标时间编码]\n",
    "    data_loader = DataLoader(TensorDataset(X, y, X_stamp, y_stamp), shuffle=True, batch_size=batch_size)\n",
    "    \n",
    "    return X, y, data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "8e773fe4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:29:03.993775Z",
     "start_time": "2024-04-14T13:29:03.065331Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_size: torch.Size([23368, 6, 2]),y_size: torch.Size([23368, 3, 2]),loader_len: 731\n",
      "X_size: torch.Size([1456, 6, 2]),y_size: torch.Size([1456, 3, 2]),loader_len: 46\n",
      "X_size: torch.Size([1456, 6, 2]),y_size: torch.Size([1456, 3, 2]),loader_len: 46\n"
     ]
    }
   ],
   "source": [
    "# 构造参数字典\n",
    "params2 = {\n",
    "    \"seq_len\": 6,\n",
    "    \"pred_len\": 3,\n",
    "    \"label_len\": 0,\n",
    "    \"batch_size\": 32,\n",
    "}\n",
    "X_train, y_train, train_loader = create_dataset(train, **params2)\n",
    "X_valid, y_valid, valid_loader = create_dataset(valid, **params2)\n",
    "X_test, y_test, test_loader = create_dataset(test, **params2)\n",
    "print(\"X_size: {0},y_size: {1},loader_len: {2}\".format(X_train.shape, y_train.shape, len(train_loader)))\n",
    "print(\"X_size: {0},y_size: {1},loader_len: {2}\".format(X_valid.shape, y_valid.shape, len(valid_loader)))\n",
    "print(\"X_size: {0},y_size: {1},loader_len: {2}\".format(X_test.shape, y_test.shape, len(test_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0f17fa",
   "metadata": {},
   "source": [
    "### 模型定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d258926d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:29:06.458141Z",
     "start_time": "2024-04-14T13:29:06.282756Z"
    }
   },
   "outputs": [],
   "source": [
    "# DataEmbedding编码类\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model).float()\n",
    "        pe.require_grad = False\n",
    "\n",
    "        position = torch.arange(0, max_len).float().unsqueeze(1)\n",
    "        div_term = (torch.arange(0, d_model, 2).float()\n",
    "                    * -(math.log(10000.0) / d_model)).exp()\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pe[:, :x.size(1)]\n",
    "\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, c_in, d_model):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        padding = 1 if torch.__version__ >= '1.5.0' else 2\n",
    "        self.tokenConv = nn.Conv1d(in_channels=c_in, out_channels=d_model,\n",
    "                                   kernel_size=3, padding=padding, padding_mode='circular', bias=False)\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                nn.init.kaiming_normal_(\n",
    "                    m.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.tokenConv(x.permute(0, 2, 1)).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class FixedEmbedding(nn.Module):\n",
    "    def __init__(self, c_in, d_model):\n",
    "        super(FixedEmbedding, self).__init__()\n",
    "\n",
    "        w = torch.zeros(c_in, d_model).float()\n",
    "        w.require_grad = False\n",
    "\n",
    "        position = torch.arange(0, c_in).float().unsqueeze(1)\n",
    "        div_term = (torch.arange(0, d_model, 2).float()\n",
    "                    * -(math.log(10000.0) / d_model)).exp()\n",
    "\n",
    "        w[:, 0::2] = torch.sin(position * div_term)\n",
    "        w[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        self.emb = nn.Embedding(c_in, d_model)\n",
    "        self.emb.weight = nn.Parameter(w, requires_grad=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.emb(x).detach()\n",
    "\n",
    "\n",
    "class TemporalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, embed_type='fixed', freq='h'):\n",
    "        super(TemporalEmbedding, self).__init__()\n",
    "\n",
    "        minute_size = 4\n",
    "        hour_size = 24\n",
    "        weekday_size = 7\n",
    "        day_size = 32\n",
    "        month_size = 13\n",
    "\n",
    "        Embed = FixedEmbedding if embed_type == 'fixed' else nn.Embedding\n",
    "        if freq == 't':\n",
    "            self.minute_embed = Embed(minute_size, d_model)\n",
    "        self.hour_embed = Embed(hour_size, d_model)\n",
    "        self.weekday_embed = Embed(weekday_size, d_model)\n",
    "        self.day_embed = Embed(day_size, d_model)\n",
    "        self.month_embed = Embed(month_size, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.long()\n",
    "        minute_x = self.minute_embed(x[:, :, 4]) if hasattr(\n",
    "            self, 'minute_embed') else 0.\n",
    "        hour_x = self.hour_embed(x[:, :, 3])\n",
    "        weekday_x = self.weekday_embed(x[:, :, 2])\n",
    "        day_x = self.day_embed(x[:, :, 1])\n",
    "        month_x = self.month_embed(x[:, :, 0])\n",
    "\n",
    "        return hour_x + weekday_x + day_x + month_x + minute_x\n",
    "\n",
    "\n",
    "class TimeFeatureEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, embed_type='timeF', freq='h'):\n",
    "        super(TimeFeatureEmbedding, self).__init__()\n",
    "\n",
    "        freq_map = {'h': 4, 't': 5, 's': 6,\n",
    "                    'm': 1, 'a': 1, 'w': 2, 'd': 3, 'b': 3}\n",
    "        d_inp = freq_map[freq]\n",
    "        self.embed = nn.Linear(d_inp, d_model, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embed(x)\n",
    "\n",
    "\n",
    "class DataEmbedding(nn.Module):\n",
    "    def __init__(self, c_in, d_model, embed_type='fixed', freq='h', dropout=0.1):\n",
    "        super(DataEmbedding, self).__init__()\n",
    "\n",
    "        self.value_embedding = TokenEmbedding(c_in=c_in, d_model=d_model)\n",
    "        self.position_embedding = PositionalEmbedding(d_model=d_model)\n",
    "        self.temporal_embedding = TemporalEmbedding(d_model=d_model, embed_type=embed_type,\n",
    "                                                    freq=freq) if embed_type != 'timeF' else TimeFeatureEmbedding(\n",
    "            d_model=d_model, embed_type=embed_type, freq=freq)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, x_mark):\n",
    "        if x_mark is None:\n",
    "            x = self.value_embedding(x) + self.position_embedding(x)\n",
    "        else:\n",
    "            x = self.value_embedding(x) + self.temporal_embedding(x_mark) + self.position_embedding(x)\n",
    "        return self.dropout(x)\n",
    "    \n",
    "    \n",
    "# Transformer_EncDec类\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, attention, d_model, d_ff=None, dropout=0.1, activation=\"relu\"):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        d_ff = d_ff or 4 * d_model\n",
    "        self.attention = attention\n",
    "        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = F.relu if activation == \"relu\" else F.gelu\n",
    "\n",
    "    def forward(self, x, attn_mask=None, tau=None, delta=None):\n",
    "        new_x, attn = self.attention(\n",
    "            x, x, x,\n",
    "            attn_mask=attn_mask,\n",
    "            tau=tau, delta=delta\n",
    "        )\n",
    "        x = x + self.dropout(new_x)\n",
    "\n",
    "        y = x = self.norm1(x)\n",
    "        y = self.dropout(self.activation(self.conv1(y.transpose(-1, 1))))\n",
    "        y = self.dropout(self.conv2(y).transpose(-1, 1))\n",
    "\n",
    "        return self.norm2(x + y), attn\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, attn_layers, conv_layers=None, norm_layer=None):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.attn_layers = nn.ModuleList(attn_layers)\n",
    "        self.conv_layers = nn.ModuleList(conv_layers) if conv_layers is not None else None\n",
    "        self.norm = norm_layer\n",
    "\n",
    "    def forward(self, x, attn_mask=None, tau=None, delta=None):\n",
    "        # x [B, L, D]\n",
    "        attns = []\n",
    "        if self.conv_layers is not None:\n",
    "            for i, (attn_layer, conv_layer) in enumerate(zip(self.attn_layers, self.conv_layers)):\n",
    "                delta = delta if i == 0 else None\n",
    "                x, attn = attn_layer(x, attn_mask=attn_mask, tau=tau, delta=delta)\n",
    "                x = conv_layer(x)\n",
    "                attns.append(attn)\n",
    "            x, attn = self.attn_layers[-1](x, tau=tau, delta=None)\n",
    "            attns.append(attn)\n",
    "        else:\n",
    "            for attn_layer in self.attn_layers:\n",
    "                x, attn = attn_layer(x, attn_mask=attn_mask, tau=tau, delta=delta)\n",
    "                attns.append(attn)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "\n",
    "        return x, attns\n",
    "    \n",
    "    \n",
    "# 自编码类\n",
    "TOKEN_SELF_ATTN_VALUE = -5e4 # carefully set for half precision to work\n",
    "\n",
    "def max_neg_value(tensor):\n",
    "    return -torch.finfo(tensor.dtype).max\n",
    "\n",
    "\n",
    "def batched_index_select(values, indices):\n",
    "    last_dim = values.shape[-1]\n",
    "    return values.gather(1, indices[:, :, None].expand(-1, -1, last_dim))\n",
    "\n",
    "\n",
    "def sort_key_val(t1, t2, dim=-1):\n",
    "    values, indices = t1.sort(dim=dim)\n",
    "    t2 = t2.expand_as(t1)\n",
    "    return values, t2.gather(dim, indices)\n",
    "\n",
    "\n",
    "def process_inputs_chunk(fn, chunks=1, dim=0):\n",
    "    def inner_fn(*args, **kwargs):\n",
    "        keys, values, len_args = kwargs.keys(), kwargs.values(), len(args)\n",
    "        chunked_args = list(zip(*map(lambda x: x.chunk(chunks, dim=dim), list(args) + list(values))))\n",
    "        all_args = map(lambda x: (x[:len_args], dict(zip(keys, x[len_args:]))), chunked_args)\n",
    "        outputs = [fn(*c_args, **c_kwargs) for c_args, c_kwargs in all_args]\n",
    "        return tuple(map(lambda x: torch.cat(x, dim=dim), zip(*outputs)))\n",
    "    return inner_fn\n",
    "\n",
    "\n",
    "def split_at_index(dim, index, t):\n",
    "    pre_slices = (slice(None),) * dim\n",
    "    l = (*pre_slices, slice(None, index))\n",
    "    r = (*pre_slices, slice(index, None))\n",
    "    return t[l], t[r]\n",
    "\n",
    "\n",
    "def merge_dims(ind_from, ind_to, tensor):\n",
    "    shape = list(tensor.shape)\n",
    "    arr_slice = slice(ind_from, ind_to + 1)\n",
    "    shape[arr_slice] = [functools.reduce(mul, shape[arr_slice])]\n",
    "    return tensor.reshape(*shape)\n",
    "\n",
    "\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "\n",
    "def default(val, default_val):\n",
    "    return default_val if val is None else val\n",
    "\n",
    "\n",
    "def cache_method_decorator(cache_attr, cache_namespace, reexecute = False):\n",
    "    def inner_fn(fn):\n",
    "        @wraps(fn)\n",
    "        def wrapper(self, *args, key_namespace=None, fetch=False, set_cache=True, **kwargs):\n",
    "            namespace_str = str(default(key_namespace, ''))\n",
    "            _cache = getattr(self, cache_attr)\n",
    "            _keyname = f'{cache_namespace}:{namespace_str}'\n",
    "\n",
    "            if fetch:\n",
    "                val = _cache[_keyname]\n",
    "                if reexecute:\n",
    "                    fn(self, *args, **kwargs)\n",
    "            else:\n",
    "                val = fn(self, *args, **kwargs)\n",
    "                if set_cache:\n",
    "                    setattr(self, cache_attr, {**_cache, **{_keyname: val}})\n",
    "            return val\n",
    "        return wrapper\n",
    "    return inner_fn\n",
    "\n",
    "class Always(nn.Module):\n",
    "    def __init__(self, val):\n",
    "        super().__init__()\n",
    "        self.val = val\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        return self.val\n",
    "\n",
    "class MatrixMultiply(nn.Module):\n",
    "    def __init__(self, tensor, transpose = False, normalize = False):\n",
    "        super().__init__()\n",
    "        self.tensor = tensor\n",
    "        self.transpose = transpose\n",
    "        self.normalize = normalize\n",
    "\n",
    "    def forward(self, x):\n",
    "        tensor = self.tensor\n",
    "        if self.normalize:\n",
    "            tensor = F.normalize(tensor, dim=-1)\n",
    "        if self.transpose:\n",
    "            tensor = tensor.t()\n",
    "        return x @ tensor\n",
    "\n",
    "class ReZero(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.g = nn.Parameter(torch.zeros(1))\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(x, **kwargs) * self.g\n",
    "\n",
    "class ScaleNorm(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.g = nn.Parameter(torch.ones(1))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        n = torch.norm(x, dim=-1, keepdim=True).clamp(min=self.eps)\n",
    "        return x / n * self.g\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, norm_class, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = norm_class(dim)\n",
    "        self.fn = fn\n",
    "    def forward(self, x, **kwargs):\n",
    "        x = self.norm(x)\n",
    "        return self.fn(x, **kwargs)\n",
    "\n",
    "class Chunk(nn.Module):\n",
    "    def __init__(self, chunks, fn, along_dim = -1):\n",
    "        super().__init__()\n",
    "        self.dim = along_dim\n",
    "        self.chunks = chunks\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        if self.chunks == 1:\n",
    "            return self.fn(x, **kwargs)\n",
    "        chunks = x.chunk(self.chunks, dim = self.dim)\n",
    "        return torch.cat([self.fn(c, **kwargs) for c in chunks], dim = self.dim)\n",
    "\n",
    "# LSH attention as described in https://openreview.net/pdf?id=rkgNKkHtvB\n",
    "# adapted from trax, stripped to what paper said needed to work\n",
    "# namely that buckets need to be at least 64 with 8 rounds of hashing\n",
    "# https://github.com/google/trax/blob/master/trax/layers/research/efficient_attention.py#L442\n",
    "\n",
    "class LSHAttention(nn.Module):\n",
    "    def __init__( self,\n",
    "                  dropout = 0.,\n",
    "                  bucket_size = 64,\n",
    "                  n_hashes = 8,\n",
    "                  causal = False,\n",
    "                  allow_duplicate_attention = True,\n",
    "                  attend_across_buckets = True,\n",
    "                  rehash_each_round = True,\n",
    "                  drop_for_hash_rate = 0.0,\n",
    "                  random_rotations_per_head = False,\n",
    "                  return_attn = False):\n",
    "        super().__init__()\n",
    "        if dropout >= 1.0:\n",
    "            raise ValueError('Dropout rates must be lower than 1.')\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.dropout_for_hash = nn.Dropout(drop_for_hash_rate)\n",
    "\n",
    "        assert rehash_each_round or allow_duplicate_attention, (\n",
    "            'The setting {allow_duplicate_attention=False, rehash_each_round=False}'\n",
    "            ' is not implemented.')\n",
    "\n",
    "        self.causal = causal\n",
    "        self.bucket_size = bucket_size\n",
    "\n",
    "        self.n_hashes = n_hashes\n",
    "\n",
    "        self._allow_duplicate_attention = allow_duplicate_attention\n",
    "        self._attend_across_buckets = attend_across_buckets\n",
    "        self._rehash_each_round = rehash_each_round\n",
    "        self._random_rotations_per_head = random_rotations_per_head\n",
    "\n",
    "        # will expend extra computation to return attention matrix\n",
    "        self._return_attn = return_attn\n",
    "\n",
    "        # cache buckets for reversible network, reported by authors to make Reformer work at depth\n",
    "        self._cache = {}\n",
    "\n",
    "    @cache_method_decorator('_cache', 'buckets', reexecute=True)\n",
    "    def hash_vectors(self, n_buckets, vecs):\n",
    "        batch_size = vecs.shape[0]\n",
    "        device = vecs.device\n",
    "\n",
    "        # See https://arxiv.org/pdf/1509.02897.pdf\n",
    "        # We sample a different random rotation for each round of hashing to\n",
    "        # decrease the probability of hash misses.\n",
    "        assert n_buckets % 2 == 0\n",
    "\n",
    "        rot_size = n_buckets\n",
    "\n",
    "        rotations_shape = (\n",
    "            batch_size if self._random_rotations_per_head else 1,\n",
    "            vecs.shape[-1],\n",
    "            self.n_hashes if self._rehash_each_round else 1,\n",
    "            rot_size // 2)\n",
    "\n",
    "        random_rotations = torch.randn(rotations_shape, dtype=vecs.dtype, device=device).expand(batch_size, -1, -1, -1)\n",
    "\n",
    "        dropped_vecs = self.dropout_for_hash(vecs)\n",
    "        rotated_vecs = torch.einsum('btf,bfhi->bhti', dropped_vecs, random_rotations)\n",
    "\n",
    "        if self._rehash_each_round:\n",
    "            # rotated_vectors size [batch,n_hash,seq_len,buckets]\n",
    "            rotated_vecs = torch.cat([rotated_vecs, -rotated_vecs], dim=-1)\n",
    "            buckets = torch.argmax(rotated_vecs, dim=-1)\n",
    "        else:\n",
    "            rotated_vecs = torch.cat([rotated_vecs, -rotated_vecs], dim=-1)\n",
    "            # In this configuration, we map each item to the top self.n_hashes buckets\n",
    "            rotated_vecs = torch.squeeze(rotated_vecs, 1)\n",
    "            bucket_range = torch.arange(rotated_vecs.shape[-1], device=device)\n",
    "            bucket_range = torch.reshape(bucket_range, (1, -1))\n",
    "            bucket_range = bucket_range.expand_as(rotated_vecs)\n",
    "\n",
    "            _, buckets = sort_key_val(rotated_vecs, bucket_range, dim=-1)\n",
    "            # buckets size [batch size, seq_len, buckets]\n",
    "            buckets = buckets[... , -self.n_hashes:].transpose(1, 2)\n",
    "\n",
    "        # buckets is now (self.n_hashes, seq_len). Next we add offsets so that\n",
    "        # bucket numbers from different hashing rounds don't overlap.\n",
    "        offsets = torch.arange(self.n_hashes, device=device)\n",
    "        offsets = torch.reshape(offsets * n_buckets, (1, -1, 1))\n",
    "        buckets = torch.reshape(buckets + offsets, (batch_size, -1,))\n",
    "        return buckets\n",
    "\n",
    "    def forward(self, qk, v, query_len = None, input_mask = None, input_attn_mask = None, pos_emb = None, **kwargs):\n",
    "        batch_size, seqlen, dim, device = *qk.shape, qk.device\n",
    "\n",
    "        query_len = default(query_len, seqlen)\n",
    "        is_reverse = kwargs.pop('_reverse', False)\n",
    "        depth = kwargs.pop('_depth', None)\n",
    "\n",
    "        assert seqlen % (self.bucket_size * 2) == 0, f'Sequence length ({seqlen}) needs to be divisible by target bucket size  x 2 - {self.bucket_size * 2}'\n",
    "\n",
    "        n_buckets = seqlen // self.bucket_size\n",
    "        buckets = self.hash_vectors(n_buckets, qk, key_namespace=depth, fetch=is_reverse, set_cache=self.training)\n",
    "\n",
    "        # We use the same vector as both a query and a key.\n",
    "        assert int(buckets.shape[1]) == self.n_hashes * seqlen\n",
    "\n",
    "        total_hashes = self.n_hashes\n",
    "\n",
    "        ticker = torch.arange(total_hashes * seqlen, device=device).unsqueeze(0).expand_as(buckets)\n",
    "        buckets_and_t = seqlen * buckets + (ticker % seqlen)\n",
    "        buckets_and_t = buckets_and_t.detach()\n",
    "\n",
    "        # Hash-based sort (\"s\" at the start of variable names means \"sorted\")\n",
    "        sbuckets_and_t, sticker = sort_key_val(buckets_and_t, ticker, dim=-1)\n",
    "        _, undo_sort = sticker.sort(dim=-1)\n",
    "        del ticker\n",
    "\n",
    "        sbuckets_and_t = sbuckets_and_t.detach()\n",
    "        sticker = sticker.detach()\n",
    "        undo_sort = undo_sort.detach()\n",
    "\n",
    "        if exists(pos_emb):\n",
    "            qk = apply_rotary_pos_emb(qk, pos_emb)\n",
    "\n",
    "        st = (sticker % seqlen)\n",
    "        sqk = batched_index_select(qk, st)\n",
    "        sv = batched_index_select(v, st)\n",
    "\n",
    "        # Split off a \"bin\" axis so that attention only occurs within chunks.\n",
    "        chunk_size = total_hashes * n_buckets\n",
    "        bq_t = bkv_t = torch.reshape(st, (batch_size, chunk_size, -1))\n",
    "        bqk = torch.reshape(sqk, (batch_size, chunk_size, -1, dim))\n",
    "        bv = torch.reshape(sv, (batch_size, chunk_size, -1, dim))\n",
    "\n",
    "        # Hashing operates on unit-length vectors. Unnormalized query vectors are\n",
    "        # fine because they effectively provide a learnable temperature for the\n",
    "        # attention softmax, but normalizing keys is needed so that similarity for\n",
    "        # the purposes of attention correctly corresponds to hash locality.\n",
    "        bq = bqk\n",
    "        bk = F.normalize(bqk, p=2, dim=-1).type_as(bq)\n",
    "\n",
    "        # Allow each chunk to attend within itself, and also one chunk back. Chunk\n",
    "        # boundaries might occur in the middle of a sequence of items from the\n",
    "        # same bucket, so this increases the chances of attending to relevant items.\n",
    "        def look_one_back(x):\n",
    "            x_extra = torch.cat([x[:, -1:, ...], x[:, :-1, ...]], dim=1)\n",
    "            return torch.cat([x, x_extra], dim=2)\n",
    "\n",
    "        bk = look_one_back(bk)\n",
    "        bv = look_one_back(bv)\n",
    "        bkv_t = look_one_back(bkv_t)\n",
    "\n",
    "        # Dot-product attention.\n",
    "        dots = torch.einsum('bhie,bhje->bhij', bq, bk) * (dim ** -0.5)\n",
    "        masked_value = max_neg_value(dots)\n",
    "\n",
    "        # Mask for post qk attention logits of the input sequence\n",
    "        if input_attn_mask is not None:\n",
    "            input_attn_mask = F.pad(input_attn_mask, (0, seqlen - input_attn_mask.shape[-1], 0, seqlen - input_attn_mask.shape[-2]), value=True)\n",
    "            dot_attn_indices = ((bq_t * seqlen)[:, :, :, None] + bkv_t[:, :, None, :])\n",
    "            input_attn_mask = input_attn_mask.reshape(batch_size, -1)\n",
    "            dot_attn_indices = dot_attn_indices.reshape(batch_size, -1)\n",
    "            mask = input_attn_mask.gather(1, dot_attn_indices).reshape_as(dots)\n",
    "            dots.masked_fill_(~mask, masked_value)\n",
    "            del mask\n",
    "\n",
    "        # Input mask for padding in variable lengthed sequences\n",
    "        if input_mask is not None:\n",
    "            input_mask = F.pad(input_mask, (0, seqlen - input_mask.shape[1]), value=True)\n",
    "            mq = input_mask.gather(1, st).reshape((batch_size, chunk_size, -1))\n",
    "            mkv = look_one_back(mq)\n",
    "            mask = mq[:, :, :, None] * mkv[:, :, None, :]\n",
    "            dots.masked_fill_(~mask, masked_value)\n",
    "            del mask\n",
    "\n",
    "        # Causal masking\n",
    "        if self.causal:\n",
    "            mask = bq_t[:, :, :, None] < bkv_t[:, :, None, :]\n",
    "            if seqlen > query_len:\n",
    "                mask = mask & (bkv_t[:, :, None, :] < query_len)\n",
    "            dots.masked_fill_(mask, masked_value)\n",
    "            del mask\n",
    "\n",
    "        # Mask out attention to self except when no other targets are available.\n",
    "        self_mask = bq_t[:, :, :, None] == bkv_t[:, :, None, :]\n",
    "        dots.masked_fill_(self_mask, TOKEN_SELF_ATTN_VALUE)\n",
    "        del self_mask\n",
    "\n",
    "        # Mask out attention to other hash buckets.\n",
    "        if not self._attend_across_buckets:\n",
    "            bq_buckets = bkv_buckets = torch.reshape(sbuckets_and_t // seqlen, (batch_size, chunk_size, -1))\n",
    "            bkv_buckets = look_one_back(bkv_buckets)\n",
    "            bucket_mask = bq_buckets[:, :, :, None] != bkv_buckets[:, :, None, :]\n",
    "            dots.masked_fill_(bucket_mask, masked_value)\n",
    "            del bucket_mask\n",
    "\n",
    "        # Don't double-count query-key pairs across multiple rounds of hashing.\n",
    "        # There are two possible strategies here. (1) The default is to count how\n",
    "        # many times a query-key pair is repeated, and to lower its log-prob\n",
    "        # correspondingly at each repetition. (2) When hard_k is set, the code\n",
    "        # instead masks all but the first occurence of each query-key pair.\n",
    "        if not self._allow_duplicate_attention:\n",
    "            locs1 = undo_sort // bq_t.shape[-1]\n",
    "            locs2 = (locs1 + 1) % chunk_size\n",
    "            if not self._attend_across_buckets:\n",
    "                locs1 = buckets * chunk_size + locs1\n",
    "                locs2 = buckets * chunk_size + locs2\n",
    "            locs = torch.cat([\n",
    "                torch.reshape(locs1, (batch_size, total_hashes, seqlen)),\n",
    "                torch.reshape(locs2, (batch_size, total_hashes, seqlen)),\n",
    "            ], 1).permute((0, 2, 1))\n",
    "\n",
    "            slocs = batched_index_select(locs, st)\n",
    "            b_locs = torch.reshape(slocs, (batch_size, chunk_size, -1, 2 * total_hashes))\n",
    "\n",
    "            b_locs1 = b_locs[:, :, :, None, :total_hashes]\n",
    "\n",
    "            bq_locs = b_locs1.expand(b_locs.shape[:3] + (2, total_hashes))\n",
    "            bq_locs = torch.reshape(bq_locs, b_locs.shape)\n",
    "            bkv_locs = look_one_back(b_locs)\n",
    "\n",
    "            dup_counts = (bq_locs[:, :, :, None, :] == bkv_locs[:, :, None, :, :])\n",
    "            # for memory considerations, chunk summation of last dimension for counting duplicates\n",
    "            dup_counts = chunked_sum(dup_counts, chunks=(total_hashes * batch_size))\n",
    "            dup_counts = dup_counts.detach()\n",
    "            assert dup_counts.shape == dots.shape\n",
    "            dots = dots - torch.log(dup_counts + 1e-9)\n",
    "            del dup_counts\n",
    "\n",
    "        # Softmax.\n",
    "        dots_logsumexp = torch.logsumexp(dots, dim=-1, keepdim=True)\n",
    "        dots = torch.exp(dots - dots_logsumexp).type_as(dots)\n",
    "        dropped_dots = self.dropout(dots)\n",
    "\n",
    "        bo = torch.einsum('buij,buje->buie', dropped_dots, bv)\n",
    "        so = torch.reshape(bo, (batch_size, -1, dim))\n",
    "        slogits = torch.reshape(dots_logsumexp, (batch_size, -1,))\n",
    "\n",
    "        # unsort logits\n",
    "        o = batched_index_select(so, undo_sort)\n",
    "        logits = slogits.gather(1, undo_sort)\n",
    "\n",
    "        o = torch.reshape(o, (batch_size, total_hashes, seqlen, dim))\n",
    "        logits = torch.reshape(logits, (batch_size, total_hashes, seqlen, 1))\n",
    "\n",
    "        if query_len != seqlen:\n",
    "            query_slice = (slice(None), slice(None), slice(0, query_len))\n",
    "            o, logits = o[query_slice], logits[query_slice]\n",
    "\n",
    "        probs = torch.exp(logits - torch.logsumexp(logits, dim=1, keepdim=True))\n",
    "        out = torch.sum(o * probs, dim=1)\n",
    "\n",
    "        attn = torch.empty(0, device=device)\n",
    "\n",
    "        # return unsorted attention weights\n",
    "        if self._return_attn:\n",
    "            attn_unsort = ((bq_t * seqlen)[:, :, :, None] + bkv_t[:, :, None, :])\n",
    "            attn_unsort = attn_unsort.view(batch_size * total_hashes, -1).long()\n",
    "            unsorted_dots = torch.zeros(batch_size * total_hashes, seqlen * seqlen, device=device)\n",
    "            unsorted_dots.scatter_add_(1, attn_unsort, dots.view_as(attn_unsort))\n",
    "            del attn_unsort\n",
    "            unsorted_dots = unsorted_dots.reshape(batch_size, total_hashes, seqlen, seqlen)\n",
    "            attn = torch.sum(unsorted_dots[:, :, 0:query_len, :] * probs, dim=1)\n",
    "\n",
    "        # return output, attention matrix, and bucket distribution\n",
    "        return out, attn, buckets\n",
    "\n",
    "\n",
    "class FullQKAttention(nn.Module):\n",
    "    def __init__(self, causal = False, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.causal = causal\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, qk, v, query_len = None, input_mask = None, input_attn_mask = None, **kwargs):\n",
    "        b, seq_len, dim = qk.shape\n",
    "        query_len = default(query_len, seq_len)\n",
    "        t = query_len\n",
    "\n",
    "        q = qk[:, 0:query_len]\n",
    "        qk = F.normalize(qk, 2, dim=-1).type_as(q)\n",
    "\n",
    "        dot = torch.einsum('bie,bje->bij', q, qk) * (dim ** -0.5)\n",
    "\n",
    "        # qk attention requires tokens not attend to self\n",
    "        i = torch.arange(t)\n",
    "        dot[:, i, i] = TOKEN_SELF_ATTN_VALUE\n",
    "        masked_value = max_neg_value(dot)\n",
    "\n",
    "        # Input mask for padding in variable lengthed sequences\n",
    "        if input_mask is not None:\n",
    "            mask = input_mask[:, 0:query_len, None] * input_mask[:, None, :]\n",
    "            mask = F.pad(mask, (0, seq_len - mask.shape[-1]), value=True)\n",
    "            dot.masked_fill_(~mask, masked_value)\n",
    "\n",
    "        # Mask for post qk attention logits of the input sequence\n",
    "        if input_attn_mask is not None:\n",
    "            input_attn_mask = F.pad(input_attn_mask, (0, seq_len - input_attn_mask.shape[-1]), value=True)\n",
    "            dot.masked_fill_(~input_attn_mask, masked_value)\n",
    "\n",
    "        if self.causal:\n",
    "            i, j = torch.triu_indices(t, t, 1)\n",
    "            dot[:, i, j] = masked_value\n",
    "\n",
    "        dot = dot.softmax(dim=-1)\n",
    "        dot = self.dropout(dot)\n",
    "\n",
    "        out = torch.einsum('bij,bje->bie', dot, v)\n",
    "\n",
    "        return out, dot, torch.empty(0)\n",
    "\n",
    "    \n",
    "class LocalAttention(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        window_size,\n",
    "        causal = False,\n",
    "        look_backward = 1,\n",
    "        look_forward = None,\n",
    "        dropout = 0.,\n",
    "        shared_qk = False,\n",
    "        rel_pos_emb_config = None,\n",
    "        dim = None,\n",
    "        autopad = False,\n",
    "        exact_windowsize = False,\n",
    "        scale = None,\n",
    "        use_rotary_pos_emb = True,\n",
    "        use_xpos = False,\n",
    "        xpos_scale_base = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "        look_forward = default(look_forward, 0 if causal else 1)\n",
    "        assert not (causal and look_forward > 0), 'you cannot look forward if causal'\n",
    "\n",
    "        self.scale = scale\n",
    "\n",
    "        self.window_size = window_size\n",
    "        self.autopad = autopad\n",
    "        self.exact_windowsize = exact_windowsize\n",
    "\n",
    "        self.causal = causal\n",
    "\n",
    "        self.look_backward = look_backward\n",
    "        self.look_forward = look_forward\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.shared_qk = shared_qk\n",
    "\n",
    "        # relative positions\n",
    "\n",
    "        self.rel_pos = None\n",
    "        self.use_xpos = use_xpos\n",
    "\n",
    "        if use_rotary_pos_emb and (exists(rel_pos_emb_config) or exists(dim)):  # backwards compatible with old `rel_pos_emb_config` deprecated argument\n",
    "            if exists(rel_pos_emb_config):\n",
    "                dim = rel_pos_emb_config[0]\n",
    "\n",
    "            self.rel_pos = SinusoidalEmbeddings(\n",
    "                dim,\n",
    "                use_xpos = use_xpos,\n",
    "                scale_base = default(xpos_scale_base, window_size // 2)\n",
    "            )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        q, k, v,\n",
    "        mask = None,\n",
    "        input_mask = None,\n",
    "        attn_bias = None,\n",
    "        window_size = None\n",
    "    ):\n",
    "\n",
    "        mask = default(mask, input_mask)\n",
    "\n",
    "        assert not (exists(window_size) and not self.use_xpos), 'cannot perform window size extrapolation if xpos is not turned on'\n",
    "\n",
    "        shape, autopad, pad_value, window_size, causal, look_backward, look_forward, shared_qk = q.shape, self.autopad, -1, default(window_size, self.window_size), self.causal, self.look_backward, self.look_forward, self.shared_qk\n",
    "\n",
    "        # https://github.com/arogozhnikov/einops/blob/master/docs/4-pack-and-unpack.ipynb\n",
    "        (q, packed_shape), (k, _), (v, _) = map(lambda t: pack([t], '* n d'), (q, k, v))\n",
    "\n",
    "        # auto padding\n",
    "\n",
    "        if autopad:\n",
    "            orig_seq_len = q.shape[1]\n",
    "            (needed_pad, q), (_, k), (_, v) = map(lambda t: pad_to_multiple(t, self.window_size, dim = -2), (q, k, v))\n",
    "\n",
    "        b, n, dim_head, device, dtype = *q.shape, q.device, q.dtype\n",
    "\n",
    "        scale = default(self.scale, dim_head ** -0.5)\n",
    "\n",
    "        assert (n % window_size) == 0, f'sequence length {n} must be divisible by window size {window_size} for local attention'\n",
    "\n",
    "        windows = n // window_size\n",
    "\n",
    "        if shared_qk:\n",
    "            k = l2norm(k)\n",
    "\n",
    "        seq = torch.arange(n, device = device)\n",
    "        b_t = rearrange(seq, '(w n) -> 1 w n', w = windows, n = window_size)\n",
    "\n",
    "        # bucketing\n",
    "\n",
    "        bq, bk, bv = map(lambda t: rearrange(t, 'b (w n) d -> b w n d', w = windows), (q, k, v))\n",
    "\n",
    "        bq = bq * scale\n",
    "\n",
    "        look_around_kwargs = dict(\n",
    "            backward =  look_backward,\n",
    "            forward =  look_forward,\n",
    "            pad_value = pad_value\n",
    "        )\n",
    "\n",
    "        bk = look_around(bk, **look_around_kwargs)\n",
    "        bv = look_around(bv, **look_around_kwargs)\n",
    "\n",
    "        # rotary embeddings\n",
    "\n",
    "        if exists(self.rel_pos):\n",
    "            pos_emb, xpos_scale = self.rel_pos(bk)\n",
    "            bq, bk = apply_rotary_pos_emb(bq, bk, pos_emb, scale = xpos_scale)\n",
    "\n",
    "        # calculate positions for masking\n",
    "\n",
    "        bq_t = b_t\n",
    "        bq_k = look_around(b_t, **look_around_kwargs)\n",
    "\n",
    "        bq_t = rearrange(bq_t, '... i -> ... i 1')\n",
    "        bq_k = rearrange(bq_k, '... j -> ... 1 j')\n",
    "\n",
    "        pad_mask = bq_k == pad_value\n",
    "\n",
    "        sim = einsum('b h i e, b h j e -> b h i j', bq, bk)\n",
    "\n",
    "        if exists(attn_bias):\n",
    "            heads = attn_bias.shape[0]\n",
    "            assert (b % heads) == 0\n",
    "\n",
    "            attn_bias = repeat(attn_bias, 'h i j -> (b h) 1 i j', b = b // heads)\n",
    "            sim = sim + attn_bias\n",
    "\n",
    "        mask_value = max_neg_value(sim)\n",
    "\n",
    "        if shared_qk:\n",
    "            self_mask = bq_t == bq_k\n",
    "            sim = sim.masked_fill(self_mask, TOKEN_SELF_ATTN_VALUE)\n",
    "            del self_mask\n",
    "\n",
    "        if causal:\n",
    "            causal_mask = bq_t < bq_k\n",
    "\n",
    "            if self.exact_windowsize:\n",
    "                max_causal_window_size = (self.window_size * self.look_backward)\n",
    "                causal_mask = causal_mask | (bq_t > (bq_k + max_causal_window_size))\n",
    "\n",
    "            sim = sim.masked_fill(causal_mask, mask_value)\n",
    "            del causal_mask\n",
    "\n",
    "        # masking out for exact window size for non-causal\n",
    "        # as well as masking out for padding value\n",
    "\n",
    "        if not causal and self.exact_windowsize:\n",
    "            max_backward_window_size = (self.window_size * self.look_backward)\n",
    "            max_forward_window_size = (self.window_size * self.look_forward)\n",
    "            window_mask = ((bq_k - max_forward_window_size) > bq_t) | (bq_t > (bq_k + max_backward_window_size)) | pad_mask\n",
    "            sim = sim.masked_fill(window_mask, mask_value)\n",
    "        else:\n",
    "            sim = sim.masked_fill(pad_mask, mask_value)\n",
    "\n",
    "        # take care of key padding mask passed in\n",
    "\n",
    "        if exists(mask):\n",
    "            batch = mask.shape[0]\n",
    "            assert (b % batch) == 0\n",
    "\n",
    "            h = b // mask.shape[0]\n",
    "\n",
    "            if autopad:\n",
    "                _, mask = pad_to_multiple(mask, window_size, dim = -1, value = False)\n",
    "\n",
    "            mask = rearrange(mask, '... (w n) -> (...) w n', w = windows, n = window_size)\n",
    "            mask = look_around(mask, **{**look_around_kwargs, 'pad_value': False})\n",
    "            mask = rearrange(mask, '... j -> ... 1 j')\n",
    "            mask = repeat(mask, 'b ... -> (b h) ...', h = h)\n",
    "            sim = sim.masked_fill(~mask, mask_value)\n",
    "            del mask\n",
    "\n",
    "        # attention\n",
    "\n",
    "        attn = sim.softmax(dim = -1)\n",
    "        attn = self.dropout(attn)\n",
    "\n",
    "        # aggregation\n",
    "\n",
    "        out = einsum('b h i j, b h j e -> b h i e', attn, bv)\n",
    "        out = rearrange(out, 'b w n d -> b (w n) d')\n",
    "\n",
    "        if autopad:\n",
    "            out = out[:, :orig_seq_len, :]\n",
    "\n",
    "        out, *_ = unpack(out, packed_shape, '* n d')\n",
    "        return out\n",
    "\n",
    "\n",
    "class LSHSelfAttention(nn.Module):\n",
    "    def __init__(self, dim, heads = 8, bucket_size = 64, n_hashes = 8, causal = False, dim_head = None, attn_chunks = 1, random_rotations_per_head = False, attend_across_buckets = True, allow_duplicate_attention = True, num_mem_kv = 0, one_value_head = False, use_full_attn = False, full_attn_thres = None, return_attn = False, post_attn_dropout = 0., dropout = 0., n_local_attn_heads = 0, **kwargs):\n",
    "        super().__init__()\n",
    "        assert dim_head or (dim % heads) == 0, 'dimensions must be divisible by number of heads'\n",
    "        assert n_local_attn_heads < heads, 'local attention heads must be less than number of heads'\n",
    "\n",
    "        dim_head = default(dim_head, dim // heads)\n",
    "        dim_heads = dim_head * heads\n",
    "\n",
    "        self.dim = dim\n",
    "        self.heads = heads\n",
    "        self.dim_head = dim_head\n",
    "        self.attn_chunks = default(attn_chunks, 1)\n",
    "\n",
    "        self.v_head_repeats = (heads if one_value_head else 1)\n",
    "        v_dim = dim_heads // self.v_head_repeats\n",
    "\n",
    "        self.toqk = nn.Linear(dim, dim_heads, bias = False)\n",
    "        self.tov = nn.Linear(dim, v_dim, bias = False)\n",
    "        self.to_out = nn.Linear(dim_heads, dim)\n",
    "\n",
    "        self.bucket_size = bucket_size\n",
    "        self.lsh_attn = LSHAttention(bucket_size=bucket_size, n_hashes=n_hashes, causal=causal, random_rotations_per_head=random_rotations_per_head, attend_across_buckets = attend_across_buckets,  allow_duplicate_attention = allow_duplicate_attention, return_attn = return_attn, dropout = dropout, **kwargs)\n",
    "        self.full_attn = FullQKAttention(causal=causal, dropout=dropout)\n",
    "        self.post_attn_dropout = nn.Dropout(post_attn_dropout)\n",
    "\n",
    "        self.use_full_attn = use_full_attn\n",
    "        self.full_attn_thres = default(full_attn_thres, bucket_size)\n",
    "\n",
    "        self.num_mem_kv = num_mem_kv\n",
    "        self.mem_kv = nn.Parameter(torch.randn(1, num_mem_kv, dim, requires_grad=True)) if num_mem_kv > 0 else None\n",
    "\n",
    "        self.n_local_attn_heads = n_local_attn_heads\n",
    "        self.local_attn = LocalAttention(window_size=bucket_size * 2, causal=causal, dropout=dropout, shared_qk=True, look_forward=(1 if not causal else 0))\n",
    "\n",
    "        self.callback = None\n",
    "\n",
    "    def forward(self, x, keys = None, input_mask = None, input_attn_mask = None, context_mask = None, pos_emb = None, **kwargs):\n",
    "        device, dtype = x.device, x.dtype\n",
    "        b, t, e, h, dh, m, l_h = *x.shape, self.heads, self.dim_head, self.num_mem_kv, self.n_local_attn_heads\n",
    "\n",
    "        mem_kv = default(self.mem_kv, torch.empty(b, 0, e, dtype=dtype, device=device))\n",
    "        mem = mem_kv.expand(b, m, -1)\n",
    "\n",
    "        keys = default(keys, torch.empty(b, 0, e, dtype=dtype, device=device))\n",
    "        c = keys.shape[1]\n",
    "\n",
    "        kv_len = t + m + c\n",
    "        use_full_attn = self.use_full_attn or kv_len <= self.full_attn_thres\n",
    "\n",
    "        x = torch.cat((x, mem, keys), dim=1)\n",
    "        qk = self.toqk(x)\n",
    "        v = self.tov(x)\n",
    "        v = v.repeat(1, 1, self.v_head_repeats)\n",
    "\n",
    "        def merge_heads(v):\n",
    "            return v.view(b, kv_len, h, -1).transpose(1, 2)\n",
    "\n",
    "        def split_heads(v):\n",
    "            return v.view(b, h, t, -1).transpose(1, 2).contiguous()\n",
    "\n",
    "        merge_batch_and_heads = partial(merge_dims, 0, 1)\n",
    "\n",
    "        qk, v = map(merge_heads, (qk, v))\n",
    "\n",
    "        has_local = l_h > 0\n",
    "        lsh_h = h - l_h\n",
    "\n",
    "        split_index_fn = partial(split_at_index, 1, l_h)\n",
    "        (lqk, qk), (lv, v) = map(split_index_fn, (qk, v))\n",
    "        lqk, qk, lv, v = map(merge_batch_and_heads, (lqk, qk, lv, v))\n",
    "\n",
    "        masks = {}\n",
    "        if input_mask is not None or context_mask is not None:\n",
    "            default_mask = torch.tensor([True], device=device)\n",
    "            i_mask = default(input_mask, default_mask.expand(b, t))\n",
    "            m_mask = default_mask.expand(b, m)\n",
    "            c_mask = default(context_mask, default_mask.expand(b, c))\n",
    "            mask = torch.cat((i_mask, m_mask, c_mask), dim=1)\n",
    "            mask = merge_batch_and_heads(expand_dim(1, lsh_h, mask))\n",
    "            masks['input_mask'] = mask\n",
    "\n",
    "        if input_attn_mask is not None:\n",
    "            input_attn_mask = merge_batch_and_heads(expand_dim(1, lsh_h, input_attn_mask))\n",
    "            masks['input_attn_mask'] = input_attn_mask\n",
    "\n",
    "        attn_fn = self.lsh_attn if not use_full_attn else self.full_attn\n",
    "        partial_attn_fn = partial(attn_fn, query_len = t, pos_emb = pos_emb, **kwargs)\n",
    "        attn_fn_in_chunks = process_inputs_chunk(partial_attn_fn, chunks = self.attn_chunks)\n",
    "\n",
    "        out, attn, buckets = attn_fn_in_chunks(qk, v, **masks)\n",
    "\n",
    "        if self.callback is not None:\n",
    "            self.callback(attn.reshape(b, lsh_h, t, -1), buckets.reshape(b, lsh_h, -1))\n",
    "\n",
    "        if has_local:\n",
    "            lqk, lv = lqk[:, :t], lv[:, :t]\n",
    "            local_out = self.local_attn(lqk, lqk, lv, input_mask=input_mask)\n",
    "            local_out = local_out.reshape(b, l_h, t, -1)\n",
    "            out = out.reshape(b, lsh_h, t, -1)\n",
    "            out = torch.cat((local_out, out), dim=1)\n",
    "\n",
    "        out = split_heads(out).view(b, t, -1)\n",
    "        out = self.to_out(out)\n",
    "        return self.post_attn_dropout(out)\n",
    "\n",
    "\n",
    "class ReformerLayer(nn.Module):\n",
    "    def __init__(self, attention, d_model, n_heads, d_keys=None,\n",
    "                 d_values=None, causal=False, bucket_size=4, n_hashes=4):\n",
    "        super().__init__()\n",
    "        self.bucket_size = bucket_size\n",
    "        self.attn = LSHSelfAttention(\n",
    "            dim=d_model,\n",
    "            heads=n_heads,\n",
    "            bucket_size=bucket_size,\n",
    "            n_hashes=n_hashes,\n",
    "            causal=causal\n",
    "        )\n",
    "\n",
    "    def fit_length(self, queries):\n",
    "        # inside reformer: assert N % (bucket_size * 2) == 0\n",
    "        B, N, C = queries.shape\n",
    "        if N % (self.bucket_size * 2) == 0:\n",
    "            return queries\n",
    "        else:\n",
    "            # fill the time series\n",
    "            fill_len = (self.bucket_size * 2) - (N % (self.bucket_size * 2))\n",
    "            return torch.cat([queries, torch.zeros([B, fill_len, C]).to(queries.device)], dim=1)\n",
    "\n",
    "    def forward(self, queries, keys, values, attn_mask, tau, delta):\n",
    "        # in Reformer: defalut queries=keys\n",
    "        B, N, C = queries.shape\n",
    "        queries = self.attn(self.fit_length(queries))[:, :N, :]\n",
    "        return queries, None\n",
    "\n",
    "    \n",
    "# Reformer模型\n",
    "class Reformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Reformer with O(LlogL) complexity\n",
    "    \"\"\"\n",
    "    def __init__(self, task_name, pred_len, seq_len, label_len, enc_in, d_model, dropout, n_heads, d_ff, \n",
    "                 e_layers, c_out, embed, freq, bucket_size=4, n_hashes=4):\n",
    "        \"\"\"\n",
    "        bucket_size: int, \n",
    "        n_hashes: int, \n",
    "        \"\"\"\n",
    "        super(Reformer, self).__init__()\n",
    "        self.task_name = task_name\n",
    "        self.pred_len = pred_len\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        self.enc_embedding = DataEmbedding(enc_in, d_model, embed, freq, dropout)\n",
    "        # Encoder\n",
    "        self.encoder = Encoder(\n",
    "            [\n",
    "                EncoderLayer(\n",
    "                    ReformerLayer(None, d_model, n_heads,\n",
    "                                  bucket_size=bucket_size, n_hashes=n_hashes),\n",
    "                    d_model,\n",
    "                    d_ff,\n",
    "                    dropout=dropout,\n",
    "                    activation='relu'\n",
    "                ) for l in range(e_layers)\n",
    "            ],\n",
    "            norm_layer=torch.nn.LayerNorm(d_model)\n",
    "        )\n",
    "        \n",
    "        self.projection = nn.Linear(d_model, c_out, bias=True)\n",
    "\n",
    "    def long_forecast(self, x_enc, x_mark_enc, x_dec, x_mark_dec):\n",
    "        # add placeholder\n",
    "        x_enc = torch.cat([x_enc, x_dec[:, -self.pred_len:, :]], dim=1)\n",
    "        if x_mark_enc is not None:\n",
    "            x_mark_enc = torch.cat(\n",
    "                [x_mark_enc, x_mark_dec[:, -self.pred_len:, :]], dim=1)\n",
    "\n",
    "        enc_out = self.enc_embedding(x_enc, x_mark_enc)  # [B,T,C]\n",
    "        enc_out, attns = self.encoder(enc_out, attn_mask=None)\n",
    "        dec_out = self.projection(enc_out)\n",
    "\n",
    "        return dec_out  # [B, L, D]\n",
    "    \n",
    "    def short_forecast(self, x_enc, x_mark_enc, x_dec, x_mark_dec):\n",
    "        # Normalization\n",
    "        mean_enc = x_enc.mean(1, keepdim=True).detach()  # B x 1 x E\n",
    "        x_enc = x_enc - mean_enc\n",
    "        std_enc = torch.sqrt(torch.var(x_enc, dim=1, keepdim=True, unbiased=False) + 1e-5).detach()  # B x 1 x E\n",
    "        x_enc = x_enc / std_enc\n",
    "\n",
    "        # add placeholder\n",
    "        x_enc = torch.cat([x_enc, x_dec[:, -self.pred_len:, :]], dim=1)\n",
    "        if x_mark_enc is not None:\n",
    "            x_mark_enc = torch.cat(\n",
    "                [x_mark_enc, x_mark_dec[:, -self.pred_len:, :]], dim=1)\n",
    "\n",
    "        enc_out = self.enc_embedding(x_enc, x_mark_enc)  # [B,T,C]\n",
    "        enc_out, attns = self.encoder(enc_out, attn_mask=None)\n",
    "        dec_out = self.projection(enc_out)\n",
    "\n",
    "        dec_out = dec_out * std_enc + mean_enc\n",
    "        return dec_out  # [B, L, D]\n",
    "\n",
    "    def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec, mask=None):\n",
    "        if self.task_name == 'long_term_forecast':\n",
    "            dec_out = self.long_forecast(x_enc, x_mark_enc, x_dec, x_mark_dec)\n",
    "            return dec_out[:, -self.pred_len:, :]  # [B, L, D]\n",
    "        if self.task_name == 'short_term_forecast':\n",
    "            dec_out = self.short_forecast(x_enc, x_mark_enc, x_dec, x_mark_dec)\n",
    "            return dec_out[:, -self.pred_len:, :]  # [B, L, D]\n",
    "        \n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83970fc4",
   "metadata": {},
   "source": [
    "### 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "2226bc48",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:29:09.838035Z",
     "start_time": "2024-04-14T13:29:09.783632Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(task_args, train_args, model_args):\n",
    "    # 参数配置\n",
    "    columns = task_args['columns'] # 模型全部特征\n",
    "    target = task_args['target'] # 模型预测特征\n",
    "    features = task_args['features'] # 模型预测模式\n",
    "    model_name = train_args['model_name'] # 模型名称\n",
    "    train_loader = train_args['train_loader'] # 训练集\n",
    "    valid_loader = train_args['valid_loader'] # 验证集\n",
    "    n_epochs = train_args['n_epochs'] # 训练次数\n",
    "    learning_rate = train_args['learning_rate'] # 学习率\n",
    "    loss = train_args['loss'] # 损失函数\n",
    "    patience = train_args['patience'] # 最大早停次数阈值，超过就会早停\n",
    "    lradj = train_args['lradj'] # 学习率函数\n",
    "    path = train_args['path'] # 模型保存路径\n",
    "    verbose = train_args['verbose'] # 打印训练过程\n",
    "    plots = train_args['plots'] # 绘制损失图\n",
    "    \n",
    "    #检查是否可用GPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # 创建模型和优化器\n",
    "    model = model_name(**model_args).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_fn = loss\n",
    "    \n",
    "    # 损失函数值\n",
    "    train_losses, val_losses = [], []\n",
    "    \n",
    "    # 调整学习率\n",
    "    def adjust_learning_rate(optimizer, epoch, lradj, learning_rate, train_epochs):\n",
    "        # lr = learning_rate * (0.2 ** (epoch // 2))\n",
    "        if lradj == 'type1':\n",
    "            lr_adjust = {epoch: learning_rate * (0.5 ** ((epoch - 1) // 1))}\n",
    "        elif lradj == 'type2':\n",
    "            lr_adjust = {\n",
    "                2: 5e-5, 4: 1e-5, 6: 5e-6, 8: 1e-6,\n",
    "                10: 5e-7, 15: 1e-7, 20: 5e-8\n",
    "            }\n",
    "        elif lradj == \"cosine\":\n",
    "            lr_adjust = {epoch: learning_rate /2 * (1 + math.cos(epoch / train_epochs * math.pi))}\n",
    "        if epoch in lr_adjust.keys():\n",
    "            lr = lr_adjust[epoch]\n",
    "            # 参数组(param_groups)是用来指定不同的参数组以便对它们进行不同的优化设置，比如'lr'\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "            print('Updating learning rate to {}'.format(lr))\n",
    "    \n",
    "    # 设置早停\n",
    "    class EarlyStopping():\n",
    "        def __init__(self, patience=7, verbose=False, delta=0):\n",
    "            self.patience = patience # 连续超限次数，如果满足条件，则早停\n",
    "            self.verbose = verbose\n",
    "            self.counter = 0\n",
    "            self.best_score = None\n",
    "            self.early_stop = False\n",
    "            self.val_loss_min = np.Inf\n",
    "            self.delta = delta\n",
    "\n",
    "        def __call__(self, val_loss, model, path):\n",
    "            score = -val_loss\n",
    "            if self.best_score is None:\n",
    "                self.best_score = score\n",
    "                self.save_checkpoint(val_loss, model, path)\n",
    "            elif score < self.best_score + self.delta:\n",
    "                self.counter += 1\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "                if self.counter >= self.patience:\n",
    "                    self.early_stop = True\n",
    "            else:\n",
    "                self.best_score = score\n",
    "                self.save_checkpoint(val_loss, model, path)\n",
    "                self.counter = 0\n",
    "\n",
    "        def save_checkpoint(self, val_loss, model, path):\n",
    "            if self.verbose:\n",
    "                print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "            torch.save(model.state_dict(), path + '/' + 'checkpoint.pth')\n",
    "            self.val_loss_min = val_loss\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=verbose)\n",
    "    \n",
    "    # 训练任务\n",
    "    def forecasting_task(columns, target, features):\n",
    "        \"\"\"\n",
    "        features: [M, S, MS]; \n",
    "            M:multivariate predict multivariate, \n",
    "            S:univariate predict univariate, \n",
    "            MS:multivariate predict univariate'\n",
    "        \"\"\"\n",
    "        # 字典索引生成\n",
    "        col_dict = {}\n",
    "        for i,j in enumerate(columns):\n",
    "            col_dict[j] = i\n",
    "\n",
    "        if features == 'MS':\n",
    "            target = target[0]\n",
    "            f_dim = col_dict[target]\n",
    "        elif features == 'S':\n",
    "            f_dim = 0\n",
    "        else:\n",
    "            f_dim = 0\n",
    "        return f_dim\n",
    "    f_dim = forecasting_task(columns, target, features)\n",
    "    \n",
    "    # 设置保存模型路径\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    # 模型训练和验证\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for batch_x, batch_y, batch_x_mark, batch_y_mark in train_loader:\n",
    "            #将数据移至 GPU\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            batch_x_mark = batch_x_mark.to(device)\n",
    "            batch_y_mark = batch_y_mark.to(device)\n",
    "            # 清空梯度\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x, batch_x_mark, batch_y, batch_y_mark)\n",
    "            if features == 'MS':\n",
    "                outputs = outputs[:, :, f_dim: f_dim+1] \n",
    "            else:\n",
    "                outputs = outputs[:, :, f_dim:]\n",
    "            train_loss = loss_fn(outputs, batch_y)\n",
    "            # 反向传播计算得到每个参数的梯度值\n",
    "            train_loss.backward()\n",
    "            # 通过梯度下降执行一步参数更新\n",
    "            optimizer.step()\n",
    "            #每个batch的loss和\n",
    "            total_train_loss += train_loss.item() # .item()表示只包含一个元素的tensor中提取值\n",
    "                \n",
    "        #每个epoch的损失平均\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        \n",
    "        #所有epoch的loss\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        #评估模型\n",
    "        model.eval()\n",
    "        #关闭自动求导功能，只使用训练好的模型进行预测或评估，不需要进行梯度计算和参数更新\n",
    "        with torch.no_grad():\n",
    "            total_val_loss = 0\n",
    "            for batch_x, batch_y, batch_x_mark, batch_y_mark in valid_loader:\n",
    "                #将数据移至 GPU\n",
    "                batch_x = batch_x.to(device)\n",
    "                batch_y = batch_y.to(device)\n",
    "                batch_x_mark = batch_x_mark.to(device)\n",
    "                batch_y_mark = batch_y_mark.to(device)\n",
    "                val_outputs = model(batch_x, batch_x_mark, batch_y, batch_y_mark)\n",
    "                if features == 'MS':\n",
    "                    val_outputs = val_outputs[:, :, f_dim: f_dim+1] \n",
    "                else:\n",
    "                    val_outputs = val_outputs[:, :, f_dim:]\n",
    "                val_loss = loss_fn(val_outputs, batch_y)\n",
    "                #每个batch的loss和\n",
    "                total_val_loss += val_loss.item()\n",
    "                \n",
    "        #每个epoch的损失平均\n",
    "        avg_val_loss = total_val_loss / len(valid_loader)\n",
    "        \n",
    "        #所有epoch的loss\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        #打印训练过程\n",
    "        if verbose:\n",
    "            print(f'Epoch [{epoch+1}/{n_epochs}], Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}')\n",
    "            \n",
    "        # 设置早停\n",
    "        early_stopping(avg_val_loss, model, path)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping!\")\n",
    "            break\n",
    "            \n",
    "        # 调整学习率\n",
    "        adjust_learning_rate(optimizer, epoch+1, lradj, learning_rate, n_epochs)\n",
    "        \n",
    "    # 加载最佳模型\n",
    "    best_model_path = path + '/' + 'checkpoint.pth'\n",
    "    model.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "    #绘制损失函数图\n",
    "    def plot_loss(train_losses, val_losses):\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.style.use('seaborn-v0_8-paper') #绘制背景色\n",
    "        plt.grid(axis = 'y',linewidth=0.35) #绘制网格\n",
    "        plt.plot(val_losses, linestyle='-',color = '#11b3b6')\n",
    "        plt.plot(train_losses, linestyle='-',color = '#f14643')\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Training and Validation Progress\")\n",
    "        plt.legend([\"Validation\", \"Training\"])\n",
    "        plt.show()\n",
    "    if plots:\n",
    "        plot_loss(train_losses, val_losses)\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "360b3cca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:29:27.674419Z",
     "start_time": "2024-04-14T13:29:11.672146Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|████▏                                                                              | 1/20 [00:11<03:39, 11.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Training Loss: 0.0050, Validation Loss: 0.0065\n",
      "Validation loss decreased (inf --> 0.006532).  Saving model ...\n",
      "Updating learning rate to 0.0009938441702975688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|████▏                                                                              | 1/20 [00:14<04:43, 14.93s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[111], line 37\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 构造参数字典\u001b[39;00m\n\u001b[0;32m      2\u001b[0m params3 \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtask_args\u001b[39m\u001b[38;5;124m\"\u001b[39m:{\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mload\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemp\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     35\u001b[0m     },\n\u001b[0;32m     36\u001b[0m }\n\u001b[1;32m---> 37\u001b[0m model \u001b[38;5;241m=\u001b[39m train(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams3)\n",
      "Cell \u001b[1;32mIn[110], line 127\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(task_args, train_args, model_args)\u001b[0m\n\u001b[0;32m    125\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs, batch_y)\n\u001b[0;32m    126\u001b[0m \u001b[38;5;66;03m# 反向传播计算得到每个参数的梯度值\u001b[39;00m\n\u001b[1;32m--> 127\u001b[0m \u001b[43mtrain_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;66;03m# 通过梯度下降执行一步参数更新\u001b[39;00m\n\u001b[0;32m    129\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 构造参数字典\n",
    "params3 = {\n",
    "    \"task_args\":{\n",
    "        \"columns\": ['load', 'temp'],\n",
    "        \"target\": ['load', 'temp'],\n",
    "        \"features\": 'M'\n",
    "    },\n",
    "    \"train_args\": {\n",
    "        \"model_name\": Reformer,\n",
    "        \"train_loader\": train_loader,\n",
    "        \"valid_loader\": valid_loader,\n",
    "        \"n_epochs\": 20,\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"loss\": nn.MSELoss(),\n",
    "        \"patience\": 3,\n",
    "        \"lradj\": 'cosine',\n",
    "        \"path\": \"../models/test\",\n",
    "        \"verbose\": True,\n",
    "        \"plots\": True,\n",
    "    },\n",
    "    \"model_args\": {\n",
    "        'task_name': 'short_term_forecast',\n",
    "        'pred_len': 3, \n",
    "        'seq_len': 6,\n",
    "        \"label_len\": 0,\n",
    "        'd_model': 128,\n",
    "        'enc_in': 2,\n",
    "        'dropout': 0.1,\n",
    "        'n_heads': 8,\n",
    "        'd_ff': 128,\n",
    "        'e_layers': 1,\n",
    "        'c_out': 2,\n",
    "        'embed': 'fixed',\n",
    "        'freq': 'h',\n",
    "    },\n",
    "}\n",
    "model = train(**params3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdfd732",
   "metadata": {},
   "source": [
    "# 基于TiDE的时间序列预测"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed3c261",
   "metadata": {},
   "source": [
    "## 多变量多步预测"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5698dcc0",
   "metadata": {},
   "source": [
    "### 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "7378f4c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:29:30.875563Z",
     "start_time": "2024-04-14T13:29:30.866151Z"
    }
   },
   "outputs": [],
   "source": [
    "# 读取数据\n",
    "def get_dataset(path, time_col=None):\n",
    "    data = pd.read_csv(path)\n",
    "    rows = data.shape[0]\n",
    "    now = datetime.now()\n",
    "    newtime = now.replace(microsecond=0)\n",
    "    if time_col == None:\n",
    "        # 如果没有时间列，生成时间戳范围\n",
    "        time_index = pd.date_range(start=datetime.now() -\n",
    "                                   timedelta(seconds=rows - 1),\n",
    "                                   end=datetime.now(),\n",
    "                                   freq='S')\n",
    "        full_data = pd.DataFrame(data=data.values,\n",
    "                                 index=pd.to_datetime(time_index, unit='s'),\n",
    "                                 columns=data.columns)\n",
    "    else:\n",
    "        columns = [i for i in data.columns if i != time_col] # 去除时间列\n",
    "        full_data = pd.DataFrame(data=data.drop([time_col], axis=1).values,\n",
    "                                 index=pd.to_datetime(data[time_col].values),\n",
    "                                 columns=columns)\n",
    "    return full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "bb102d44",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:29:32.390905Z",
     "start_time": "2024-04-14T13:29:32.312564Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>load</th>\n",
       "      <th>temp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-01-01 00:00:00</th>\n",
       "      <td>2698.0</td>\n",
       "      <td>32.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-01 01:00:00</th>\n",
       "      <td>2558.0</td>\n",
       "      <td>32.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-01 02:00:00</th>\n",
       "      <td>2444.0</td>\n",
       "      <td>30.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-01 03:00:00</th>\n",
       "      <td>2402.0</td>\n",
       "      <td>31.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-01 04:00:00</th>\n",
       "      <td>2403.0</td>\n",
       "      <td>32.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-12-31 19:00:00</th>\n",
       "      <td>4012.0</td>\n",
       "      <td>18.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-12-31 20:00:00</th>\n",
       "      <td>3856.0</td>\n",
       "      <td>16.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-12-31 21:00:00</th>\n",
       "      <td>3671.0</td>\n",
       "      <td>17.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-12-31 22:00:00</th>\n",
       "      <td>3499.0</td>\n",
       "      <td>15.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-12-31 23:00:00</th>\n",
       "      <td>3345.0</td>\n",
       "      <td>15.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26304 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       load   temp\n",
       "2012-01-01 00:00:00  2698.0  32.00\n",
       "2012-01-01 01:00:00  2558.0  32.67\n",
       "2012-01-01 02:00:00  2444.0  30.00\n",
       "2012-01-01 03:00:00  2402.0  31.00\n",
       "2012-01-01 04:00:00  2403.0  32.00\n",
       "...                     ...    ...\n",
       "2014-12-31 19:00:00  4012.0  18.00\n",
       "2014-12-31 20:00:00  3856.0  16.67\n",
       "2014-12-31 21:00:00  3671.0  17.00\n",
       "2014-12-31 22:00:00  3499.0  15.33\n",
       "2014-12-31 23:00:00  3345.0  15.33\n",
       "\n",
       "[26304 rows x 2 columns]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"../data/energy.csv\"\n",
    "ts_data = get_dataset(path, time_col='time')\n",
    "ts_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "08a58347",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:29:33.398587Z",
     "start_time": "2024-04-14T13:29:33.361912Z"
    }
   },
   "outputs": [],
   "source": [
    "# 时间格式编码\n",
    "def time_features_from_frequency_str(freq_str: str):\n",
    "    \"\"\"\n",
    "    Returns a list of time features that will be appropriate for the given frequency string.\n",
    "    Parameters\n",
    "    ----------\n",
    "    freq_str\n",
    "        Frequency string of the form [multiple][granularity] such as \"12H\", \"5min\", \"1D\" etc.\n",
    "    \"\"\"\n",
    "\n",
    "    class TimeFeature:\n",
    "        def __init__(self):\n",
    "            pass\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            pass\n",
    "\n",
    "        def __repr__(self):\n",
    "            return self.__class__.__name__ + \"()\"\n",
    "\n",
    "\n",
    "    class SecondOfMinute(TimeFeature):\n",
    "        \"\"\"Minute of hour encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return index.second / 59.0 - 0.5\n",
    "\n",
    "\n",
    "    class MinuteOfHour(TimeFeature):\n",
    "        \"\"\"Minute of hour encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return index.minute / 59.0 - 0.5\n",
    "\n",
    "\n",
    "    class HourOfDay(TimeFeature):\n",
    "        \"\"\"Hour of day encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return index.hour / 23.0 - 0.5\n",
    "\n",
    "\n",
    "    class DayOfWeek(TimeFeature):\n",
    "        \"\"\"Hour of day encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return index.dayofweek / 6.0 - 0.5\n",
    "\n",
    "\n",
    "    class DayOfMonth(TimeFeature):\n",
    "        \"\"\"Day of month encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return (index.day - 1) / 30.0 - 0.5\n",
    "\n",
    "\n",
    "    class DayOfYear(TimeFeature):\n",
    "        \"\"\"Day of year encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return (index.dayofyear - 1) / 365.0 - 0.5\n",
    "\n",
    "\n",
    "    class MonthOfYear(TimeFeature):\n",
    "        \"\"\"Month of year encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return (index.month - 1) / 11.0 - 0.5\n",
    "\n",
    "\n",
    "    class WeekOfYear(TimeFeature):\n",
    "        \"\"\"Week of year encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return (index.isocalendar().week - 1) / 52.0 - 0.5\n",
    "\n",
    "    \n",
    "    features_by_offsets = {\n",
    "        offsets.YearEnd: [],\n",
    "        offsets.QuarterEnd: [MonthOfYear],\n",
    "        offsets.MonthEnd: [MonthOfYear],\n",
    "        offsets.Week: [DayOfMonth, WeekOfYear],\n",
    "        offsets.Day: [DayOfWeek, DayOfMonth, DayOfYear],\n",
    "        offsets.BusinessDay: [DayOfWeek, DayOfMonth, DayOfYear],\n",
    "        offsets.Hour: [HourOfDay, DayOfWeek, DayOfMonth, DayOfYear],\n",
    "        offsets.Minute: [\n",
    "            MinuteOfHour,\n",
    "            HourOfDay,\n",
    "            DayOfWeek,\n",
    "            DayOfMonth,\n",
    "            DayOfYear,\n",
    "        ],\n",
    "        offsets.Second: [\n",
    "            SecondOfMinute,\n",
    "            MinuteOfHour,\n",
    "            HourOfDay,\n",
    "            DayOfWeek,\n",
    "            DayOfMonth,\n",
    "            DayOfYear,\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    offset = to_offset(freq_str)\n",
    "\n",
    "    for offset_type, feature_classes in features_by_offsets.items():\n",
    "        if isinstance(offset, offset_type):\n",
    "            return [cls() for cls in feature_classes]\n",
    "\n",
    "    supported_freq_msg = f\"\"\"\n",
    "    Unsupported frequency {freq_str}\n",
    "    The following frequencies are supported:\n",
    "        Y   - yearly\n",
    "            alias: A\n",
    "        M   - monthly\n",
    "        W   - weekly\n",
    "        D   - daily\n",
    "        B   - business days\n",
    "        H   - hourly\n",
    "        T   - minutely\n",
    "            alias: min\n",
    "        S   - secondly\n",
    "    \"\"\"\n",
    "    raise RuntimeError(supported_freq_msg)\n",
    "    \n",
    "def time_features(dates, freq='h'):\n",
    "    return np.vstack([feat(dates) for feat in time_features_from_frequency_str(freq)])\n",
    "\n",
    "# 包含时间维度的数据集划分\n",
    "def divide_dataset(df, valid_date, test_date, x_feature_list, y_feature_list, freq):\n",
    "    #归一化\n",
    "    x_scaler = MinMaxScaler()\n",
    "    y_scaler = MinMaxScaler()\n",
    "\n",
    "    #测试集\n",
    "    train = df.copy()[df.index < valid_date][x_feature_list]\n",
    "    train_stamp = time_features(pd.to_datetime(train.index), freq=freq)\n",
    "    train_stamp = train_stamp.transpose(1, 0)\n",
    "    train[x_feature_list] = x_scaler.fit_transform(train)\n",
    "    xtr = train.values.astype('float32')\n",
    "\n",
    "    #验证集\n",
    "    valid = df.copy()[(df.index >= valid_date) & (df.index < test_date)][x_feature_list]\n",
    "    valid_stamp = time_features(pd.to_datetime(valid.index), freq=freq)\n",
    "    valid_stamp = valid_stamp.transpose(1, 0)\n",
    "    valid[x_feature_list] = x_scaler.fit_transform(valid)\n",
    "    xva = valid.values.astype('float32')\n",
    "\n",
    "    #测试集\n",
    "    test = df.copy()[test_date:][x_feature_list]\n",
    "    test_stamp = time_features(pd.to_datetime(test.index), freq=freq)\n",
    "    test_stamp = test_stamp.transpose(1, 0)\n",
    "    test[x_feature_list] = x_scaler.fit_transform(test)\n",
    "    xte = test.values.astype('float32')\n",
    "\n",
    "    #标签\n",
    "    ytr = df.copy()[df.index < valid_date][y_feature_list]\n",
    "    ytr[y_feature_list] = y_scaler.fit_transform(ytr)\n",
    "    ytr = ytr.values.astype('float32')\n",
    "\n",
    "    yva = df.copy()[(df.index >= valid_date) & (df.index < test_date)][y_feature_list]\n",
    "    yva[y_feature_list] = y_scaler.fit_transform(yva)\n",
    "    yva = yva.values.astype('float32')\n",
    "\n",
    "    yte = df.copy()[test_date:][y_feature_list]\n",
    "    yte[y_feature_list] = y_scaler.fit_transform(yte)\n",
    "    yte = yte.values.astype('float32')\n",
    "    \n",
    "    # 数据合并，[训练集，测试集，时间戳]\n",
    "    train = [xtr, ytr, train_stamp]\n",
    "    valid = [xva, yva, valid_stamp]\n",
    "    test = [xte, yte, test_stamp]\n",
    "    \n",
    "    return y_scaler, train, valid, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "495eb6f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:29:34.608193Z",
     "start_time": "2024-04-14T13:29:34.278114Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (23376, 2) y_train shape: (23376, 2) stamp_train shape: (23376, 4)\n",
      "x_valid shape: (1464, 2) y_valid shape: (1464, 2) stamp_valid shape: (1464, 4)\n",
      "x_test shape: (1464, 2) y_test shape: (1464, 2) stamp_test shape: (1464, 4)\n"
     ]
    }
   ],
   "source": [
    "# 构造参数字典\n",
    "params1 = {\n",
    "    \"df\": ts_data,\n",
    "    \"valid_date\": \"2014-09-01 00:00:00\",\n",
    "    \"test_date\": \"2014-11-01 00:00:00\",\n",
    "    \"x_feature_list\": [\"load\", 'temp'],\n",
    "    \"y_feature_list\": [\"load\", 'temp'],\n",
    "    \"freq\": 'h'\n",
    "}\n",
    "\n",
    "#函数传参\n",
    "scaler, train, valid, test = divide_dataset(**params1)\n",
    "print(\"x_train shape: {0} y_train shape: {1} stamp_train shape: {2}\".format(train[0].shape, train[1].shape, train[2].shape))\n",
    "print(\"x_valid shape: {0} y_valid shape: {1} stamp_valid shape: {2}\".format(valid[0].shape, valid[1].shape, valid[2].shape))\n",
    "print(\"x_test shape: {0} y_test shape: {1} stamp_test shape: {2}\".format(test[0].shape, test[1].shape, test[2].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "23961587",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:29:35.584160Z",
     "start_time": "2024-04-14T13:29:35.560708Z"
    }
   },
   "outputs": [],
   "source": [
    "#利用前seq_len个数据，预测下pred_len个数据\n",
    "def create_dataset(data_list, seq_len, pred_len, label_len, batch_size):\n",
    "    \"\"\"\n",
    "    @参数设置：\n",
    "    data_list：特征，目标，时间戳\n",
    "    seq_len：输入数据包含过去多少个时间步\n",
    "    pred_len：目标应该在未来多少个时间步之后\n",
    "    label_len：先验时间步\n",
    "    \"\"\"\n",
    "    # 获取数据\n",
    "    feature = data_list[0] # 特征\n",
    "    target = data_list[1] # 目标\n",
    "    stamp = data_list[2] # 时间戳\n",
    "    \n",
    "    # 循环生成数据\n",
    "    X, y = [], []\n",
    "    X_stamp, y_stamp = [], []\n",
    "    seq_len = seq_len - 1 # 包含当前时间点\n",
    "    for i in range(seq_len, len(feature) - pred_len):\n",
    "        # 数据维度\n",
    "        feat = feature[i - seq_len:i + 1]\n",
    "        tar = target[i + 1:i + 1 + pred_len]\n",
    "        X.append(feat)\n",
    "        y.append(tar)\n",
    "        \n",
    "        # 时间维度\n",
    "        xs = stamp[i - seq_len:i + 1]\n",
    "        ys = stamp[i + 1 - label_len:i + 1 + pred_len]\n",
    "        X_stamp.append(xs)\n",
    "        y_stamp.append(ys)\n",
    "        \n",
    "    # 转为张量，数据维度\n",
    "    X = torch.as_tensor(X).float()\n",
    "    y = torch.as_tensor(y).float()\n",
    "    \n",
    "    # 转为张量，时间维度\n",
    "    X_stamp = torch.as_tensor(X_stamp).float()\n",
    "    y_stamp = torch.as_tensor(y_stamp).float()\n",
    "    \n",
    "    # 创建dataloader，[特征，目标，特征时间编码，目标时间编码]\n",
    "    data_loader = DataLoader(TensorDataset(X, y, X_stamp, y_stamp), shuffle=True, batch_size=batch_size)\n",
    "    \n",
    "    return X, y, data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "d8ebc2c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:29:37.525780Z",
     "start_time": "2024-04-14T13:29:36.648788Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_size: torch.Size([23368, 6, 2]),y_size: torch.Size([23368, 3, 2]),loader_len: 731\n",
      "X_size: torch.Size([1456, 6, 2]),y_size: torch.Size([1456, 3, 2]),loader_len: 46\n",
      "X_size: torch.Size([1456, 6, 2]),y_size: torch.Size([1456, 3, 2]),loader_len: 46\n"
     ]
    }
   ],
   "source": [
    "# 构造参数字典\n",
    "params2 = {\n",
    "    \"seq_len\": 6,\n",
    "    \"pred_len\": 3,\n",
    "    \"label_len\": 0,\n",
    "    \"batch_size\": 32,\n",
    "}\n",
    "X_train, y_train, train_loader = create_dataset(train, **params2)\n",
    "X_valid, y_valid, valid_loader = create_dataset(valid, **params2)\n",
    "X_test, y_test, test_loader = create_dataset(test, **params2)\n",
    "print(\"X_size: {0},y_size: {1},loader_len: {2}\".format(X_train.shape, y_train.shape, len(train_loader)))\n",
    "print(\"X_size: {0},y_size: {1},loader_len: {2}\".format(X_valid.shape, y_valid.shape, len(valid_loader)))\n",
    "print(\"X_size: {0},y_size: {1},loader_len: {2}\".format(X_test.shape, y_test.shape, len(test_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9396a65",
   "metadata": {},
   "source": [
    "### 模型定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "52c5cf50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:29:39.642782Z",
     "start_time": "2024-04-14T13:29:39.614925Z"
    }
   },
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n",
    "    def __init__(self, ndim, bias):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias,\n",
    "                            1e-5)\n",
    "\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim,\n",
    "                 hidden_dim,\n",
    "                 output_dim,\n",
    "                 dropout=0.1,\n",
    "                 bias=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim, bias=bias)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim, bias=bias)\n",
    "        self.fc3 = nn.Linear(input_dim, output_dim, bias=bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.ln = LayerNorm(output_dim, bias=bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.dropout(out)\n",
    "        out = out + self.fc3(x)\n",
    "        out = self.ln(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "# TiDE模型\n",
    "class TiDE(nn.Module):\n",
    "    def __init__(self, seq_len, pred_len, label_len, d_model, e_layers, d_layers, freq, d_ff, dropout, c_out, \n",
    "                 bias=True, feature_encode_dim=2):\n",
    "        super(TiDE, self).__init__()\n",
    "        self.seq_len = seq_len  #L\n",
    "        self.pred_len = pred_len  #H\n",
    "        self.hidden_dim = d_model\n",
    "        self.res_hidden = d_model\n",
    "        self.encoder_num = e_layers\n",
    "        self.decoder_num = d_layers\n",
    "        self.freq = freq\n",
    "        self.feature_encode_dim = feature_encode_dim\n",
    "        self.decode_dim = c_out\n",
    "        self.temporalDecoderHidden = d_ff\n",
    "        dropout = dropout\n",
    "\n",
    "        freq_map = {'h': 4, 't': 5, 's': 6,\n",
    "                    'm': 1, 'a': 1, 'w': 2, 'd': 3, 'b': 3}\n",
    "\n",
    "        self.feature_dim = freq_map[self.freq]\n",
    "\n",
    "        flatten_dim = self.seq_len + (self.seq_len +\n",
    "                                      self.pred_len) * self.feature_encode_dim\n",
    "\n",
    "        self.feature_encoder = ResBlock(self.feature_dim, self.res_hidden,\n",
    "                                        self.feature_encode_dim, dropout, bias)\n",
    "        self.encoders = nn.Sequential(\n",
    "            ResBlock(flatten_dim, self.res_hidden, self.hidden_dim, dropout,\n",
    "                     bias),\n",
    "            *([\n",
    "                ResBlock(self.hidden_dim, self.res_hidden, self.hidden_dim,\n",
    "                         dropout, bias)\n",
    "            ] * (self.encoder_num - 1)))\n",
    "\n",
    "        self.decoders = nn.Sequential(\n",
    "            *([\n",
    "                ResBlock(self.hidden_dim, self.res_hidden, self.hidden_dim,\n",
    "                         dropout, bias)\n",
    "            ] * (self.decoder_num - 1)),\n",
    "            ResBlock(self.hidden_dim, self.res_hidden,\n",
    "                     self.decode_dim * self.pred_len, dropout, bias))\n",
    "        self.temporalDecoder = ResBlock(\n",
    "            self.decode_dim + self.feature_encode_dim,\n",
    "            self.temporalDecoderHidden, 1, dropout, bias)\n",
    "        self.residual_proj = nn.Linear(self.seq_len,\n",
    "                                       self.pred_len,\n",
    "                                       bias=bias)\n",
    "\n",
    "    def forecast(self, x_enc, x_mark_enc, x_dec, batch_y_mark):\n",
    "        # Normalization\n",
    "        means = x_enc.mean(1, keepdim=True).detach()\n",
    "        x_enc = x_enc - means\n",
    "        stdev = torch.sqrt(\n",
    "            torch.var(x_enc, dim=1, keepdim=True, unbiased=False) + 1e-5)\n",
    "        x_enc /= stdev\n",
    "\n",
    "        feature = self.feature_encoder(batch_y_mark)\n",
    "        hidden = self.encoders(\n",
    "            torch.cat([x_enc, feature.reshape(feature.shape[0], -1)], dim=-1))\n",
    "        decoded = self.decoders(hidden).reshape(hidden.shape[0], self.pred_len,\n",
    "                                                self.decode_dim)\n",
    "        dec_out = self.temporalDecoder(\n",
    "            torch.cat([feature[:, self.seq_len:], decoded],\n",
    "                      dim=-1)).squeeze(-1) + self.residual_proj(x_enc)\n",
    "\n",
    "        # De-Normalization\n",
    "        dec_out = dec_out * (stdev[:, 0].unsqueeze(1).repeat(1, self.pred_len))\n",
    "        dec_out = dec_out + (means[:, 0].unsqueeze(1).repeat(1, self.pred_len))\n",
    "        return dec_out\n",
    "\n",
    "    def forward(self, x_enc, x_mark_enc, x_dec, batch_y_mark, mask=None):\n",
    "        '''x_mark_enc is the exogenous dynamic feature described in the original paper'''\n",
    "        batch_y_mark = torch.concat(\n",
    "            [x_mark_enc, batch_y_mark[:, -self.pred_len:, :]], dim=1)\n",
    "        dec_out = torch.stack([\n",
    "            self.forecast(x_enc[:, :, feature], x_mark_enc, x_dec,\n",
    "                          batch_y_mark)\n",
    "            for feature in range(x_enc.shape[-1])\n",
    "        ], dim=-1)\n",
    "        \n",
    "        return dec_out  # [B, L, D]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e33d0b",
   "metadata": {},
   "source": [
    "### 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "4e592e47",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:29:42.136476Z",
     "start_time": "2024-04-14T13:29:42.096487Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(task_args, train_args, model_args):\n",
    "    # 参数配置\n",
    "    columns = task_args['columns'] # 模型全部特征\n",
    "    target = task_args['target'] # 模型预测特征\n",
    "    features = task_args['features'] # 模型预测模式\n",
    "    model_name = train_args['model_name'] # 模型名称\n",
    "    train_loader = train_args['train_loader'] # 训练集\n",
    "    valid_loader = train_args['valid_loader'] # 验证集\n",
    "    n_epochs = train_args['n_epochs'] # 训练次数\n",
    "    learning_rate = train_args['learning_rate'] # 学习率\n",
    "    loss = train_args['loss'] # 损失函数\n",
    "    patience = train_args['patience'] # 最大早停次数阈值，超过就会早停\n",
    "    lradj = train_args['lradj'] # 学习率函数\n",
    "    path = train_args['path'] # 模型保存路径\n",
    "    verbose = train_args['verbose'] # 打印训练过程\n",
    "    plots = train_args['plots'] # 绘制损失图\n",
    "    \n",
    "    #检查是否可用GPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # 创建模型和优化器\n",
    "    model = model_name(**model_args).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_fn = loss\n",
    "    \n",
    "    # 损失函数值\n",
    "    train_losses, val_losses = [], []\n",
    "    \n",
    "    # 调整学习率\n",
    "    def adjust_learning_rate(optimizer, epoch, lradj, learning_rate, train_epochs):\n",
    "        # lr = learning_rate * (0.2 ** (epoch // 2))\n",
    "        if lradj == 'type1':\n",
    "            lr_adjust = {epoch: learning_rate * (0.5 ** ((epoch - 1) // 1))}\n",
    "        elif lradj == 'type2':\n",
    "            lr_adjust = {\n",
    "                2: 5e-5, 4: 1e-5, 6: 5e-6, 8: 1e-6,\n",
    "                10: 5e-7, 15: 1e-7, 20: 5e-8\n",
    "            }\n",
    "        elif lradj == \"cosine\":\n",
    "            lr_adjust = {epoch: learning_rate /2 * (1 + math.cos(epoch / train_epochs * math.pi))}\n",
    "        if epoch in lr_adjust.keys():\n",
    "            lr = lr_adjust[epoch]\n",
    "            # 参数组(param_groups)是用来指定不同的参数组以便对它们进行不同的优化设置，比如'lr'\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "            print('Updating learning rate to {}'.format(lr))\n",
    "    \n",
    "    # 设置早停\n",
    "    class EarlyStopping():\n",
    "        def __init__(self, patience=7, verbose=False, delta=0):\n",
    "            self.patience = patience # 连续超限次数，如果满足条件，则早停\n",
    "            self.verbose = verbose\n",
    "            self.counter = 0\n",
    "            self.best_score = None\n",
    "            self.early_stop = False\n",
    "            self.val_loss_min = np.Inf\n",
    "            self.delta = delta\n",
    "\n",
    "        def __call__(self, val_loss, model, path):\n",
    "            score = -val_loss\n",
    "            if self.best_score is None:\n",
    "                self.best_score = score\n",
    "                self.save_checkpoint(val_loss, model, path)\n",
    "            elif score < self.best_score + self.delta:\n",
    "                self.counter += 1\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "                if self.counter >= self.patience:\n",
    "                    self.early_stop = True\n",
    "            else:\n",
    "                self.best_score = score\n",
    "                self.save_checkpoint(val_loss, model, path)\n",
    "                self.counter = 0\n",
    "\n",
    "        def save_checkpoint(self, val_loss, model, path):\n",
    "            if self.verbose:\n",
    "                print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "            torch.save(model.state_dict(), path + '/' + 'checkpoint.pth')\n",
    "            self.val_loss_min = val_loss\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=verbose)\n",
    "    \n",
    "    # 训练任务\n",
    "    def forecasting_task(columns, target, features):\n",
    "        \"\"\"\n",
    "        features: [M, S, MS]; \n",
    "            M:multivariate predict multivariate, \n",
    "            S:univariate predict univariate, \n",
    "            MS:multivariate predict univariate'\n",
    "        \"\"\"\n",
    "        # 字典索引生成\n",
    "        col_dict = {}\n",
    "        for i,j in enumerate(columns):\n",
    "            col_dict[j] = i\n",
    "\n",
    "        if features == 'MS':\n",
    "            target = target[0]\n",
    "            f_dim = col_dict[target]\n",
    "        elif features == 'S':\n",
    "            f_dim = 0\n",
    "        else:\n",
    "            f_dim = 0\n",
    "        return f_dim\n",
    "    f_dim = forecasting_task(columns, target, features)\n",
    "    \n",
    "    # 设置保存模型路径\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    # 模型训练和验证\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for batch_x, batch_y, batch_x_mark, batch_y_mark in train_loader:\n",
    "            #将数据移至 GPU\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            batch_x_mark = batch_x_mark.to(device)\n",
    "            batch_y_mark = batch_y_mark.to(device)\n",
    "            # 清空梯度\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x, batch_x_mark, batch_y, batch_y_mark)\n",
    "            if features == 'MS':\n",
    "                outputs = outputs[:, :, f_dim: f_dim+1] \n",
    "            else:\n",
    "                outputs = outputs[:, :, f_dim:]\n",
    "            train_loss = loss_fn(outputs, batch_y)\n",
    "            # 反向传播计算得到每个参数的梯度值\n",
    "            train_loss.backward()\n",
    "            # 通过梯度下降执行一步参数更新\n",
    "            optimizer.step()\n",
    "            #每个batch的loss和\n",
    "            total_train_loss += train_loss.item() # .item()表示只包含一个元素的tensor中提取值\n",
    "                \n",
    "        #每个epoch的损失平均\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        \n",
    "        #所有epoch的loss\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        #评估模型\n",
    "        model.eval()\n",
    "        #关闭自动求导功能，只使用训练好的模型进行预测或评估，不需要进行梯度计算和参数更新\n",
    "        with torch.no_grad():\n",
    "            total_val_loss = 0\n",
    "            for batch_x, batch_y, batch_x_mark, batch_y_mark in valid_loader:\n",
    "                #将数据移至 GPU\n",
    "                batch_x = batch_x.to(device)\n",
    "                batch_y = batch_y.to(device)\n",
    "                batch_x_mark = batch_x_mark.to(device)\n",
    "                batch_y_mark = batch_y_mark.to(device)\n",
    "                val_outputs = model(batch_x, batch_x_mark, batch_y, batch_y_mark)\n",
    "                if features == 'MS':\n",
    "                    val_outputs = val_outputs[:, :, f_dim: f_dim+1] \n",
    "                else:\n",
    "                    val_outputs = val_outputs[:, :, f_dim:]\n",
    "                val_loss = loss_fn(val_outputs, batch_y)\n",
    "                #每个batch的loss和\n",
    "                total_val_loss += val_loss.item()\n",
    "                \n",
    "        #每个epoch的损失平均\n",
    "        avg_val_loss = total_val_loss / len(valid_loader)\n",
    "        \n",
    "        #所有epoch的loss\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        #打印训练过程\n",
    "        if verbose:\n",
    "            print(f'Epoch [{epoch+1}/{n_epochs}], Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}')\n",
    "            \n",
    "        # 设置早停\n",
    "        early_stopping(avg_val_loss, model, path)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping!\")\n",
    "            break\n",
    "            \n",
    "        # 调整学习率\n",
    "        adjust_learning_rate(optimizer, epoch+1, lradj, learning_rate, n_epochs)\n",
    "        \n",
    "    # 加载最佳模型\n",
    "    best_model_path = path + '/' + 'checkpoint.pth'\n",
    "    model.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "    #绘制损失函数图\n",
    "    def plot_loss(train_losses, val_losses):\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.style.use('seaborn-v0_8-paper') #绘制背景色\n",
    "        plt.grid(axis = 'y',linewidth=0.35) #绘制网格\n",
    "        plt.plot(val_losses, linestyle='-',color = '#11b3b6')\n",
    "        plt.plot(train_losses, linestyle='-',color = '#f14643')\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Training and Validation Progress\")\n",
    "        plt.legend([\"Validation\", \"Training\"])\n",
    "        plt.show()\n",
    "    if plots:\n",
    "        plot_loss(train_losses, val_losses)\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "b6ab60f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:29:57.376340Z",
     "start_time": "2024-04-14T13:29:45.780919Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|████▏                                                                              | 1/20 [00:08<02:36,  8.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Training Loss: 0.0075, Validation Loss: 0.0115\n",
      "Validation loss decreased (inf --> 0.011546).  Saving model ...\n",
      "Updating learning rate to 0.0009938441702975688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|████▏                                                                              | 1/20 [00:10<03:17, 10.37s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[120], line 34\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 构造参数字典\u001b[39;00m\n\u001b[0;32m      2\u001b[0m params3 \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtask_args\u001b[39m\u001b[38;5;124m\"\u001b[39m:{\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mload\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemp\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     32\u001b[0m     },\n\u001b[0;32m     33\u001b[0m }\n\u001b[1;32m---> 34\u001b[0m model \u001b[38;5;241m=\u001b[39m train(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams3)\n",
      "Cell \u001b[1;32mIn[119], line 120\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(task_args, train_args, model_args)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;66;03m# 清空梯度\u001b[39;00m\n\u001b[0;32m    119\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 120\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_x_mark\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_y_mark\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m features \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMS\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    122\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m outputs[:, :, f_dim: f_dim\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m] \n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[118], line 115\u001b[0m, in \u001b[0;36mTiDE.forward\u001b[1;34m(self, x_enc, x_mark_enc, x_dec, batch_y_mark, mask)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''x_mark_enc is the exogenous dynamic feature described in the original paper'''\u001b[39;00m\n\u001b[0;32m    113\u001b[0m batch_y_mark \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mconcat(\n\u001b[0;32m    114\u001b[0m     [x_mark_enc, batch_y_mark[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpred_len:, :]], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 115\u001b[0m dec_out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforecast(x_enc[:, :, feature], x_mark_enc, x_dec,\n\u001b[0;32m    117\u001b[0m                   batch_y_mark)\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(x_enc\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    119\u001b[0m ], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dec_out\n",
      "Cell \u001b[1;32mIn[118], line 116\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''x_mark_enc is the exogenous dynamic feature described in the original paper'''\u001b[39;00m\n\u001b[0;32m    113\u001b[0m batch_y_mark \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mconcat(\n\u001b[0;32m    114\u001b[0m     [x_mark_enc, batch_y_mark[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpred_len:, :]], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    115\u001b[0m dec_out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforecast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_enc\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_mark_enc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_dec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[43m                  \u001b[49m\u001b[43mbatch_y_mark\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(x_enc\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    119\u001b[0m ], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dec_out\n",
      "Cell \u001b[1;32mIn[118], line 107\u001b[0m, in \u001b[0;36mTiDE.forecast\u001b[1;34m(self, x_enc, x_mark_enc, x_dec, batch_y_mark)\u001b[0m\n\u001b[0;32m    102\u001b[0m dec_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemporalDecoder(\n\u001b[0;32m    103\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcat([feature[:, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseq_len:], decoded],\n\u001b[0;32m    104\u001b[0m               dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual_proj(x_enc)\n\u001b[0;32m    106\u001b[0m \u001b[38;5;66;03m# De-Normalization\u001b[39;00m\n\u001b[1;32m--> 107\u001b[0m dec_out \u001b[38;5;241m=\u001b[39m dec_out \u001b[38;5;241m*\u001b[39m (\u001b[43mstdev\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepeat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpred_len\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    108\u001b[0m dec_out \u001b[38;5;241m=\u001b[39m dec_out \u001b[38;5;241m+\u001b[39m (means[:, \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpred_len))\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dec_out\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 构造参数字典\n",
    "params3 = {\n",
    "    \"task_args\":{\n",
    "        \"columns\": ['load', 'temp'],\n",
    "        \"target\": ['load', 'temp'],\n",
    "        \"features\": 'M'\n",
    "    },\n",
    "    \"train_args\": {\n",
    "        \"model_name\": TiDE,\n",
    "        \"train_loader\": train_loader,\n",
    "        \"valid_loader\": valid_loader,\n",
    "        \"n_epochs\": 20,\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"loss\": nn.MSELoss(),\n",
    "        \"patience\": 7,\n",
    "        \"lradj\": 'cosine',\n",
    "        \"path\": \"../models/test\",\n",
    "        \"verbose\": True,\n",
    "        \"plots\": True,\n",
    "    },\n",
    "    \"model_args\": {\n",
    "        'seq_len': 6,\n",
    "        'pred_len': 3,\n",
    "        \"label_len\": 0,\n",
    "        'd_model': 128,\n",
    "        'd_layers': 1,\n",
    "        'd_ff': 128,\n",
    "        'dropout': 0.1,\n",
    "        'e_layers': 1,\n",
    "        'freq': 'h',\n",
    "        'c_out': 2\n",
    "    },\n",
    "}\n",
    "model = train(**params3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0572f0",
   "metadata": {},
   "source": [
    "# 基于Transformer的时间序列预测"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068db957",
   "metadata": {},
   "source": [
    "## 多变量多步预测"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b100a10",
   "metadata": {},
   "source": [
    "### 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "861d9802",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:30:01.747288Z",
     "start_time": "2024-04-14T13:30:01.738536Z"
    }
   },
   "outputs": [],
   "source": [
    "# 读取数据\n",
    "def get_dataset(path, time_col=None):\n",
    "    data = pd.read_csv(path)\n",
    "    rows = data.shape[0]\n",
    "    now = datetime.now()\n",
    "    newtime = now.replace(microsecond=0)\n",
    "    if time_col == None:\n",
    "        # 如果没有时间列，生成时间戳范围\n",
    "        time_index = pd.date_range(start=datetime.now() -\n",
    "                                   timedelta(seconds=rows - 1),\n",
    "                                   end=datetime.now(),\n",
    "                                   freq='S')\n",
    "        full_data = pd.DataFrame(data=data.values,\n",
    "                                 index=pd.to_datetime(time_index, unit='s'),\n",
    "                                 columns=data.columns)\n",
    "    else:\n",
    "        columns = [i for i in data.columns if i != time_col] # 去除时间列\n",
    "        full_data = pd.DataFrame(data=data.drop([time_col], axis=1).values,\n",
    "                                 index=pd.to_datetime(data[time_col].values),\n",
    "                                 columns=columns)\n",
    "    return full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "ac992088",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:30:02.878328Z",
     "start_time": "2024-04-14T13:30:02.796284Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>load</th>\n",
       "      <th>temp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-01-01 00:00:00</th>\n",
       "      <td>2698.0</td>\n",
       "      <td>32.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-01 01:00:00</th>\n",
       "      <td>2558.0</td>\n",
       "      <td>32.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-01 02:00:00</th>\n",
       "      <td>2444.0</td>\n",
       "      <td>30.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-01 03:00:00</th>\n",
       "      <td>2402.0</td>\n",
       "      <td>31.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-01 04:00:00</th>\n",
       "      <td>2403.0</td>\n",
       "      <td>32.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-12-31 19:00:00</th>\n",
       "      <td>4012.0</td>\n",
       "      <td>18.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-12-31 20:00:00</th>\n",
       "      <td>3856.0</td>\n",
       "      <td>16.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-12-31 21:00:00</th>\n",
       "      <td>3671.0</td>\n",
       "      <td>17.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-12-31 22:00:00</th>\n",
       "      <td>3499.0</td>\n",
       "      <td>15.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-12-31 23:00:00</th>\n",
       "      <td>3345.0</td>\n",
       "      <td>15.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26304 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       load   temp\n",
       "2012-01-01 00:00:00  2698.0  32.00\n",
       "2012-01-01 01:00:00  2558.0  32.67\n",
       "2012-01-01 02:00:00  2444.0  30.00\n",
       "2012-01-01 03:00:00  2402.0  31.00\n",
       "2012-01-01 04:00:00  2403.0  32.00\n",
       "...                     ...    ...\n",
       "2014-12-31 19:00:00  4012.0  18.00\n",
       "2014-12-31 20:00:00  3856.0  16.67\n",
       "2014-12-31 21:00:00  3671.0  17.00\n",
       "2014-12-31 22:00:00  3499.0  15.33\n",
       "2014-12-31 23:00:00  3345.0  15.33\n",
       "\n",
       "[26304 rows x 2 columns]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"../data/energy.csv\"\n",
    "ts_data = get_dataset(path, time_col='time')\n",
    "ts_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "1e83d4b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:30:03.896097Z",
     "start_time": "2024-04-14T13:30:03.858768Z"
    }
   },
   "outputs": [],
   "source": [
    "# 时间格式编码\n",
    "def time_features_from_frequency_str(freq_str: str):\n",
    "    \"\"\"\n",
    "    Returns a list of time features that will be appropriate for the given frequency string.\n",
    "    Parameters\n",
    "    ----------\n",
    "    freq_str\n",
    "        Frequency string of the form [multiple][granularity] such as \"12H\", \"5min\", \"1D\" etc.\n",
    "    \"\"\"\n",
    "\n",
    "    class TimeFeature:\n",
    "        def __init__(self):\n",
    "            pass\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            pass\n",
    "\n",
    "        def __repr__(self):\n",
    "            return self.__class__.__name__ + \"()\"\n",
    "\n",
    "\n",
    "    class SecondOfMinute(TimeFeature):\n",
    "        \"\"\"Minute of hour encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return index.second / 59.0 - 0.5\n",
    "\n",
    "\n",
    "    class MinuteOfHour(TimeFeature):\n",
    "        \"\"\"Minute of hour encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return index.minute / 59.0 - 0.5\n",
    "\n",
    "\n",
    "    class HourOfDay(TimeFeature):\n",
    "        \"\"\"Hour of day encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return index.hour / 23.0 - 0.5\n",
    "\n",
    "\n",
    "    class DayOfWeek(TimeFeature):\n",
    "        \"\"\"Hour of day encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return index.dayofweek / 6.0 - 0.5\n",
    "\n",
    "\n",
    "    class DayOfMonth(TimeFeature):\n",
    "        \"\"\"Day of month encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return (index.day - 1) / 30.0 - 0.5\n",
    "\n",
    "\n",
    "    class DayOfYear(TimeFeature):\n",
    "        \"\"\"Day of year encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return (index.dayofyear - 1) / 365.0 - 0.5\n",
    "\n",
    "\n",
    "    class MonthOfYear(TimeFeature):\n",
    "        \"\"\"Month of year encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return (index.month - 1) / 11.0 - 0.5\n",
    "\n",
    "\n",
    "    class WeekOfYear(TimeFeature):\n",
    "        \"\"\"Week of year encoded as value between [-0.5, 0.5]\"\"\"\n",
    "\n",
    "        def __call__(self, index: pd.DatetimeIndex) -> np.ndarray:\n",
    "            return (index.isocalendar().week - 1) / 52.0 - 0.5\n",
    "\n",
    "    \n",
    "    features_by_offsets = {\n",
    "        offsets.YearEnd: [],\n",
    "        offsets.QuarterEnd: [MonthOfYear],\n",
    "        offsets.MonthEnd: [MonthOfYear],\n",
    "        offsets.Week: [DayOfMonth, WeekOfYear],\n",
    "        offsets.Day: [DayOfWeek, DayOfMonth, DayOfYear],\n",
    "        offsets.BusinessDay: [DayOfWeek, DayOfMonth, DayOfYear],\n",
    "        offsets.Hour: [HourOfDay, DayOfWeek, DayOfMonth, DayOfYear],\n",
    "        offsets.Minute: [\n",
    "            MinuteOfHour,\n",
    "            HourOfDay,\n",
    "            DayOfWeek,\n",
    "            DayOfMonth,\n",
    "            DayOfYear,\n",
    "        ],\n",
    "        offsets.Second: [\n",
    "            SecondOfMinute,\n",
    "            MinuteOfHour,\n",
    "            HourOfDay,\n",
    "            DayOfWeek,\n",
    "            DayOfMonth,\n",
    "            DayOfYear,\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    offset = to_offset(freq_str)\n",
    "\n",
    "    for offset_type, feature_classes in features_by_offsets.items():\n",
    "        if isinstance(offset, offset_type):\n",
    "            return [cls() for cls in feature_classes]\n",
    "\n",
    "    supported_freq_msg = f\"\"\"\n",
    "    Unsupported frequency {freq_str}\n",
    "    The following frequencies are supported:\n",
    "        Y   - yearly\n",
    "            alias: A\n",
    "        M   - monthly\n",
    "        W   - weekly\n",
    "        D   - daily\n",
    "        B   - business days\n",
    "        H   - hourly\n",
    "        T   - minutely\n",
    "            alias: min\n",
    "        S   - secondly\n",
    "    \"\"\"\n",
    "    raise RuntimeError(supported_freq_msg)\n",
    "    \n",
    "def time_features(dates, freq='h'):\n",
    "    return np.vstack([feat(dates) for feat in time_features_from_frequency_str(freq)])\n",
    "\n",
    "# 包含时间维度的数据集划分\n",
    "def divide_dataset(df, valid_date, test_date, x_feature_list, y_feature_list, freq):\n",
    "    #归一化\n",
    "    x_scaler = MinMaxScaler()\n",
    "    y_scaler = MinMaxScaler()\n",
    "\n",
    "    #测试集\n",
    "    train = df.copy()[df.index < valid_date][x_feature_list]\n",
    "    train_stamp = time_features(pd.to_datetime(train.index), freq=freq)\n",
    "    train_stamp = train_stamp.transpose(1, 0)\n",
    "    train[x_feature_list] = x_scaler.fit_transform(train)\n",
    "    xtr = train.values.astype('float32')\n",
    "\n",
    "    #验证集\n",
    "    valid = df.copy()[(df.index >= valid_date) & (df.index < test_date)][x_feature_list]\n",
    "    valid_stamp = time_features(pd.to_datetime(valid.index), freq=freq)\n",
    "    valid_stamp = valid_stamp.transpose(1, 0)\n",
    "    valid[x_feature_list] = x_scaler.fit_transform(valid)\n",
    "    xva = valid.values.astype('float32')\n",
    "\n",
    "    #测试集\n",
    "    test = df.copy()[test_date:][x_feature_list]\n",
    "    test_stamp = time_features(pd.to_datetime(test.index), freq=freq)\n",
    "    test_stamp = test_stamp.transpose(1, 0)\n",
    "    test[x_feature_list] = x_scaler.fit_transform(test)\n",
    "    xte = test.values.astype('float32')\n",
    "\n",
    "    #标签\n",
    "    ytr = df.copy()[df.index < valid_date][y_feature_list]\n",
    "    ytr[y_feature_list] = y_scaler.fit_transform(ytr)\n",
    "    ytr = ytr.values.astype('float32')\n",
    "\n",
    "    yva = df.copy()[(df.index >= valid_date) & (df.index < test_date)][y_feature_list]\n",
    "    yva[y_feature_list] = y_scaler.fit_transform(yva)\n",
    "    yva = yva.values.astype('float32')\n",
    "\n",
    "    yte = df.copy()[test_date:][y_feature_list]\n",
    "    yte[y_feature_list] = y_scaler.fit_transform(yte)\n",
    "    yte = yte.values.astype('float32')\n",
    "    \n",
    "    # 数据合并，[训练集，测试集，时间戳]\n",
    "    train = [xtr, ytr, train_stamp]\n",
    "    valid = [xva, yva, valid_stamp]\n",
    "    test = [xte, yte, test_stamp]\n",
    "    \n",
    "    return y_scaler, train, valid, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "19692d14",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:30:05.048531Z",
     "start_time": "2024-04-14T13:30:04.968650Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (23376, 2) y_train shape: (23376, 1) stamp_train shape: (23376, 4)\n",
      "x_valid shape: (1464, 2) y_valid shape: (1464, 1) stamp_valid shape: (1464, 4)\n",
      "x_test shape: (1464, 2) y_test shape: (1464, 1) stamp_test shape: (1464, 4)\n"
     ]
    }
   ],
   "source": [
    "# 构造参数字典\n",
    "params1 = {\n",
    "    \"df\": ts_data,\n",
    "    \"valid_date\": \"2014-09-01 00:00:00\",\n",
    "    \"test_date\": \"2014-11-01 00:00:00\",\n",
    "    \"x_feature_list\": [\"load\", 'temp'],\n",
    "    \"y_feature_list\": [\"load\"],\n",
    "    \"freq\": 'h'\n",
    "}\n",
    "\n",
    "#函数传参\n",
    "scaler, train, valid, test = divide_dataset(**params1)\n",
    "print(\"x_train shape: {0} y_train shape: {1} stamp_train shape: {2}\".format(train[0].shape, train[1].shape, train[2].shape))\n",
    "print(\"x_valid shape: {0} y_valid shape: {1} stamp_valid shape: {2}\".format(valid[0].shape, valid[1].shape, valid[2].shape))\n",
    "print(\"x_test shape: {0} y_test shape: {1} stamp_test shape: {2}\".format(test[0].shape, test[1].shape, test[2].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "861e19ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:30:06.289302Z",
     "start_time": "2024-04-14T13:30:06.273728Z"
    }
   },
   "outputs": [],
   "source": [
    "#利用前seq_len个数据，预测下pred_len个数据\n",
    "def create_dataset(data_list, seq_len, pred_len, label_len, batch_size):\n",
    "    \"\"\"\n",
    "    @参数设置：\n",
    "    data_list：特征，目标，时间戳\n",
    "    seq_len：输入数据包含过去多少个时间步\n",
    "    pred_len：目标应该在未来多少个时间步之后\n",
    "    label_len：先验时间步\n",
    "    \"\"\"\n",
    "    # 获取数据\n",
    "    feature = data_list[0] # 特征\n",
    "    target = data_list[1] # 目标\n",
    "    stamp = data_list[2] # 时间戳\n",
    "    \n",
    "    # 循环生成数据\n",
    "    X, y = [], []\n",
    "    X_stamp, y_stamp = [], []\n",
    "    seq_len = seq_len - 1 # 包含当前时间点\n",
    "    for i in range(seq_len, len(feature) - pred_len):\n",
    "        # 数据维度\n",
    "        feat = feature[i - seq_len:i + 1]\n",
    "        tar = target[i + 1:i + 1 + pred_len]\n",
    "        X.append(feat)\n",
    "        y.append(tar)\n",
    "        \n",
    "        # 时间维度\n",
    "        xs = stamp[i - seq_len:i + 1]\n",
    "        ys = stamp[i + 1 - label_len:i + 1 + pred_len]\n",
    "        X_stamp.append(xs)\n",
    "        y_stamp.append(ys)\n",
    "        \n",
    "    # 转为张量，数据维度\n",
    "    X = torch.as_tensor(X).float()\n",
    "    y = torch.as_tensor(y).float()\n",
    "    \n",
    "    # 转为张量，时间维度\n",
    "    X_stamp = torch.as_tensor(X_stamp).float()\n",
    "    y_stamp = torch.as_tensor(y_stamp).float()\n",
    "    \n",
    "    # 创建dataloader，[特征，目标，特征时间编码，目标时间编码]\n",
    "    data_loader = DataLoader(TensorDataset(X, y, X_stamp, y_stamp), shuffle=True, batch_size=batch_size)\n",
    "    \n",
    "    return X, y, data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "072de476",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:30:08.319480Z",
     "start_time": "2024-04-14T13:30:07.506687Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_size: torch.Size([23368, 6, 2]),y_size: torch.Size([23368, 3, 1]),loader_len: 731\n",
      "X_size: torch.Size([1456, 6, 2]),y_size: torch.Size([1456, 3, 1]),loader_len: 46\n",
      "X_size: torch.Size([1456, 6, 2]),y_size: torch.Size([1456, 3, 1]),loader_len: 46\n"
     ]
    }
   ],
   "source": [
    "# 构造参数字典\n",
    "params2 = {\n",
    "    \"seq_len\": 6,\n",
    "    \"pred_len\": 3,\n",
    "    \"label_len\": 0,\n",
    "    \"batch_size\": 32,\n",
    "}\n",
    "X_train, y_train, train_loader = create_dataset(train, **params2)\n",
    "X_valid, y_valid, valid_loader = create_dataset(valid, **params2)\n",
    "X_test, y_test, test_loader = create_dataset(test, **params2)\n",
    "print(\"X_size: {0},y_size: {1},loader_len: {2}\".format(X_train.shape, y_train.shape, len(train_loader)))\n",
    "print(\"X_size: {0},y_size: {1},loader_len: {2}\".format(X_valid.shape, y_valid.shape, len(valid_loader)))\n",
    "print(\"X_size: {0},y_size: {1},loader_len: {2}\".format(X_test.shape, y_test.shape, len(test_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b497a3",
   "metadata": {},
   "source": [
    "### 模型定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "f0713ebc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:30:10.753507Z",
     "start_time": "2024-04-14T13:30:10.686129Z"
    }
   },
   "outputs": [],
   "source": [
    "# DataEmbedding编码类\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model).float()\n",
    "        pe.require_grad = False\n",
    "\n",
    "        position = torch.arange(0, max_len).float().unsqueeze(1)\n",
    "        div_term = (torch.arange(0, d_model, 2).float()\n",
    "                    * -(math.log(10000.0) / d_model)).exp()\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pe[:, :x.size(1)]\n",
    "\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, c_in, d_model):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        padding = 1 if torch.__version__ >= '1.5.0' else 2\n",
    "        self.tokenConv = nn.Conv1d(in_channels=c_in, out_channels=d_model,\n",
    "                                   kernel_size=3, padding=padding, padding_mode='circular', bias=False)\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                nn.init.kaiming_normal_(\n",
    "                    m.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.tokenConv(x.permute(0, 2, 1)).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class FixedEmbedding(nn.Module):\n",
    "    def __init__(self, c_in, d_model):\n",
    "        super(FixedEmbedding, self).__init__()\n",
    "\n",
    "        w = torch.zeros(c_in, d_model).float()\n",
    "        w.require_grad = False\n",
    "\n",
    "        position = torch.arange(0, c_in).float().unsqueeze(1)\n",
    "        div_term = (torch.arange(0, d_model, 2).float()\n",
    "                    * -(math.log(10000.0) / d_model)).exp()\n",
    "\n",
    "        w[:, 0::2] = torch.sin(position * div_term)\n",
    "        w[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        self.emb = nn.Embedding(c_in, d_model)\n",
    "        self.emb.weight = nn.Parameter(w, requires_grad=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.emb(x).detach()\n",
    "\n",
    "\n",
    "class TemporalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, embed_type='fixed', freq='h'):\n",
    "        super(TemporalEmbedding, self).__init__()\n",
    "\n",
    "        minute_size = 4\n",
    "        hour_size = 24\n",
    "        weekday_size = 7\n",
    "        day_size = 32\n",
    "        month_size = 13\n",
    "\n",
    "        Embed = FixedEmbedding if embed_type == 'fixed' else nn.Embedding\n",
    "        if freq == 't':\n",
    "            self.minute_embed = Embed(minute_size, d_model)\n",
    "        self.hour_embed = Embed(hour_size, d_model)\n",
    "        self.weekday_embed = Embed(weekday_size, d_model)\n",
    "        self.day_embed = Embed(day_size, d_model)\n",
    "        self.month_embed = Embed(month_size, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.long()\n",
    "        minute_x = self.minute_embed(x[:, :, 4]) if hasattr(\n",
    "            self, 'minute_embed') else 0.\n",
    "        hour_x = self.hour_embed(x[:, :, 3])\n",
    "        weekday_x = self.weekday_embed(x[:, :, 2])\n",
    "        day_x = self.day_embed(x[:, :, 1])\n",
    "        month_x = self.month_embed(x[:, :, 0])\n",
    "\n",
    "        return hour_x + weekday_x + day_x + month_x + minute_x\n",
    "\n",
    "\n",
    "class TimeFeatureEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, embed_type='timeF', freq='h'):\n",
    "        super(TimeFeatureEmbedding, self).__init__()\n",
    "\n",
    "        freq_map = {'h': 4, 't': 5, 's': 6,\n",
    "                    'm': 1, 'a': 1, 'w': 2, 'd': 3, 'b': 3}\n",
    "        d_inp = freq_map[freq]\n",
    "        self.embed = nn.Linear(d_inp, d_model, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embed(x)\n",
    "\n",
    "\n",
    "class DataEmbedding(nn.Module):\n",
    "    def __init__(self, c_in, d_model, embed_type='fixed', freq='h', dropout=0.1):\n",
    "        super(DataEmbedding, self).__init__()\n",
    "\n",
    "        self.value_embedding = TokenEmbedding(c_in=c_in, d_model=d_model)\n",
    "        self.position_embedding = PositionalEmbedding(d_model=d_model)\n",
    "        self.temporal_embedding = TemporalEmbedding(d_model=d_model, embed_type=embed_type,\n",
    "                                                    freq=freq) if embed_type != 'timeF' else TimeFeatureEmbedding(\n",
    "            d_model=d_model, embed_type=embed_type, freq=freq)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, x_mark):\n",
    "        if x_mark is None:\n",
    "            x = self.value_embedding(x) + self.position_embedding(x)\n",
    "        else:\n",
    "            x = self.value_embedding(x) + self.temporal_embedding(x_mark) + self.position_embedding(x)\n",
    "        return self.dropout(x)\n",
    "    \n",
    "    \n",
    "# Transformer_EncDec类\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, attention, d_model, d_ff=None, dropout=0.1, activation=\"relu\"):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        d_ff = d_ff or 4 * d_model\n",
    "        self.attention = attention\n",
    "        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = F.relu if activation == \"relu\" else F.gelu\n",
    "\n",
    "    def forward(self, x, attn_mask=None, tau=None, delta=None):\n",
    "        new_x, attn = self.attention(\n",
    "            x, x, x,\n",
    "            attn_mask=attn_mask,\n",
    "            tau=tau, delta=delta\n",
    "        )\n",
    "        x = x + self.dropout(new_x)\n",
    "\n",
    "        y = x = self.norm1(x)\n",
    "        y = self.dropout(self.activation(self.conv1(y.transpose(-1, 1))))\n",
    "        y = self.dropout(self.conv2(y).transpose(-1, 1))\n",
    "\n",
    "        return self.norm2(x + y), attn\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, attn_layers, conv_layers=None, norm_layer=None):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.attn_layers = nn.ModuleList(attn_layers)\n",
    "        self.conv_layers = nn.ModuleList(conv_layers) if conv_layers is not None else None\n",
    "        self.norm = norm_layer\n",
    "\n",
    "    def forward(self, x, attn_mask=None, tau=None, delta=None):\n",
    "        # x [B, L, D]\n",
    "        attns = []\n",
    "        if self.conv_layers is not None:\n",
    "            for i, (attn_layer, conv_layer) in enumerate(zip(self.attn_layers, self.conv_layers)):\n",
    "                delta = delta if i == 0 else None\n",
    "                x, attn = attn_layer(x, attn_mask=attn_mask, tau=tau, delta=delta)\n",
    "                x = conv_layer(x)\n",
    "                attns.append(attn)\n",
    "            x, attn = self.attn_layers[-1](x, tau=tau, delta=None)\n",
    "            attns.append(attn)\n",
    "        else:\n",
    "            for attn_layer in self.attn_layers:\n",
    "                x, attn = attn_layer(x, attn_mask=attn_mask, tau=tau, delta=delta)\n",
    "                attns.append(attn)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "\n",
    "        return x, attns\n",
    "    \n",
    "    \n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, self_attention, cross_attention, d_model, d_ff=None,\n",
    "                 dropout=0.1, activation=\"relu\"):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        d_ff = d_ff or 4 * d_model\n",
    "        self.self_attention = self_attention\n",
    "        self.cross_attention = cross_attention\n",
    "        self.conv1 = nn.Conv1d(in_channels=d_model, out_channels=d_ff, kernel_size=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=d_ff, out_channels=d_model, kernel_size=1)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = F.relu if activation == \"relu\" else F.gelu\n",
    "\n",
    "    def forward(self, x, cross, x_mask=None, cross_mask=None, tau=None, delta=None):\n",
    "        x = x + self.dropout(self.self_attention(\n",
    "            x, x, x,\n",
    "            attn_mask=x_mask,\n",
    "            tau=tau, delta=None\n",
    "        )[0])\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        x = x + self.dropout(self.cross_attention(\n",
    "            x, cross, cross,\n",
    "            attn_mask=cross_mask,\n",
    "            tau=tau, delta=delta\n",
    "        )[0])\n",
    "\n",
    "        y = x = self.norm2(x)\n",
    "        y = self.dropout(self.activation(self.conv1(y.transpose(-1, 1))))\n",
    "        y = self.dropout(self.conv2(y).transpose(-1, 1))\n",
    "\n",
    "        return self.norm3(x + y)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, layers, norm_layer=None, projection=None):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.norm = norm_layer\n",
    "        self.projection = projection\n",
    "\n",
    "    def forward(self, x, cross, x_mask=None, cross_mask=None, tau=None, delta=None):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, cross, x_mask=x_mask, cross_mask=cross_mask, tau=tau, delta=delta)\n",
    "\n",
    "        if self.norm is not None:\n",
    "            x = self.norm(x)\n",
    "\n",
    "        if self.projection is not None:\n",
    "            x = self.projection(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "# 自编码类\n",
    "class TriangularCausalMask():\n",
    "    def __init__(self, B, L, device=\"cpu\"):\n",
    "        mask_shape = [B, 1, L, L]\n",
    "        with torch.no_grad():\n",
    "            self._mask = torch.triu(torch.ones(mask_shape, dtype=torch.bool), diagonal=1).to(device)\n",
    "\n",
    "    @property\n",
    "    def mask(self):\n",
    "        return self._mask\n",
    "    \n",
    "    \n",
    "class FullAttention(nn.Module):\n",
    "    def __init__(self, mask_flag=True, factor=5, scale=None, attention_dropout=0.1, output_attention=False):\n",
    "        super(FullAttention, self).__init__()\n",
    "        self.scale = scale\n",
    "        self.mask_flag = mask_flag\n",
    "        self.output_attention = output_attention\n",
    "        self.dropout = nn.Dropout(attention_dropout)\n",
    "\n",
    "    def forward(self, queries, keys, values, attn_mask, tau=None, delta=None):\n",
    "        B, L, H, E = queries.shape\n",
    "        _, S, _, D = values.shape\n",
    "        scale = self.scale or 1. / math.sqrt(E)\n",
    "\n",
    "        scores = torch.einsum(\"blhe,bshe->bhls\", queries, keys)\n",
    "\n",
    "        if self.mask_flag:\n",
    "            if attn_mask is None:\n",
    "                attn_mask = TriangularCausalMask(B, L, device=queries.device)\n",
    "\n",
    "            scores.masked_fill_(attn_mask.mask, -np.inf)\n",
    "\n",
    "        A = self.dropout(torch.softmax(scale * scores, dim=-1))\n",
    "        V = torch.einsum(\"bhls,bshd->blhd\", A, values)\n",
    "\n",
    "        if self.output_attention:\n",
    "            return V.contiguous(), A\n",
    "        else:\n",
    "            return V.contiguous(), None\n",
    "        \n",
    "        \n",
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, attention, d_model, n_heads, d_keys=None,\n",
    "                 d_values=None):\n",
    "        super(AttentionLayer, self).__init__()\n",
    "\n",
    "        d_keys = d_keys or (d_model // n_heads)\n",
    "        d_values = d_values or (d_model // n_heads)\n",
    "\n",
    "        self.inner_attention = attention\n",
    "        self.query_projection = nn.Linear(d_model, d_keys * n_heads)\n",
    "        self.key_projection = nn.Linear(d_model, d_keys * n_heads)\n",
    "        self.value_projection = nn.Linear(d_model, d_values * n_heads)\n",
    "        self.out_projection = nn.Linear(d_values * n_heads, d_model)\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "    def forward(self, queries, keys, values, attn_mask, tau=None, delta=None):\n",
    "        B, L, _ = queries.shape\n",
    "        _, S, _ = keys.shape\n",
    "        H = self.n_heads\n",
    "\n",
    "        queries = self.query_projection(queries).view(B, L, H, -1)\n",
    "        keys = self.key_projection(keys).view(B, S, H, -1)\n",
    "        values = self.value_projection(values).view(B, S, H, -1)\n",
    "\n",
    "        out, attn = self.inner_attention(\n",
    "            queries,\n",
    "            keys,\n",
    "            values,\n",
    "            attn_mask,\n",
    "            tau=tau,\n",
    "            delta=delta\n",
    "        )\n",
    "        out = out.view(B, L, -1)\n",
    "\n",
    "        return self.out_projection(out), attn\n",
    "    \n",
    "    \n",
    "# Transformer模型\n",
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "        embed: time features encoding, options:[timeF, fixed, learned]\n",
    "        freq: 'freq for time features encoding, options:[s:secondly, \n",
    "            t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], \n",
    "            you can also use more detailed freq like 15min or 3h'\n",
    "    \"\"\"\n",
    "    def __init__(self, pred_len, label_len, output_attention, enc_in, d_model, dropout, factor, n_heads, d_ff, \n",
    "                e_layers, dec_in, d_layers, c_out, embed, freq):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.pred_len = pred_len\n",
    "        self.output_attention = output_attention\n",
    "        # Embedding\n",
    "        self.enc_embedding = DataEmbedding(enc_in, d_model, embed, freq, dropout)\n",
    "        # Encoder\n",
    "        self.encoder = Encoder(\n",
    "            [\n",
    "                EncoderLayer(\n",
    "                    AttentionLayer(\n",
    "                        FullAttention(False, factor, attention_dropout=dropout,\n",
    "                                      output_attention=output_attention), d_model, n_heads),\n",
    "                    d_model,\n",
    "                    d_ff,\n",
    "                    dropout=dropout,\n",
    "                    activation='relu'\n",
    "                ) for l in range(e_layers)\n",
    "            ],\n",
    "            norm_layer=torch.nn.LayerNorm(d_model)\n",
    "        )\n",
    "        # Decoder\n",
    "        self.dec_embedding = DataEmbedding(dec_in, d_model, embed, freq, dropout)\n",
    "        self.decoder = Decoder(\n",
    "            [\n",
    "                DecoderLayer(\n",
    "                    AttentionLayer(\n",
    "                        FullAttention(True, factor, attention_dropout=dropout,\n",
    "                                      output_attention=False),\n",
    "                        d_model, n_heads),\n",
    "                    AttentionLayer(\n",
    "                        FullAttention(False, factor, attention_dropout=dropout,\n",
    "                                      output_attention=False),\n",
    "                        d_model, n_heads),\n",
    "                    d_model,\n",
    "                    d_ff,\n",
    "                    dropout=dropout,\n",
    "                    activation='relu',\n",
    "                )\n",
    "                for l in range(d_layers)\n",
    "            ],\n",
    "            norm_layer=torch.nn.LayerNorm(d_model),\n",
    "            projection=nn.Linear(d_model, c_out, bias=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec):\n",
    "        # Embedding\n",
    "        enc_out = self.enc_embedding(x_enc, x_mark_enc)\n",
    "        enc_out, attns = self.encoder(enc_out, attn_mask=None)\n",
    "\n",
    "        dec_out = self.dec_embedding(x_dec, x_mark_dec)\n",
    "        dec_out = self.decoder(dec_out, enc_out, x_mask=None, cross_mask=None)\n",
    "        \n",
    "        output = dec_out[:, -self.pred_len:, :]  # [B, L, D]\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426e292d",
   "metadata": {},
   "source": [
    "### 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "d3f66bff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:30:13.333548Z",
     "start_time": "2024-04-14T13:30:13.296351Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(task_args, train_args, model_args):\n",
    "    # 参数配置\n",
    "    columns = task_args['columns'] # 模型全部特征\n",
    "    target = task_args['target'] # 模型预测特征\n",
    "    features = task_args['features'] # 模型预测模式\n",
    "    model_name = train_args['model_name'] # 模型名称\n",
    "    train_loader = train_args['train_loader'] # 训练集\n",
    "    valid_loader = train_args['valid_loader'] # 验证集\n",
    "    n_epochs = train_args['n_epochs'] # 训练次数\n",
    "    learning_rate = train_args['learning_rate'] # 学习率\n",
    "    loss = train_args['loss'] # 损失函数\n",
    "    patience = train_args['patience'] # 最大早停次数阈值，超过就会早停\n",
    "    lradj = train_args['lradj'] # 学习率函数\n",
    "    path = train_args['path'] # 模型保存路径\n",
    "    verbose = train_args['verbose'] # 打印训练过程\n",
    "    plots = train_args['plots'] # 绘制损失图\n",
    "    \n",
    "    #检查是否可用GPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # 创建模型和优化器\n",
    "    model = model_name(**model_args).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    loss_fn = loss\n",
    "    \n",
    "    # 损失函数值\n",
    "    train_losses, val_losses = [], []\n",
    "    \n",
    "    # 调整学习率\n",
    "    def adjust_learning_rate(optimizer, epoch, lradj, learning_rate, train_epochs):\n",
    "        # lr = learning_rate * (0.2 ** (epoch // 2))\n",
    "        if lradj == 'type1':\n",
    "            lr_adjust = {epoch: learning_rate * (0.5 ** ((epoch - 1) // 1))}\n",
    "        elif lradj == 'type2':\n",
    "            lr_adjust = {\n",
    "                2: 5e-5, 4: 1e-5, 6: 5e-6, 8: 1e-6,\n",
    "                10: 5e-7, 15: 1e-7, 20: 5e-8\n",
    "            }\n",
    "        elif lradj == \"cosine\":\n",
    "            lr_adjust = {epoch: learning_rate /2 * (1 + math.cos(epoch / train_epochs * math.pi))}\n",
    "        if epoch in lr_adjust.keys():\n",
    "            lr = lr_adjust[epoch]\n",
    "            # 参数组(param_groups)是用来指定不同的参数组以便对它们进行不同的优化设置，比如'lr'\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = lr\n",
    "            print('Updating learning rate to {}'.format(lr))\n",
    "    \n",
    "    # 设置早停\n",
    "    class EarlyStopping():\n",
    "        def __init__(self, patience=7, verbose=False, delta=0):\n",
    "            self.patience = patience # 连续超限次数，如果满足条件，则早停\n",
    "            self.verbose = verbose\n",
    "            self.counter = 0\n",
    "            self.best_score = None\n",
    "            self.early_stop = False\n",
    "            self.val_loss_min = np.Inf\n",
    "            self.delta = delta\n",
    "\n",
    "        def __call__(self, val_loss, model, path):\n",
    "            score = -val_loss\n",
    "            if self.best_score is None:\n",
    "                self.best_score = score\n",
    "                self.save_checkpoint(val_loss, model, path)\n",
    "            elif score < self.best_score + self.delta:\n",
    "                self.counter += 1\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "                if self.counter >= self.patience:\n",
    "                    self.early_stop = True\n",
    "            else:\n",
    "                self.best_score = score\n",
    "                self.save_checkpoint(val_loss, model, path)\n",
    "                self.counter = 0\n",
    "\n",
    "        def save_checkpoint(self, val_loss, model, path):\n",
    "            if self.verbose:\n",
    "                print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "            torch.save(model.state_dict(), path + '/' + 'checkpoint.pth')\n",
    "            self.val_loss_min = val_loss\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=verbose)\n",
    "    \n",
    "    # 训练任务\n",
    "    def forecasting_task(columns, target, features):\n",
    "        \"\"\"\n",
    "        features: [M, S, MS]; \n",
    "            M:multivariate predict multivariate, \n",
    "            S:univariate predict univariate, \n",
    "            MS:multivariate predict univariate'\n",
    "        \"\"\"\n",
    "        # 字典索引生成\n",
    "        col_dict = {}\n",
    "        for i,j in enumerate(columns):\n",
    "            col_dict[j] = i\n",
    "\n",
    "        if features == 'MS':\n",
    "            target = target[0]\n",
    "            f_dim = col_dict[target]\n",
    "        elif features == 'S':\n",
    "            f_dim = 0\n",
    "        else:\n",
    "            f_dim = 0\n",
    "        return f_dim\n",
    "    f_dim = forecasting_task(columns, target, features)\n",
    "    \n",
    "    # 设置保存模型路径\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "\n",
    "    # 模型训练和验证\n",
    "    for epoch in tqdm(range(n_epochs)):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for batch_x, batch_y, batch_x_mark, batch_y_mark in train_loader:\n",
    "            #将数据移至 GPU\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            batch_x_mark = batch_x_mark.to(device)\n",
    "            batch_y_mark = batch_y_mark.to(device)\n",
    "            # 清空梯度\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x, batch_x_mark, batch_y, batch_y_mark)\n",
    "            if features == 'MS':\n",
    "                outputs = outputs[:, :, f_dim: f_dim+1] \n",
    "            else:\n",
    "                outputs = outputs[:, :, f_dim:]\n",
    "            train_loss = loss_fn(outputs, batch_y)\n",
    "            # 反向传播计算得到每个参数的梯度值\n",
    "            train_loss.backward()\n",
    "            # 通过梯度下降执行一步参数更新\n",
    "            optimizer.step()\n",
    "            #每个batch的loss和\n",
    "            total_train_loss += train_loss.item() # .item()表示只包含一个元素的tensor中提取值\n",
    "                \n",
    "        #每个epoch的损失平均\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        \n",
    "        #所有epoch的loss\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        #评估模型\n",
    "        model.eval()\n",
    "        #关闭自动求导功能，只使用训练好的模型进行预测或评估，不需要进行梯度计算和参数更新\n",
    "        with torch.no_grad():\n",
    "            total_val_loss = 0\n",
    "            for batch_x, batch_y, batch_x_mark, batch_y_mark in valid_loader:\n",
    "                #将数据移至 GPU\n",
    "                batch_x = batch_x.to(device)\n",
    "                batch_y = batch_y.to(device)\n",
    "                batch_x_mark = batch_x_mark.to(device)\n",
    "                batch_y_mark = batch_y_mark.to(device)\n",
    "                val_outputs = model(batch_x, batch_x_mark, batch_y, batch_y_mark)\n",
    "                if features == 'MS':\n",
    "                    val_outputs = val_outputs[:, :, f_dim: f_dim+1] \n",
    "                else:\n",
    "                    val_outputs = val_outputs[:, :, f_dim:]\n",
    "                val_loss = loss_fn(val_outputs, batch_y)\n",
    "                #每个batch的loss和\n",
    "                total_val_loss += val_loss.item()\n",
    "                \n",
    "        #每个epoch的损失平均\n",
    "        avg_val_loss = total_val_loss / len(valid_loader)\n",
    "        \n",
    "        #所有epoch的loss\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        #打印训练过程\n",
    "        if verbose:\n",
    "            print(f'Epoch [{epoch+1}/{n_epochs}], Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}')\n",
    "            \n",
    "        # 设置早停\n",
    "        early_stopping(avg_val_loss, model, path)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping!\")\n",
    "            break\n",
    "            \n",
    "        # 调整学习率\n",
    "        adjust_learning_rate(optimizer, epoch+1, lradj, learning_rate, n_epochs)\n",
    "        \n",
    "    # 加载最佳模型\n",
    "    best_model_path = path + '/' + 'checkpoint.pth'\n",
    "    model.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "    #绘制损失函数图\n",
    "    def plot_loss(train_losses, val_losses):\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.style.use('seaborn-v0_8-paper') #绘制背景色\n",
    "        plt.grid(axis = 'y',linewidth=0.35) #绘制网格\n",
    "        plt.plot(val_losses, linestyle='-',color = '#11b3b6')\n",
    "        plt.plot(train_losses, linestyle='-',color = '#f14643')\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Training and Validation Progress\")\n",
    "        plt.legend([\"Validation\", \"Training\"])\n",
    "        plt.show()\n",
    "    if plots:\n",
    "        plot_loss(train_losses, val_losses)\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "ed24cc1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-14T13:30:31.594036Z",
     "start_time": "2024-04-14T13:30:15.559450Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|████▏                                                                              | 1/20 [00:11<03:39, 11.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Training Loss: 0.0183, Validation Loss: 0.0013\n",
      "Validation loss decreased (inf --> 0.001266).  Saving model ...\n",
      "Updating learning rate to 0.0009938441702975688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|████▏                                                                              | 1/20 [00:13<04:23, 13.89s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[129], line 39\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 构造参数字典\u001b[39;00m\n\u001b[0;32m      2\u001b[0m params3 \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtask_args\u001b[39m\u001b[38;5;124m\"\u001b[39m:{\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mload\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtemp\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m     },\n\u001b[0;32m     38\u001b[0m }\n\u001b[1;32m---> 39\u001b[0m model \u001b[38;5;241m=\u001b[39m train(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams3)\n",
      "Cell \u001b[1;32mIn[128], line 120\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(task_args, train_args, model_args)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;66;03m# 清空梯度\u001b[39;00m\n\u001b[0;32m    119\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 120\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_x_mark\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_y_mark\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m features \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMS\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    122\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m outputs[:, :, f_dim: f_dim\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m] \n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[127], line 373\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, x_enc, x_mark_enc, x_dec, x_mark_dec)\u001b[0m\n\u001b[0;32m    370\u001b[0m enc_out, attns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(enc_out, attn_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    372\u001b[0m dec_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdec_embedding(x_dec, x_mark_dec)\n\u001b[1;32m--> 373\u001b[0m dec_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdec_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcross_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    375\u001b[0m output \u001b[38;5;241m=\u001b[39m dec_out[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpred_len:, :]  \u001b[38;5;66;03m# [B, L, D]\u001b[39;00m\n\u001b[0;32m    376\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[127], line 224\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[1;34m(self, x, cross, x_mask, cross_mask, tau, delta)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, cross, x_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, cross_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, tau\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, delta\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m--> 224\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcross\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcross_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtau\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtau\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdelta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    226\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    227\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[127], line 195\u001b[0m, in \u001b[0;36mDecoderLayer.forward\u001b[1;34m(self, x, cross, x_mask, cross_mask, tau, delta)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, cross, x_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, cross_mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, tau\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, delta\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 195\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtau\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtau\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[0;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    200\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x)\n\u001b[0;32m    202\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcross_attention(\n\u001b[0;32m    203\u001b[0m         x, cross, cross,\n\u001b[0;32m    204\u001b[0m         attn_mask\u001b[38;5;241m=\u001b[39mcross_mask,\n\u001b[0;32m    205\u001b[0m         tau\u001b[38;5;241m=\u001b[39mtau, delta\u001b[38;5;241m=\u001b[39mdelta\n\u001b[0;32m    206\u001b[0m     )[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[127], line 300\u001b[0m, in \u001b[0;36mAttentionLayer.forward\u001b[1;34m(self, queries, keys, values, attn_mask, tau, delta)\u001b[0m\n\u001b[0;32m    297\u001b[0m keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_projection(keys)\u001b[38;5;241m.\u001b[39mview(B, S, H, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    298\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_projection(values)\u001b[38;5;241m.\u001b[39mview(B, S, H, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 300\u001b[0m out, attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minner_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    301\u001b[0m \u001b[43m    \u001b[49m\u001b[43mqueries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    302\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    303\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    304\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    305\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtau\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtau\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    306\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdelta\u001b[49m\n\u001b[0;32m    307\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    308\u001b[0m out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mview(B, L, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_projection(out), attn\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[127], line 267\u001b[0m, in \u001b[0;36mFullAttention.forward\u001b[1;34m(self, queries, keys, values, attn_mask, tau, delta)\u001b[0m\n\u001b[0;32m    263\u001b[0m         attn_mask \u001b[38;5;241m=\u001b[39m TriangularCausalMask(B, L, device\u001b[38;5;241m=\u001b[39mqueries\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    265\u001b[0m     scores\u001b[38;5;241m.\u001b[39mmasked_fill_(attn_mask\u001b[38;5;241m.\u001b[39mmask, \u001b[38;5;241m-\u001b[39mnp\u001b[38;5;241m.\u001b[39minf)\n\u001b[1;32m--> 267\u001b[0m A \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscale\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mscores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    268\u001b[0m V \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbhls,bshd->blhd\u001b[39m\u001b[38;5;124m\"\u001b[39m, A, values)\n\u001b[0;32m    270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_attention:\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\nn\\modules\\dropout.py:59\u001b[0m, in \u001b[0;36mDropout.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda\\lib\\site-packages\\torch\\nn\\functional.py:1252\u001b[0m, in \u001b[0;36mdropout\u001b[1;34m(input, p, training, inplace)\u001b[0m\n\u001b[0;32m   1250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[0;32m   1251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(p))\n\u001b[1;32m-> 1252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 构造参数字典\n",
    "params3 = {\n",
    "    \"task_args\":{\n",
    "        \"columns\": ['load', 'temp'],\n",
    "        \"target\": ['load'],\n",
    "        \"features\": 'MS'\n",
    "    },\n",
    "    \"train_args\": {\n",
    "        \"model_name\": Transformer,\n",
    "        \"train_loader\": train_loader,\n",
    "        \"valid_loader\": valid_loader,\n",
    "        \"n_epochs\": 20,\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"loss\": nn.MSELoss(),\n",
    "        \"patience\": 3,\n",
    "        \"lradj\": 'cosine',\n",
    "        \"path\": \"../models/test\",\n",
    "        \"verbose\": True,\n",
    "        \"plots\": True,\n",
    "    },\n",
    "    \"model_args\": {\n",
    "        'pred_len': 3, \n",
    "        \"label_len\": 0,\n",
    "        'output_attention': True,\n",
    "        'embed': 'fixed',\n",
    "        'freq': 'h',\n",
    "        'd_model': 128,\n",
    "        'enc_in': 2,\n",
    "        'dec_in':1,\n",
    "        'dropout': 0.1,\n",
    "        'factor': 3,\n",
    "        'n_heads': 8,\n",
    "        'd_ff': 128,\n",
    "        'e_layers': 1,\n",
    "        'd_layers': 1,\n",
    "        'c_out': 1\n",
    "    },\n",
    "}\n",
    "model = train(**params3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f426be23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc": {
   "base_numbering": "7",
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "198.661px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
