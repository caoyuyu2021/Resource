{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58f83253",
   "metadata": {},
   "source": [
    "# Gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c3cb29",
   "metadata": {},
   "source": [
    "Gym库是OpenAI推出的强化学习实验环境库。它用Python语言实现了离散之间智能体-环境接口中的环境部分。本文中“环境”一词均指强化学习基本框架模型之“智能体-环境”接口中的“环境”，每个环境就代表着一类强化学习问题，用户通过设计和训练自己的智能体来解决这些强化学习问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2f68f8b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-01T06:20:33.977802Z",
     "start_time": "2023-09-01T06:20:33.973359Z"
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import envs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384513bf",
   "metadata": {},
   "source": [
    "## 查看gym所有的可用环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c110900",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-01T06:27:03.767876Z",
     "start_time": "2023-09-01T06:27:03.760235Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 44 envs in gym\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['CartPole-v0',\n",
       " 'CartPole-v1',\n",
       " 'MountainCar-v0',\n",
       " 'MountainCarContinuous-v0',\n",
       " 'Pendulum-v1',\n",
       " 'Acrobot-v1',\n",
       " 'LunarLander-v2',\n",
       " 'LunarLanderContinuous-v2',\n",
       " 'BipedalWalker-v3',\n",
       " 'BipedalWalkerHardcore-v3',\n",
       " 'CarRacing-v2',\n",
       " 'Blackjack-v1',\n",
       " 'FrozenLake-v1',\n",
       " 'FrozenLake8x8-v1',\n",
       " 'CliffWalking-v0',\n",
       " 'Taxi-v3',\n",
       " 'Reacher-v2',\n",
       " 'Reacher-v4',\n",
       " 'Pusher-v2',\n",
       " 'Pusher-v4',\n",
       " 'InvertedPendulum-v2',\n",
       " 'InvertedPendulum-v4',\n",
       " 'InvertedDoublePendulum-v2',\n",
       " 'InvertedDoublePendulum-v4',\n",
       " 'HalfCheetah-v2',\n",
       " 'HalfCheetah-v3',\n",
       " 'HalfCheetah-v4',\n",
       " 'Hopper-v2',\n",
       " 'Hopper-v3',\n",
       " 'Hopper-v4',\n",
       " 'Swimmer-v2',\n",
       " 'Swimmer-v3',\n",
       " 'Swimmer-v4',\n",
       " 'Walker2d-v2',\n",
       " 'Walker2d-v3',\n",
       " 'Walker2d-v4',\n",
       " 'Ant-v2',\n",
       " 'Ant-v3',\n",
       " 'Ant-v4',\n",
       " 'Humanoid-v2',\n",
       " 'Humanoid-v3',\n",
       " 'Humanoid-v4',\n",
       " 'HumanoidStandup-v2',\n",
       " 'HumanoidStandup-v4']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_list = envs.registry.keys()\n",
    "env_ids = [env_item for env_item in env_list]\n",
    "print('There are {0} envs in gym'.format(len(env_ids)))\n",
    "env_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7f5bdf",
   "metadata": {},
   "source": [
    "## gym基本函数接口"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5dbd2d",
   "metadata": {},
   "source": [
    "使用gym环境，首先第一件事情就是通过调用`gym.make()`来创建环境对象。\n",
    "\n",
    "然后，智能体通过调用env类的方法来与环境进行交互，常用的env类方法有：`reset()`, `close()`, `render()`, `step()`，等等。以下分别介绍。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afec3270",
   "metadata": {},
   "source": [
    "### make(): 生成环境对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad156dbe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-01T06:36:32.220728Z",
     "start_time": "2023-09-01T06:36:32.214563Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nenv = gym.make(id)\\n    说明：生成环境\\n    参数：Id(str类型)  环境ID\\n    返回值：env(Env类型)  环境\\n    \\n    环境ID是OpenAI Gym提供的环境的ID，可以通过上一节所述方式进行查看有哪些可用的环境\\n    例如，如果是“CartPole”环境，则ID可以用“CartPole-v1”。返回“Env”对象作为返回值\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "env = gym.make(id)\n",
    "    说明：生成环境\n",
    "    参数：Id(str类型)  环境ID\n",
    "    返回值：env(Env类型)  环境\n",
    "    \n",
    "    环境ID是OpenAI Gym提供的环境的ID，可以通过上一节所述方式进行查看有哪些可用的环境\n",
    "    例如，如果是“CartPole”环境，则ID可以用“CartPole-v1”。返回“Env”对象作为返回值\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b6efff",
   "metadata": {},
   "source": [
    "通过make()创建完环境对象后，可以查看环境的属性和当前状态，代码示例如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f063c0b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-01T06:38:31.426353Z",
     "start_time": "2023-09-01T06:38:31.402624Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "观测空间 = Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n",
      "动作空间 = Discrete(2)\n",
      "动作数 = 2\n",
      "初始状态 = None\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "print('观测空间 = {}'.format(env.observation_space))\n",
    "print('动作空间 = {}'.format(env.action_space))\n",
    "print('动作数 = {}'.format(env.action_space.n))\n",
    "print('初始状态 = {}'.format(env.state))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdbbac90",
   "metadata": {},
   "source": [
    "### reset()函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e599616",
   "metadata": {},
   "source": [
    " reset()为环境复位初始化函数。将环境的状态恢复到初始状态。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cf0b84",
   "metadata": {},
   "source": [
    "函数接口说明如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "35e99600",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-01T06:40:45.691709Z",
     "start_time": "2023-09-01T06:40:45.685570Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nstate = env.reset()\\n    说明：重置环境，回到初始状态\\n    返回值：state（object类型） 环境的最初状态。类型由属性“observation_space”决定\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "state = env.reset()\n",
    "    说明：重置环境，回到初始状态\n",
    "    返回值：state（object类型） 环境的最初状态。类型由属性“observation_space”决定\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "72fdb0b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-01T06:41:02.225195Z",
     "start_time": "2023-09-01T06:41:02.219248Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始状态 = (array([ 0.00718721,  0.02455014, -0.01747924, -0.00514615], dtype=float32), {})\n",
      "初始状态 = [ 0.00718721  0.02455014 -0.01747924 -0.00514615]\n"
     ]
    }
   ],
   "source": [
    "init_state = env.reset()\n",
    "print('初始状态 = {}'.format(init_state))\n",
    "print('初始状态 = {}'.format(env.state))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d11255b",
   "metadata": {},
   "source": [
    "### env.state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a749ae",
   "metadata": {},
   "source": [
    "env.state用于查看环境当前状态。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ae917b",
   "metadata": {},
   "source": [
    "### step()：单步执行"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbeb545",
   "metadata": {},
   "source": [
    "单步执行指智能体与环境之间的一次交互，即智能体在当前状态s下执行一次动作a，环境相应地更新至状态s'，并向智能体反馈及时奖励r。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c312e95",
   "metadata": {},
   "source": [
    "函数接口说明如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7e872a7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-01T06:47:55.762717Z",
     "start_time": "2023-09-01T06:47:55.756074Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nObservation, reward, terminated, truncated, info = env.step(action)\\n    说明：环境执行一步动作\\n    参数：action（object 类型） 动作\\n    返回值：results（tuple 类型） (下一状态，报酬，episode 是否完成，日志信息)\\n    \\n    将“动作”传递给环境，返回值返回“下一个状态”（object）、“报酬”（float）、“ episode 是否完成”（bool）、“日志信息”（dict）\\n    传递给环境的“动作”类型，由属性“action_space”决定\\n'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Observation, reward, terminated, truncated, info = env.step(action)\n",
    "    说明：环境执行一步动作\n",
    "    参数：action（object 类型） 动作\n",
    "    返回值：results（tuple 类型） (下一状态，报酬，episode 是否完成，日志信息)\n",
    "    \n",
    "    将“动作”传递给环境，返回值返回“下一个状态”（object）、“报酬”（float）、“ episode 是否完成”（bool）、“日志信息”（dict）\n",
    "    传递给环境的“动作”类型，由属性“action_space”决定\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b915d852",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-01T06:49:28.048053Z",
     "start_time": "2023-09-01T06:49:28.042150Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "动作 = 1: 当前状态 = [ 0.00767821  0.21991836 -0.01758216 -0.30329233], 奖励 = 1.0, 结束标志 = False, 日志信息 = {}\n",
      "动作 = 1: 当前状态 = [ 0.01207658  0.41528642 -0.02364801 -0.601468  ], 奖励 = 1.0, 结束标志 = False, 日志信息 = {}\n",
      "动作 = 0: 当前状态 = [ 0.02038231  0.2205031  -0.03567737 -0.31632662], 奖励 = 1.0, 结束标志 = False, 日志信息 = {}\n",
      "动作 = 0: 当前状态 = [ 0.02479237  0.02590702 -0.0420039  -0.03510516], 奖励 = 1.0, 结束标志 = False, 日志信息 = {}\n",
      "动作 = 0: 当前状态 = [ 0.02531051 -0.1685882  -0.04270601  0.24403483], 奖励 = 1.0, 结束标志 = False, 日志信息 = {}\n"
     ]
    }
   ],
   "source": [
    "for k in range(5):\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, truncated, info = env.step(action)\n",
    "    print('动作 = {0}: 当前状态 = {1}, 奖励 = {2}, 结束标志 = {3}, 日志信息 = {4}'.format(action, state, reward, done, info))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ce5ee8",
   "metadata": {},
   "source": [
    "### close()：关闭环境"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d44c031",
   "metadata": {},
   "source": [
    "关闭环境。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ebc0dc",
   "metadata": {},
   "source": [
    "### sample_space.sample(): 对动作空间进行随机采样"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29b8ef1",
   "metadata": {},
   "source": [
    "动作的选择应该基于策略进行。但是，完全随机的选择动作也是一种策略，或者可以说是一种基线(baseline)策略。任何一种能够体现有效的学习效果的策略都不应该比这种基线策略的效果差。这就好比任何一个有效的预测（股票涨跌、球赛胜负啊随便什么的）算法不能比随机扔硬币决定要更差。如果一种基于一种习得的策略来选取动作其最终得到的回报不比以上随机采样策略好的话，就说明这个习得的策略没有任何价值。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a55331",
   "metadata": {},
   "source": [
    "## 一个随机策略的完整示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c7fcfda",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-01T08:18:38.024538Z",
     "start_time": "2023-09-01T08:18:34.576638Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state = [ 0.00720253 -0.15701821 -0.02658597  0.2511829 ]; reward = 1.0\n",
      "state = [ 0.00406217  0.0384731  -0.02156231 -0.04976577]; reward = 1.0\n",
      "state = [ 0.00483163 -0.15633313 -0.02255762  0.23603684]; reward = 1.0\n",
      "state = [ 0.00170497 -0.35112566 -0.01783689  0.5215199 ]; reward = 1.0\n",
      "state = [-0.00531754 -0.15575725 -0.00740649  0.22327013]; reward = 1.0\n",
      "state = [-0.00843269  0.03946978 -0.00294109 -0.07173986]; reward = 1.0\n",
      "state = [-0.00764329  0.23463377 -0.00437588 -0.36534926]; reward = 1.0\n",
      "state = [-0.00295062  0.03957428 -0.01168287 -0.07404933]; reward = 1.0\n",
      "state = [-0.00215913 -0.15537825 -0.01316386  0.21492483]; reward = 1.0\n",
      "state = [-0.0052667  -0.35030955 -0.00886536  0.5034264 ]; reward = 1.0\n",
      "state = [-0.01227289 -0.1550638   0.00120317  0.20796286]; reward = 1.0\n",
      "state = [-0.01537417  0.04004094  0.00536243 -0.08434029]; reward = 1.0\n",
      "state = [-0.01457335  0.23508562  0.00367562 -0.37532654]; reward = 1.0\n",
      "state = [-0.00987163  0.03991165 -0.00383091 -0.08148695]; reward = 1.0\n",
      "state = [-0.0090734   0.2350883  -0.00546065 -0.37537608]; reward = 1.0\n",
      "state = [-0.00437163  0.04004434 -0.01296817 -0.08441991]; reward = 1.0\n",
      "state = [-0.00357075  0.23534976 -0.01465657 -0.38116595]; reward = 1.0\n",
      "state = [ 0.00113625  0.04043896 -0.02227989 -0.09314002]; reward = 1.0\n",
      "state = [ 0.00194503 -0.15435669 -0.02414269  0.19243118]; reward = 1.0\n",
      "state = [-0.00114211 -0.3491251  -0.02029406  0.4774014 ]; reward = 1.0\n",
      "state = [-0.00812461 -0.15372257 -0.01074604  0.17839193]; reward = 1.0\n",
      "state = [-0.01119906 -0.3486891  -0.0071782   0.46766558]; reward = 1.0\n",
      "state = [-0.01817284 -0.5437089   0.00217511  0.7580774 ]; reward = 1.0\n",
      "state = [-0.02904702 -0.7388608   0.01733666  1.0514439 ]; reward = 1.0\n",
      "state = [-0.04382424 -0.93420833  0.03836554  1.349518  ]; reward = 1.0\n",
      "state = [-0.0625084  -0.73958886  0.0653559   1.0690802 ]; reward = 1.0\n",
      "state = [-0.07730018 -0.93551147  0.08673751  1.3815377 ]; reward = 1.0\n",
      "state = [-0.09601041 -1.1316022   0.11436826  1.7000369 ]; reward = 1.0\n",
      "state = [-0.11864245 -0.93796885  0.148369    1.4450355 ]; reward = 1.0\n",
      "state = [-0.13740183 -0.7449511   0.17726971  1.2021503 ]; reward = 1.0\n",
      "state = [-0.15230085 -0.5525078   0.20131272  0.9698539 ]; reward = 1.0\n",
      "state = [-0.16335101 -0.36057362  0.22070979  0.7465537 ]; reward = 1.0\n",
      "terminated\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import time\n",
    "# 生成环境，倒立摆模型\n",
    "env = gym.make('CartPole-v1', render_mode='human')\n",
    "# 环境初始化\n",
    "state = env.reset()\n",
    "# 循环交互\n",
    " \n",
    "while True:\n",
    "    # 从动作空间随机获取一个动作\n",
    "    action = env.action_space.sample()\n",
    "    # agent与环境进行一步交互\n",
    "    state, reward, terminated, truncated, info = env.step(action)\n",
    "    print('state = {0}; reward = {1}'.format(state, reward))\n",
    "    # 判断当前episode 是否完成\n",
    "    if terminated:\n",
    "        print('terminated')\n",
    "        break\n",
    "    time.sleep(0.1)\n",
    "# 环境结束\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc98027d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea131a3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174c968b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3832acd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
