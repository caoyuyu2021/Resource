{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00ebd87a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T08:32:27.071317Z",
     "start_time": "2023-05-22T08:32:26.154012Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch_gpu version: 1.13.1+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "version = torch.__version__  #查看PyTorch版本\n",
    "print(\"torch_gpu version:\", version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046d1e6a",
   "metadata": {},
   "source": [
    "# GRU+Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6ec7cf8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T08:33:10.844380Z",
     "start_time": "2023-05-22T08:33:10.825920Z"
    }
   },
   "outputs": [],
   "source": [
    "# 1. 导入所需库\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    " \n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "        self.v = nn.Linear(hidden_size, 1, bias=False)\n",
    " \n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        max_len = encoder_outputs.size(1)\n",
    "        repeated_hidden = hidden.unsqueeze(1).repeat(1, max_len, 1)\n",
    "        energy = torch.tanh(self.attn(torch.cat((repeated_hidden, encoder_outputs), dim=2)))\n",
    "        attention_scores = self.v(energy).squeeze(2)\n",
    "        attention_weights = nn.functional.softmax(attention_scores, dim=1)\n",
    "        context_vector = (encoder_outputs * attention_weights.unsqueeze(2)).sum(dim=1)\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87f879ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T08:36:36.280212Z",
     "start_time": "2023-05-22T08:36:36.265262Z"
    }
   },
   "outputs": [],
   "source": [
    "# 2. GRU模型构建+注意力机制\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers, dropout=0.5):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.attention = Attention(hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    " \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size)\n",
    "        out, hidden = self.gru(x, h0)\n",
    "        out, attention_weights = self.attention(hidden[-1], out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b772ed76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T08:36:01.405477Z",
     "start_time": "2023-05-22T08:36:01.329335Z"
    }
   },
   "outputs": [],
   "source": [
    "# 3. 准备数据集\n",
    "class SampleDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.sequences = []\n",
    "        self.labels = []\n",
    "        for _ in range(1000):\n",
    "            seq = torch.randn(10, 5)\n",
    "            label = torch.zeros(2)\n",
    "            if seq.sum() > 0:\n",
    "                label[0] = 1\n",
    "            else:\n",
    "                label[1] = 1\n",
    "            self.sequences.append(seq)\n",
    "            self.labels.append(label)\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    " \n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.labels[idx]\n",
    " \n",
    " \n",
    "train_set_split = int(0.8 * len(SampleDataset()))\n",
    "train_set, test_set = torch.utils.data.random_split(SampleDataset(),\n",
    "                                                    [train_set_split, len(SampleDataset()) - train_set_split])\n",
    "train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "546fb916",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-22T08:36:56.990222Z",
     "start_time": "2023-05-22T08:36:52.001695Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\TongYuan\\.julia\\miniforge3\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:67: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "Train Loss: 0.6976, Acc: 51.38%\n",
      "Test Loss: 0.6814, Acc: 57.00%\n",
      "Epoch 2/100\n",
      "Train Loss: 0.6859, Acc: 54.88%\n",
      "Test Loss: 0.6714, Acc: 59.00%\n",
      "Epoch 3/100\n",
      "Train Loss: 0.6728, Acc: 60.38%\n",
      "Test Loss: 0.6588, Acc: 69.50%\n",
      "Epoch 4/100\n",
      "Train Loss: 0.6563, Acc: 66.00%\n",
      "Test Loss: 0.6403, Acc: 76.50%\n",
      "Epoch 5/100\n",
      "Train Loss: 0.6324, Acc: 75.25%\n",
      "Test Loss: 0.6099, Acc: 87.00%\n",
      "Epoch 6/100\n",
      "Train Loss: 0.5907, Acc: 81.62%\n",
      "Test Loss: 0.5588, Acc: 87.00%\n",
      "Epoch 7/100\n",
      "Train Loss: 0.5409, Acc: 84.62%\n",
      "Test Loss: 0.4783, Acc: 87.50%\n",
      "Epoch 8/100\n",
      "Train Loss: 0.4677, Acc: 85.00%\n",
      "Test Loss: 0.3977, Acc: 89.00%\n",
      "Epoch 9/100\n",
      "Train Loss: 0.4176, Acc: 85.00%\n",
      "Test Loss: 0.3464, Acc: 90.50%\n",
      "Epoch 10/100\n",
      "Train Loss: 0.3750, Acc: 87.62%\n",
      "Test Loss: 0.3066, Acc: 92.50%\n",
      "Epoch 11/100\n",
      "Train Loss: 0.3330, Acc: 91.00%\n",
      "Test Loss: 0.2672, Acc: 93.50%\n",
      "Epoch 12/100\n",
      "Train Loss: 0.2948, Acc: 91.00%\n",
      "Test Loss: 0.2471, Acc: 93.50%\n",
      "Epoch 13/100\n",
      "Train Loss: 0.2746, Acc: 91.50%\n",
      "Test Loss: 0.2248, Acc: 93.50%\n",
      "Epoch 14/100\n",
      "Train Loss: 0.2535, Acc: 92.50%\n",
      "Test Loss: 0.2162, Acc: 94.50%\n",
      "Epoch 15/100\n",
      "Train Loss: 0.2318, Acc: 93.25%\n",
      "Test Loss: 0.2074, Acc: 92.50%\n",
      "Epoch 16/100\n",
      "Train Loss: 0.2172, Acc: 93.75%\n",
      "Test Loss: 0.1951, Acc: 93.00%\n",
      "Epoch 17/100\n",
      "Train Loss: 0.1999, Acc: 94.25%\n",
      "Test Loss: 0.1989, Acc: 93.50%\n",
      "Epoch 18/100\n",
      "Train Loss: 0.1941, Acc: 94.50%\n",
      "Test Loss: 0.1878, Acc: 93.50%\n",
      "Epoch 19/100\n",
      "Train Loss: 0.1943, Acc: 94.12%\n",
      "Test Loss: 0.1823, Acc: 94.00%\n",
      "Epoch 20/100\n",
      "Train Loss: 0.1788, Acc: 94.75%\n",
      "Test Loss: 0.1860, Acc: 93.00%\n",
      "Epoch 21/100\n",
      "Train Loss: 0.1758, Acc: 95.00%\n",
      "Test Loss: 0.1820, Acc: 93.00%\n",
      "Epoch 22/100\n",
      "Train Loss: 0.1658, Acc: 95.38%\n",
      "Test Loss: 0.1879, Acc: 94.00%\n",
      "Epoch 23/100\n",
      "Train Loss: 0.1599, Acc: 96.00%\n",
      "Test Loss: 0.1805, Acc: 93.50%\n",
      "Epoch 24/100\n",
      "Train Loss: 0.1483, Acc: 96.25%\n",
      "Test Loss: 0.1722, Acc: 94.00%\n",
      "Epoch 25/100\n",
      "Train Loss: 0.1506, Acc: 95.50%\n",
      "Test Loss: 0.1683, Acc: 94.00%\n",
      "Epoch 26/100\n",
      "Train Loss: 0.1435, Acc: 96.50%\n",
      "Test Loss: 0.1708, Acc: 94.00%\n",
      "Epoch 27/100\n",
      "Train Loss: 0.1490, Acc: 96.88%\n",
      "Test Loss: 0.1674, Acc: 94.00%\n",
      "Epoch 28/100\n",
      "Train Loss: 0.1343, Acc: 96.75%\n",
      "Test Loss: 0.1660, Acc: 94.00%\n",
      "Epoch 29/100\n",
      "Train Loss: 0.1373, Acc: 96.88%\n",
      "Test Loss: 0.1646, Acc: 94.00%\n",
      "Epoch 30/100\n",
      "Train Loss: 0.1290, Acc: 96.50%\n",
      "Test Loss: 0.1645, Acc: 93.50%\n",
      "Epoch 31/100\n",
      "Train Loss: 0.1307, Acc: 97.00%\n",
      "Test Loss: 0.1676, Acc: 93.00%\n",
      "Epoch 32/100\n",
      "Train Loss: 0.1226, Acc: 96.88%\n",
      "Test Loss: 0.1672, Acc: 93.50%\n",
      "Epoch 33/100\n",
      "Train Loss: 0.1227, Acc: 96.88%\n",
      "Test Loss: 0.1749, Acc: 94.00%\n",
      "Epoch 34/100\n",
      "Train Loss: 0.1142, Acc: 97.50%\n",
      "Test Loss: 0.1589, Acc: 94.50%\n",
      "Epoch 35/100\n",
      "Train Loss: 0.1148, Acc: 97.25%\n",
      "Test Loss: 0.1646, Acc: 93.50%\n",
      "Epoch 36/100\n",
      "Train Loss: 0.1120, Acc: 97.38%\n",
      "Test Loss: 0.1679, Acc: 93.50%\n",
      "Epoch 37/100\n",
      "Train Loss: 0.1104, Acc: 97.12%\n",
      "Test Loss: 0.1696, Acc: 93.50%\n",
      "Epoch 38/100\n",
      "Train Loss: 0.1043, Acc: 97.62%\n",
      "Test Loss: 0.1734, Acc: 94.00%\n",
      "Epoch 39/100\n",
      "Train Loss: 0.1117, Acc: 97.38%\n",
      "Test Loss: 0.1686, Acc: 94.00%\n",
      "Epoch 40/100\n",
      "Train Loss: 0.1005, Acc: 98.00%\n",
      "Test Loss: 0.1573, Acc: 94.50%\n",
      "Epoch 41/100\n",
      "Train Loss: 0.0992, Acc: 97.75%\n",
      "Test Loss: 0.1689, Acc: 94.00%\n",
      "Epoch 42/100\n",
      "Train Loss: 0.0984, Acc: 97.88%\n",
      "Test Loss: 0.1722, Acc: 93.50%\n",
      "Epoch 43/100\n",
      "Train Loss: 0.0804, Acc: 97.62%\n",
      "Test Loss: 0.1667, Acc: 94.00%\n",
      "Epoch 44/100\n",
      "Train Loss: 0.1000, Acc: 97.25%\n",
      "Test Loss: 0.1725, Acc: 93.50%\n",
      "Epoch 45/100\n",
      "Train Loss: 0.0959, Acc: 98.00%\n",
      "Test Loss: 0.1727, Acc: 93.50%\n",
      "Epoch 46/100\n",
      "Train Loss: 0.0900, Acc: 98.12%\n",
      "Test Loss: 0.1729, Acc: 93.50%\n",
      "Epoch 47/100\n",
      "Train Loss: 0.0885, Acc: 98.12%\n",
      "Test Loss: 0.1623, Acc: 94.50%\n",
      "Epoch 48/100\n",
      "Train Loss: 0.0872, Acc: 98.00%\n",
      "Test Loss: 0.1694, Acc: 93.50%\n",
      "Epoch 49/100\n",
      "Train Loss: 0.0852, Acc: 98.50%\n",
      "Test Loss: 0.1739, Acc: 94.00%\n",
      "Epoch 50/100\n",
      "Train Loss: 0.0853, Acc: 98.62%\n",
      "Test Loss: 0.1631, Acc: 94.50%\n",
      "Epoch 51/100\n",
      "Train Loss: 0.0886, Acc: 98.38%\n",
      "Test Loss: 0.1698, Acc: 94.00%\n",
      "Epoch 52/100\n",
      "Train Loss: 0.0806, Acc: 98.00%\n",
      "Test Loss: 0.1882, Acc: 93.50%\n",
      "Epoch 53/100\n",
      "Train Loss: 0.0811, Acc: 98.50%\n",
      "Test Loss: 0.1672, Acc: 94.50%\n",
      "Epoch 54/100\n",
      "Train Loss: 0.0821, Acc: 98.25%\n",
      "Test Loss: 0.1830, Acc: 94.50%\n",
      "Epoch 55/100\n",
      "Train Loss: 0.0826, Acc: 98.38%\n",
      "Test Loss: 0.1814, Acc: 94.50%\n",
      "Epoch 56/100\n",
      "Train Loss: 0.0755, Acc: 98.88%\n",
      "Test Loss: 0.1787, Acc: 94.50%\n",
      "Epoch 57/100\n",
      "Train Loss: 0.0784, Acc: 98.75%\n",
      "Test Loss: 0.1728, Acc: 94.50%\n",
      "Epoch 58/100\n",
      "Train Loss: 0.0702, Acc: 98.12%\n",
      "Test Loss: 0.1823, Acc: 95.00%\n",
      "Epoch 59/100\n",
      "Train Loss: 0.0743, Acc: 98.88%\n",
      "Test Loss: 0.1763, Acc: 94.00%\n",
      "Epoch 60/100\n",
      "Train Loss: 0.0735, Acc: 98.50%\n",
      "Test Loss: 0.1860, Acc: 94.50%\n",
      "Epoch 61/100\n",
      "Train Loss: 0.0675, Acc: 98.75%\n",
      "Test Loss: 0.1788, Acc: 94.50%\n",
      "Epoch 62/100\n",
      "Train Loss: 0.0586, Acc: 99.25%\n",
      "Test Loss: 0.1836, Acc: 94.50%\n",
      "Epoch 63/100\n",
      "Train Loss: 0.0611, Acc: 99.12%\n",
      "Test Loss: 0.1701, Acc: 95.00%\n",
      "Epoch 64/100\n",
      "Train Loss: 0.0665, Acc: 98.12%\n",
      "Test Loss: 0.1651, Acc: 94.00%\n",
      "Epoch 65/100\n",
      "Train Loss: 0.0776, Acc: 97.88%\n",
      "Test Loss: 0.1722, Acc: 94.50%\n",
      "Epoch 66/100\n",
      "Train Loss: 0.0719, Acc: 98.88%\n",
      "Test Loss: 0.1658, Acc: 94.50%\n",
      "Epoch 67/100\n",
      "Train Loss: 0.0677, Acc: 98.62%\n",
      "Test Loss: 0.1812, Acc: 95.00%\n",
      "Epoch 68/100\n",
      "Train Loss: 0.0659, Acc: 99.00%\n",
      "Test Loss: 0.1717, Acc: 95.00%\n",
      "Epoch 69/100\n",
      "Train Loss: 0.0549, Acc: 99.38%\n",
      "Test Loss: 0.1799, Acc: 94.50%\n",
      "Epoch 70/100\n",
      "Train Loss: 0.0542, Acc: 98.88%\n",
      "Test Loss: 0.1709, Acc: 94.50%\n",
      "Epoch 71/100\n",
      "Train Loss: 0.0571, Acc: 99.00%\n",
      "Test Loss: 0.1905, Acc: 95.00%\n",
      "Epoch 72/100\n",
      "Train Loss: 0.0561, Acc: 99.12%\n",
      "Test Loss: 0.1856, Acc: 94.50%\n",
      "Epoch 73/100\n",
      "Train Loss: 0.0590, Acc: 99.12%\n",
      "Test Loss: 0.1773, Acc: 95.00%\n",
      "Epoch 74/100\n",
      "Train Loss: 0.0601, Acc: 99.12%\n",
      "Test Loss: 0.1738, Acc: 95.00%\n",
      "Epoch 75/100\n",
      "Train Loss: 0.0588, Acc: 99.00%\n",
      "Test Loss: 0.1708, Acc: 95.00%\n",
      "Epoch 76/100\n",
      "Train Loss: 0.0558, Acc: 99.25%\n",
      "Test Loss: 0.1796, Acc: 95.00%\n",
      "Epoch 77/100\n",
      "Train Loss: 0.0447, Acc: 99.50%\n",
      "Test Loss: 0.1907, Acc: 94.50%\n",
      "Epoch 78/100\n",
      "Train Loss: 0.0501, Acc: 99.12%\n",
      "Test Loss: 0.1763, Acc: 95.00%\n",
      "Epoch 79/100\n",
      "Train Loss: 0.0430, Acc: 99.25%\n",
      "Test Loss: 0.1687, Acc: 95.00%\n",
      "Epoch 80/100\n",
      "Train Loss: 0.0434, Acc: 99.00%\n",
      "Test Loss: 0.1798, Acc: 95.00%\n",
      "Epoch 81/100\n",
      "Train Loss: 0.0496, Acc: 98.88%\n",
      "Test Loss: 0.1713, Acc: 95.00%\n",
      "Epoch 82/100\n",
      "Train Loss: 0.0504, Acc: 99.00%\n",
      "Test Loss: 0.1756, Acc: 95.00%\n",
      "Epoch 83/100\n",
      "Train Loss: 0.0418, Acc: 99.50%\n",
      "Test Loss: 0.1740, Acc: 95.00%\n",
      "Epoch 84/100\n",
      "Train Loss: 0.0464, Acc: 98.88%\n",
      "Test Loss: 0.1667, Acc: 95.00%\n",
      "Epoch 85/100\n",
      "Train Loss: 0.0489, Acc: 98.75%\n",
      "Test Loss: 0.1718, Acc: 95.00%\n",
      "Epoch 86/100\n",
      "Train Loss: 0.0401, Acc: 99.25%\n",
      "Test Loss: 0.2065, Acc: 95.00%\n",
      "Epoch 87/100\n",
      "Train Loss: 0.0475, Acc: 99.38%\n",
      "Test Loss: 0.1711, Acc: 95.00%\n",
      "Epoch 88/100\n",
      "Train Loss: 0.0419, Acc: 99.38%\n",
      "Test Loss: 0.1906, Acc: 95.50%\n",
      "Epoch 89/100\n",
      "Train Loss: 0.0440, Acc: 98.88%\n",
      "Test Loss: 0.1826, Acc: 95.50%\n",
      "Epoch 90/100\n",
      "Train Loss: 0.0360, Acc: 99.62%\n",
      "Test Loss: 0.1723, Acc: 95.50%\n",
      "Epoch 91/100\n",
      "Train Loss: 0.0464, Acc: 99.50%\n",
      "Test Loss: 0.1576, Acc: 95.50%\n",
      "Epoch 92/100\n",
      "Train Loss: 0.0404, Acc: 99.00%\n",
      "Test Loss: 0.1583, Acc: 95.00%\n",
      "Epoch 93/100\n",
      "Train Loss: 0.0387, Acc: 99.25%\n",
      "Test Loss: 0.1670, Acc: 95.00%\n",
      "Epoch 94/100\n",
      "Train Loss: 0.0313, Acc: 99.62%\n",
      "Test Loss: 0.1695, Acc: 95.00%\n",
      "Epoch 95/100\n",
      "Train Loss: 0.0356, Acc: 99.50%\n",
      "Test Loss: 0.1909, Acc: 95.50%\n",
      "Epoch 96/100\n",
      "Train Loss: 0.0305, Acc: 99.62%\n",
      "Test Loss: 0.1658, Acc: 95.00%\n",
      "Epoch 97/100\n",
      "Train Loss: 0.0257, Acc: 99.75%\n",
      "Test Loss: 0.1793, Acc: 95.50%\n",
      "Epoch 98/100\n",
      "Train Loss: 0.0301, Acc: 99.50%\n",
      "Test Loss: 0.1958, Acc: 95.50%\n",
      "Epoch 99/100\n",
      "Train Loss: 0.0296, Acc: 99.62%\n",
      "Test Loss: 0.1644, Acc: 95.00%\n",
      "Epoch 100/100\n",
      "Train Loss: 0.0318, Acc: 99.50%\n",
      "Test Loss: 0.1841, Acc: 95.50%\n"
     ]
    }
   ],
   "source": [
    "# 4. 定义训练过程\n",
    "def train(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    " \n",
    "    for batch_idx, (inputs, labels) in enumerate(loader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    " \n",
    "        optimizer.zero_grad()\n",
    " \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    " \n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        _, true_labels = torch.max(labels, 1)\n",
    "        total += true_labels.size(0)\n",
    "        correct += (predicted == true_labels).sum().item()\n",
    " \n",
    "    print(\"Train Loss: {:.4f}, Acc: {:.2f}%\".format(running_loss / (batch_idx + 1), 100 * correct / total))\n",
    " \n",
    " \n",
    "# 5. 定义评估过程\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    " \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, labels) in enumerate(loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    " \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    " \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            _, true_labels = torch.max(labels, 1)\n",
    "            total += true_labels.size(0)\n",
    "            correct += (predicted == true_labels).sum().item()\n",
    " \n",
    "    print(\"Test Loss: {:.4f}, Acc: {:.2f}%\".format(running_loss / (batch_idx + 1), 100 * correct / total))\n",
    " \n",
    " \n",
    "# 6. 训练模型并评估\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = GRUModel(input_size=5, hidden_size=10, output_size=2, num_layers=1).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    " \n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    print(\"Epoch {}/{}\".format(epoch + 1, num_epochs))\n",
    "    train(model, train_loader, criterion, optimizer, device)\n",
    "    evaluate(model, test_loader, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc34369",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
