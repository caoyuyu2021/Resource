{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-01T14:14:37.921326Z",
     "start_time": "2022-06-01T14:14:37.882756Z"
    }
   },
   "outputs": [],
   "source": [
    "import jieba #jieba适用中文场景"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-01T14:14:39.388786Z",
     "start_time": "2022-06-01T14:14:38.723385Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache D:\\ProgramData\\Temp\\jieba.cache\n",
      "Loading model cost 0.651 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "未/按照/规定/对/被/派遣/劳动者/进行/安全/生产/教育/和/培训/。\n",
      "['未', '按照', '规定', '对', '被', '派遣', '劳动者', '进行', '安全', '生产', '教育', '和', '培训', '。']\n"
     ]
    }
   ],
   "source": [
    "text = \"未按照规定对被派遣劳动者进行安全生产教育和培训。\"\n",
    "# jieba.cut直接得到generator形式的分词结果\n",
    "seg = jieba.cut(text)  \n",
    "print('/'.join(seg)) \n",
    "\n",
    "# 也可以使用jieba.lcut得到list的分词结果\n",
    "seg = jieba.lcut(text)\n",
    "print(seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-01T14:14:40.065895Z",
     "start_time": "2022-06-01T14:14:40.060904Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "此时 相望 不 相闻 ， 愿逐 月华 流照 君\n"
     ]
    }
   ],
   "source": [
    "text = \"此时相望不相闻，愿逐月华流照君\"\n",
    "seg = jieba.cut(text)\n",
    "print(' '.join(seg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-01T14:14:42.795017Z",
     "start_time": "2022-06-01T14:14:40.675054Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'world', '.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk #nltk适用英文场景\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "word_tokenize('hello world.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-01T14:14:44.032644Z",
     "start_time": "2022-06-01T14:14:44.019296Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', \"'s\", 'a', 'test']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokenizer.tokenize(\"this's a test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-01T14:14:44.891538Z",
     "start_time": "2022-06-01T14:14:44.882887Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中文:['粗', '缯', '大布', '裹', '生涯', '，', '腹有', '诗书', '气自华']\n",
      "英文:['all', ' ', 'is', ' ', 'well']\n"
     ]
    }
   ],
   "source": [
    "import jieba \n",
    "text1 = \"粗缯大布裹生涯，腹有诗书气自华\"\n",
    "text2 = \"all is well\"\n",
    "seg1 = jieba.lcut(text1)\n",
    "seg2 = jieba.lcut(text2)\n",
    "print('中文:{}'.format(seg1))\n",
    "print('英文:{}'.format(seg2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-01T14:14:45.939765Z",
     "start_time": "2022-06-01T14:14:45.931920Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中文:['粗缯大布裹生涯，腹有诗书气自华']\n",
      "英文:['all', 'is', 'well']\n",
      "句子:['粗缯大布裹生涯，腹有诗书气自华']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "seg1 = word_tokenize(text1)\n",
    "seg2 = word_tokenize(text2)\n",
    "seg3 = sent_tokenize(text1)\n",
    "print('中文:{}'.format(seg1))\n",
    "print('英文:{}'.format(seg2))\n",
    "print('句子:{}'.format(seg3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-01T14:15:19.731537Z",
     "start_time": "2022-06-01T14:15:19.315695Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "去除停用词：\n",
      " [['Once', 'dreamt', 'that', 'were', 'strangers', 'We', 'wake', 'up', 'find', 'that', 'were', 'dear', 'each', 'other'], ['We', 'come', 'nearest', 'the', 'great', 'when', 'are', 'great', 'in', 'humility'], ['I', 'love'], ['My', 'heart', ',', 'the', 'bird', 'the', 'wilderness', ',', 'has', 'found', 'its', 'sky', 'in', 'your', 'eyes'], ['It', 'the', 'tears', 'the', 'earth', 'that', 'keep', 'her', 'smiles', 'in', 'bloom'], ['The', 'perfect', 'decks', 'itself', 'in', 'beauty', 'for', 'the', 'love', 'the', 'Imperfect'], ['What', 'are', 'do', 'see', ',', 'what', 'see', 'your', 'shadow'], ['Like', 'the', 'meeting', 'the', 'seagulls', 'the', 'waves', 'meet', 'come', 'near', 'The', 'seagulls', 'fly', 'off', ',', 'the', 'waves', 'roll', 'away', 'depart']]\n",
      "分词：\n",
      " ['Once dreamt that were strangers We wake up find that were dear each other', 'We come nearest the great when are great in humility', 'I love', 'My heart , the bird the wilderness , has found its sky in your eyes', 'It the tears the earth that keep her smiles in bloom', 'The perfect decks itself in beauty for the love the Imperfect', 'What are do see , what see your shadow', 'Like the meeting the seagulls the waves meet come near The seagulls fly off , the waves roll away depart']\n",
      "词频统计矩阵:\n",
      "   (0, 38)\t0.24609444828604737\n",
      "  (0, 11)\t0.24609444828604737\n",
      "  (0, 6)\t0.24609444828604737\n",
      "  (0, 14)\t0.24609444828604737\n",
      "  (0, 50)\t0.24609444828604737\n",
      "  (0, 51)\t0.24609444828604737\n",
      "  (0, 53)\t0.2062463540507891\n",
      "  (0, 46)\t0.24609444828604737\n",
      "  (0, 54)\t0.49218889657209475\n",
      "  (0, 48)\t0.4124927081015782\n",
      "  (0, 10)\t0.24609444828604737\n",
      "  (0, 37)\t0.24609444828604737\n",
      "  (1, 22)\t0.319044485389035\n",
      "  (1, 24)\t0.20229988899841042\n",
      "  (1, 0)\t0.26738417851269114\n",
      "  (1, 56)\t0.319044485389035\n",
      "  (1, 18)\t0.63808897077807\n",
      "  (1, 49)\t0.17907030059289994\n",
      "  (1, 35)\t0.319044485389035\n",
      "  (1, 5)\t0.26738417851269114\n",
      "  (1, 53)\t0.26738417851269114\n",
      "  (2, 30)\t1.0\n",
      "  (3, 13)\t0.29663625252587983\n",
      "  (3, 58)\t0.24860433052776257\n",
      "  (3, 44)\t0.29663625252587983\n",
      "  :\t:\n",
      "  (5, 27)\t0.31718623768788784\n",
      "  (5, 7)\t0.31718623768788784\n",
      "  (5, 39)\t0.31718623768788784\n",
      "  (5, 30)\t0.2658268219125967\n",
      "  (5, 24)\t0.20112161035424195\n",
      "  (5, 49)\t0.5340819621825673\n",
      "  (6, 43)\t0.29611276010237\n",
      "  (6, 42)\t0.59222552020474\n",
      "  (6, 9)\t0.29611276010237\n",
      "  (6, 55)\t0.59222552020474\n",
      "  (6, 58)\t0.24816560302101023\n",
      "  (6, 0)\t0.24816560302101023\n",
      "  (7, 8)\t0.19772736033398933\n",
      "  (7, 1)\t0.19772736033398933\n",
      "  (7, 40)\t0.19772736033398933\n",
      "  (7, 36)\t0.19772736033398933\n",
      "  (7, 15)\t0.19772736033398933\n",
      "  (7, 34)\t0.19772736033398933\n",
      "  (7, 31)\t0.19772736033398933\n",
      "  (7, 52)\t0.39545472066797865\n",
      "  (7, 41)\t0.39545472066797865\n",
      "  (7, 32)\t0.19772736033398933\n",
      "  (7, 29)\t0.19772736033398933\n",
      "  (7, 49)\t0.5548928044826344\n",
      "  (7, 5)\t0.16571095954822487\n",
      "稀疏矩阵：\n",
      " [[0.         0.         0.         0.         0.         0.\n",
      "  0.24609445 0.         0.         0.         0.24609445 0.24609445\n",
      "  0.         0.         0.24609445 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.24609445 0.24609445 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.24609445 0.\n",
      "  0.41249271 0.         0.24609445 0.24609445 0.         0.20624635\n",
      "  0.4921889  0.         0.         0.         0.        ]\n",
      " [0.26738418 0.         0.         0.         0.         0.26738418\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.63808897 0.         0.         0.         0.31904449 0.\n",
      "  0.20229989 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.31904449\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.1790703  0.         0.         0.         0.26738418\n",
      "  0.         0.         0.31904449 0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  1.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.29663625 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.29663625 0.         0.         0.         0.29663625\n",
      "  0.         0.29663625 0.29663625 0.         0.         0.\n",
      "  0.18809127 0.         0.29663625 0.         0.         0.\n",
      "  0.         0.         0.         0.29663625 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.29663625 0.         0.         0.\n",
      "  0.         0.33298644 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.29663625 0.24860433]\n",
      " [0.         0.         0.         0.         0.32678113 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.32678113 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.32678113 0.         0.\n",
      "  0.20720554 0.32678113 0.         0.         0.32678113 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.32678113 0.         0.32678113\n",
      "  0.27386809 0.3668253  0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.31718624 0.         0.         0.\n",
      "  0.         0.31718624 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.31718624 0.\n",
      "  0.         0.         0.         0.         0.         0.31718624\n",
      "  0.20112161 0.         0.         0.31718624 0.         0.\n",
      "  0.26582682 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.31718624 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.53408196 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.2481656  0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.29611276 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.59222552 0.29611276 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.59222552 0.         0.         0.2481656 ]\n",
      " [0.         0.19772736 0.         0.         0.         0.16571096\n",
      "  0.         0.         0.19772736 0.         0.         0.\n",
      "  0.         0.         0.         0.19772736 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.19772736\n",
      "  0.         0.19772736 0.19772736 0.         0.19772736 0.\n",
      "  0.19772736 0.         0.         0.         0.19772736 0.39545472\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.5548928  0.         0.         0.39545472 0.\n",
      "  0.         0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import jieba\n",
    "import pandas as pd\n",
    "data = pd.read_csv('D:\\\\Jupyter notebook\\\\Dataset\\\\stu_data\\\\engdata.csv',encoding = 'gb2312')\n",
    "text = list(data[\"字段\"].values) \n",
    " \n",
    "docs = [jieba.lcut(doc) for doc in text] #分词\n",
    "stopwords = ['we','i','to','and','not','is','you','.','?','of'] #停用词表\n",
    "docs = [[w for w in doc if w not in stopwords ] for doc in docs] \n",
    "docs = [[s for s in doc if s != ' '] for doc in docs] #去掉多余的空格\n",
    "print(\"去除停用词：\\n\",docs)\n",
    "\n",
    "corpus = [' '.join(doc) for doc in docs]#处理之后，每个文档的词语列表，加上空格\n",
    "print(\"分词：\\n\",corpus)\n",
    "\n",
    "tfidf = TfidfVectorizer() #TF-IDF词频统计\n",
    "tfidf_matrix = tfidf.fit_transform(corpus)\n",
    "print(\"词频统计矩阵:\\n\",tfidf_matrix)  #词频统计\n",
    "print(\"稀疏矩阵：\\n\",tfidf_matrix.toarray()) #稀疏矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jumped': 3, 'over': 5, 'lazy': 4, 'dog': 1}\n",
      "(1, 8)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "[[1 1 1 1 1 1 1 2]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# 下面是一个文本文档的列表\n",
    "text = [\"The quick brown fox jumped over the lazy dog.\"]\n",
    "# 实例化 CountVectorizer 类\n",
    "vectorizer = CountVectorizer()\n",
    "# 标记并建立索引\n",
    "vectorizer.fit(text)\n",
    "# 查看结果\n",
    "print(vectorizer.vocabulary_)\n",
    "# 编码文档\n",
    "vector = vectorizer.transform(text)\n",
    "# 查看编码后的向量\n",
    "print(vector.shape)\n",
    "print(type(vector))\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 9, 'quick': 8, 'brown': 1, 'fox': 3, 'jumped': 5, 'over': 7, 'lazy': 6, 'dog': 2, 'boy': 0, 'is': 4}\n",
      "(2, 10)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "[[0 1 1 1 0 1 1 1 1 2]\n",
      " [1 0 0 0 1 0 1 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# 下面是一个文本文档的列表\n",
    "text = [\"The quick brown fox jumped over the lazy dog.\",\"The boy is lazy\"]\n",
    "# 实例化 CountVectorizer 类\n",
    "vectorizer = CountVectorizer()\n",
    "# 标记并建立索引\n",
    "vectorizer.fit(text)\n",
    "# 查看结果\n",
    "print(vectorizer.vocabulary_)\n",
    "# 编码文档\n",
    "vector = vectorizer.transform(text)\n",
    "# 查看编码后的向量\n",
    "print(vector.shape)\n",
    "print(type(vector))\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 10, 'quick': 9, 'brown': 1, 'fox': 3, 'jumped': 6, 'over': 8, 'lazy': 7, 'dog': 2, 'boy': 0, 'is': 5, 'girl': 4}\n",
      "(3, 11)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "[[0 1 1 1 0 0 1 1 1 1 2]\n",
      " [1 0 0 0 0 1 0 1 0 0 1]\n",
      " [0 0 0 0 1 1 0 1 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# 下面是一个文本文档的列表\n",
    "text = [\"The quick brown fox jumped over the lazy dog.\",\"The boy is lazy\",\"The girl is lazy\"]\n",
    "# 实例化 CountVectorizer 类\n",
    "vectorizer = CountVectorizer()\n",
    "# 标记并建立索引\n",
    "vectorizer.fit(text)\n",
    "# 查看结果\n",
    "print(vectorizer.vocabulary_)\n",
    "# 编码文档\n",
    "vector = vectorizer.transform(text)\n",
    "# 查看编码后的向量\n",
    "print(vector.shape)\n",
    "print(type(vector))\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Once we dreamt that we were strangers. We wake up to find that we were dear to each other.',\n",
       " 'We come nearest to the great when we are great in humility.',\n",
       " 'I love you.',\n",
       " 'My heart, the bird of the wilderness, has found its sky in your eyes.',\n",
       " 'It is the tears of the earth that keep her smiles in?bloom.',\n",
       " 'The perfect decks itself in beauty for the love of the Imperfect.',\n",
       " 'What you are you do not see, what you see is your shadow.',\n",
       " 'Like the meeting of the seagulls and the waves we meet and come near.The seagulls fly off, the waves roll away and we depart.']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('D:\\\\Jupyter notebook\\\\Dataset\\\\stu_data\\\\engdata.csv',encoding = 'gb2312')\n",
    "text = list(data[\"字段\"].values)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原数据：\n",
      "Once we dreamt that we were strangers. We wake up to find that we were dear to each other.\n",
      "We come nearest to the great when we are great in humility.\n",
      "I love you.\n",
      "My heart, the bird of the wilderness, has found its sky in your eyes.\n",
      "It is the tears of the earth that keep her smiles in?bloom.\n",
      "The perfect decks itself in beauty for the love of the Imperfect.\n",
      "What you are you do not see, what you see is your shadow.\n",
      "Like the meeting of the seagulls and the waves we meet and come near.The seagulls fly off, the waves roll away and we depart.\n",
      "\n",
      "\n",
      "索引：\n",
      "{'once': 41, 'we': 58, 'dreamt': 11, 'that': 52, 'were': 59, 'strangers': 50, 'wake': 56, 'up': 55, 'to': 54, 'find': 15, 'dear': 7, 'each': 12, 'other': 42, 'come': 6, 'nearest': 37, 'the': 53, 'great': 19, 'when': 61, 'are': 1, 'in': 25, 'humility': 23, 'love': 32, 'you': 63, 'my': 35, 'heart': 21, 'bird': 4, 'of': 39, 'wilderness': 62, 'has': 20, 'found': 18, 'its': 28, 'sky': 48, 'your': 64, 'eyes': 14, 'it': 27, 'is': 26, 'tears': 51, 'earth': 13, 'keep': 30, 'her': 22, 'smiles': 49, 'bloom': 5, 'perfect': 43, 'decks': 8, 'itself': 29, 'beauty': 3, 'for': 17, 'imperfect': 24, 'what': 60, 'do': 10, 'not': 38, 'see': 46, 'shadow': 47, 'like': 31, 'meeting': 34, 'seagulls': 45, 'and': 0, 'waves': 57, 'meet': 33, 'near': 36, 'fly': 16, 'off': 40, 'roll': 44, 'away': 2, 'depart': 9}\n",
      "\n",
      "\n",
      "向量形状：\n",
      "(8, 65)\n",
      "\n",
      "\n",
      "向量格式：\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "\n",
      "\n",
      "稀疏矩阵：\n",
      "[[0 0 0 0 0 0 0 1 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 2 0 2 1 1 0 4 2 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0\n",
      "  0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 2 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 1\n",
      "  0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 2 0 0 0 0 0 0 0 0 1 0 1]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1 1 0 0 1 0 0 0 0 0\n",
      "  0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 1 2 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 0 1 0 0 1 0 0 0\n",
      "  0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      "  0 0 1 0 0 0 0 0 0 0 2 1 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 3 1]\n",
      " [3 0 1 0 0 0 1 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0\n",
      "  1 0 0 1 1 0 0 0 1 2 0 0 0 0 0 0 0 5 0 0 0 2 2 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# 下面是一个文本文档的列表\n",
    "import pandas as pd\n",
    "data = pd.read_csv('D:\\\\Jupyter notebook\\\\Dataset\\\\stu_data\\\\engdata.csv',encoding = 'gb2312')\n",
    "text = list(data[\"字段\"].values)\n",
    "print(\"原数据：\")\n",
    "for i in range(0,len(text)):\n",
    "    print(text[i])\n",
    "print('\\n')\n",
    "# 实例化 CountVectorizer 类\n",
    "vectorizer = CountVectorizer()\n",
    "# 标记并建立索引\n",
    "vectorizer.fit(text)\n",
    "# 查看结果\n",
    "print('索引：')\n",
    "print(vectorizer.vocabulary_)\n",
    "print('\\n')\n",
    "# 编码文档\n",
    "vector = vectorizer.transform(text)\n",
    "# 查看编码后的向量\n",
    "print('向量形状：')\n",
    "print(vector.shape)\n",
    "print('\\n')\n",
    "print('向量格式：')\n",
    "print(type(vector))\n",
    "print('\\n')\n",
    "print('稀疏矩阵：')\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "向量形状：\n",
      "(8, 1048576)\n",
      "\n",
      "\n",
      "向量格式：\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "\n",
      "\n",
      "稀疏矩阵：\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#特征哈希\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "import pandas as pd\n",
    "data = pd.read_csv('D:\\\\Jupyter notebook\\\\Dataset\\\\stu_data\\\\engdata.csv',encoding = 'gb2312')\n",
    "text = list(data[\"字段\"].values)\n",
    "hv = HashingVectorizer()\n",
    "x_hv = hv.transform(text)\n",
    "\n",
    "print('向量形状：')\n",
    "print(x_hv.shape)\n",
    "print('\\n')\n",
    "print('向量格式：')\n",
    "print(type(x_hv))\n",
    "print('\\n')\n",
    "print('稀疏矩阵：')\n",
    "print(x_hv.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['computer', 'human', 'interface', 'response', 'survey', 'system', 'time', 'user', 'eps', 'trees', 'graph', 'minors']\n",
      "[[(0, 1), (1, 1), (2, 1)], [(0, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)], [(2, 1), (5, 1), (7, 1), (8, 1)], [(1, 1), (5, 2), (8, 1)], [(3, 1), (6, 1), (7, 1)], [(9, 1)], [(9, 1), (10, 1)], [(9, 1), (10, 1), (11, 1)], [(4, 1), (10, 1), (11, 1)]]\n"
     ]
    }
   ],
   "source": [
    "import gensim  #Gensim是一款开源的第三方Python工具包，用于从原始的非结构化的文本中，无监督地学习到文本隐层的主题向量表达。\n",
    "from gensim import corpora\n",
    "texts = [['human', 'interface', 'computer'],\n",
    "['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
    "['eps', 'user', 'interface', 'system'],\n",
    "['system', 'human', 'system', 'eps'],\n",
    "['user', 'response', 'time'],\n",
    "['trees'],\n",
    "['graph', 'trees'],\n",
    "['graph', 'minors', 'trees'],\n",
    "['graph', 'minors', 'survey']]\n",
    "dictionary = corpora.Dictionary(texts)#所有文档中不同词汇的集合\n",
    "print([dictionary[i] for i in dictionary])\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "print (corpus) #每个词汇对应的索引及在该文档中出现的次数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['我将要', '将要去', '去美国']\n"
     ]
    }
   ],
   "source": [
    "#用python3实现bigram模型\n",
    "import jieba\n",
    "\n",
    "text = \"我将要去美国\"\n",
    "cut = jieba.cut(text)\n",
    "sent = list(cut)\n",
    "bigram=[]\n",
    "for i in range(len(sent)-1):\n",
    "   bigram.append(sent[i] + sent[i+1])\n",
    "print(bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['我将要去', '将要去美国']\n"
     ]
    }
   ],
   "source": [
    "#用python3实现trigram模型\n",
    "import jieba\n",
    " \n",
    "text = \"我将要去美国\"\n",
    "cut = jieba.cut(text)\n",
    "sent = list(cut)\n",
    "#sent=[“我”，“将要”，“去”，“美国”]\n",
    " \n",
    "trigram=[]\n",
    "for i in range(len(sent)-2):\n",
    "   trigram.append(sent[i] + sent[i+1] + sent[i+2])\n",
    "print(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache C:\\Users\\Cyy\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.672 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "去除停用词：\n",
      " [['Once', 'dreamt', 'that', 'were', 'strangers', 'We', 'wake', 'up', 'find', 'that', 'were', 'dear', 'each', 'other'], ['We', 'come', 'nearest', 'the', 'great', 'when', 'are', 'great', 'in', 'humility'], ['I', 'love'], ['My', 'heart', ',', 'the', 'bird', 'the', 'wilderness', ',', 'has', 'found', 'its', 'sky', 'in', 'your', 'eyes'], ['It', 'the', 'tears', 'the', 'earth', 'that', 'keep', 'her', 'smiles', 'in', 'bloom'], ['The', 'perfect', 'decks', 'itself', 'in', 'beauty', 'for', 'the', 'love', 'the', 'Imperfect'], ['What', 'are', 'do', 'see', ',', 'what', 'see', 'your', 'shadow'], ['Like', 'the', 'meeting', 'the', 'seagulls', 'the', 'waves', 'meet', 'come', 'near', 'The', 'seagulls', 'fly', 'off', ',', 'the', 'waves', 'roll', 'away', 'depart']]\n",
      "分词：\n",
      " ['Once dreamt that were strangers We wake up find that were dear each other', 'We come nearest the great when are great in humility', 'I love', 'My heart , the bird the wilderness , has found its sky in your eyes', 'It the tears the earth that keep her smiles in bloom', 'The perfect decks itself in beauty for the love the Imperfect', 'What are do see , what see your shadow', 'Like the meeting the seagulls the waves meet come near The seagulls fly off , the waves roll away depart']\n",
      "词频统计矩阵:\n",
      "   (0, 38)\t0.24609444828604737\n",
      "  (0, 11)\t0.24609444828604737\n",
      "  (0, 6)\t0.24609444828604737\n",
      "  (0, 14)\t0.24609444828604737\n",
      "  (0, 50)\t0.24609444828604737\n",
      "  (0, 51)\t0.24609444828604737\n",
      "  (0, 53)\t0.2062463540507891\n",
      "  (0, 46)\t0.24609444828604737\n",
      "  (0, 54)\t0.49218889657209475\n",
      "  (0, 48)\t0.4124927081015782\n",
      "  (0, 10)\t0.24609444828604737\n",
      "  (0, 37)\t0.24609444828604737\n",
      "  (1, 22)\t0.319044485389035\n",
      "  (1, 24)\t0.20229988899841042\n",
      "  (1, 0)\t0.26738417851269114\n",
      "  (1, 56)\t0.319044485389035\n",
      "  (1, 18)\t0.63808897077807\n",
      "  (1, 49)\t0.17907030059289994\n",
      "  (1, 35)\t0.319044485389035\n",
      "  (1, 5)\t0.26738417851269114\n",
      "  (1, 53)\t0.26738417851269114\n",
      "  (2, 30)\t1.0\n",
      "  (3, 13)\t0.29663625252587983\n",
      "  (3, 58)\t0.24860433052776257\n",
      "  (3, 44)\t0.29663625252587983\n",
      "  :\t:\n",
      "  (5, 27)\t0.31718623768788784\n",
      "  (5, 7)\t0.31718623768788784\n",
      "  (5, 39)\t0.31718623768788784\n",
      "  (5, 30)\t0.2658268219125967\n",
      "  (5, 24)\t0.20112161035424195\n",
      "  (5, 49)\t0.5340819621825673\n",
      "  (6, 43)\t0.29611276010237\n",
      "  (6, 42)\t0.59222552020474\n",
      "  (6, 9)\t0.29611276010237\n",
      "  (6, 55)\t0.59222552020474\n",
      "  (6, 58)\t0.24816560302101023\n",
      "  (6, 0)\t0.24816560302101023\n",
      "  (7, 8)\t0.19772736033398933\n",
      "  (7, 1)\t0.19772736033398933\n",
      "  (7, 40)\t0.19772736033398933\n",
      "  (7, 36)\t0.19772736033398933\n",
      "  (7, 15)\t0.19772736033398933\n",
      "  (7, 34)\t0.19772736033398933\n",
      "  (7, 31)\t0.19772736033398933\n",
      "  (7, 52)\t0.39545472066797865\n",
      "  (7, 41)\t0.39545472066797865\n",
      "  (7, 32)\t0.19772736033398933\n",
      "  (7, 29)\t0.19772736033398933\n",
      "  (7, 49)\t0.5548928044826344\n",
      "  (7, 5)\t0.16571095954822487\n",
      "稀疏矩阵：\n",
      " [[0.         0.         0.         0.         0.         0.\n",
      "  0.24609445 0.         0.         0.         0.24609445 0.24609445\n",
      "  0.         0.         0.24609445 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.24609445 0.24609445 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.24609445 0.\n",
      "  0.41249271 0.         0.24609445 0.24609445 0.         0.20624635\n",
      "  0.4921889  0.         0.         0.         0.        ]\n",
      " [0.26738418 0.         0.         0.         0.         0.26738418\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.63808897 0.         0.         0.         0.31904449 0.\n",
      "  0.20229989 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.31904449\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.1790703  0.         0.         0.         0.26738418\n",
      "  0.         0.         0.31904449 0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  1.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.29663625 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.29663625 0.         0.         0.         0.29663625\n",
      "  0.         0.29663625 0.29663625 0.         0.         0.\n",
      "  0.18809127 0.         0.29663625 0.         0.         0.\n",
      "  0.         0.         0.         0.29663625 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.29663625 0.         0.         0.\n",
      "  0.         0.33298644 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.29663625 0.24860433]\n",
      " [0.         0.         0.         0.         0.32678113 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.32678113 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.32678113 0.         0.\n",
      "  0.20720554 0.32678113 0.         0.         0.32678113 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.32678113 0.         0.32678113\n",
      "  0.27386809 0.3668253  0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.         0.         0.31718624 0.         0.         0.\n",
      "  0.         0.31718624 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.31718624 0.\n",
      "  0.         0.         0.         0.         0.         0.31718624\n",
      "  0.20112161 0.         0.         0.31718624 0.         0.\n",
      "  0.26582682 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.31718624 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.53408196 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]\n",
      " [0.2481656  0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.29611276 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.59222552 0.29611276 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.59222552 0.         0.         0.2481656 ]\n",
      " [0.         0.19772736 0.         0.         0.         0.16571096\n",
      "  0.         0.         0.19772736 0.         0.         0.\n",
      "  0.         0.         0.         0.19772736 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.19772736\n",
      "  0.         0.19772736 0.19772736 0.         0.19772736 0.\n",
      "  0.19772736 0.         0.         0.         0.19772736 0.39545472\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.5548928  0.         0.         0.39545472 0.\n",
      "  0.         0.         0.         0.         0.        ]]\n",
      "分类结果矩阵：\n",
      " [[0.13761848 0.86238152]\n",
      " [0.16914015 0.83085985]\n",
      " [0.7413503  0.2586497 ]\n",
      " [0.1557526  0.8442474 ]\n",
      " [0.85506269 0.14493731]\n",
      " [0.85408532 0.14591468]\n",
      " [0.16836178 0.83163822]\n",
      " [0.86147079 0.13852921]]\n",
      "属于第一类的概率：[0.13761848 0.16914015 0.7413503  0.1557526  0.85506269 0.85408532\n",
      " 0.16836178 0.86147079]\n",
      "属于第二类的概率：[0.86238152 0.83085985 0.2586497  0.8442474  0.14493731 0.14591468\n",
      " 0.83163822 0.13852921]\n"
     ]
    }
   ],
   "source": [
    "#LDA主题分类（聚类）\n",
    "import jieba\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "data = pd.read_csv('D:\\\\Jupyter notebook\\\\Dataset\\\\stu_data\\\\engdata.csv',encoding = 'gb2312')\n",
    "text = list(data[\"字段\"].values) \n",
    " \n",
    "docs = [jieba.lcut(doc) for doc in text] #分词\n",
    "stopwords = ['we','i','to','and','not','is','you','.','?','of'] #停用词表\n",
    "docs = [[w for w in doc if w not in stopwords ] for doc in docs] \n",
    "docs = [[s for s in doc if s != ' '] for doc in docs] #去掉多余的空格\n",
    "print(\"去除停用词：\\n\",docs)\n",
    "\n",
    "corpus = [' '.join(doc) for doc in docs]#处理之后，每个文档的词语列表，加上空格\n",
    "print(\"分词：\\n\",corpus)\n",
    " \n",
    "tfidf = TfidfVectorizer() #TF-IDF词频统计\n",
    "tfidf_matrix = tfidf.fit_transform(corpus)\n",
    "print(\"词频统计矩阵:\\n\",tfidf_matrix)  #词频统计\n",
    "print(\"稀疏矩阵：\\n\",tfidf_matrix.toarray()) #稀疏矩阵\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=2,random_state=123456) #n_components，话题数K等于2，也就是聚类数为2\n",
    "docres = lda.fit_transform(tfidf_matrix)\n",
    "print(\"分类结果矩阵：\\n\",docres)\n",
    "print(\"属于第一类的概率：{0}\".format(docres[:,0]))\n",
    "print(\"属于第二类的概率：{0}\".format(docres[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 59)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
