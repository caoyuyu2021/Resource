{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font face=\"微软雅黑\" color=green size=5>Julia查库</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-05T17:04:01.128000+08:00",
     "start_time": "2022-07-05T09:04:00.664Z"
    }
   },
   "outputs": [],
   "source": [
    "using GLM  #广义线性回归"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T10:32:59.442000+08:00",
     "start_time": "2022-06-13T02:32:57.485Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1mG\u001b[22m\u001b[0m\u001b[1mL\u001b[22m\u001b[0m\u001b[1mM\u001b[22m \u001b[0m\u001b[1mg\u001b[22m\u001b[0m\u001b[1ml\u001b[22m\u001b[0m\u001b[1mm\u001b[22m Cate\u001b[0m\u001b[1mg\u001b[22morica\u001b[0m\u001b[1ml\u001b[22mTer\u001b[0m\u001b[1mm\u001b[22m \u001b[0m\u001b[1mG\u001b[22menera\u001b[0m\u001b[1ml\u001b[22mizedLinear\u001b[0m\u001b[1mM\u001b[22model\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "No docstring found for module \\texttt{GLM}.\n",
       "\n",
       "\\section{Exported names}\n",
       "\\texttt{@formula}, \\texttt{AbstractContrasts}, \\texttt{AbstractTerm}, \\texttt{Bernoulli}, \\texttt{Binomial}, \\texttt{CategoricalTerm}, \\texttt{CauchitLink}, \\texttt{CloglogLink}, \\texttt{ConstantTerm}, \\texttt{ContinuousTerm}, \\texttt{ContrastsCoding}, \\texttt{DummyCoding}, \\texttt{EffectsCoding}, \\texttt{FormulaTerm}, \\texttt{FunctionTerm}, \\texttt{Gamma}, \\texttt{GeneralizedLinearModel}, \\texttt{Geometric}, \\texttt{HelmertCoding}, \\texttt{HypothesisCoding}, \\texttt{IdentityLink}, \\texttt{InteractionTerm}, \\texttt{InterceptTerm}, \\texttt{InverseGaussian}, \\texttt{InverseLink}, \\texttt{InverseSquareLink}, \\texttt{LinearModel}, \\texttt{Link}, \\texttt{LogLink}, \\texttt{LogitLink}, \\texttt{MatrixTerm}, \\texttt{ModelFrame}, \\texttt{ModelMatrix}, \\texttt{NegativeBinomial}, \\texttt{NegativeBinomialLink}, \\texttt{Normal}, \\texttt{Poisson}, \\texttt{PowerLink}, \\texttt{ProbitLink}, \\texttt{RegressionModel}, \\texttt{SeqDiffCoding}, \\texttt{SqrtLink}, \\texttt{StatisticalModel}, \\texttt{StatsModels}, \\texttt{Term}, \\texttt{adjr2}, \\texttt{adjr²}, \\texttt{apply\\_schema}, \\texttt{canonicallink}, \\texttt{coef}, \\texttt{coefnames}, \\texttt{coeftable}, \\texttt{concrete\\_term}, \\texttt{confint}, \\texttt{cooksdistance}, \\texttt{deviance}, \\texttt{devresid}, \\texttt{dof}, \\texttt{dof\\_residual}, \\texttt{drop\\_term}, \\texttt{fit}, \\texttt{fit!}, \\texttt{fitted}, \\texttt{formula}, \\texttt{ftest}, \\texttt{glm}, \\texttt{hasintercept}, \\texttt{issubmodel}, \\texttt{lag}, \\texttt{lead}, \\texttt{linpred}, \\texttt{lm}, \\texttt{loglikelihood}, \\texttt{lrtest}, \\texttt{model\\_response}, \\texttt{modelcols}, \\texttt{modelmatrix}, \\texttt{negbin}, \\texttt{nobs}, \\texttt{nulldeviance}, \\texttt{nullloglikelihood}, \\texttt{predict}, \\texttt{r2}, \\texttt{residuals}, \\texttt{response}, \\texttt{r²}, \\texttt{schema}, \\texttt{setcontrasts!}, \\texttt{stderror}, \\texttt{term}, \\texttt{terms}, \\texttt{vcov}, \\texttt{width}\n",
       "\n",
       "\\section{Displaying contents of readme found at \\texttt{D:{\\textbackslash}TongYuan{\\textbackslash}.julia{\\textbackslash}packages{\\textbackslash}GLM{\\textbackslash}P0Ris{\\textbackslash}README.md}}\n",
       "\\section{Linear and generalized linear models in Julia}\n",
       "\\begin{tabular}\n",
       "{c | c | c | c}\n",
       "Documentation & CI Status & Coverage & DOI \\\\\n",
       "\\hline\n",
       "[![][docs-stable-img]][docs-stable-url] [![][docs-latest-img]][docs-latest-url] & [![][ci-img]][ci-url] & [![][codecov-img]][codecov-url] & [![][DOI-img]][DOI-url] \\\\\n",
       "\\end{tabular}\n",
       "[docs-latest-img]: https://img.shields.io/badge/docs-latest-blue.svg [docs-latest-url]: https://JuliaStats.github.io/GLM.jl/dev\n",
       "\n",
       "[docs-stable-img]: https://img.shields.io/badge/docs-stable-blue.svg [docs-stable-url]: https://JuliaStats.github.io/GLM.jl/stable\n",
       "\n",
       "[ci-img]: https://github.com/JuliaStats/GLM.jl/workflows/CI-stable/badge.svg [ci-url]: https://github.com/JuliaStats/GLM.jl/actions?query=workflow\\%3ACI-stable+branch\\%3Amaster\n",
       "\n",
       "[codecov-img]: https://codecov.io/gh/JuliaStats/GLM.jl/branch/master/graph/badge.svg?token=cVkd4c3M8H [codecov-url]: https://codecov.io/gh/JuliaStats/GLM.jl\n",
       "\n",
       "[DOI-img]: https://zenodo.org/badge/DOI/10.5281/zenodo.3376013.svg [DOI-url]: https://doi.org/10.5281/zenodo.3376013\n",
       "\n"
      ],
      "text/markdown": [
       "No docstring found for module `GLM`.\n",
       "\n",
       "# Exported names\n",
       "\n",
       "`@formula`, `AbstractContrasts`, `AbstractTerm`, `Bernoulli`, `Binomial`, `CategoricalTerm`, `CauchitLink`, `CloglogLink`, `ConstantTerm`, `ContinuousTerm`, `ContrastsCoding`, `DummyCoding`, `EffectsCoding`, `FormulaTerm`, `FunctionTerm`, `Gamma`, `GeneralizedLinearModel`, `Geometric`, `HelmertCoding`, `HypothesisCoding`, `IdentityLink`, `InteractionTerm`, `InterceptTerm`, `InverseGaussian`, `InverseLink`, `InverseSquareLink`, `LinearModel`, `Link`, `LogLink`, `LogitLink`, `MatrixTerm`, `ModelFrame`, `ModelMatrix`, `NegativeBinomial`, `NegativeBinomialLink`, `Normal`, `Poisson`, `PowerLink`, `ProbitLink`, `RegressionModel`, `SeqDiffCoding`, `SqrtLink`, `StatisticalModel`, `StatsModels`, `Term`, `adjr2`, `adjr²`, `apply_schema`, `canonicallink`, `coef`, `coefnames`, `coeftable`, `concrete_term`, `confint`, `cooksdistance`, `deviance`, `devresid`, `dof`, `dof_residual`, `drop_term`, `fit`, `fit!`, `fitted`, `formula`, `ftest`, `glm`, `hasintercept`, `issubmodel`, `lag`, `lead`, `linpred`, `lm`, `loglikelihood`, `lrtest`, `model_response`, `modelcols`, `modelmatrix`, `negbin`, `nobs`, `nulldeviance`, `nullloglikelihood`, `predict`, `r2`, `residuals`, `response`, `r²`, `schema`, `setcontrasts!`, `stderror`, `term`, `terms`, `vcov`, `width`\n",
       "\n",
       "# Displaying contents of readme found at `D:\\TongYuan\\.julia\\packages\\GLM\\P0Ris\\README.md`\n",
       "\n",
       "# Linear and generalized linear models in Julia\n",
       "\n",
       "|                                  Documentation                                  |       CI Status       |            Coverage             |           DOI           |\n",
       "|:-------------------------------------------------------------------------------:|:---------------------:|:-------------------------------:|:-----------------------:|\n",
       "| [![][docs-stable-img]][docs-stable-url] [![][docs-latest-img]][docs-latest-url] | [![][ci-img]][ci-url] | [![][codecov-img]][codecov-url] | [![][DOI-img]][DOI-url] |\n",
       "\n",
       "[docs-latest-img]: https://img.shields.io/badge/docs-latest-blue.svg [docs-latest-url]: https://JuliaStats.github.io/GLM.jl/dev\n",
       "\n",
       "[docs-stable-img]: https://img.shields.io/badge/docs-stable-blue.svg [docs-stable-url]: https://JuliaStats.github.io/GLM.jl/stable\n",
       "\n",
       "[ci-img]: https://github.com/JuliaStats/GLM.jl/workflows/CI-stable/badge.svg [ci-url]: https://github.com/JuliaStats/GLM.jl/actions?query=workflow%3ACI-stable+branch%3Amaster\n",
       "\n",
       "[codecov-img]: https://codecov.io/gh/JuliaStats/GLM.jl/branch/master/graph/badge.svg?token=cVkd4c3M8H [codecov-url]: https://codecov.io/gh/JuliaStats/GLM.jl\n",
       "\n",
       "[DOI-img]: https://zenodo.org/badge/DOI/10.5281/zenodo.3376013.svg [DOI-url]: https://doi.org/10.5281/zenodo.3376013\n"
      ],
      "text/plain": [
       "  No docstring found for module \u001b[36mGLM\u001b[39m.\n",
       "\n",
       "\u001b[1m  Exported names\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "  \u001b[36m@formula\u001b[39m, \u001b[36mAbstractContrasts\u001b[39m, \u001b[36mAbstractTerm\u001b[39m, \u001b[36mBernoulli\u001b[39m, \u001b[36mBinomial\u001b[39m,\n",
       "  \u001b[36mCategoricalTerm\u001b[39m, \u001b[36mCauchitLink\u001b[39m, \u001b[36mCloglogLink\u001b[39m, \u001b[36mConstantTerm\u001b[39m, \u001b[36mContinuousTerm\u001b[39m,\n",
       "  \u001b[36mContrastsCoding\u001b[39m, \u001b[36mDummyCoding\u001b[39m, \u001b[36mEffectsCoding\u001b[39m, \u001b[36mFormulaTerm\u001b[39m, \u001b[36mFunctionTerm\u001b[39m,\n",
       "  \u001b[36mGamma\u001b[39m, \u001b[36mGeneralizedLinearModel\u001b[39m, \u001b[36mGeometric\u001b[39m, \u001b[36mHelmertCoding\u001b[39m, \u001b[36mHypothesisCoding\u001b[39m,\n",
       "  \u001b[36mIdentityLink\u001b[39m, \u001b[36mInteractionTerm\u001b[39m, \u001b[36mInterceptTerm\u001b[39m, \u001b[36mInverseGaussian\u001b[39m, \u001b[36mInverseLink\u001b[39m,\n",
       "  \u001b[36mInverseSquareLink\u001b[39m, \u001b[36mLinearModel\u001b[39m, \u001b[36mLink\u001b[39m, \u001b[36mLogLink\u001b[39m, \u001b[36mLogitLink\u001b[39m, \u001b[36mMatrixTerm\u001b[39m,\n",
       "  \u001b[36mModelFrame\u001b[39m, \u001b[36mModelMatrix\u001b[39m, \u001b[36mNegativeBinomial\u001b[39m, \u001b[36mNegativeBinomialLink\u001b[39m, \u001b[36mNormal\u001b[39m,\n",
       "  \u001b[36mPoisson\u001b[39m, \u001b[36mPowerLink\u001b[39m, \u001b[36mProbitLink\u001b[39m, \u001b[36mRegressionModel\u001b[39m, \u001b[36mSeqDiffCoding\u001b[39m, \u001b[36mSqrtLink\u001b[39m,\n",
       "  \u001b[36mStatisticalModel\u001b[39m, \u001b[36mStatsModels\u001b[39m, \u001b[36mTerm\u001b[39m, \u001b[36madjr2\u001b[39m, \u001b[36madjr²\u001b[39m, \u001b[36mapply_schema\u001b[39m,\n",
       "  \u001b[36mcanonicallink\u001b[39m, \u001b[36mcoef\u001b[39m, \u001b[36mcoefnames\u001b[39m, \u001b[36mcoeftable\u001b[39m, \u001b[36mconcrete_term\u001b[39m, \u001b[36mconfint\u001b[39m,\n",
       "  \u001b[36mcooksdistance\u001b[39m, \u001b[36mdeviance\u001b[39m, \u001b[36mdevresid\u001b[39m, \u001b[36mdof\u001b[39m, \u001b[36mdof_residual\u001b[39m, \u001b[36mdrop_term\u001b[39m, \u001b[36mfit\u001b[39m, \u001b[36mfit!\u001b[39m,\n",
       "  \u001b[36mfitted\u001b[39m, \u001b[36mformula\u001b[39m, \u001b[36mftest\u001b[39m, \u001b[36mglm\u001b[39m, \u001b[36mhasintercept\u001b[39m, \u001b[36missubmodel\u001b[39m, \u001b[36mlag\u001b[39m, \u001b[36mlead\u001b[39m, \u001b[36mlinpred\u001b[39m,\n",
       "  \u001b[36mlm\u001b[39m, \u001b[36mloglikelihood\u001b[39m, \u001b[36mlrtest\u001b[39m, \u001b[36mmodel_response\u001b[39m, \u001b[36mmodelcols\u001b[39m, \u001b[36mmodelmatrix\u001b[39m, \u001b[36mnegbin\u001b[39m,\n",
       "  \u001b[36mnobs\u001b[39m, \u001b[36mnulldeviance\u001b[39m, \u001b[36mnullloglikelihood\u001b[39m, \u001b[36mpredict\u001b[39m, \u001b[36mr2\u001b[39m, \u001b[36mresiduals\u001b[39m, \u001b[36mresponse\u001b[39m, \u001b[36mr²\u001b[39m,\n",
       "  \u001b[36mschema\u001b[39m, \u001b[36msetcontrasts!\u001b[39m, \u001b[36mstderror\u001b[39m, \u001b[36mterm\u001b[39m, \u001b[36mterms\u001b[39m, \u001b[36mvcov\u001b[39m, \u001b[36mwidth\u001b[39m\n",
       "\n",
       "\u001b[1m  Displaying contents of readme found at\u001b[22m\n",
       "\u001b[1m \u001b[36mD:\\TongYuan\\.julia\\packages\\GLM\\P0Ris\\README.md\u001b[39m\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "\u001b[1m  Linear and generalized linear models in Julia\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "                                   Documentation                                        CI Status                  Coverage                       DOI          \n",
       "  ––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––– ––––––––––––––––––––– ––––––––––––––––––––––––––––––– –––––––––––––––––––––––\n",
       "  [![][docs-stable-img]][docs-stable-url] [![][docs-latest-img]][docs-latest-url] [![][ci-img]][ci-url] [![][codecov-img]][codecov-url] [![][DOI-img]][DOI-url]\n",
       "\n",
       "  [docs-latest-img]: https://img.shields.io/badge/docs-latest-blue.svg\n",
       "  [docs-latest-url]: https://JuliaStats.github.io/GLM.jl/dev\n",
       "\n",
       "  [docs-stable-img]: https://img.shields.io/badge/docs-stable-blue.svg\n",
       "  [docs-stable-url]: https://JuliaStats.github.io/GLM.jl/stable\n",
       "\n",
       "  [ci-img]: https://github.com/JuliaStats/GLM.jl/workflows/CI-stable/badge.svg\n",
       "  [ci-url]:\n",
       "  https://github.com/JuliaStats/GLM.jl/actions?query=workflow%3ACI-stable+branch%3Amaster\n",
       "\n",
       "  [codecov-img]:\n",
       "  https://codecov.io/gh/JuliaStats/GLM.jl/branch/master/graph/badge.svg?token=cVkd4c3M8H\n",
       "  [codecov-url]: https://codecov.io/gh/JuliaStats/GLM.jl\n",
       "\n",
       "  [DOI-img]: https://zenodo.org/badge/DOI/10.5281/zenodo.3376013.svg\n",
       "  [DOI-url]: https://doi.org/10.5281/zenodo.3376013"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?GLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T14:53:30.593000+08:00",
     "start_time": "2022-06-08T06:53:24.659Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1ml\u001b[22m\u001b[0m\u001b[1mm\u001b[22m g\u001b[0m\u001b[1ml\u001b[22m\u001b[0m\u001b[1mm\u001b[22m G\u001b[0m\u001b[1mL\u001b[22m\u001b[0m\u001b[1mM\u001b[22m He\u001b[0m\u001b[1ml\u001b[22m\u001b[0m\u001b[1mm\u001b[22mertCoding mode\u001b[0m\u001b[1ml\u001b[22m\u001b[0m\u001b[1mm\u001b[22matrix Mode\u001b[0m\u001b[1ml\u001b[22m\u001b[0m\u001b[1mM\u001b[22matrix \u001b[0m\u001b[1ml\u001b[22mc\u001b[0m\u001b[1mm\u001b[22m c\u001b[0m\u001b[1ml\u001b[22ma\u001b[0m\u001b[1mm\u001b[22mp f\u001b[0m\u001b[1ml\u001b[22md\u001b[0m\u001b[1mm\u001b[22mod c\u001b[0m\u001b[1ml\u001b[22ma\u001b[0m\u001b[1mm\u001b[22mp!\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "lm(formula, data, allowrankdeficient=false;\n",
       "   [wts::AbstractVector], dropcollinear::Bool=true)\n",
       "lm(X::AbstractMatrix, y::AbstractVector;\n",
       "   wts::AbstractVector=similar(y, 0), dropcollinear::Bool=true)\n",
       "\\end{verbatim}\n",
       "Fit a linear model to data. An alias for \\texttt{fit(LinearModel, X, y; wts=wts, dropcollinear=dropcollinear)}\n",
       "\n",
       "In the first method, \\texttt{formula} must be a \\href{https://juliastats.org/StatsModels.jl/stable/formula/}{StatsModels.jl \\texttt{Formula} object} and \\texttt{data} a table (in the \\href{https://tables.juliadata.org/stable/}{Tables.jl} definition, e.g. a data frame). In the second method, \\texttt{X} must be a matrix holding values of the independent variable(s) in columns (including if appropriate the intercept), and \\texttt{y} must be a vector holding values of the dependent variable.\n",
       "\n",
       "The keyword argument \\texttt{wts} can be a \\texttt{Vector} specifying frequency weights for observations. Such weights are equivalent to repeating each observation a number of times equal to its weight. Do note that this interpretation gives equal point estimates but different standard errors from analytical (a.k.a. inverse variance) weights and from probability (a.k.a. sampling) weights which are the default in some other software.\n",
       "\n",
       "\\texttt{dropcollinear} controls whether or not \\texttt{lm} accepts a model matrix which is less-than-full rank. If \\texttt{true} (the default), only the first of each set of linearly-dependent columns is used. The coefficient for redundant linearly dependent columns is \\texttt{0.0} and all associated statistics are set to \\texttt{NaN}.\n",
       "\n"
      ],
      "text/markdown": [
       "```\n",
       "lm(formula, data, allowrankdeficient=false;\n",
       "   [wts::AbstractVector], dropcollinear::Bool=true)\n",
       "lm(X::AbstractMatrix, y::AbstractVector;\n",
       "   wts::AbstractVector=similar(y, 0), dropcollinear::Bool=true)\n",
       "```\n",
       "\n",
       "Fit a linear model to data. An alias for `fit(LinearModel, X, y; wts=wts, dropcollinear=dropcollinear)`\n",
       "\n",
       "In the first method, `formula` must be a [StatsModels.jl `Formula` object](https://juliastats.org/StatsModels.jl/stable/formula/) and `data` a table (in the [Tables.jl](https://tables.juliadata.org/stable/) definition, e.g. a data frame). In the second method, `X` must be a matrix holding values of the independent variable(s) in columns (including if appropriate the intercept), and `y` must be a vector holding values of the dependent variable.\n",
       "\n",
       "The keyword argument `wts` can be a `Vector` specifying frequency weights for observations. Such weights are equivalent to repeating each observation a number of times equal to its weight. Do note that this interpretation gives equal point estimates but different standard errors from analytical (a.k.a. inverse variance) weights and from probability (a.k.a. sampling) weights which are the default in some other software.\n",
       "\n",
       "`dropcollinear` controls whether or not `lm` accepts a model matrix which is less-than-full rank. If `true` (the default), only the first of each set of linearly-dependent columns is used. The coefficient for redundant linearly dependent columns is `0.0` and all associated statistics are set to `NaN`.\n"
      ],
      "text/plain": [
       "\u001b[36m  lm(formula, data, allowrankdeficient=false;\u001b[39m\n",
       "\u001b[36m     [wts::AbstractVector], dropcollinear::Bool=true)\u001b[39m\n",
       "\u001b[36m  lm(X::AbstractMatrix, y::AbstractVector;\u001b[39m\n",
       "\u001b[36m     wts::AbstractVector=similar(y, 0), dropcollinear::Bool=true)\u001b[39m\n",
       "\n",
       "  Fit a linear model to data. An alias for \u001b[36mfit(LinearModel, X, y; wts=wts,\n",
       "  dropcollinear=dropcollinear)\u001b[39m\n",
       "\n",
       "  In the first method, \u001b[36mformula\u001b[39m must be a StatsModels.jl \u001b[36mFormula\u001b[39m object\n",
       "  (https://juliastats.org/StatsModels.jl/stable/formula/) and \u001b[36mdata\u001b[39m a table (in\n",
       "  the Tables.jl (https://tables.juliadata.org/stable/) definition, e.g. a data\n",
       "  frame). In the second method, \u001b[36mX\u001b[39m must be a matrix holding values of the\n",
       "  independent variable(s) in columns (including if appropriate the intercept),\n",
       "  and \u001b[36my\u001b[39m must be a vector holding values of the dependent variable.\n",
       "\n",
       "  The keyword argument \u001b[36mwts\u001b[39m can be a \u001b[36mVector\u001b[39m specifying frequency weights for\n",
       "  observations. Such weights are equivalent to repeating each observation a\n",
       "  number of times equal to its weight. Do note that this interpretation gives\n",
       "  equal point estimates but different standard errors from analytical (a.k.a.\n",
       "  inverse variance) weights and from probability (a.k.a. sampling) weights\n",
       "  which are the default in some other software.\n",
       "\n",
       "  \u001b[36mdropcollinear\u001b[39m controls whether or not \u001b[36mlm\u001b[39m accepts a model matrix which is\n",
       "  less-than-full rank. If \u001b[36mtrue\u001b[39m (the default), only the first of each set of\n",
       "  linearly-dependent columns is used. The coefficient for redundant linearly\n",
       "  dependent columns is \u001b[36m0.0\u001b[39m and all associated statistics are set to \u001b[36mNaN\u001b[39m."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T14:53:30.742000+08:00",
     "start_time": "2022-06-08T06:53:24.660Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1mp\u001b[22m\u001b[0m\u001b[1mr\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1md\u001b[22m\u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1mc\u001b[22m\u001b[0m\u001b[1mt\u001b[22m \u001b[0m\u001b[1mp\u001b[22ma\u001b[0m\u001b[1mr\u001b[22m\u001b[0m\u001b[1me\u001b[22mntin\u001b[0m\u001b[1md\u001b[22m\u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1mc\u001b[22mes \u001b[0m\u001b[1mp\u001b[22me\u001b[0m\u001b[1mr\u001b[22mmut\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1md\u001b[22m\u001b[0m\u001b[1mi\u001b[22mms \u001b[0m\u001b[1mp\u001b[22me\u001b[0m\u001b[1mr\u001b[22mmut\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1md\u001b[22m\u001b[0m\u001b[1mi\u001b[22mms! \u001b[0m\u001b[1mP\u001b[22me\u001b[0m\u001b[1mr\u001b[22mmut\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1md\u001b[22mD\u001b[0m\u001b[1mi\u001b[22mmsArray\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "predict(model::RegressionModel, [newX])\n",
       "\\end{verbatim}\n",
       "Form the predicted response of \\texttt{model}. An object with new covariate values \\texttt{newX} can be supplied, which should have the same type and structure as that used to fit \\texttt{model}; e.g. for a GLM it would generally be a \\texttt{DataFrame} with the same variable names as the original predictors.\n",
       "\n",
       "\\rule{\\textwidth}{1pt}\n",
       "\\begin{verbatim}\n",
       "predict(mm::LinearModel, newx::AbstractMatrix;\n",
       "        interval::Union{Symbol,Nothing} = nothing, level::Real = 0.95)\n",
       "\\end{verbatim}\n",
       "If \\texttt{interval} is \\texttt{nothing} (the default), return a vector with the predicted values for model \\texttt{mm} and new data \\texttt{newx}. Otherwise, return a vector with the predicted values, as well as vectors with the lower and upper confidence bounds for a given \\texttt{level} (0.95 equates alpha = 0.05). Valid values of \\texttt{interval} are \\texttt{:confidence} delimiting the  uncertainty of the predicted relationship, and \\texttt{:prediction} delimiting estimated bounds for new data points.\n",
       "\n",
       "\\rule{\\textwidth}{1pt}\n",
       "\\begin{verbatim}\n",
       "predict(mm::AbstractGLM, newX::AbstractMatrix; offset::FPVector=eltype(newX)[],\n",
       "        interval::Union{Symbol,Nothing}=nothing, level::Real = 0.95,\n",
       "        interval_method::Symbol = :transformation)\n",
       "\\end{verbatim}\n",
       "Return the predicted response of model \\texttt{mm} from covariate values \\texttt{newX} and, optionally, an \\texttt{offset}.\n",
       "\n",
       "If \\texttt{interval=:confidence}, also return upper and lower bounds for a given coverage \\texttt{level}. By default (\\texttt{interval\\_method = :transformation}) the intervals are constructed by applying the inverse link to intervals for the linear predictor. If \\texttt{interval\\_method = :delta}, the intervals are constructed by the delta method, i.e., by linearization of the predicted response around the linear predictor. The \\texttt{:delta} method intervals are symmetric around the point estimates, but do not respect natural parameter constraints (e.g., the lower bound for a probability could be negative).\n",
       "\n"
      ],
      "text/markdown": [
       "```\n",
       "predict(model::RegressionModel, [newX])\n",
       "```\n",
       "\n",
       "Form the predicted response of `model`. An object with new covariate values `newX` can be supplied, which should have the same type and structure as that used to fit `model`; e.g. for a GLM it would generally be a `DataFrame` with the same variable names as the original predictors.\n",
       "\n",
       "---\n",
       "\n",
       "```\n",
       "predict(mm::LinearModel, newx::AbstractMatrix;\n",
       "        interval::Union{Symbol,Nothing} = nothing, level::Real = 0.95)\n",
       "```\n",
       "\n",
       "If `interval` is `nothing` (the default), return a vector with the predicted values for model `mm` and new data `newx`. Otherwise, return a vector with the predicted values, as well as vectors with the lower and upper confidence bounds for a given `level` (0.95 equates alpha = 0.05). Valid values of `interval` are `:confidence` delimiting the  uncertainty of the predicted relationship, and `:prediction` delimiting estimated bounds for new data points.\n",
       "\n",
       "---\n",
       "\n",
       "```\n",
       "predict(mm::AbstractGLM, newX::AbstractMatrix; offset::FPVector=eltype(newX)[],\n",
       "        interval::Union{Symbol,Nothing}=nothing, level::Real = 0.95,\n",
       "        interval_method::Symbol = :transformation)\n",
       "```\n",
       "\n",
       "Return the predicted response of model `mm` from covariate values `newX` and, optionally, an `offset`.\n",
       "\n",
       "If `interval=:confidence`, also return upper and lower bounds for a given coverage `level`. By default (`interval_method = :transformation`) the intervals are constructed by applying the inverse link to intervals for the linear predictor. If `interval_method = :delta`, the intervals are constructed by the delta method, i.e., by linearization of the predicted response around the linear predictor. The `:delta` method intervals are symmetric around the point estimates, but do not respect natural parameter constraints (e.g., the lower bound for a probability could be negative).\n"
      ],
      "text/plain": [
       "\u001b[36m  predict(model::RegressionModel, [newX])\u001b[39m\n",
       "\n",
       "  Form the predicted response of \u001b[36mmodel\u001b[39m. An object with new covariate values\n",
       "  \u001b[36mnewX\u001b[39m can be supplied, which should have the same type and structure as that\n",
       "  used to fit \u001b[36mmodel\u001b[39m; e.g. for a GLM it would generally be a \u001b[36mDataFrame\u001b[39m with the\n",
       "  same variable names as the original predictors.\n",
       "\n",
       "  ────────────────────────────────────────────────────────────────────────────\n",
       "\n",
       "\u001b[36m  predict(mm::LinearModel, newx::AbstractMatrix;\u001b[39m\n",
       "\u001b[36m          interval::Union{Symbol,Nothing} = nothing, level::Real = 0.95)\u001b[39m\n",
       "\n",
       "  If \u001b[36minterval\u001b[39m is \u001b[36mnothing\u001b[39m (the default), return a vector with the predicted\n",
       "  values for model \u001b[36mmm\u001b[39m and new data \u001b[36mnewx\u001b[39m. Otherwise, return a vector with the\n",
       "  predicted values, as well as vectors with the lower and upper confidence\n",
       "  bounds for a given \u001b[36mlevel\u001b[39m (0.95 equates alpha = 0.05). Valid values of\n",
       "  \u001b[36minterval\u001b[39m are \u001b[36m:confidence\u001b[39m delimiting the uncertainty of the predicted\n",
       "  relationship, and \u001b[36m:prediction\u001b[39m delimiting estimated bounds for new data\n",
       "  points.\n",
       "\n",
       "  ────────────────────────────────────────────────────────────────────────────\n",
       "\n",
       "\u001b[36m  predict(mm::AbstractGLM, newX::AbstractMatrix; offset::FPVector=eltype(newX)[],\u001b[39m\n",
       "\u001b[36m          interval::Union{Symbol,Nothing}=nothing, level::Real = 0.95,\u001b[39m\n",
       "\u001b[36m          interval_method::Symbol = :transformation)\u001b[39m\n",
       "\n",
       "  Return the predicted response of model \u001b[36mmm\u001b[39m from covariate values \u001b[36mnewX\u001b[39m and,\n",
       "  optionally, an \u001b[36moffset\u001b[39m.\n",
       "\n",
       "  If \u001b[36minterval=:confidence\u001b[39m, also return upper and lower bounds for a given\n",
       "  coverage \u001b[36mlevel\u001b[39m. By default (\u001b[36minterval_method = :transformation\u001b[39m) the intervals\n",
       "  are constructed by applying the inverse link to intervals for the linear\n",
       "  predictor. If \u001b[36minterval_method = :delta\u001b[39m, the intervals are constructed by the\n",
       "  delta method, i.e., by linearization of the predicted response around the\n",
       "  linear predictor. The \u001b[36m:delta\u001b[39m method intervals are symmetric around the point\n",
       "  estimates, but do not respect natural parameter constraints (e.g., the lower\n",
       "  bound for a probability could be negative)."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T14:53:30.979000+08:00",
     "start_time": "2022-06-08T06:53:24.662Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1mf\u001b[22m\u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1mt\u001b[22m \u001b[0m\u001b[1mf\u001b[22m\u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1mt\u001b[22m! \u001b[0m\u001b[1mf\u001b[22m\u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1mt\u001b[22mted \u001b[0m\u001b[1mf\u001b[22m\u001b[0m\u001b[1mi\u001b[22ml\u001b[0m\u001b[1mt\u001b[22mer \u001b[0m\u001b[1mf\u001b[22m\u001b[0m\u001b[1mi\u001b[22ml\u001b[0m\u001b[1mt\u001b[22mer! con\u001b[0m\u001b[1mf\u001b[22m\u001b[0m\u001b[1mi\u001b[22mn\u001b[0m\u001b[1mt\u001b[22m \u001b[0m\u001b[1mf\u001b[22m\u001b[0m\u001b[1mi\u001b[22mrs\u001b[0m\u001b[1mt\u001b[22m \u001b[0m\u001b[1mf\u001b[22m\u001b[0m\u001b[1mi\u001b[22mrs\u001b[0m\u001b[1mt\u001b[22mindex is\u001b[0m\u001b[1mf\u001b[22m\u001b[0m\u001b[1mi\u001b[22mni\u001b[0m\u001b[1mt\u001b[22me\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "Fit a statistical model.\n",
       "\n",
       "\\rule{\\textwidth}{1pt}\n",
       "\\begin{verbatim}\n",
       "fit(Histogram, data[, weight][, edges]; closed=:left, nbins)\n",
       "\\end{verbatim}\n",
       "Fit a histogram to \\texttt{data}.\n",
       "\n",
       "\\section{Arguments}\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{data}: either a vector (for a 1-dimensional histogram), or a tuple of vectors of equal length (for an \\emph{n}-dimensional histogram).\n",
       "\n",
       "\n",
       "\\item \\texttt{weight}: an optional \\texttt{AbstractWeights} (of the same length as the data vectors), denoting the weight each observation contributes to the bin. If no weight vector is supplied, each observation has weight 1.\n",
       "\n",
       "\n",
       "\\item \\texttt{edges}: a vector (typically an \\texttt{AbstractRange} object), or tuple of vectors, that gives the edges of the bins along each dimension. If no edges are provided, these are determined from the data.\n",
       "\n",
       "\\end{itemize}\n",
       "\\section{Keyword arguments}\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{closed}: if \\texttt{:left} (the default), the bin intervals are left-closed [a,b); if \\texttt{:right}, intervals are right-closed (a,b].\n",
       "\n",
       "\n",
       "\\item \\texttt{nbins}: if no \\texttt{edges} argument is supplied, the approximate number of bins to use along each dimension (can be either a single integer, or a tuple of integers).\n",
       "\n",
       "\\end{itemize}\n",
       "\\section{Examples}\n",
       "\\begin{verbatim}\n",
       "# Univariate\n",
       "h = fit(Histogram, rand(100))\n",
       "h = fit(Histogram, rand(100), 0:0.1:1.0)\n",
       "h = fit(Histogram, rand(100), nbins=10)\n",
       "h = fit(Histogram, rand(100), weights(rand(100)), 0:0.1:1.0)\n",
       "h = fit(Histogram, [20], 0:20:100)\n",
       "h = fit(Histogram, [20], 0:20:100, closed=:right)\n",
       "\n",
       "# Multivariate\n",
       "h = fit(Histogram, (rand(100),rand(100)))\n",
       "h = fit(Histogram, (rand(100),rand(100)),nbins=10)\n",
       "\\end{verbatim}\n",
       "\\rule{\\textwidth}{1pt}\n",
       "\\begin{verbatim}\n",
       "fit(ZScoreTransform, X; dims=nothing, center=true, scale=true)\n",
       "\\end{verbatim}\n",
       "Fit standardization parameters to vector or matrix \\texttt{X} and return a \\texttt{ZScoreTransform} transformation object.\n",
       "\n",
       "\\section{Keyword arguments}\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{dims}: if \\texttt{1} fit standardization parameters in column-wise fashion; if \\texttt{2} fit in row-wise fashion. The default is \\texttt{nothing}, which is equivalent to \\texttt{dims=2} with a deprecation warning.\n",
       "\n",
       "\n",
       "\\item \\texttt{center}: if \\texttt{true} (the default) center data so that its mean is zero.\n",
       "\n",
       "\n",
       "\\item \\texttt{scale}: if \\texttt{true} (the default) scale the data so that its variance is equal to one.\n",
       "\n",
       "\\end{itemize}\n",
       "\\section{Examples}\n",
       "\\begin{verbatim}\n",
       "julia> using StatsBase\n",
       "\n",
       "julia> X = [0.0 -0.5 0.5; 0.0 1.0 2.0]\n",
       "2×3 Matrix{Float64}:\n",
       " 0.0  -0.5  0.5\n",
       " 0.0   1.0  2.0\n",
       "\n",
       "julia> dt = fit(ZScoreTransform, X, dims=2)\n",
       "ZScoreTransform{Float64, Vector{Float64}}(2, 2, [0.0, 1.0], [0.5, 1.0])\n",
       "\n",
       "julia> StatsBase.transform(dt, X)\n",
       "2×3 Matrix{Float64}:\n",
       "  0.0  -1.0  1.0\n",
       " -1.0   0.0  1.0\n",
       "\\end{verbatim}\n",
       "\\rule{\\textwidth}{1pt}\n",
       "\\begin{verbatim}\n",
       "fit(UnitRangeTransform, X; dims=nothing, unit=true)\n",
       "\\end{verbatim}\n",
       "Fit a scaling parameters to vector or matrix \\texttt{X} and return a \\texttt{UnitRangeTransform} transformation object.\n",
       "\n",
       "\\section{Keyword arguments}\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{dims}: if \\texttt{1} fit standardization parameters in column-wise fashion;\n",
       "\n",
       "\\end{itemize}\n",
       "if \\texttt{2} fit in row-wise fashion. The default is \\texttt{nothing}.\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{unit}: if \\texttt{true} (the default) shift the minimum data to zero.\n",
       "\n",
       "\\end{itemize}\n",
       "\\section{Examples}\n",
       "\\begin{verbatim}\n",
       "julia> using StatsBase\n",
       "\n",
       "julia> X = [0.0 -0.5 0.5; 0.0 1.0 2.0]\n",
       "2×3 Matrix{Float64}:\n",
       " 0.0  -0.5  0.5\n",
       " 0.0   1.0  2.0\n",
       "\n",
       "julia> dt = fit(UnitRangeTransform, X, dims=2)\n",
       "UnitRangeTransform{Float64, Vector{Float64}}(2, 2, true, [-0.5, 0.0], [1.0, 0.5])\n",
       "\n",
       "julia> StatsBase.transform(dt, X)\n",
       "2×3 Matrix{Float64}:\n",
       " 0.5  0.0  1.0\n",
       " 0.0  0.5  1.0\n",
       "\\end{verbatim}\n",
       "\\rule{\\textwidth}{1pt}\n",
       "\\begin{verbatim}\n",
       "fit(::Type{<:Beta}, x::AbstractArray{T})\n",
       "\\end{verbatim}\n",
       "fit a \\texttt{Beta} distribution\n",
       "\n",
       "\\rule{\\textwidth}{1pt}\n",
       "\\begin{verbatim}\n",
       "fit(Mod::Type{<:StatisticalModel}, f::FormulaTerm, data, args...;\n",
       "    contrasts::Dict{Symbol}, kwargs...)\n",
       "\\end{verbatim}\n",
       "Convert tabular data into a numeric response vector and predictor matrix using the formula \\texttt{f}, and then \\texttt{fit} the specified model type, wrapping the result in a \\href{@ref}{\\texttt{TableRegressionModel}} or \\href{@ref}{\\texttt{TableStatisticalModel}} (as appropriate).\n",
       "\n",
       "This is intended as a backstop for modeling packages that implement model types that are subtypes of \\texttt{StatsBase.StatisticalModel} but do not explicitly support the full StatsModels terms-based interface.  Currently this works by creating a \\href{@ref}{\\texttt{ModelFrame}} from the formula and data, and then converting this to a \\href{@ref}{\\texttt{ModelMatrix}}, but this is an internal implementation detail which may change in the near future.\n",
       "\n",
       "\\rule{\\textwidth}{1pt}\n",
       "\\begin{verbatim}\n",
       "fit(LinearModel, formula, data, allowrankdeficient=false;\n",
       "   [wts::AbstractVector], dropcollinear::Bool=true)\n",
       "fit(LinearModel, X::AbstractMatrix, y::AbstractVector;\n",
       "    wts::AbstractVector=similar(y, 0), dropcollinear::Bool=true)\n",
       "\\end{verbatim}\n",
       "Fit a linear model to data.\n",
       "\n",
       "In the first method, \\texttt{formula} must be a \\href{https://juliastats.org/StatsModels.jl/stable/formula/}{StatsModels.jl \\texttt{Formula} object} and \\texttt{data} a table (in the \\href{https://tables.juliadata.org/stable/}{Tables.jl} definition, e.g. a data frame). In the second method, \\texttt{X} must be a matrix holding values of the independent variable(s) in columns (including if appropriate the intercept), and \\texttt{y} must be a vector holding values of the dependent variable.\n",
       "\n",
       "The keyword argument \\texttt{wts} can be a \\texttt{Vector} specifying frequency weights for observations. Such weights are equivalent to repeating each observation a number of times equal to its weight. Do note that this interpretation gives equal point estimates but different standard errors from analytical (a.k.a. inverse variance) weights and from probability (a.k.a. sampling) weights which are the default in some other software.\n",
       "\n",
       "\\texttt{dropcollinear} controls whether or not \\texttt{lm} accepts a model matrix which is less-than-full rank. If \\texttt{true} (the default), only the first of each set of linearly-dependent columns is used. The coefficient for redundant linearly dependent columns is \\texttt{0.0} and all associated statistics are set to \\texttt{NaN}.\n",
       "\n",
       "\\rule{\\textwidth}{1pt}\n",
       "\\begin{verbatim}\n",
       "fit(GeneralizedLinearModel, formula, data,\n",
       "    distr::UnivariateDistribution, link::Link = canonicallink(d); <keyword arguments>)\n",
       "fit(GeneralizedLinearModel, X::AbstractMatrix, y::AbstractVector,\n",
       "    distr::UnivariateDistribution, link::Link = canonicallink(d); <keyword arguments>)\n",
       "\\end{verbatim}\n",
       "Fit a generalized linear model to data.\n",
       "\n",
       "In the first method, \\texttt{formula} must be a \\href{https://juliastats.org/StatsModels.jl/stable/formula/}{StatsModels.jl \\texttt{Formula} object} and \\texttt{data} a table (in the \\href{https://tables.juliadata.org/stable/}{Tables.jl} definition, e.g. a data frame). In the second method, \\texttt{X} must be a matrix holding values of the independent variable(s) in columns (including if appropriate the intercept), and \\texttt{y} must be a vector holding values of the dependent variable. In both cases, \\texttt{distr} must specify the distribution, and \\texttt{link} may specify the link function (if omitted, it is taken to be the canonical link for \\texttt{distr}; see \\href{@ref}{\\texttt{Link}} for a list of built-in links).\n",
       "\n",
       "\\section{Keyword Arguments}\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{dofit::Bool=true}: Determines whether model will be fit\n",
       "\n",
       "\n",
       "\\item \\texttt{wts::Vector=similar(y,0)}: Prior frequency (a.k.a. case) weights of observations. Such weights are equivalent to repeating each observation a number of times equal to its weight. Do note that this interpretation gives equal point estimates but different standard errors from analytical (a.k.a. inverse variance) weights and from probability (a.k.a. sampling) weights which are the default in some other software. Can be length 0 to indicate no weighting (default).\n",
       "\n",
       "\n",
       "\\item \\texttt{offset::Vector=similar(y,0)}: offset added to \\texttt{Xβ} to form \\texttt{eta}.  Can be of length 0\n",
       "\n",
       "\n",
       "\\item \\texttt{verbose::Bool=false}: Display convergence information for each iteration\n",
       "\n",
       "\n",
       "\\item \\texttt{maxiter::Integer=30}: Maximum number of iterations allowed to achieve convergence\n",
       "\n",
       "\n",
       "\\item \\texttt{atol::Real=1e-6}: Convergence is achieved when the relative change in deviance is less than \\texttt{max(rtol*dev, atol)}.\n",
       "\n",
       "\n",
       "\\item \\texttt{rtol::Real=1e-6}: Convergence is achieved when the relative change in deviance is less than \\texttt{max(rtol*dev, atol)}.\n",
       "\n",
       "\n",
       "\\item \\texttt{minstepfac::Real=0.001}: Minimum line step fraction. Must be between 0 and 1.\n",
       "\n",
       "\n",
       "\\item \\texttt{start::AbstractVector=nothing}: Starting values for beta. Should have the same length as the number of columns in the model matrix.\n",
       "\n",
       "\\end{itemize}\n"
      ],
      "text/markdown": [
       "Fit a statistical model.\n",
       "\n",
       "---\n",
       "\n",
       "```\n",
       "fit(Histogram, data[, weight][, edges]; closed=:left, nbins)\n",
       "```\n",
       "\n",
       "Fit a histogram to `data`.\n",
       "\n",
       "# Arguments\n",
       "\n",
       "  * `data`: either a vector (for a 1-dimensional histogram), or a tuple of vectors of equal length (for an *n*-dimensional histogram).\n",
       "  * `weight`: an optional `AbstractWeights` (of the same length as the data vectors), denoting the weight each observation contributes to the bin. If no weight vector is supplied, each observation has weight 1.\n",
       "  * `edges`: a vector (typically an `AbstractRange` object), or tuple of vectors, that gives the edges of the bins along each dimension. If no edges are provided, these are determined from the data.\n",
       "\n",
       "# Keyword arguments\n",
       "\n",
       "  * `closed`: if `:left` (the default), the bin intervals are left-closed [a,b); if `:right`, intervals are right-closed (a,b].\n",
       "  * `nbins`: if no `edges` argument is supplied, the approximate number of bins to use along each dimension (can be either a single integer, or a tuple of integers).\n",
       "\n",
       "# Examples\n",
       "\n",
       "```julia\n",
       "# Univariate\n",
       "h = fit(Histogram, rand(100))\n",
       "h = fit(Histogram, rand(100), 0:0.1:1.0)\n",
       "h = fit(Histogram, rand(100), nbins=10)\n",
       "h = fit(Histogram, rand(100), weights(rand(100)), 0:0.1:1.0)\n",
       "h = fit(Histogram, [20], 0:20:100)\n",
       "h = fit(Histogram, [20], 0:20:100, closed=:right)\n",
       "\n",
       "# Multivariate\n",
       "h = fit(Histogram, (rand(100),rand(100)))\n",
       "h = fit(Histogram, (rand(100),rand(100)),nbins=10)\n",
       "```\n",
       "\n",
       "---\n",
       "\n",
       "```\n",
       "fit(ZScoreTransform, X; dims=nothing, center=true, scale=true)\n",
       "```\n",
       "\n",
       "Fit standardization parameters to vector or matrix `X` and return a `ZScoreTransform` transformation object.\n",
       "\n",
       "# Keyword arguments\n",
       "\n",
       "  * `dims`: if `1` fit standardization parameters in column-wise fashion; if `2` fit in row-wise fashion. The default is `nothing`, which is equivalent to `dims=2` with a deprecation warning.\n",
       "  * `center`: if `true` (the default) center data so that its mean is zero.\n",
       "  * `scale`: if `true` (the default) scale the data so that its variance is equal to one.\n",
       "\n",
       "# Examples\n",
       "\n",
       "```jldoctest\n",
       "julia> using StatsBase\n",
       "\n",
       "julia> X = [0.0 -0.5 0.5; 0.0 1.0 2.0]\n",
       "2×3 Matrix{Float64}:\n",
       " 0.0  -0.5  0.5\n",
       " 0.0   1.0  2.0\n",
       "\n",
       "julia> dt = fit(ZScoreTransform, X, dims=2)\n",
       "ZScoreTransform{Float64, Vector{Float64}}(2, 2, [0.0, 1.0], [0.5, 1.0])\n",
       "\n",
       "julia> StatsBase.transform(dt, X)\n",
       "2×3 Matrix{Float64}:\n",
       "  0.0  -1.0  1.0\n",
       " -1.0   0.0  1.0\n",
       "```\n",
       "\n",
       "---\n",
       "\n",
       "```\n",
       "fit(UnitRangeTransform, X; dims=nothing, unit=true)\n",
       "```\n",
       "\n",
       "Fit a scaling parameters to vector or matrix `X` and return a `UnitRangeTransform` transformation object.\n",
       "\n",
       "# Keyword arguments\n",
       "\n",
       "  * `dims`: if `1` fit standardization parameters in column-wise fashion;\n",
       "\n",
       "if `2` fit in row-wise fashion. The default is `nothing`.\n",
       "\n",
       "  * `unit`: if `true` (the default) shift the minimum data to zero.\n",
       "\n",
       "# Examples\n",
       "\n",
       "```jldoctest\n",
       "julia> using StatsBase\n",
       "\n",
       "julia> X = [0.0 -0.5 0.5; 0.0 1.0 2.0]\n",
       "2×3 Matrix{Float64}:\n",
       " 0.0  -0.5  0.5\n",
       " 0.0   1.0  2.0\n",
       "\n",
       "julia> dt = fit(UnitRangeTransform, X, dims=2)\n",
       "UnitRangeTransform{Float64, Vector{Float64}}(2, 2, true, [-0.5, 0.0], [1.0, 0.5])\n",
       "\n",
       "julia> StatsBase.transform(dt, X)\n",
       "2×3 Matrix{Float64}:\n",
       " 0.5  0.0  1.0\n",
       " 0.0  0.5  1.0\n",
       "```\n",
       "\n",
       "---\n",
       "\n",
       "```\n",
       "fit(::Type{<:Beta}, x::AbstractArray{T})\n",
       "```\n",
       "\n",
       "fit a `Beta` distribution\n",
       "\n",
       "---\n",
       "\n",
       "```\n",
       "fit(Mod::Type{<:StatisticalModel}, f::FormulaTerm, data, args...;\n",
       "    contrasts::Dict{Symbol}, kwargs...)\n",
       "```\n",
       "\n",
       "Convert tabular data into a numeric response vector and predictor matrix using the formula `f`, and then `fit` the specified model type, wrapping the result in a [`TableRegressionModel`](@ref) or [`TableStatisticalModel`](@ref) (as appropriate).\n",
       "\n",
       "This is intended as a backstop for modeling packages that implement model types that are subtypes of `StatsBase.StatisticalModel` but do not explicitly support the full StatsModels terms-based interface.  Currently this works by creating a [`ModelFrame`](@ref) from the formula and data, and then converting this to a [`ModelMatrix`](@ref), but this is an internal implementation detail which may change in the near future.\n",
       "\n",
       "---\n",
       "\n",
       "```\n",
       "fit(LinearModel, formula, data, allowrankdeficient=false;\n",
       "   [wts::AbstractVector], dropcollinear::Bool=true)\n",
       "fit(LinearModel, X::AbstractMatrix, y::AbstractVector;\n",
       "    wts::AbstractVector=similar(y, 0), dropcollinear::Bool=true)\n",
       "```\n",
       "\n",
       "Fit a linear model to data.\n",
       "\n",
       "In the first method, `formula` must be a [StatsModels.jl `Formula` object](https://juliastats.org/StatsModels.jl/stable/formula/) and `data` a table (in the [Tables.jl](https://tables.juliadata.org/stable/) definition, e.g. a data frame). In the second method, `X` must be a matrix holding values of the independent variable(s) in columns (including if appropriate the intercept), and `y` must be a vector holding values of the dependent variable.\n",
       "\n",
       "The keyword argument `wts` can be a `Vector` specifying frequency weights for observations. Such weights are equivalent to repeating each observation a number of times equal to its weight. Do note that this interpretation gives equal point estimates but different standard errors from analytical (a.k.a. inverse variance) weights and from probability (a.k.a. sampling) weights which are the default in some other software.\n",
       "\n",
       "`dropcollinear` controls whether or not `lm` accepts a model matrix which is less-than-full rank. If `true` (the default), only the first of each set of linearly-dependent columns is used. The coefficient for redundant linearly dependent columns is `0.0` and all associated statistics are set to `NaN`.\n",
       "\n",
       "---\n",
       "\n",
       "```\n",
       "fit(GeneralizedLinearModel, formula, data,\n",
       "    distr::UnivariateDistribution, link::Link = canonicallink(d); <keyword arguments>)\n",
       "fit(GeneralizedLinearModel, X::AbstractMatrix, y::AbstractVector,\n",
       "    distr::UnivariateDistribution, link::Link = canonicallink(d); <keyword arguments>)\n",
       "```\n",
       "\n",
       "Fit a generalized linear model to data.\n",
       "\n",
       "In the first method, `formula` must be a [StatsModels.jl `Formula` object](https://juliastats.org/StatsModels.jl/stable/formula/) and `data` a table (in the [Tables.jl](https://tables.juliadata.org/stable/) definition, e.g. a data frame). In the second method, `X` must be a matrix holding values of the independent variable(s) in columns (including if appropriate the intercept), and `y` must be a vector holding values of the dependent variable. In both cases, `distr` must specify the distribution, and `link` may specify the link function (if omitted, it is taken to be the canonical link for `distr`; see [`Link`](@ref) for a list of built-in links).\n",
       "\n",
       "# Keyword Arguments\n",
       "\n",
       "  * `dofit::Bool=true`: Determines whether model will be fit\n",
       "  * `wts::Vector=similar(y,0)`: Prior frequency (a.k.a. case) weights of observations. Such weights are equivalent to repeating each observation a number of times equal to its weight. Do note that this interpretation gives equal point estimates but different standard errors from analytical (a.k.a. inverse variance) weights and from probability (a.k.a. sampling) weights which are the default in some other software. Can be length 0 to indicate no weighting (default).\n",
       "  * `offset::Vector=similar(y,0)`: offset added to `Xβ` to form `eta`.  Can be of length 0\n",
       "  * `verbose::Bool=false`: Display convergence information for each iteration\n",
       "  * `maxiter::Integer=30`: Maximum number of iterations allowed to achieve convergence\n",
       "  * `atol::Real=1e-6`: Convergence is achieved when the relative change in deviance is less than `max(rtol*dev, atol)`.\n",
       "  * `rtol::Real=1e-6`: Convergence is achieved when the relative change in deviance is less than `max(rtol*dev, atol)`.\n",
       "  * `minstepfac::Real=0.001`: Minimum line step fraction. Must be between 0 and 1.\n",
       "  * `start::AbstractVector=nothing`: Starting values for beta. Should have the same length as the number of columns in the model matrix.\n"
      ],
      "text/plain": [
       "  Fit a statistical model.\n",
       "\n",
       "  ────────────────────────────────────────────────────────────────────────────\n",
       "\n",
       "\u001b[36m  fit(Histogram, data[, weight][, edges]; closed=:left, nbins)\u001b[39m\n",
       "\n",
       "  Fit a histogram to \u001b[36mdata\u001b[39m.\n",
       "\n",
       "\u001b[1m  Arguments\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "    •  \u001b[36mdata\u001b[39m: either a vector (for a 1-dimensional histogram), or a tuple\n",
       "       of vectors of equal length (for an \u001b[4mn\u001b[24m-dimensional histogram).\n",
       "\n",
       "    •  \u001b[36mweight\u001b[39m: an optional \u001b[36mAbstractWeights\u001b[39m (of the same length as the\n",
       "       data vectors), denoting the weight each observation contributes to\n",
       "       the bin. If no weight vector is supplied, each observation has\n",
       "       weight 1.\n",
       "\n",
       "    •  \u001b[36medges\u001b[39m: a vector (typically an \u001b[36mAbstractRange\u001b[39m object), or tuple of\n",
       "       vectors, that gives the edges of the bins along each dimension. If\n",
       "       no edges are provided, these are determined from the data.\n",
       "\n",
       "\u001b[1m  Keyword arguments\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "    •  \u001b[36mclosed\u001b[39m: if \u001b[36m:left\u001b[39m (the default), the bin intervals are left-closed\n",
       "       [a,b); if \u001b[36m:right\u001b[39m, intervals are right-closed (a,b].\n",
       "\n",
       "    •  \u001b[36mnbins\u001b[39m: if no \u001b[36medges\u001b[39m argument is supplied, the approximate number of\n",
       "       bins to use along each dimension (can be either a single integer,\n",
       "       or a tuple of integers).\n",
       "\n",
       "\u001b[1m  Examples\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "\u001b[36m  # Univariate\u001b[39m\n",
       "\u001b[36m  h = fit(Histogram, rand(100))\u001b[39m\n",
       "\u001b[36m  h = fit(Histogram, rand(100), 0:0.1:1.0)\u001b[39m\n",
       "\u001b[36m  h = fit(Histogram, rand(100), nbins=10)\u001b[39m\n",
       "\u001b[36m  h = fit(Histogram, rand(100), weights(rand(100)), 0:0.1:1.0)\u001b[39m\n",
       "\u001b[36m  h = fit(Histogram, [20], 0:20:100)\u001b[39m\n",
       "\u001b[36m  h = fit(Histogram, [20], 0:20:100, closed=:right)\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  # Multivariate\u001b[39m\n",
       "\u001b[36m  h = fit(Histogram, (rand(100),rand(100)))\u001b[39m\n",
       "\u001b[36m  h = fit(Histogram, (rand(100),rand(100)),nbins=10)\u001b[39m\n",
       "\n",
       "  ────────────────────────────────────────────────────────────────────────────\n",
       "\n",
       "\u001b[36m  fit(ZScoreTransform, X; dims=nothing, center=true, scale=true)\u001b[39m\n",
       "\n",
       "  Fit standardization parameters to vector or matrix \u001b[36mX\u001b[39m and return a\n",
       "  \u001b[36mZScoreTransform\u001b[39m transformation object.\n",
       "\n",
       "\u001b[1m  Keyword arguments\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "    •  \u001b[36mdims\u001b[39m: if \u001b[36m1\u001b[39m fit standardization parameters in column-wise fashion;\n",
       "       if \u001b[36m2\u001b[39m fit in row-wise fashion. The default is \u001b[36mnothing\u001b[39m, which is\n",
       "       equivalent to \u001b[36mdims=2\u001b[39m with a deprecation warning.\n",
       "\n",
       "    •  \u001b[36mcenter\u001b[39m: if \u001b[36mtrue\u001b[39m (the default) center data so that its mean is\n",
       "       zero.\n",
       "\n",
       "    •  \u001b[36mscale\u001b[39m: if \u001b[36mtrue\u001b[39m (the default) scale the data so that its variance\n",
       "       is equal to one.\n",
       "\n",
       "\u001b[1m  Examples\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "\u001b[36m  julia> using StatsBase\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> X = [0.0 -0.5 0.5; 0.0 1.0 2.0]\u001b[39m\n",
       "\u001b[36m  2×3 Matrix{Float64}:\u001b[39m\n",
       "\u001b[36m   0.0  -0.5  0.5\u001b[39m\n",
       "\u001b[36m   0.0   1.0  2.0\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> dt = fit(ZScoreTransform, X, dims=2)\u001b[39m\n",
       "\u001b[36m  ZScoreTransform{Float64, Vector{Float64}}(2, 2, [0.0, 1.0], [0.5, 1.0])\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> StatsBase.transform(dt, X)\u001b[39m\n",
       "\u001b[36m  2×3 Matrix{Float64}:\u001b[39m\n",
       "\u001b[36m    0.0  -1.0  1.0\u001b[39m\n",
       "\u001b[36m   -1.0   0.0  1.0\u001b[39m\n",
       "\n",
       "  ────────────────────────────────────────────────────────────────────────────\n",
       "\n",
       "\u001b[36m  fit(UnitRangeTransform, X; dims=nothing, unit=true)\u001b[39m\n",
       "\n",
       "  Fit a scaling parameters to vector or matrix \u001b[36mX\u001b[39m and return a\n",
       "  \u001b[36mUnitRangeTransform\u001b[39m transformation object.\n",
       "\n",
       "\u001b[1m  Keyword arguments\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "    •  \u001b[36mdims\u001b[39m: if \u001b[36m1\u001b[39m fit standardization parameters in column-wise fashion;\n",
       "\n",
       "  if \u001b[36m2\u001b[39m fit in row-wise fashion. The default is \u001b[36mnothing\u001b[39m.\n",
       "\n",
       "    •  \u001b[36munit\u001b[39m: if \u001b[36mtrue\u001b[39m (the default) shift the minimum data to zero.\n",
       "\n",
       "\u001b[1m  Examples\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "\u001b[36m  julia> using StatsBase\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> X = [0.0 -0.5 0.5; 0.0 1.0 2.0]\u001b[39m\n",
       "\u001b[36m  2×3 Matrix{Float64}:\u001b[39m\n",
       "\u001b[36m   0.0  -0.5  0.5\u001b[39m\n",
       "\u001b[36m   0.0   1.0  2.0\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> dt = fit(UnitRangeTransform, X, dims=2)\u001b[39m\n",
       "\u001b[36m  UnitRangeTransform{Float64, Vector{Float64}}(2, 2, true, [-0.5, 0.0], [1.0, 0.5])\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> StatsBase.transform(dt, X)\u001b[39m\n",
       "\u001b[36m  2×3 Matrix{Float64}:\u001b[39m\n",
       "\u001b[36m   0.5  0.0  1.0\u001b[39m\n",
       "\u001b[36m   0.0  0.5  1.0\u001b[39m\n",
       "\n",
       "  ────────────────────────────────────────────────────────────────────────────\n",
       "\n",
       "\u001b[36m  fit(::Type{<:Beta}, x::AbstractArray{T})\u001b[39m\n",
       "\n",
       "  fit a \u001b[36mBeta\u001b[39m distribution\n",
       "\n",
       "  ────────────────────────────────────────────────────────────────────────────\n",
       "\n",
       "\u001b[36m  fit(Mod::Type{<:StatisticalModel}, f::FormulaTerm, data, args...;\u001b[39m\n",
       "\u001b[36m      contrasts::Dict{Symbol}, kwargs...)\u001b[39m\n",
       "\n",
       "  Convert tabular data into a numeric response vector and predictor matrix\n",
       "  using the formula \u001b[36mf\u001b[39m, and then \u001b[36mfit\u001b[39m the specified model type, wrapping the\n",
       "  result in a \u001b[36mTableRegressionModel\u001b[39m or \u001b[36mTableStatisticalModel\u001b[39m (as appropriate).\n",
       "\n",
       "  This is intended as a backstop for modeling packages that implement model\n",
       "  types that are subtypes of \u001b[36mStatsBase.StatisticalModel\u001b[39m but do not explicitly\n",
       "  support the full StatsModels terms-based interface. Currently this works by\n",
       "  creating a \u001b[36mModelFrame\u001b[39m from the formula and data, and then converting this to\n",
       "  a \u001b[36mModelMatrix\u001b[39m, but this is an internal implementation detail which may\n",
       "  change in the near future.\n",
       "\n",
       "  ────────────────────────────────────────────────────────────────────────────\n",
       "\n",
       "\u001b[36m  fit(LinearModel, formula, data, allowrankdeficient=false;\u001b[39m\n",
       "\u001b[36m     [wts::AbstractVector], dropcollinear::Bool=true)\u001b[39m\n",
       "\u001b[36m  fit(LinearModel, X::AbstractMatrix, y::AbstractVector;\u001b[39m\n",
       "\u001b[36m      wts::AbstractVector=similar(y, 0), dropcollinear::Bool=true)\u001b[39m\n",
       "\n",
       "  Fit a linear model to data.\n",
       "\n",
       "  In the first method, \u001b[36mformula\u001b[39m must be a StatsModels.jl \u001b[36mFormula\u001b[39m object\n",
       "  (https://juliastats.org/StatsModels.jl/stable/formula/) and \u001b[36mdata\u001b[39m a table (in\n",
       "  the Tables.jl (https://tables.juliadata.org/stable/) definition, e.g. a data\n",
       "  frame). In the second method, \u001b[36mX\u001b[39m must be a matrix holding values of the\n",
       "  independent variable(s) in columns (including if appropriate the intercept),\n",
       "  and \u001b[36my\u001b[39m must be a vector holding values of the dependent variable.\n",
       "\n",
       "  The keyword argument \u001b[36mwts\u001b[39m can be a \u001b[36mVector\u001b[39m specifying frequency weights for\n",
       "  observations. Such weights are equivalent to repeating each observation a\n",
       "  number of times equal to its weight. Do note that this interpretation gives\n",
       "  equal point estimates but different standard errors from analytical (a.k.a.\n",
       "  inverse variance) weights and from probability (a.k.a. sampling) weights\n",
       "  which are the default in some other software.\n",
       "\n",
       "  \u001b[36mdropcollinear\u001b[39m controls whether or not \u001b[36mlm\u001b[39m accepts a model matrix which is\n",
       "  less-than-full rank. If \u001b[36mtrue\u001b[39m (the default), only the first of each set of\n",
       "  linearly-dependent columns is used. The coefficient for redundant linearly\n",
       "  dependent columns is \u001b[36m0.0\u001b[39m and all associated statistics are set to \u001b[36mNaN\u001b[39m.\n",
       "\n",
       "  ────────────────────────────────────────────────────────────────────────────\n",
       "\n",
       "\u001b[36m  fit(GeneralizedLinearModel, formula, data,\u001b[39m\n",
       "\u001b[36m      distr::UnivariateDistribution, link::Link = canonicallink(d); <keyword arguments>)\u001b[39m\n",
       "\u001b[36m  fit(GeneralizedLinearModel, X::AbstractMatrix, y::AbstractVector,\u001b[39m\n",
       "\u001b[36m      distr::UnivariateDistribution, link::Link = canonicallink(d); <keyword arguments>)\u001b[39m\n",
       "\n",
       "  Fit a generalized linear model to data.\n",
       "\n",
       "  In the first method, \u001b[36mformula\u001b[39m must be a StatsModels.jl \u001b[36mFormula\u001b[39m object\n",
       "  (https://juliastats.org/StatsModels.jl/stable/formula/) and \u001b[36mdata\u001b[39m a table (in\n",
       "  the Tables.jl (https://tables.juliadata.org/stable/) definition, e.g. a data\n",
       "  frame). In the second method, \u001b[36mX\u001b[39m must be a matrix holding values of the\n",
       "  independent variable(s) in columns (including if appropriate the intercept),\n",
       "  and \u001b[36my\u001b[39m must be a vector holding values of the dependent variable. In both\n",
       "  cases, \u001b[36mdistr\u001b[39m must specify the distribution, and \u001b[36mlink\u001b[39m may specify the link\n",
       "  function (if omitted, it is taken to be the canonical link for \u001b[36mdistr\u001b[39m; see\n",
       "  \u001b[36mLink\u001b[39m for a list of built-in links).\n",
       "\n",
       "\u001b[1m  Keyword Arguments\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "    •  \u001b[36mdofit::Bool=true\u001b[39m: Determines whether model will be fit\n",
       "\n",
       "    •  \u001b[36mwts::Vector=similar(y,0)\u001b[39m: Prior frequency (a.k.a. case) weights of\n",
       "       observations. Such weights are equivalent to repeating each\n",
       "       observation a number of times equal to its weight. Do note that\n",
       "       this interpretation gives equal point estimates but different\n",
       "       standard errors from analytical (a.k.a. inverse variance) weights\n",
       "       and from probability (a.k.a. sampling) weights which are the\n",
       "       default in some other software. Can be length 0 to indicate no\n",
       "       weighting (default).\n",
       "\n",
       "    •  \u001b[36moffset::Vector=similar(y,0)\u001b[39m: offset added to \u001b[36mXβ\u001b[39m to form \u001b[36meta\u001b[39m. Can\n",
       "       be of length 0\n",
       "\n",
       "    •  \u001b[36mverbose::Bool=false\u001b[39m: Display convergence information for each\n",
       "       iteration\n",
       "\n",
       "    •  \u001b[36mmaxiter::Integer=30\u001b[39m: Maximum number of iterations allowed to\n",
       "       achieve convergence\n",
       "\n",
       "    •  \u001b[36matol::Real=1e-6\u001b[39m: Convergence is achieved when the relative change\n",
       "       in deviance is less than \u001b[36mmax(rtol*dev, atol)\u001b[39m.\n",
       "\n",
       "    •  \u001b[36mrtol::Real=1e-6\u001b[39m: Convergence is achieved when the relative change\n",
       "       in deviance is less than \u001b[36mmax(rtol*dev, atol)\u001b[39m.\n",
       "\n",
       "    •  \u001b[36mminstepfac::Real=0.001\u001b[39m: Minimum line step fraction. Must be\n",
       "       between 0 and 1.\n",
       "\n",
       "    •  \u001b[36mstart::AbstractVector=nothing\u001b[39m: Starting values for beta. Should\n",
       "       have the same length as the number of columns in the model matrix."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T14:53:31.012000+08:00",
     "start_time": "2022-06-08T06:53:24.665Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1ml\u001b[22m\u001b[0m\u001b[1mm\u001b[22m g\u001b[0m\u001b[1ml\u001b[22m\u001b[0m\u001b[1mm\u001b[22m G\u001b[0m\u001b[1mL\u001b[22m\u001b[0m\u001b[1mM\u001b[22m He\u001b[0m\u001b[1ml\u001b[22m\u001b[0m\u001b[1mm\u001b[22mertCoding mode\u001b[0m\u001b[1ml\u001b[22m\u001b[0m\u001b[1mm\u001b[22matrix Mode\u001b[0m\u001b[1ml\u001b[22m\u001b[0m\u001b[1mM\u001b[22matrix \u001b[0m\u001b[1ml\u001b[22mc\u001b[0m\u001b[1mm\u001b[22m c\u001b[0m\u001b[1ml\u001b[22ma\u001b[0m\u001b[1mm\u001b[22mp f\u001b[0m\u001b[1ml\u001b[22md\u001b[0m\u001b[1mm\u001b[22mod c\u001b[0m\u001b[1ml\u001b[22ma\u001b[0m\u001b[1mm\u001b[22mp!\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "lm(formula, data, allowrankdeficient=false;\n",
       "   [wts::AbstractVector], dropcollinear::Bool=true)\n",
       "lm(X::AbstractMatrix, y::AbstractVector;\n",
       "   wts::AbstractVector=similar(y, 0), dropcollinear::Bool=true)\n",
       "\\end{verbatim}\n",
       "Fit a linear model to data. An alias for \\texttt{fit(LinearModel, X, y; wts=wts, dropcollinear=dropcollinear)}\n",
       "\n",
       "In the first method, \\texttt{formula} must be a \\href{https://juliastats.org/StatsModels.jl/stable/formula/}{StatsModels.jl \\texttt{Formula} object} and \\texttt{data} a table (in the \\href{https://tables.juliadata.org/stable/}{Tables.jl} definition, e.g. a data frame). In the second method, \\texttt{X} must be a matrix holding values of the independent variable(s) in columns (including if appropriate the intercept), and \\texttt{y} must be a vector holding values of the dependent variable.\n",
       "\n",
       "The keyword argument \\texttt{wts} can be a \\texttt{Vector} specifying frequency weights for observations. Such weights are equivalent to repeating each observation a number of times equal to its weight. Do note that this interpretation gives equal point estimates but different standard errors from analytical (a.k.a. inverse variance) weights and from probability (a.k.a. sampling) weights which are the default in some other software.\n",
       "\n",
       "\\texttt{dropcollinear} controls whether or not \\texttt{lm} accepts a model matrix which is less-than-full rank. If \\texttt{true} (the default), only the first of each set of linearly-dependent columns is used. The coefficient for redundant linearly dependent columns is \\texttt{0.0} and all associated statistics are set to \\texttt{NaN}.\n",
       "\n"
      ],
      "text/markdown": [
       "```\n",
       "lm(formula, data, allowrankdeficient=false;\n",
       "   [wts::AbstractVector], dropcollinear::Bool=true)\n",
       "lm(X::AbstractMatrix, y::AbstractVector;\n",
       "   wts::AbstractVector=similar(y, 0), dropcollinear::Bool=true)\n",
       "```\n",
       "\n",
       "Fit a linear model to data. An alias for `fit(LinearModel, X, y; wts=wts, dropcollinear=dropcollinear)`\n",
       "\n",
       "In the first method, `formula` must be a [StatsModels.jl `Formula` object](https://juliastats.org/StatsModels.jl/stable/formula/) and `data` a table (in the [Tables.jl](https://tables.juliadata.org/stable/) definition, e.g. a data frame). In the second method, `X` must be a matrix holding values of the independent variable(s) in columns (including if appropriate the intercept), and `y` must be a vector holding values of the dependent variable.\n",
       "\n",
       "The keyword argument `wts` can be a `Vector` specifying frequency weights for observations. Such weights are equivalent to repeating each observation a number of times equal to its weight. Do note that this interpretation gives equal point estimates but different standard errors from analytical (a.k.a. inverse variance) weights and from probability (a.k.a. sampling) weights which are the default in some other software.\n",
       "\n",
       "`dropcollinear` controls whether or not `lm` accepts a model matrix which is less-than-full rank. If `true` (the default), only the first of each set of linearly-dependent columns is used. The coefficient for redundant linearly dependent columns is `0.0` and all associated statistics are set to `NaN`.\n"
      ],
      "text/plain": [
       "\u001b[36m  lm(formula, data, allowrankdeficient=false;\u001b[39m\n",
       "\u001b[36m     [wts::AbstractVector], dropcollinear::Bool=true)\u001b[39m\n",
       "\u001b[36m  lm(X::AbstractMatrix, y::AbstractVector;\u001b[39m\n",
       "\u001b[36m     wts::AbstractVector=similar(y, 0), dropcollinear::Bool=true)\u001b[39m\n",
       "\n",
       "  Fit a linear model to data. An alias for \u001b[36mfit(LinearModel, X, y; wts=wts,\n",
       "  dropcollinear=dropcollinear)\u001b[39m\n",
       "\n",
       "  In the first method, \u001b[36mformula\u001b[39m must be a StatsModels.jl \u001b[36mFormula\u001b[39m object\n",
       "  (https://juliastats.org/StatsModels.jl/stable/formula/) and \u001b[36mdata\u001b[39m a table (in\n",
       "  the Tables.jl (https://tables.juliadata.org/stable/) definition, e.g. a data\n",
       "  frame). In the second method, \u001b[36mX\u001b[39m must be a matrix holding values of the\n",
       "  independent variable(s) in columns (including if appropriate the intercept),\n",
       "  and \u001b[36my\u001b[39m must be a vector holding values of the dependent variable.\n",
       "\n",
       "  The keyword argument \u001b[36mwts\u001b[39m can be a \u001b[36mVector\u001b[39m specifying frequency weights for\n",
       "  observations. Such weights are equivalent to repeating each observation a\n",
       "  number of times equal to its weight. Do note that this interpretation gives\n",
       "  equal point estimates but different standard errors from analytical (a.k.a.\n",
       "  inverse variance) weights and from probability (a.k.a. sampling) weights\n",
       "  which are the default in some other software.\n",
       "\n",
       "  \u001b[36mdropcollinear\u001b[39m controls whether or not \u001b[36mlm\u001b[39m accepts a model matrix which is\n",
       "  less-than-full rank. If \u001b[36mtrue\u001b[39m (the default), only the first of each set of\n",
       "  linearly-dependent columns is used. The coefficient for redundant linearly\n",
       "  dependent columns is \u001b[36m0.0\u001b[39m and all associated statistics are set to \u001b[36mNaN\u001b[39m."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T14:53:31.029000+08:00",
     "start_time": "2022-06-08T06:53:24.666Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1mg\u001b[22m\u001b[0m\u001b[1ml\u001b[22m\u001b[0m\u001b[1mm\u001b[22m \u001b[0m\u001b[1mG\u001b[22m\u001b[0m\u001b[1mL\u001b[22m\u001b[0m\u001b[1mM\u001b[22m Cate\u001b[0m\u001b[1mg\u001b[22morica\u001b[0m\u001b[1ml\u001b[22mTer\u001b[0m\u001b[1mm\u001b[22m \u001b[0m\u001b[1mG\u001b[22menera\u001b[0m\u001b[1ml\u001b[22mizedLinear\u001b[0m\u001b[1mM\u001b[22model\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "glm(formula, data,\n",
       "    distr::UnivariateDistribution, link::Link = canonicallink(d); <keyword arguments>)\n",
       "glm(X::AbstractMatrix, y::AbstractVector,\n",
       "    distr::UnivariateDistribution, link::Link = canonicallink(d); <keyword arguments>)\n",
       "\\end{verbatim}\n",
       "Fit a generalized linear model to data. Alias for \\texttt{fit(GeneralizedLinearModel, ...)}.\n",
       "\n",
       "In the first method, \\texttt{formula} must be a \\href{https://juliastats.org/StatsModels.jl/stable/formula/}{StatsModels.jl \\texttt{Formula} object} and \\texttt{data} a table (in the \\href{https://tables.juliadata.org/stable/}{Tables.jl} definition, e.g. a data frame). In the second method, \\texttt{X} must be a matrix holding values of the independent variable(s) in columns (including if appropriate the intercept), and \\texttt{y} must be a vector holding values of the dependent variable. In both cases, \\texttt{distr} must specify the distribution, and \\texttt{link} may specify the link function (if omitted, it is taken to be the canonical link for \\texttt{distr}; see \\href{@ref}{\\texttt{Link}} for a list of built-in links).\n",
       "\n",
       "\\section{Keyword Arguments}\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{dofit::Bool=true}: Determines whether model will be fit\n",
       "\n",
       "\n",
       "\\item \\texttt{wts::Vector=similar(y,0)}: Prior frequency (a.k.a. case) weights of observations. Such weights are equivalent to repeating each observation a number of times equal to its weight. Do note that this interpretation gives equal point estimates but different standard errors from analytical (a.k.a. inverse variance) weights and from probability (a.k.a. sampling) weights which are the default in some other software. Can be length 0 to indicate no weighting (default).\n",
       "\n",
       "\n",
       "\\item \\texttt{offset::Vector=similar(y,0)}: offset added to \\texttt{Xβ} to form \\texttt{eta}.  Can be of length 0\n",
       "\n",
       "\n",
       "\\item \\texttt{verbose::Bool=false}: Display convergence information for each iteration\n",
       "\n",
       "\n",
       "\\item \\texttt{maxiter::Integer=30}: Maximum number of iterations allowed to achieve convergence\n",
       "\n",
       "\n",
       "\\item \\texttt{atol::Real=1e-6}: Convergence is achieved when the relative change in deviance is less than \\texttt{max(rtol*dev, atol)}.\n",
       "\n",
       "\n",
       "\\item \\texttt{rtol::Real=1e-6}: Convergence is achieved when the relative change in deviance is less than \\texttt{max(rtol*dev, atol)}.\n",
       "\n",
       "\n",
       "\\item \\texttt{minstepfac::Real=0.001}: Minimum line step fraction. Must be between 0 and 1.\n",
       "\n",
       "\n",
       "\\item \\texttt{start::AbstractVector=nothing}: Starting values for beta. Should have the same length as the number of columns in the model matrix.\n",
       "\n",
       "\\end{itemize}\n"
      ],
      "text/markdown": [
       "```\n",
       "glm(formula, data,\n",
       "    distr::UnivariateDistribution, link::Link = canonicallink(d); <keyword arguments>)\n",
       "glm(X::AbstractMatrix, y::AbstractVector,\n",
       "    distr::UnivariateDistribution, link::Link = canonicallink(d); <keyword arguments>)\n",
       "```\n",
       "\n",
       "Fit a generalized linear model to data. Alias for `fit(GeneralizedLinearModel, ...)`.\n",
       "\n",
       "In the first method, `formula` must be a [StatsModels.jl `Formula` object](https://juliastats.org/StatsModels.jl/stable/formula/) and `data` a table (in the [Tables.jl](https://tables.juliadata.org/stable/) definition, e.g. a data frame). In the second method, `X` must be a matrix holding values of the independent variable(s) in columns (including if appropriate the intercept), and `y` must be a vector holding values of the dependent variable. In both cases, `distr` must specify the distribution, and `link` may specify the link function (if omitted, it is taken to be the canonical link for `distr`; see [`Link`](@ref) for a list of built-in links).\n",
       "\n",
       "# Keyword Arguments\n",
       "\n",
       "  * `dofit::Bool=true`: Determines whether model will be fit\n",
       "  * `wts::Vector=similar(y,0)`: Prior frequency (a.k.a. case) weights of observations. Such weights are equivalent to repeating each observation a number of times equal to its weight. Do note that this interpretation gives equal point estimates but different standard errors from analytical (a.k.a. inverse variance) weights and from probability (a.k.a. sampling) weights which are the default in some other software. Can be length 0 to indicate no weighting (default).\n",
       "  * `offset::Vector=similar(y,0)`: offset added to `Xβ` to form `eta`.  Can be of length 0\n",
       "  * `verbose::Bool=false`: Display convergence information for each iteration\n",
       "  * `maxiter::Integer=30`: Maximum number of iterations allowed to achieve convergence\n",
       "  * `atol::Real=1e-6`: Convergence is achieved when the relative change in deviance is less than `max(rtol*dev, atol)`.\n",
       "  * `rtol::Real=1e-6`: Convergence is achieved when the relative change in deviance is less than `max(rtol*dev, atol)`.\n",
       "  * `minstepfac::Real=0.001`: Minimum line step fraction. Must be between 0 and 1.\n",
       "  * `start::AbstractVector=nothing`: Starting values for beta. Should have the same length as the number of columns in the model matrix.\n"
      ],
      "text/plain": [
       "\u001b[36m  glm(formula, data,\u001b[39m\n",
       "\u001b[36m      distr::UnivariateDistribution, link::Link = canonicallink(d); <keyword arguments>)\u001b[39m\n",
       "\u001b[36m  glm(X::AbstractMatrix, y::AbstractVector,\u001b[39m\n",
       "\u001b[36m      distr::UnivariateDistribution, link::Link = canonicallink(d); <keyword arguments>)\u001b[39m\n",
       "\n",
       "  Fit a generalized linear model to data. Alias for\n",
       "  \u001b[36mfit(GeneralizedLinearModel, ...)\u001b[39m.\n",
       "\n",
       "  In the first method, \u001b[36mformula\u001b[39m must be a StatsModels.jl \u001b[36mFormula\u001b[39m object\n",
       "  (https://juliastats.org/StatsModels.jl/stable/formula/) and \u001b[36mdata\u001b[39m a table (in\n",
       "  the Tables.jl (https://tables.juliadata.org/stable/) definition, e.g. a data\n",
       "  frame). In the second method, \u001b[36mX\u001b[39m must be a matrix holding values of the\n",
       "  independent variable(s) in columns (including if appropriate the intercept),\n",
       "  and \u001b[36my\u001b[39m must be a vector holding values of the dependent variable. In both\n",
       "  cases, \u001b[36mdistr\u001b[39m must specify the distribution, and \u001b[36mlink\u001b[39m may specify the link\n",
       "  function (if omitted, it is taken to be the canonical link for \u001b[36mdistr\u001b[39m; see\n",
       "  \u001b[36mLink\u001b[39m for a list of built-in links).\n",
       "\n",
       "\u001b[1m  Keyword Arguments\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "    •  \u001b[36mdofit::Bool=true\u001b[39m: Determines whether model will be fit\n",
       "\n",
       "    •  \u001b[36mwts::Vector=similar(y,0)\u001b[39m: Prior frequency (a.k.a. case) weights of\n",
       "       observations. Such weights are equivalent to repeating each\n",
       "       observation a number of times equal to its weight. Do note that\n",
       "       this interpretation gives equal point estimates but different\n",
       "       standard errors from analytical (a.k.a. inverse variance) weights\n",
       "       and from probability (a.k.a. sampling) weights which are the\n",
       "       default in some other software. Can be length 0 to indicate no\n",
       "       weighting (default).\n",
       "\n",
       "    •  \u001b[36moffset::Vector=similar(y,0)\u001b[39m: offset added to \u001b[36mXβ\u001b[39m to form \u001b[36meta\u001b[39m. Can\n",
       "       be of length 0\n",
       "\n",
       "    •  \u001b[36mverbose::Bool=false\u001b[39m: Display convergence information for each\n",
       "       iteration\n",
       "\n",
       "    •  \u001b[36mmaxiter::Integer=30\u001b[39m: Maximum number of iterations allowed to\n",
       "       achieve convergence\n",
       "\n",
       "    •  \u001b[36matol::Real=1e-6\u001b[39m: Convergence is achieved when the relative change\n",
       "       in deviance is less than \u001b[36mmax(rtol*dev, atol)\u001b[39m.\n",
       "\n",
       "    •  \u001b[36mrtol::Real=1e-6\u001b[39m: Convergence is achieved when the relative change\n",
       "       in deviance is less than \u001b[36mmax(rtol*dev, atol)\u001b[39m.\n",
       "\n",
       "    •  \u001b[36mminstepfac::Real=0.001\u001b[39m: Minimum line step fraction. Must be\n",
       "       between 0 and 1.\n",
       "\n",
       "    •  \u001b[36mstart::AbstractVector=nothing\u001b[39m: Starting values for beta. Should\n",
       "       have the same length as the number of columns in the model matrix."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?glm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T14:53:31.030000+08:00",
     "start_time": "2022-06-08T06:53:24.668Z"
    }
   },
   "outputs": [],
   "source": [
    "using Random #随机数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T14:53:31.055000+08:00",
     "start_time": "2022-06-08T06:53:24.670Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1mR\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1md\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mm\u001b[22m \u001b[0m\u001b[1mR\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1md\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mm\u001b[22mDevice\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "Random\n",
       "\\end{verbatim}\n",
       "Support for generating random numbers. Provides \\href{@ref}{\\texttt{rand}}, \\href{@ref}{\\texttt{randn}}, \\href{@ref}{\\texttt{AbstractRNG}}, \\href{@ref}{\\texttt{MersenneTwister}}, and \\href{@ref}{\\texttt{RandomDevice}}.\n",
       "\n"
      ],
      "text/markdown": [
       "```\n",
       "Random\n",
       "```\n",
       "\n",
       "Support for generating random numbers. Provides [`rand`](@ref), [`randn`](@ref), [`AbstractRNG`](@ref), [`MersenneTwister`](@ref), and [`RandomDevice`](@ref).\n"
      ],
      "text/plain": [
       "\u001b[36m  Random\u001b[39m\n",
       "\n",
       "  Support for generating random numbers. Provides \u001b[36mrand\u001b[39m, \u001b[36mrandn\u001b[39m, \u001b[36mAbstractRNG\u001b[39m,\n",
       "  \u001b[36mMersenneTwister\u001b[39m, and \u001b[36mRandomDevice\u001b[39m."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T14:53:31.312000+08:00",
     "start_time": "2022-06-08T06:53:24.671Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1mr\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1md\u001b[22m \u001b[0m\u001b[1mr\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1md\u001b[22mn \u001b[0m\u001b[1mr\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1md\u001b[22m! \u001b[0m\u001b[1mr\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1md\u001b[22mn! \u001b[0m\u001b[1mr\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1md\u001b[22mexp \u001b[0m\u001b[1mR\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1md\u001b[22mom \u001b[0m\u001b[1mr\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1md\u001b[22mperm \u001b[0m\u001b[1mr\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1md\u001b[22mexp! \u001b[0m\u001b[1mr\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1md\u001b[22mperm!\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "rand([rng=GLOBAL_RNG], [S], [dims...])\n",
       "\\end{verbatim}\n",
       "Pick a random element or array of random elements from the set of values specified by \\texttt{S}; \\texttt{S} can be\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item an indexable collection (for example \\texttt{1:9} or \\texttt{('x', \"y\", :z)}),\n",
       "\n",
       "\n",
       "\\item an \\texttt{AbstractDict} or \\texttt{AbstractSet} object,\n",
       "\n",
       "\n",
       "\\item a string (considered as a collection of characters), or\n",
       "\n",
       "\n",
       "\\item a type: the set of values to pick from is then equivalent to \\texttt{typemin(S):typemax(S)} for integers (this is not applicable to \\href{@ref}{\\texttt{BigInt}}), to $[0, 1)$ for floating point numbers and to $[0, 1)+i[0, 1)$ for complex floating point numbers;\n",
       "\n",
       "\\end{itemize}\n",
       "\\texttt{S} defaults to \\href{@ref}{\\texttt{Float64}}. When only one argument is passed besides the optional \\texttt{rng} and is a \\texttt{Tuple}, it is interpreted as a collection of values (\\texttt{S}) and not as \\texttt{dims}.\n",
       "\n",
       "\\begin{quote}\n",
       "\\textbf{compat}\n",
       "\n",
       "Julia 1.1\n",
       "\n",
       "Support for \\texttt{S} as a tuple requires at least Julia 1.1.\n",
       "\n",
       "\\end{quote}\n",
       "\\section{Examples}\n",
       "\\begin{verbatim}\n",
       "julia> rand(Int, 2)\n",
       "2-element Array{Int64,1}:\n",
       " 1339893410598768192\n",
       " 1575814717733606317\n",
       "\n",
       "julia> using Random\n",
       "\n",
       "julia> rand(MersenneTwister(0), Dict(1=>2, 3=>4))\n",
       "1=>2\n",
       "\n",
       "julia> rand((2, 3))\n",
       "3\n",
       "\n",
       "julia> rand(Float64, (2, 3))\n",
       "2×3 Array{Float64,2}:\n",
       " 0.999717  0.0143835  0.540787\n",
       " 0.696556  0.783855   0.938235\n",
       "\\end{verbatim}\n",
       "\\begin{quote}\n",
       "\\textbf{note}\n",
       "\n",
       "Note\n",
       "\n",
       "The complexity of \\texttt{rand(rng, s::Union\\{AbstractDict,AbstractSet\\})} is linear in the length of \\texttt{s}, unless an optimized method with constant complexity is available, which is the case for \\texttt{Dict}, \\texttt{Set} and \\texttt{BitSet}. For more than a few calls, use \\texttt{rand(rng, collect(s))} instead, or either \\texttt{rand(rng, Dict(s))} or \\texttt{rand(rng, Set(s))} as appropriate.\n",
       "\n",
       "\\end{quote}\n",
       "\\rule{\\textwidth}{1pt}\n",
       "\\begin{verbatim}\n",
       "rand([rng::AbstractRNG,] s::Sampleable)\n",
       "\\end{verbatim}\n",
       "Generate one sample for \\texttt{s}.\n",
       "\n",
       "\\begin{verbatim}\n",
       "rand([rng::AbstractRNG,] s::Sampleable, n::Int)\n",
       "\\end{verbatim}\n",
       "Generate \\texttt{n} samples from \\texttt{s}. The form of the returned object depends on the variate form of \\texttt{s}:\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item When \\texttt{s} is univariate, it returns a vector of length \\texttt{n}.\n",
       "\n",
       "\n",
       "\\item When \\texttt{s} is multivariate, it returns a matrix with \\texttt{n} columns.\n",
       "\n",
       "\n",
       "\\item When \\texttt{s} is matrix-variate, it returns an array, where each element is a sample matrix.\n",
       "\n",
       "rand([rng::AbstractRNG,] s::Sampleable, dim1::Int, dim2::Int...)   rand([rng::AbstractRNG,] s::Sampleable, dims::Dims)\n",
       "\n",
       "\\end{itemize}\n",
       "Generate an array of samples from \\texttt{s} whose shape is determined by the given dimensions.\n",
       "\n",
       "\\rule{\\textwidth}{1pt}\n",
       "\\begin{verbatim}\n",
       "rand(rng::AbstractRNG, d::UnivariateDistribution)\n",
       "\\end{verbatim}\n",
       "Generate a scalar sample from \\texttt{d}. The general fallback is \\texttt{quantile(d, rand())}.\n",
       "\n",
       "\\rule{\\textwidth}{1pt}\n",
       "\\begin{verbatim}\n",
       "rand(rng, d)\n",
       "\\end{verbatim}\n",
       "Extract a sample from the p-Generalized Gaussian distribution 'd'. The sampling procedure is implemented from from [1]. [1]  Gonzalez-Farias, G., Molina, J. A. D., \\& Rodríguez-Dagnino, R. M. (2009). Efficiency of the approximated shape parameter estimator in the generalized Gaussian distribution. IEEE Transactions on Vehicular Technology, 58(8), 4214-4223.\n",
       "\n",
       "\\rule{\\textwidth}{1pt}\n",
       "\\begin{verbatim}\n",
       "rand(::AbstractRNG, ::Distributions.AbstractMvNormal)\n",
       "\\end{verbatim}\n",
       "Sample a random vector from the provided multi-variate normal distribution.\n",
       "\n",
       "\\rule{\\textwidth}{1pt}\n",
       "\\begin{verbatim}\n",
       "rand(::AbstractRNG, ::Sampleable)\n",
       "\\end{verbatim}\n",
       "Samples from the sampler and returns the result.\n",
       "\n",
       "\\rule{\\textwidth}{1pt}\n",
       "\\begin{verbatim}\n",
       "rand(d::Union{UnivariateMixture, MultivariateMixture})\n",
       "\\end{verbatim}\n",
       "Draw a sample from the mixture model \\texttt{d}.\n",
       "\n",
       "\\begin{verbatim}\n",
       "rand(d::Union{UnivariateMixture, MultivariateMixture}, n)\n",
       "\\end{verbatim}\n",
       "Draw \\texttt{n} samples from \\texttt{d}.\n",
       "\n"
      ],
      "text/markdown": [
       "```\n",
       "rand([rng=GLOBAL_RNG], [S], [dims...])\n",
       "```\n",
       "\n",
       "Pick a random element or array of random elements from the set of values specified by `S`; `S` can be\n",
       "\n",
       "  * an indexable collection (for example `1:9` or `('x', \"y\", :z)`),\n",
       "  * an `AbstractDict` or `AbstractSet` object,\n",
       "  * a string (considered as a collection of characters), or\n",
       "  * a type: the set of values to pick from is then equivalent to `typemin(S):typemax(S)` for integers (this is not applicable to [`BigInt`](@ref)), to $[0, 1)$ for floating point numbers and to $[0, 1)+i[0, 1)$ for complex floating point numbers;\n",
       "\n",
       "`S` defaults to [`Float64`](@ref). When only one argument is passed besides the optional `rng` and is a `Tuple`, it is interpreted as a collection of values (`S`) and not as `dims`.\n",
       "\n",
       "!!! compat \"Julia 1.1\"\n",
       "    Support for `S` as a tuple requires at least Julia 1.1.\n",
       "\n",
       "\n",
       "# Examples\n",
       "\n",
       "```julia-repl\n",
       "julia> rand(Int, 2)\n",
       "2-element Array{Int64,1}:\n",
       " 1339893410598768192\n",
       " 1575814717733606317\n",
       "\n",
       "julia> using Random\n",
       "\n",
       "julia> rand(MersenneTwister(0), Dict(1=>2, 3=>4))\n",
       "1=>2\n",
       "\n",
       "julia> rand((2, 3))\n",
       "3\n",
       "\n",
       "julia> rand(Float64, (2, 3))\n",
       "2×3 Array{Float64,2}:\n",
       " 0.999717  0.0143835  0.540787\n",
       " 0.696556  0.783855   0.938235\n",
       "```\n",
       "\n",
       "!!! note\n",
       "    The complexity of `rand(rng, s::Union{AbstractDict,AbstractSet})` is linear in the length of `s`, unless an optimized method with constant complexity is available, which is the case for `Dict`, `Set` and `BitSet`. For more than a few calls, use `rand(rng, collect(s))` instead, or either `rand(rng, Dict(s))` or `rand(rng, Set(s))` as appropriate.\n",
       "\n",
       "\n",
       "---\n",
       "\n",
       "```\n",
       "rand([rng::AbstractRNG,] s::Sampleable)\n",
       "```\n",
       "\n",
       "Generate one sample for `s`.\n",
       "\n",
       "```\n",
       "rand([rng::AbstractRNG,] s::Sampleable, n::Int)\n",
       "```\n",
       "\n",
       "Generate `n` samples from `s`. The form of the returned object depends on the variate form of `s`:\n",
       "\n",
       "  * When `s` is univariate, it returns a vector of length `n`.\n",
       "  * When `s` is multivariate, it returns a matrix with `n` columns.\n",
       "  * When `s` is matrix-variate, it returns an array, where each element is a sample matrix.\n",
       "\n",
       "    rand([rng::AbstractRNG,] s::Sampleable, dim1::Int, dim2::Int...)   rand([rng::AbstractRNG,] s::Sampleable, dims::Dims)\n",
       "\n",
       "Generate an array of samples from `s` whose shape is determined by the given dimensions.\n",
       "\n",
       "---\n",
       "\n",
       "```\n",
       "rand(rng::AbstractRNG, d::UnivariateDistribution)\n",
       "```\n",
       "\n",
       "Generate a scalar sample from `d`. The general fallback is `quantile(d, rand())`.\n",
       "\n",
       "---\n",
       "\n",
       "```\n",
       "rand(rng, d)\n",
       "```\n",
       "\n",
       "Extract a sample from the p-Generalized Gaussian distribution 'd'. The sampling procedure is implemented from from [1]. [1]  Gonzalez-Farias, G., Molina, J. A. D., & Rodríguez-Dagnino, R. M. (2009). Efficiency of the approximated shape parameter estimator in the generalized Gaussian distribution. IEEE Transactions on Vehicular Technology, 58(8), 4214-4223.\n",
       "\n",
       "---\n",
       "\n",
       "```\n",
       "rand(::AbstractRNG, ::Distributions.AbstractMvNormal)\n",
       "```\n",
       "\n",
       "Sample a random vector from the provided multi-variate normal distribution.\n",
       "\n",
       "---\n",
       "\n",
       "```\n",
       "rand(::AbstractRNG, ::Sampleable)\n",
       "```\n",
       "\n",
       "Samples from the sampler and returns the result.\n",
       "\n",
       "---\n",
       "\n",
       "```\n",
       "rand(d::Union{UnivariateMixture, MultivariateMixture})\n",
       "```\n",
       "\n",
       "Draw a sample from the mixture model `d`.\n",
       "\n",
       "```\n",
       "rand(d::Union{UnivariateMixture, MultivariateMixture}, n)\n",
       "```\n",
       "\n",
       "Draw `n` samples from `d`.\n"
      ],
      "text/plain": [
       "\u001b[36m  rand([rng=GLOBAL_RNG], [S], [dims...])\u001b[39m\n",
       "\n",
       "  Pick a random element or array of random elements from the set of values\n",
       "  specified by \u001b[36mS\u001b[39m; \u001b[36mS\u001b[39m can be\n",
       "\n",
       "    •  an indexable collection (for example \u001b[36m1:9\u001b[39m or \u001b[36m('x', \"y\", :z)\u001b[39m),\n",
       "\n",
       "    •  an \u001b[36mAbstractDict\u001b[39m or \u001b[36mAbstractSet\u001b[39m object,\n",
       "\n",
       "    •  a string (considered as a collection of characters), or\n",
       "\n",
       "    •  a type: the set of values to pick from is then equivalent to\n",
       "       \u001b[36mtypemin(S):typemax(S)\u001b[39m for integers (this is not applicable to\n",
       "       \u001b[36mBigInt\u001b[39m), to \u001b[35m[0, 1)\u001b[39m for floating point numbers and to \u001b[35m[0, 1)+i[0,\n",
       "       1)\u001b[39m for complex floating point numbers;\n",
       "\n",
       "  \u001b[36mS\u001b[39m defaults to \u001b[36mFloat64\u001b[39m. When only one argument is passed besides the optional\n",
       "  \u001b[36mrng\u001b[39m and is a \u001b[36mTuple\u001b[39m, it is interpreted as a collection of values (\u001b[36mS\u001b[39m) and not\n",
       "  as \u001b[36mdims\u001b[39m.\n",
       "\n",
       "\u001b[39m\u001b[1m  │ \u001b[22m\u001b[39m\u001b[1mJulia 1.1\u001b[22m\n",
       "\u001b[39m\u001b[1m  │\u001b[22m\n",
       "\u001b[39m\u001b[1m  │\u001b[22m  Support for \u001b[36mS\u001b[39m as a tuple requires at least Julia 1.1.\n",
       "\n",
       "\u001b[1m  Examples\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "\u001b[36m  julia> rand(Int, 2)\u001b[39m\n",
       "\u001b[36m  2-element Array{Int64,1}:\u001b[39m\n",
       "\u001b[36m   1339893410598768192\u001b[39m\n",
       "\u001b[36m   1575814717733606317\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> using Random\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> rand(MersenneTwister(0), Dict(1=>2, 3=>4))\u001b[39m\n",
       "\u001b[36m  1=>2\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> rand((2, 3))\u001b[39m\n",
       "\u001b[36m  3\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> rand(Float64, (2, 3))\u001b[39m\n",
       "\u001b[36m  2×3 Array{Float64,2}:\u001b[39m\n",
       "\u001b[36m   0.999717  0.0143835  0.540787\u001b[39m\n",
       "\u001b[36m   0.696556  0.783855   0.938235\u001b[39m\n",
       "\n",
       "\u001b[36m\u001b[1m  │ \u001b[22m\u001b[39m\u001b[36m\u001b[1mNote\u001b[22m\u001b[39m\n",
       "\u001b[36m\u001b[1m  │\u001b[22m\u001b[39m\n",
       "\u001b[36m\u001b[1m  │\u001b[22m\u001b[39m  The complexity of \u001b[36mrand(rng, s::Union{AbstractDict,AbstractSet})\u001b[39m is\n",
       "\u001b[36m\u001b[1m  │\u001b[22m\u001b[39m  linear in the length of \u001b[36ms\u001b[39m, unless an optimized method with\n",
       "\u001b[36m\u001b[1m  │\u001b[22m\u001b[39m  constant complexity is available, which is the case for \u001b[36mDict\u001b[39m, \u001b[36mSet\u001b[39m\n",
       "\u001b[36m\u001b[1m  │\u001b[22m\u001b[39m  and \u001b[36mBitSet\u001b[39m. For more than a few calls, use \u001b[36mrand(rng, collect(s))\u001b[39m\n",
       "\u001b[36m\u001b[1m  │\u001b[22m\u001b[39m  instead, or either \u001b[36mrand(rng, Dict(s))\u001b[39m or \u001b[36mrand(rng, Set(s))\u001b[39m as\n",
       "\u001b[36m\u001b[1m  │\u001b[22m\u001b[39m  appropriate.\n",
       "\n",
       "  ────────────────────────────────────────────────────────────────────────────\n",
       "\n",
       "\u001b[36m  rand([rng::AbstractRNG,] s::Sampleable)\u001b[39m\n",
       "\n",
       "  Generate one sample for \u001b[36ms\u001b[39m.\n",
       "\n",
       "\u001b[36m  rand([rng::AbstractRNG,] s::Sampleable, n::Int)\u001b[39m\n",
       "\n",
       "  Generate \u001b[36mn\u001b[39m samples from \u001b[36ms\u001b[39m. The form of the returned object depends on the\n",
       "  variate form of \u001b[36ms\u001b[39m:\n",
       "\n",
       "    •  When \u001b[36ms\u001b[39m is univariate, it returns a vector of length \u001b[36mn\u001b[39m.\n",
       "\n",
       "    •  When \u001b[36ms\u001b[39m is multivariate, it returns a matrix with \u001b[36mn\u001b[39m columns.\n",
       "\n",
       "    •  When \u001b[36ms\u001b[39m is matrix-variate, it returns an array, where each element\n",
       "       is a sample matrix.\n",
       "       rand([rng::AbstractRNG,] s::Sampleable, dim1::Int, dim2::Int...)\n",
       "       rand([rng::AbstractRNG,] s::Sampleable, dims::Dims)\n",
       "\n",
       "  Generate an array of samples from \u001b[36ms\u001b[39m whose shape is determined by the given\n",
       "  dimensions.\n",
       "\n",
       "  ────────────────────────────────────────────────────────────────────────────\n",
       "\n",
       "\u001b[36m  rand(rng::AbstractRNG, d::UnivariateDistribution)\u001b[39m\n",
       "\n",
       "  Generate a scalar sample from \u001b[36md\u001b[39m. The general fallback is \u001b[36mquantile(d,\n",
       "  rand())\u001b[39m.\n",
       "\n",
       "  ────────────────────────────────────────────────────────────────────────────\n",
       "\n",
       "\u001b[36m  rand(rng, d)\u001b[39m\n",
       "\n",
       "  Extract a sample from the p-Generalized Gaussian distribution 'd'. The\n",
       "  sampling procedure is implemented from from [1]. [1] Gonzalez-Farias, G.,\n",
       "  Molina, J. A. D., & Rodríguez-Dagnino, R. M. (2009). Efficiency of the\n",
       "  approximated shape parameter estimator in the generalized Gaussian\n",
       "  distribution. IEEE Transactions on Vehicular Technology, 58(8), 4214-4223.\n",
       "\n",
       "  ────────────────────────────────────────────────────────────────────────────\n",
       "\n",
       "\u001b[36m  rand(::AbstractRNG, ::Distributions.AbstractMvNormal)\u001b[39m\n",
       "\n",
       "  Sample a random vector from the provided multi-variate normal distribution.\n",
       "\n",
       "  ────────────────────────────────────────────────────────────────────────────\n",
       "\n",
       "\u001b[36m  rand(::AbstractRNG, ::Sampleable)\u001b[39m\n",
       "\n",
       "  Samples from the sampler and returns the result.\n",
       "\n",
       "  ────────────────────────────────────────────────────────────────────────────\n",
       "\n",
       "\u001b[36m  rand(d::Union{UnivariateMixture, MultivariateMixture})\u001b[39m\n",
       "\n",
       "  Draw a sample from the mixture model \u001b[36md\u001b[39m.\n",
       "\n",
       "\u001b[36m  rand(d::Union{UnivariateMixture, MultivariateMixture}, n)\u001b[39m\n",
       "\n",
       "  Draw \u001b[36mn\u001b[39m samples from \u001b[36md\u001b[39m."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?rand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StatsBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-05T17:06:48.012000+08:00",
     "start_time": "2022-07-05T09:06:47.995Z"
    }
   },
   "outputs": [],
   "source": [
    "using StatsBase #统计库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T14:53:31.464000+08:00",
     "start_time": "2022-06-08T06:53:24.675Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1mS\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mB\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1me\u001b[22m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "No docstring found for module \\texttt{StatsBase}.\n",
       "\n",
       "\\section{Exported names}\n",
       "\\texttt{AbstractDataTransform}, \\texttt{AbstractHistogram}, \\texttt{AbstractWeights}, \\texttt{AnalyticWeights}, \\texttt{CoefTable}, \\texttt{ConvergenceException}, \\texttt{CovarianceEstimator}, \\texttt{CronbachAlpha}, \\texttt{ECDF}, \\texttt{FrequencyWeights}, \\texttt{Histogram}, \\texttt{L1dist}, \\texttt{L2dist}, \\texttt{Linfdist}, \\texttt{ProbabilityWeights}, \\texttt{RegressionModel}, \\texttt{SimpleCovariance}, \\texttt{StatisticalModel}, \\texttt{UnitRangeTransform}, \\texttt{UnitWeights}, \\texttt{Weights}, \\texttt{ZScoreTransform}, \\texttt{addcounts!}, \\texttt{adjr2}, \\texttt{adjr²}, \\texttt{aic}, \\texttt{aicc}, \\texttt{autocor}, \\texttt{autocor!}, \\texttt{autocov}, \\texttt{autocov!}, \\texttt{aweights}, \\texttt{bic}, \\texttt{coef}, \\texttt{coefnames}, \\texttt{coeftable}, \\texttt{competerank}, \\texttt{confint}, \\texttt{cooksdistance}, \\texttt{cor}, \\texttt{cor2cov}, \\texttt{corkendall}, \\texttt{corspearman}, \\texttt{counteq}, \\texttt{countmap}, \\texttt{countne}, \\texttt{counts}, \\texttt{cov}, \\texttt{cov2cor}, \\texttt{cronbachalpha}, \\texttt{crosscor}, \\texttt{crosscor!}, \\texttt{crosscov}, \\texttt{crosscov!}, \\texttt{crossentropy}, \\texttt{crossmodelmatrix}, \\texttt{denserank}, \\texttt{describe}, \\texttt{deviance}, \\texttt{dof}, \\texttt{dof\\_residual}, \\texttt{ecdf}, \\texttt{entropy}, \\texttt{eweights}, \\texttt{fit}, \\texttt{fit!}, \\texttt{fitted}, \\texttt{fweights}, \\texttt{genmean}, \\texttt{genvar}, \\texttt{geomean}, \\texttt{gkldiv}, \\texttt{harmmean}, \\texttt{indexmap}, \\texttt{indicatormat}, \\texttt{informationmatrix}, \\texttt{inverse\\_rle}, \\texttt{iqr}, \\texttt{isfitted}, \\texttt{islinear}, \\texttt{kldivergence}, \\texttt{kurtosis}, \\texttt{levelsmap}, \\texttt{leverage}, \\texttt{loglikelihood}, \\texttt{mad}, \\texttt{mad!}, \\texttt{maxad}, \\texttt{mean}, \\texttt{mean!}, \\texttt{mean\\_and\\_cov}, \\texttt{mean\\_and\\_std}, \\texttt{mean\\_and\\_var}, \\texttt{meanad}, \\texttt{meanresponse}, \\texttt{median}, \\texttt{median!}, \\texttt{middle}, \\texttt{midpoints}, \\texttt{mode}, \\texttt{model\\_response}, \\texttt{modelmatrix}, \\texttt{modes}, \\texttt{moment}, \\texttt{msd}, \\texttt{mss}, \\texttt{nobs}, \\texttt{norepeats}, \\texttt{nquantile}, \\texttt{nulldeviance}, \\texttt{nullloglikelihood}, \\texttt{ordinalrank}, \\texttt{pacf}, \\texttt{pacf!}, \\texttt{pairwise}, \\texttt{pairwise!}, \\texttt{partialcor}, \\texttt{percentile}, \\texttt{percentilerank}, \\texttt{predict}, \\texttt{predict!}, \\texttt{proportionmap}, \\texttt{proportions}, \\texttt{psnr}, \\texttt{pweights}, \\texttt{quantile}, \\texttt{quantile!}, \\texttt{quantilerank}, \\texttt{r2}, \\texttt{renyientropy}, \\texttt{residuals}, \\texttt{response}, \\texttt{responsename}, \\texttt{rle}, \\texttt{rmsd}, \\texttt{rss}, \\texttt{r²}, \\texttt{sample}, \\texttt{sample!}, \\texttt{samplepair}, \\texttt{scattermat}, \\texttt{scattermat\\_zm}, \\texttt{scattermatm}, \\texttt{score}, \\texttt{sem}, \\texttt{skewness}, \\texttt{span}, \\texttt{sqL2dist}, \\texttt{standardize}, \\texttt{std}, \\texttt{stderror}, \\texttt{sum}, \\texttt{summarystats}, \\texttt{tiedrank}, \\texttt{totalvar}, \\texttt{trim}, \\texttt{trim!}, \\texttt{trimvar}, \\texttt{uweights}, \\texttt{values}, \\texttt{var}, \\texttt{variation}, \\texttt{vcov}, \\texttt{weights}, \\texttt{winsor}, \\texttt{winsor!}, \\texttt{wmedian}, \\texttt{wquantile}, \\texttt{wsample}, \\texttt{wsample!}, \\texttt{wsum}, \\texttt{wsum!}, \\texttt{zscore}, \\texttt{zscore!}\n",
       "\n",
       "\\section{Displaying contents of readme found at \\texttt{D:{\\textbackslash}TongYuan{\\textbackslash}.julia{\\textbackslash}packages{\\textbackslash}StatsBase{\\textbackslash}pJqvO{\\textbackslash}README.md}}\n",
       "\\subsection{StatsBase.jl}\n",
       "\\emph{StatsBase.jl} is a Julia package that provides basic support for statistics. Particularly, it implements a variety of statistics-related functions, such as scalar statistics, high-order moment computation, counting, ranking, covariances, sampling, and empirical density estimation.\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item \\textbf{Build \\& Testing Status:} \\href{https://github.com/JuliaStats/StatsBase.jl/actions?query=workflow%3ACI+branch%3Amaster}{\\begin{figure}\n",
       "\\centering\n",
       "\\includegraphics{https://github.com/JuliaStats/StatsBase.jl/workflows/CI/badge.svg}\n",
       "\\caption{Build status}\n",
       "\\end{figure}\n",
       "} \\href{https://coveralls.io/r/JuliaStats/StatsBase.jl?branch=master}{\\begin{figure}\n",
       "\\centering\n",
       "\\includegraphics{https://coveralls.io/repos/JuliaStats/StatsBase.jl/badge.svg?branch=master}\n",
       "\\caption{Coverage Status}\n",
       "\\end{figure}\n",
       "} \\href{http://codecov.io/github/JuliaStats/StatsBase.jl?branch=master}{\\begin{figure}\n",
       "\\centering\n",
       "\\includegraphics{http://codecov.io/github/JuliaStats/StatsBase.jl/coverage.svg?branch=master}\n",
       "\\caption{Coverage Status}\n",
       "\\end{figure}\n",
       "}\n",
       "\n",
       "\n",
       "\\item \\textbf{Documentation}: [![][docs-stable-img]][docs-stable-url] [![][docs-latest-img]][docs-latest-url]\n",
       "\n",
       "\\end{itemize}\n",
       "[docs-latest-img]: https://img.shields.io/badge/docs-latest-blue.svg [docs-latest-url]: http://JuliaStats.github.io/StatsBase.jl/latest/\n",
       "\n",
       "[docs-stable-img]: https://img.shields.io/badge/docs-stable-blue.svg [docs-stable-url]: http://JuliaStats.github.io/StatsBase.jl/stable/\n",
       "\n"
      ],
      "text/markdown": [
       "No docstring found for module `StatsBase`.\n",
       "\n",
       "# Exported names\n",
       "\n",
       "`AbstractDataTransform`, `AbstractHistogram`, `AbstractWeights`, `AnalyticWeights`, `CoefTable`, `ConvergenceException`, `CovarianceEstimator`, `CronbachAlpha`, `ECDF`, `FrequencyWeights`, `Histogram`, `L1dist`, `L2dist`, `Linfdist`, `ProbabilityWeights`, `RegressionModel`, `SimpleCovariance`, `StatisticalModel`, `UnitRangeTransform`, `UnitWeights`, `Weights`, `ZScoreTransform`, `addcounts!`, `adjr2`, `adjr²`, `aic`, `aicc`, `autocor`, `autocor!`, `autocov`, `autocov!`, `aweights`, `bic`, `coef`, `coefnames`, `coeftable`, `competerank`, `confint`, `cooksdistance`, `cor`, `cor2cov`, `corkendall`, `corspearman`, `counteq`, `countmap`, `countne`, `counts`, `cov`, `cov2cor`, `cronbachalpha`, `crosscor`, `crosscor!`, `crosscov`, `crosscov!`, `crossentropy`, `crossmodelmatrix`, `denserank`, `describe`, `deviance`, `dof`, `dof_residual`, `ecdf`, `entropy`, `eweights`, `fit`, `fit!`, `fitted`, `fweights`, `genmean`, `genvar`, `geomean`, `gkldiv`, `harmmean`, `indexmap`, `indicatormat`, `informationmatrix`, `inverse_rle`, `iqr`, `isfitted`, `islinear`, `kldivergence`, `kurtosis`, `levelsmap`, `leverage`, `loglikelihood`, `mad`, `mad!`, `maxad`, `mean`, `mean!`, `mean_and_cov`, `mean_and_std`, `mean_and_var`, `meanad`, `meanresponse`, `median`, `median!`, `middle`, `midpoints`, `mode`, `model_response`, `modelmatrix`, `modes`, `moment`, `msd`, `mss`, `nobs`, `norepeats`, `nquantile`, `nulldeviance`, `nullloglikelihood`, `ordinalrank`, `pacf`, `pacf!`, `pairwise`, `pairwise!`, `partialcor`, `percentile`, `percentilerank`, `predict`, `predict!`, `proportionmap`, `proportions`, `psnr`, `pweights`, `quantile`, `quantile!`, `quantilerank`, `r2`, `renyientropy`, `residuals`, `response`, `responsename`, `rle`, `rmsd`, `rss`, `r²`, `sample`, `sample!`, `samplepair`, `scattermat`, `scattermat_zm`, `scattermatm`, `score`, `sem`, `skewness`, `span`, `sqL2dist`, `standardize`, `std`, `stderror`, `sum`, `summarystats`, `tiedrank`, `totalvar`, `trim`, `trim!`, `trimvar`, `uweights`, `values`, `var`, `variation`, `vcov`, `weights`, `winsor`, `winsor!`, `wmedian`, `wquantile`, `wsample`, `wsample!`, `wsum`, `wsum!`, `zscore`, `zscore!`\n",
       "\n",
       "# Displaying contents of readme found at `D:\\TongYuan\\.julia\\packages\\StatsBase\\pJqvO\\README.md`\n",
       "\n",
       "## StatsBase.jl\n",
       "\n",
       "*StatsBase.jl* is a Julia package that provides basic support for statistics. Particularly, it implements a variety of statistics-related functions, such as scalar statistics, high-order moment computation, counting, ranking, covariances, sampling, and empirical density estimation.\n",
       "\n",
       "  * **Build & Testing Status:** [![Build status](https://github.com/JuliaStats/StatsBase.jl/workflows/CI/badge.svg)](https://github.com/JuliaStats/StatsBase.jl/actions?query=workflow%3ACI+branch%3Amaster) [![Coverage Status](https://coveralls.io/repos/JuliaStats/StatsBase.jl/badge.svg?branch=master)](https://coveralls.io/r/JuliaStats/StatsBase.jl?branch=master) [![Coverage Status](http://codecov.io/github/JuliaStats/StatsBase.jl/coverage.svg?branch=master)](http://codecov.io/github/JuliaStats/StatsBase.jl?branch=master)\n",
       "  * **Documentation**: [![][docs-stable-img]][docs-stable-url] [![][docs-latest-img]][docs-latest-url]\n",
       "\n",
       "[docs-latest-img]: https://img.shields.io/badge/docs-latest-blue.svg [docs-latest-url]: http://JuliaStats.github.io/StatsBase.jl/latest/\n",
       "\n",
       "[docs-stable-img]: https://img.shields.io/badge/docs-stable-blue.svg [docs-stable-url]: http://JuliaStats.github.io/StatsBase.jl/stable/\n"
      ],
      "text/plain": [
       "  No docstring found for module \u001b[36mStatsBase\u001b[39m.\n",
       "\n",
       "\u001b[1m  Exported names\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "  \u001b[36mAbstractDataTransform\u001b[39m, \u001b[36mAbstractHistogram\u001b[39m, \u001b[36mAbstractWeights\u001b[39m, \u001b[36mAnalyticWeights\u001b[39m,\n",
       "  \u001b[36mCoefTable\u001b[39m, \u001b[36mConvergenceException\u001b[39m, \u001b[36mCovarianceEstimator\u001b[39m, \u001b[36mCronbachAlpha\u001b[39m, \u001b[36mECDF\u001b[39m,\n",
       "  \u001b[36mFrequencyWeights\u001b[39m, \u001b[36mHistogram\u001b[39m, \u001b[36mL1dist\u001b[39m, \u001b[36mL2dist\u001b[39m, \u001b[36mLinfdist\u001b[39m, \u001b[36mProbabilityWeights\u001b[39m,\n",
       "  \u001b[36mRegressionModel\u001b[39m, \u001b[36mSimpleCovariance\u001b[39m, \u001b[36mStatisticalModel\u001b[39m, \u001b[36mUnitRangeTransform\u001b[39m,\n",
       "  \u001b[36mUnitWeights\u001b[39m, \u001b[36mWeights\u001b[39m, \u001b[36mZScoreTransform\u001b[39m, \u001b[36maddcounts!\u001b[39m, \u001b[36madjr2\u001b[39m, \u001b[36madjr²\u001b[39m, \u001b[36maic\u001b[39m, \u001b[36maicc\u001b[39m,\n",
       "  \u001b[36mautocor\u001b[39m, \u001b[36mautocor!\u001b[39m, \u001b[36mautocov\u001b[39m, \u001b[36mautocov!\u001b[39m, \u001b[36maweights\u001b[39m, \u001b[36mbic\u001b[39m, \u001b[36mcoef\u001b[39m, \u001b[36mcoefnames\u001b[39m,\n",
       "  \u001b[36mcoeftable\u001b[39m, \u001b[36mcompeterank\u001b[39m, \u001b[36mconfint\u001b[39m, \u001b[36mcooksdistance\u001b[39m, \u001b[36mcor\u001b[39m, \u001b[36mcor2cov\u001b[39m, \u001b[36mcorkendall\u001b[39m,\n",
       "  \u001b[36mcorspearman\u001b[39m, \u001b[36mcounteq\u001b[39m, \u001b[36mcountmap\u001b[39m, \u001b[36mcountne\u001b[39m, \u001b[36mcounts\u001b[39m, \u001b[36mcov\u001b[39m, \u001b[36mcov2cor\u001b[39m,\n",
       "  \u001b[36mcronbachalpha\u001b[39m, \u001b[36mcrosscor\u001b[39m, \u001b[36mcrosscor!\u001b[39m, \u001b[36mcrosscov\u001b[39m, \u001b[36mcrosscov!\u001b[39m, \u001b[36mcrossentropy\u001b[39m,\n",
       "  \u001b[36mcrossmodelmatrix\u001b[39m, \u001b[36mdenserank\u001b[39m, \u001b[36mdescribe\u001b[39m, \u001b[36mdeviance\u001b[39m, \u001b[36mdof\u001b[39m, \u001b[36mdof_residual\u001b[39m, \u001b[36mecdf\u001b[39m,\n",
       "  \u001b[36mentropy\u001b[39m, \u001b[36meweights\u001b[39m, \u001b[36mfit\u001b[39m, \u001b[36mfit!\u001b[39m, \u001b[36mfitted\u001b[39m, \u001b[36mfweights\u001b[39m, \u001b[36mgenmean\u001b[39m, \u001b[36mgenvar\u001b[39m, \u001b[36mgeomean\u001b[39m,\n",
       "  \u001b[36mgkldiv\u001b[39m, \u001b[36mharmmean\u001b[39m, \u001b[36mindexmap\u001b[39m, \u001b[36mindicatormat\u001b[39m, \u001b[36minformationmatrix\u001b[39m, \u001b[36minverse_rle\u001b[39m,\n",
       "  \u001b[36miqr\u001b[39m, \u001b[36misfitted\u001b[39m, \u001b[36mislinear\u001b[39m, \u001b[36mkldivergence\u001b[39m, \u001b[36mkurtosis\u001b[39m, \u001b[36mlevelsmap\u001b[39m, \u001b[36mleverage\u001b[39m,\n",
       "  \u001b[36mloglikelihood\u001b[39m, \u001b[36mmad\u001b[39m, \u001b[36mmad!\u001b[39m, \u001b[36mmaxad\u001b[39m, \u001b[36mmean\u001b[39m, \u001b[36mmean!\u001b[39m, \u001b[36mmean_and_cov\u001b[39m, \u001b[36mmean_and_std\u001b[39m,\n",
       "  \u001b[36mmean_and_var\u001b[39m, \u001b[36mmeanad\u001b[39m, \u001b[36mmeanresponse\u001b[39m, \u001b[36mmedian\u001b[39m, \u001b[36mmedian!\u001b[39m, \u001b[36mmiddle\u001b[39m, \u001b[36mmidpoints\u001b[39m,\n",
       "  \u001b[36mmode\u001b[39m, \u001b[36mmodel_response\u001b[39m, \u001b[36mmodelmatrix\u001b[39m, \u001b[36mmodes\u001b[39m, \u001b[36mmoment\u001b[39m, \u001b[36mmsd\u001b[39m, \u001b[36mmss\u001b[39m, \u001b[36mnobs\u001b[39m, \u001b[36mnorepeats\u001b[39m,\n",
       "  \u001b[36mnquantile\u001b[39m, \u001b[36mnulldeviance\u001b[39m, \u001b[36mnullloglikelihood\u001b[39m, \u001b[36mordinalrank\u001b[39m, \u001b[36mpacf\u001b[39m, \u001b[36mpacf!\u001b[39m,\n",
       "  \u001b[36mpairwise\u001b[39m, \u001b[36mpairwise!\u001b[39m, \u001b[36mpartialcor\u001b[39m, \u001b[36mpercentile\u001b[39m, \u001b[36mpercentilerank\u001b[39m, \u001b[36mpredict\u001b[39m,\n",
       "  \u001b[36mpredict!\u001b[39m, \u001b[36mproportionmap\u001b[39m, \u001b[36mproportions\u001b[39m, \u001b[36mpsnr\u001b[39m, \u001b[36mpweights\u001b[39m, \u001b[36mquantile\u001b[39m, \u001b[36mquantile!\u001b[39m,\n",
       "  \u001b[36mquantilerank\u001b[39m, \u001b[36mr2\u001b[39m, \u001b[36mrenyientropy\u001b[39m, \u001b[36mresiduals\u001b[39m, \u001b[36mresponse\u001b[39m, \u001b[36mresponsename\u001b[39m, \u001b[36mrle\u001b[39m,\n",
       "  \u001b[36mrmsd\u001b[39m, \u001b[36mrss\u001b[39m, \u001b[36mr²\u001b[39m, \u001b[36msample\u001b[39m, \u001b[36msample!\u001b[39m, \u001b[36msamplepair\u001b[39m, \u001b[36mscattermat\u001b[39m, \u001b[36mscattermat_zm\u001b[39m,\n",
       "  \u001b[36mscattermatm\u001b[39m, \u001b[36mscore\u001b[39m, \u001b[36msem\u001b[39m, \u001b[36mskewness\u001b[39m, \u001b[36mspan\u001b[39m, \u001b[36msqL2dist\u001b[39m, \u001b[36mstandardize\u001b[39m, \u001b[36mstd\u001b[39m,\n",
       "  \u001b[36mstderror\u001b[39m, \u001b[36msum\u001b[39m, \u001b[36msummarystats\u001b[39m, \u001b[36mtiedrank\u001b[39m, \u001b[36mtotalvar\u001b[39m, \u001b[36mtrim\u001b[39m, \u001b[36mtrim!\u001b[39m, \u001b[36mtrimvar\u001b[39m,\n",
       "  \u001b[36muweights\u001b[39m, \u001b[36mvalues\u001b[39m, \u001b[36mvar\u001b[39m, \u001b[36mvariation\u001b[39m, \u001b[36mvcov\u001b[39m, \u001b[36mweights\u001b[39m, \u001b[36mwinsor\u001b[39m, \u001b[36mwinsor!\u001b[39m, \u001b[36mwmedian\u001b[39m,\n",
       "  \u001b[36mwquantile\u001b[39m, \u001b[36mwsample\u001b[39m, \u001b[36mwsample!\u001b[39m, \u001b[36mwsum\u001b[39m, \u001b[36mwsum!\u001b[39m, \u001b[36mzscore\u001b[39m, \u001b[36mzscore!\u001b[39m\n",
       "\n",
       "\u001b[1m  Displaying contents of readme found at\u001b[22m\n",
       "\u001b[1m \u001b[36mD:\\TongYuan\\.julia\\packages\\StatsBase\\pJqvO\\README.md\u001b[39m\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "\u001b[1m  StatsBase.jl\u001b[22m\n",
       "\u001b[1m  ==============\u001b[22m\n",
       "\n",
       "  \u001b[4mStatsBase.jl\u001b[24m is a Julia package that provides basic support for statistics.\n",
       "  Particularly, it implements a variety of statistics-related functions, such\n",
       "  as scalar statistics, high-order moment computation, counting, ranking,\n",
       "  covariances, sampling, and empirical density estimation.\n",
       "\n",
       "    •  \u001b[1mBuild & Testing Status:\u001b[22m (Image: Build status)\n",
       "       (https://github.com/JuliaStats/StatsBase.jl/actions?query=workflow%3ACI+branch%3Amaster)\n",
       "       (Image: Coverage Status)\n",
       "       (https://coveralls.io/r/JuliaStats/StatsBase.jl?branch=master)\n",
       "       (Image: Coverage Status)\n",
       "       (http://codecov.io/github/JuliaStats/StatsBase.jl?branch=master)\n",
       "\n",
       "    •  \u001b[1mDocumentation\u001b[22m: [![][docs-stable-img]][docs-stable-url]\n",
       "       [![][docs-latest-img]][docs-latest-url]\n",
       "\n",
       "  [docs-latest-img]: https://img.shields.io/badge/docs-latest-blue.svg\n",
       "  [docs-latest-url]: http://JuliaStats.github.io/StatsBase.jl/latest/\n",
       "\n",
       "  [docs-stable-img]: https://img.shields.io/badge/docs-stable-blue.svg\n",
       "  [docs-stable-url]: http://JuliaStats.github.io/StatsBase.jl/stable/"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?StatsBase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-22T10:06:43.894000+08:00",
     "start_time": "2022-07-22T02:06:43.892Z"
    }
   },
   "outputs": [],
   "source": [
    "using Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-22T10:06:44.458000+08:00",
     "start_time": "2022-07-22T02:06:44.412Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1mS\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1mc\u001b[22m\u001b[0m\u001b[1ms\u001b[22m \u001b[0m\u001b[1mS\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1mc\u001b[22malModel Ab\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mt\u001b[22mr\u001b[0m\u001b[1ma\u001b[22mc\u001b[0m\u001b[1mt\u001b[22mH\u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mt\u001b[22mogram \u001b[0m\u001b[1mS\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1mi\u001b[22mc\u001b[0m\u001b[1mS\u001b[22murroga\u001b[0m\u001b[1mt\u001b[22me\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "Statistics\n",
       "\\end{verbatim}\n",
       "Standard library module for basic statistics functionality.\n",
       "\n"
      ],
      "text/markdown": [
       "```\n",
       "Statistics\n",
       "```\n",
       "\n",
       "Standard library module for basic statistics functionality.\n"
      ],
      "text/plain": [
       "\u001b[36m  Statistics\u001b[39m\n",
       "\n",
       "  Standard library module for basic statistics functionality."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-22T10:41:25.145000+08:00",
     "start_time": "2022-07-22T02:41:25.136Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14-element Vector{Symbol}:\n",
       " :Statistics\n",
       " :cor\n",
       " :cov\n",
       " :mean\n",
       " :mean!\n",
       " :median\n",
       " :median!\n",
       " :middle\n",
       " :quantile\n",
       " :quantile!\n",
       " :std\n",
       " :stdm\n",
       " :var\n",
       " :varm"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: both MLJBase and MLBase export \"predict\"; uses of it in module Main must be qualified\n"
     ]
    }
   ],
   "source": [
    "names(Statistics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-05T17:06:56.036000+08:00",
     "start_time": "2022-07-05T09:06:55.399Z"
    }
   },
   "outputs": [],
   "source": [
    "using DataFrames #相当于Numpy之于Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T14:53:32.477000+08:00",
     "start_time": "2022-06-08T06:53:24.678Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1mD\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1mF\u001b[22m\u001b[0m\u001b[1mr\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1mm\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1ms\u001b[22m \u001b[0m\u001b[1mD\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1mF\u001b[22m\u001b[0m\u001b[1mr\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1mm\u001b[22m\u001b[0m\u001b[1me\u001b[22m \u001b[0m\u001b[1mD\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1mF\u001b[22m\u001b[0m\u001b[1mr\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1mm\u001b[22m\u001b[0m\u001b[1me\u001b[22mRow Sub\u001b[0m\u001b[1mD\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1mF\u001b[22m\u001b[0m\u001b[1mr\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1mm\u001b[22m\u001b[0m\u001b[1me\u001b[22m Groupe\u001b[0m\u001b[1md\u001b[22mD\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1mF\u001b[22m\u001b[0m\u001b[1mr\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1mm\u001b[22m\u001b[0m\u001b[1me\u001b[22m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "No docstring found for module \\texttt{DataFrames}.\n",
       "\n",
       "\\section{Exported names}\n",
       "\\texttt{AbstractDataFrame}, \\texttt{All}, \\texttt{AsTable}, \\texttt{Between}, \\texttt{ByRow}, \\texttt{Cols}, \\texttt{DataFrame}, \\texttt{DataFrameRow}, \\texttt{GroupedDataFrame}, \\texttt{InvertedIndex}, \\texttt{InvertedIndices}, \\texttt{Missing}, \\texttt{MissingException}, \\texttt{Missings}, \\texttt{Not}, \\texttt{PrettyTables}, \\texttt{SubDataFrame}, \\texttt{Tables}, \\texttt{aggregate}, \\texttt{allowmissing}, \\texttt{allowmissing!}, \\texttt{antijoin}, \\texttt{by}, \\texttt{coalesce}, \\texttt{columnindex}, \\texttt{combine}, \\texttt{completecases}, \\texttt{crossjoin}, \\texttt{delete!}, \\texttt{describe}, \\texttt{disallowmissing}, \\texttt{disallowmissing!}, \\texttt{dropmissing}, \\texttt{dropmissing!}, \\texttt{flatten}, \\texttt{groupby}, \\texttt{groupcols}, \\texttt{groupindices}, \\texttt{innerjoin}, \\texttt{insertcols!}, \\texttt{ismissing}, \\texttt{leftjoin}, \\texttt{leftjoin!}, \\texttt{levels}, \\texttt{mapcols}, \\texttt{mapcols!}, \\texttt{missing}, \\texttt{missings}, \\texttt{ncol}, \\texttt{nonmissingtype}, \\texttt{nonunique}, \\texttt{nrow}, \\texttt{order}, \\texttt{outerjoin}, \\texttt{passmissing}, \\texttt{rename}, \\texttt{rename!}, \\texttt{repeat!}, \\texttt{rightjoin}, \\texttt{rownumber}, \\texttt{select}, \\texttt{select!}, \\texttt{semijoin}, \\texttt{skipmissings}, \\texttt{stack}, \\texttt{subset}, \\texttt{subset!}, \\texttt{transform}, \\texttt{transform!}, \\texttt{unique!}, \\texttt{unstack}, \\texttt{valuecols}\n",
       "\n",
       "\\section{Displaying contents of readme found at \\texttt{D:{\\textbackslash}TongYuan{\\textbackslash}.julia{\\textbackslash}packages{\\textbackslash}DataFrames{\\textbackslash}zqFGs{\\textbackslash}README.md}}\n",
       "\\section{DataFrames.jl}\n",
       "\\href{http://codecov.io/github/JuliaData/DataFrames.jl?branch=main}{\\begin{figure}\n",
       "\\centering\n",
       "\\includegraphics{http://codecov.io/github/JuliaData/DataFrames.jl/coverage.svg?branch=main}\n",
       "\\caption{Coverage Status}\n",
       "\\end{figure}\n",
       "} \\href{https://github.com/JuliaData/DataFrames.jl/actions?query=workflow%3ACI+branch%3Amain}{\\begin{figure}\n",
       "\\centering\n",
       "\\includegraphics{https://github.com/JuliaData/DataFrames.jl/workflows/CI/badge.svg}\n",
       "\\caption{CI Testing}\n",
       "\\end{figure}\n",
       "}\n",
       "\n",
       "Tools for working with tabular data in Julia.\n",
       "\n",
       "\\textbf{Installation}: at the Julia REPL, \\texttt{using Pkg; Pkg.add(\"DataFrames\")}\n",
       "\n",
       "\\textbf{Documentation}: [![][docs-stable-img]][docs-stable-url] [![][docs-latest-img]][docs-latest-url]\n",
       "\n",
       "\\textbf{Reporting Issues and Contributing}: See \\href{CONTRIBUTING.md}{CONTRIBUTING.md}\n",
       "\n",
       "\\href{https://github.com/SciML/ColPrac}{\\begin{figure}\n",
       "\\centering\n",
       "\\includegraphics{https://img.shields.io/badge/ColPrac-Contributor's%20Guide-blueviolet}\n",
       "\\caption{ColPrac: Contributor's Guide on Collaborative Practices for Community Packages}\n",
       "\\end{figure}\n",
       "}\n",
       "\n",
       "\\textbf{Maintenance}: DataFrames is maintained collectively by the \\href{https://github.com/orgs/JuliaData/people}{JuliaData collaborators}. Responsiveness to pull requests and issues can vary, depending on the availability of key collaborators.\n",
       "\n",
       "\\textbf{Learning}: New to DataFrames.jl? Check out our \\href{https://juliaacademy.com/p/introduction-to-dataframes-jl}{free Julia Academy course} which will walk you through how to use DataFrames.jl. You can also check out \\href{https://github.com/bkamins/Julia-DataFrames-Tutorial}{Bogumił Kamiński's DataFrames.jl tutorial} that is available on GitHub.\n",
       "\n",
       "[docs-latest-img]: https://img.shields.io/badge/docs-latest-blue.svg [docs-latest-url]: http://dataframes.juliadata.org/latest/\n",
       "\n",
       "[docs-stable-img]: https://img.shields.io/badge/docs-stable-blue.svg [docs-stable-url]: http://dataframes.juliadata.org/stable/\n",
       "\n"
      ],
      "text/markdown": [
       "No docstring found for module `DataFrames`.\n",
       "\n",
       "# Exported names\n",
       "\n",
       "`AbstractDataFrame`, `All`, `AsTable`, `Between`, `ByRow`, `Cols`, `DataFrame`, `DataFrameRow`, `GroupedDataFrame`, `InvertedIndex`, `InvertedIndices`, `Missing`, `MissingException`, `Missings`, `Not`, `PrettyTables`, `SubDataFrame`, `Tables`, `aggregate`, `allowmissing`, `allowmissing!`, `antijoin`, `by`, `coalesce`, `columnindex`, `combine`, `completecases`, `crossjoin`, `delete!`, `describe`, `disallowmissing`, `disallowmissing!`, `dropmissing`, `dropmissing!`, `flatten`, `groupby`, `groupcols`, `groupindices`, `innerjoin`, `insertcols!`, `ismissing`, `leftjoin`, `leftjoin!`, `levels`, `mapcols`, `mapcols!`, `missing`, `missings`, `ncol`, `nonmissingtype`, `nonunique`, `nrow`, `order`, `outerjoin`, `passmissing`, `rename`, `rename!`, `repeat!`, `rightjoin`, `rownumber`, `select`, `select!`, `semijoin`, `skipmissings`, `stack`, `subset`, `subset!`, `transform`, `transform!`, `unique!`, `unstack`, `valuecols`\n",
       "\n",
       "# Displaying contents of readme found at `D:\\TongYuan\\.julia\\packages\\DataFrames\\zqFGs\\README.md`\n",
       "\n",
       "# DataFrames.jl\n",
       "\n",
       "[![Coverage Status](http://codecov.io/github/JuliaData/DataFrames.jl/coverage.svg?branch=main)](http://codecov.io/github/JuliaData/DataFrames.jl?branch=main) [![CI Testing](https://github.com/JuliaData/DataFrames.jl/workflows/CI/badge.svg)](https://github.com/JuliaData/DataFrames.jl/actions?query=workflow%3ACI+branch%3Amain)\n",
       "\n",
       "Tools for working with tabular data in Julia.\n",
       "\n",
       "**Installation**: at the Julia REPL, `using Pkg; Pkg.add(\"DataFrames\")`\n",
       "\n",
       "**Documentation**: [![][docs-stable-img]][docs-stable-url] [![][docs-latest-img]][docs-latest-url]\n",
       "\n",
       "**Reporting Issues and Contributing**: See [CONTRIBUTING.md](CONTRIBUTING.md)\n",
       "\n",
       "[![ColPrac: Contributor's Guide on Collaborative Practices for Community Packages](https://img.shields.io/badge/ColPrac-Contributor's%20Guide-blueviolet)](https://github.com/SciML/ColPrac)\n",
       "\n",
       "**Maintenance**: DataFrames is maintained collectively by the [JuliaData collaborators](https://github.com/orgs/JuliaData/people). Responsiveness to pull requests and issues can vary, depending on the availability of key collaborators.\n",
       "\n",
       "**Learning**: New to DataFrames.jl? Check out our [free Julia Academy course](https://juliaacademy.com/p/introduction-to-dataframes-jl) which will walk you through how to use DataFrames.jl. You can also check out [Bogumił Kamiński's DataFrames.jl tutorial](https://github.com/bkamins/Julia-DataFrames-Tutorial) that is available on GitHub.\n",
       "\n",
       "[docs-latest-img]: https://img.shields.io/badge/docs-latest-blue.svg [docs-latest-url]: http://dataframes.juliadata.org/latest/\n",
       "\n",
       "[docs-stable-img]: https://img.shields.io/badge/docs-stable-blue.svg [docs-stable-url]: http://dataframes.juliadata.org/stable/\n"
      ],
      "text/plain": [
       "  No docstring found for module \u001b[36mDataFrames\u001b[39m.\n",
       "\n",
       "\u001b[1m  Exported names\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "  \u001b[36mAbstractDataFrame\u001b[39m, \u001b[36mAll\u001b[39m, \u001b[36mAsTable\u001b[39m, \u001b[36mBetween\u001b[39m, \u001b[36mByRow\u001b[39m, \u001b[36mCols\u001b[39m, \u001b[36mDataFrame\u001b[39m,\n",
       "  \u001b[36mDataFrameRow\u001b[39m, \u001b[36mGroupedDataFrame\u001b[39m, \u001b[36mInvertedIndex\u001b[39m, \u001b[36mInvertedIndices\u001b[39m, \u001b[36mMissing\u001b[39m,\n",
       "  \u001b[36mMissingException\u001b[39m, \u001b[36mMissings\u001b[39m, \u001b[36mNot\u001b[39m, \u001b[36mPrettyTables\u001b[39m, \u001b[36mSubDataFrame\u001b[39m, \u001b[36mTables\u001b[39m,\n",
       "  \u001b[36maggregate\u001b[39m, \u001b[36mallowmissing\u001b[39m, \u001b[36mallowmissing!\u001b[39m, \u001b[36mantijoin\u001b[39m, \u001b[36mby\u001b[39m, \u001b[36mcoalesce\u001b[39m, \u001b[36mcolumnindex\u001b[39m,\n",
       "  \u001b[36mcombine\u001b[39m, \u001b[36mcompletecases\u001b[39m, \u001b[36mcrossjoin\u001b[39m, \u001b[36mdelete!\u001b[39m, \u001b[36mdescribe\u001b[39m, \u001b[36mdisallowmissing\u001b[39m,\n",
       "  \u001b[36mdisallowmissing!\u001b[39m, \u001b[36mdropmissing\u001b[39m, \u001b[36mdropmissing!\u001b[39m, \u001b[36mflatten\u001b[39m, \u001b[36mgroupby\u001b[39m, \u001b[36mgroupcols\u001b[39m,\n",
       "  \u001b[36mgroupindices\u001b[39m, \u001b[36minnerjoin\u001b[39m, \u001b[36minsertcols!\u001b[39m, \u001b[36mismissing\u001b[39m, \u001b[36mleftjoin\u001b[39m, \u001b[36mleftjoin!\u001b[39m,\n",
       "  \u001b[36mlevels\u001b[39m, \u001b[36mmapcols\u001b[39m, \u001b[36mmapcols!\u001b[39m, \u001b[36mmissing\u001b[39m, \u001b[36mmissings\u001b[39m, \u001b[36mncol\u001b[39m, \u001b[36mnonmissingtype\u001b[39m,\n",
       "  \u001b[36mnonunique\u001b[39m, \u001b[36mnrow\u001b[39m, \u001b[36morder\u001b[39m, \u001b[36mouterjoin\u001b[39m, \u001b[36mpassmissing\u001b[39m, \u001b[36mrename\u001b[39m, \u001b[36mrename!\u001b[39m, \u001b[36mrepeat!\u001b[39m,\n",
       "  \u001b[36mrightjoin\u001b[39m, \u001b[36mrownumber\u001b[39m, \u001b[36mselect\u001b[39m, \u001b[36mselect!\u001b[39m, \u001b[36msemijoin\u001b[39m, \u001b[36mskipmissings\u001b[39m, \u001b[36mstack\u001b[39m,\n",
       "  \u001b[36msubset\u001b[39m, \u001b[36msubset!\u001b[39m, \u001b[36mtransform\u001b[39m, \u001b[36mtransform!\u001b[39m, \u001b[36munique!\u001b[39m, \u001b[36munstack\u001b[39m, \u001b[36mvaluecols\u001b[39m\n",
       "\n",
       "\u001b[1m  Displaying contents of readme found at\u001b[22m\n",
       "\u001b[1m \u001b[36mD:\\TongYuan\\.julia\\packages\\DataFrames\\zqFGs\\README.md\u001b[39m\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "\u001b[1m  DataFrames.jl\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "  (Image: Coverage Status)\n",
       "  (http://codecov.io/github/JuliaData/DataFrames.jl?branch=main) (Image: CI\n",
       "  Testing)\n",
       "  (https://github.com/JuliaData/DataFrames.jl/actions?query=workflow%3ACI+branch%3Amain)\n",
       "\n",
       "  Tools for working with tabular data in Julia.\n",
       "\n",
       "  \u001b[1mInstallation\u001b[22m: at the Julia REPL, \u001b[36musing Pkg; Pkg.add(\"DataFrames\")\u001b[39m\n",
       "\n",
       "  \u001b[1mDocumentation\u001b[22m: [![][docs-stable-img]][docs-stable-url]\n",
       "  [![][docs-latest-img]][docs-latest-url]\n",
       "\n",
       "  \u001b[1mReporting Issues and Contributing\u001b[22m: See CONTRIBUTING.md (CONTRIBUTING.md)\n",
       "\n",
       "  (Image: ColPrac: Contributor's Guide on Collaborative Practices for\n",
       "  Community Packages) (https://github.com/SciML/ColPrac)\n",
       "\n",
       "  \u001b[1mMaintenance\u001b[22m: DataFrames is maintained collectively by the JuliaData\n",
       "  collaborators (https://github.com/orgs/JuliaData/people). Responsiveness to\n",
       "  pull requests and issues can vary, depending on the availability of key\n",
       "  collaborators.\n",
       "\n",
       "  \u001b[1mLearning\u001b[22m: New to DataFrames.jl? Check out our free Julia Academy course\n",
       "  (https://juliaacademy.com/p/introduction-to-dataframes-jl) which will walk\n",
       "  you through how to use DataFrames.jl. You can also check out Bogumił\n",
       "  Kamiński's DataFrames.jl tutorial\n",
       "  (https://github.com/bkamins/Julia-DataFrames-Tutorial) that is available on\n",
       "  GitHub.\n",
       "\n",
       "  [docs-latest-img]: https://img.shields.io/badge/docs-latest-blue.svg\n",
       "  [docs-latest-url]: http://dataframes.juliadata.org/latest/\n",
       "\n",
       "  [docs-stable-img]: https://img.shields.io/badge/docs-stable-blue.svg\n",
       "  [docs-stable-url]: http://dataframes.juliadata.org/stable/"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-05T17:08:26.085000+08:00",
     "start_time": "2022-07-05T09:08:24.910Z"
    }
   },
   "outputs": [],
   "source": [
    "using Plots #绘图库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T14:53:38.065000+08:00",
     "start_time": "2022-06-08T06:53:24.681Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1mP\u001b[22m\u001b[0m\u001b[1ml\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1ms\u001b[22m \u001b[0m\u001b[1mp\u001b[22m\u001b[0m\u001b[1ml\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1ms\u001b[22m_heatmap \u001b[0m\u001b[1mp\u001b[22m\u001b[0m\u001b[1ml\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1ms\u001b[22m_heatmap! \u001b[0m\u001b[1mp\u001b[22m\u001b[0m\u001b[1ml\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mt\u001b[22mlyj\u001b[0m\u001b[1ms\u001b[22m unicode\u001b[0m\u001b[1mp\u001b[22m\u001b[0m\u001b[1ml\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1ms\u001b[22m \u001b[0m\u001b[1mp\u001b[22mgfp\u001b[0m\u001b[1ml\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "No docstring found for module \\texttt{Plots}.\n",
       "\n",
       "\\section{Exported names}\n",
       "\\texttt{@animate}, \\texttt{@colorant\\_str}, \\texttt{@gif}, \\texttt{@layout}, \\texttt{@recipe}, \\texttt{@series}, \\texttt{@shorthands}, \\texttt{@userplot}, \\texttt{ABGR}, \\texttt{ADIN99}, \\texttt{ADIN99d}, \\texttt{ADIN99o}, \\texttt{AGray}, \\texttt{AGray32}, \\texttt{AHSI}, \\texttt{AHSL}, \\texttt{AHSV}, \\texttt{ALCHab}, \\texttt{ALCHuv}, \\texttt{ALMS}, \\texttt{ALab}, \\texttt{ALuv}, \\texttt{ARGB}, \\texttt{ARGB32}, \\texttt{AXYZ}, \\texttt{AYCbCr}, \\texttt{AYIQ}, \\texttt{AbstractAGray}, \\texttt{AbstractARGB}, \\texttt{AbstractBackend}, \\texttt{AbstractGray}, \\texttt{AbstractGrayA}, \\texttt{AbstractLayout}, \\texttt{AbstractPlot}, \\texttt{AbstractRGB}, \\texttt{AbstractRGBA}, \\texttt{AlphaColor}, \\texttt{Animation}, \\texttt{AxyY}, \\texttt{BGR}, \\texttt{BGRA}, \\texttt{BezierCurve}, \\texttt{CIE1931JV\\_CMF}, \\texttt{CIE1931J\\_CMF}, \\texttt{CIE1931\\_CMF}, \\texttt{CIE1964\\_CMF}, \\texttt{CIE2006\\_10\\_CMF}, \\texttt{CIE2006\\_2\\_CMF}, \\texttt{Color}, \\texttt{Color3}, \\texttt{ColorAlpha}, \\texttt{ColorGradient}, \\texttt{ColorPalette}, \\texttt{ColorTypes}, \\texttt{Colorant}, \\texttt{ColorantNormed}, \\texttt{Colors}, \\texttt{DE\\_2000}, \\texttt{DE\\_94}, \\texttt{DE\\_AB}, \\texttt{DE\\_BFD}, \\texttt{DE\\_CMC}, \\texttt{DE\\_DIN99}, \\texttt{DE\\_DIN99d}, \\texttt{DE\\_DIN99o}, \\texttt{DE\\_JPC79}, \\texttt{DIN99}, \\texttt{DIN99A}, \\texttt{DIN99d}, \\texttt{DIN99dA}, \\texttt{DIN99o}, \\texttt{DIN99oA}, \\texttt{Formatted}, \\texttt{Fractional}, \\texttt{GR}, \\texttt{Gray}, \\texttt{Gray24}, \\texttt{GrayA}, \\texttt{HSB}, \\texttt{HSI}, \\texttt{HSIA}, \\texttt{HSL}, \\texttt{HSLA}, \\texttt{HSV}, \\texttt{HSVA}, \\texttt{KW}, \\texttt{LCHab}, \\texttt{LCHabA}, \\texttt{LCHuv}, \\texttt{LCHuvA}, \\texttt{LMS}, \\texttt{LMSA}, \\texttt{Lab}, \\texttt{LabA}, \\texttt{Luv}, \\texttt{LuvA}, \\texttt{MSC}, \\texttt{OHLC}, \\texttt{PlotThemes}, \\texttt{PlotUtils}, \\texttt{RGB}, \\texttt{RGB1}, \\texttt{RGB24}, \\texttt{RGB4}, \\texttt{RGBA}, \\texttt{RGBX}, \\texttt{RecipeData}, \\texttt{RecipesBase}, \\texttt{Segments}, \\texttt{Shape}, \\texttt{Surface}, \\texttt{Transparent3}, \\texttt{TransparentColor}, \\texttt{TransparentGray}, \\texttt{TransparentRGB}, \\texttt{XRGB}, \\texttt{XYZ}, \\texttt{XYZA}, \\texttt{YCbCr}, \\texttt{YCbCrA}, \\texttt{YIQ}, \\texttt{YIQA}, \\texttt{adapted\\_grid}, \\texttt{add\\_theme}, \\texttt{aliases}, \\texttt{alpha}, \\texttt{alphacolor}, \\texttt{animate}, \\texttt{annotate!}, \\texttt{areaplot}, \\texttt{areaplot!}, \\texttt{arrow}, \\texttt{attr!}, \\texttt{backend}, \\texttt{backend\\_name}, \\texttt{backend\\_object}, \\texttt{backends}, \\texttt{bar}, \\texttt{bar!}, \\texttt{barh}, \\texttt{barh!}, \\texttt{barhist}, \\texttt{barhist!}, \\texttt{base\\_color\\_type}, \\texttt{base\\_colorant\\_type}, \\texttt{bbox}, \\texttt{blue}, \\texttt{boxplot}, \\texttt{boxplot!}, \\texttt{brush}, \\texttt{ccolor}, \\texttt{center}, \\texttt{cgrad}, \\texttt{chroma}, \\texttt{cie\\_color\\_match}, \\texttt{closeall}, \\texttt{color}, \\texttt{color\\_list}, \\texttt{color\\_type}, \\texttt{coloralpha}, \\texttt{colordiff}, \\texttt{colormap}, \\texttt{colormatch}, \\texttt{comp1}, \\texttt{comp2}, \\texttt{comp3}, \\texttt{comp4}, \\texttt{comp5}, \\texttt{contour}, \\texttt{contour!}, \\texttt{contour3d}, \\texttt{contour3d!}, \\texttt{contourf}, \\texttt{contourf!}, \\texttt{coords}, \\texttt{current}, \\texttt{curve\\_points}, \\texttt{curves}, \\texttt{curves!}, \\texttt{cvec}, \\texttt{default}, \\texttt{default\\_cgrad}, \\texttt{density}, \\texttt{density!}, \\texttt{deuteranopic}, \\texttt{distinguishable\\_colors}, \\texttt{diverging\\_palette}, \\texttt{font}, \\texttt{frame}, \\texttt{gamutmax}, \\texttt{gamutmin}, \\texttt{gaston}, \\texttt{get\\_color\\_palette}, \\texttt{gif}, \\texttt{gr}, \\texttt{gray}, \\texttt{green}, \\texttt{grid}, \\texttt{gui}, \\texttt{hdf5}, \\texttt{heatmap}, \\texttt{heatmap!}, \\texttt{hex}, \\texttt{hexbin}, \\texttt{hexbin!}, \\texttt{histogram}, \\texttt{histogram!}, \\texttt{histogram2d}, \\texttt{histogram2d!}, \\texttt{hline}, \\texttt{hline!}, \\texttt{hspan}, \\texttt{hspan!}, \\texttt{hue}, \\texttt{inline}, \\texttt{inspectdr}, \\texttt{invisible}, \\texttt{isdark}, \\texttt{iter\\_segments}, \\texttt{lens!}, \\texttt{mapc}, \\texttt{mapreducec}, \\texttt{mesh3d}, \\texttt{mesh3d!}, \\texttt{mov}, \\texttt{mp4}, \\texttt{normalize\\_hue}, \\texttt{ohlc}, \\texttt{ohlc!}, \\texttt{optimize\\_datetime\\_ticks}, \\texttt{optimize\\_ticks}, \\texttt{palette}, \\texttt{parametric\\_colorant}, \\texttt{path3d}, \\texttt{path3d!}, \\texttt{pgfplots}, \\texttt{pgfplotsx}, \\texttt{pie}, \\texttt{pie!}, \\texttt{plot}, \\texttt{plot!}, \\texttt{plot3d}, \\texttt{plot3d!}, \\texttt{plot\\_color}, \\texttt{plotarea}, \\texttt{plotattr}, \\texttt{plotly}, \\texttt{plotlyjs}, \\texttt{plots\\_heatmap}, \\texttt{plots\\_heatmap!}, \\texttt{png}, \\texttt{portfoliocomposition}, \\texttt{portfoliocomposition!}, \\texttt{protanopic}, \\texttt{pyplot}, \\texttt{quiver}, \\texttt{quiver!}, \\texttt{red}, \\texttt{reducec}, \\texttt{resetfontsizes}, \\texttt{rgb\\_string}, \\texttt{rgba\\_string}, \\texttt{rotate}, \\texttt{rotate!}, \\texttt{savefig}, \\texttt{scalefontsize}, \\texttt{scalefontsizes}, \\texttt{scatter}, \\texttt{scatter!}, \\texttt{scatter3d}, \\texttt{scatter3d!}, \\texttt{scatterhist}, \\texttt{scatterhist!}, \\texttt{sequential\\_palette}, \\texttt{set\\_theme}, \\texttt{shape\\_coords}, \\texttt{showtheme}, \\texttt{showtheme!}, \\texttt{spy}, \\texttt{spy!}, \\texttt{stephist}, \\texttt{stephist!}, \\texttt{sticks}, \\texttt{sticks!}, \\texttt{stroke}, \\texttt{surface}, \\texttt{surface!}, \\texttt{test\\_examples}, \\texttt{text}, \\texttt{theme}, \\texttt{theme\\_palette}, \\texttt{title!}, \\texttt{translate}, \\texttt{translate!}, \\texttt{tritanopic}, \\texttt{twinx}, \\texttt{unicodeplots}, \\texttt{violin}, \\texttt{violin!}, \\texttt{vline}, \\texttt{vline!}, \\texttt{vspan}, \\texttt{vspan!}, \\texttt{webm}, \\texttt{weighted\\_color\\_mean}, \\texttt{whitebalance}, \\texttt{wireframe}, \\texttt{wireframe!}, \\texttt{with}, \\texttt{wrap}, \\texttt{xaxis!}, \\texttt{xflip!}, \\texttt{xgrid!}, \\texttt{xlabel!}, \\texttt{xlims}, \\texttt{xlims!}, \\texttt{xticks}, \\texttt{xticks!}, \\texttt{xyY}, \\texttt{xyYA}, \\texttt{yaxis!}, \\texttt{yflip!}, \\texttt{ygrid!}, \\texttt{ylabel!}, \\texttt{ylims}, \\texttt{ylims!}, \\texttt{yticks}, \\texttt{yticks!}, \\texttt{zlims}, \\texttt{zlims!}, \\texttt{zscale}, \\texttt{zticks}\n",
       "\n",
       "\\section{Displaying contents of readme found at \\texttt{D:{\\textbackslash}TongYuan{\\textbackslash}.julia{\\textbackslash}packages{\\textbackslash}Plots{\\textbackslash}mnkn4{\\textbackslash}README.md}}\n",
       "\\section{Plots}\n",
       "[gh-ci-img]: https://github.com/JuliaPlots/Plots.jl/workflows/ci/badge.svg?branch=master [gh-ci-url]: https://github.com/JuliaPlots/Plots.jl/actions?query=workflow\\%3Aci\n",
       "\n",
       "[pkgeval-img]: https://juliaci.github.io/NanosoldierReports/pkgeval\\emph{badges/P/Plots.svg [pkgeval-url]: https://juliaci.github.io/NanosoldierReports/pkgeval}badges/report.html\n",
       "\n",
       "[gitter-img]: https://badges.gitter.im/tbreloff/Plots.jl.svg [gitter-url]: https://gitter.im/tbreloff/Plots.jl?utm\\emph{source=badge\\&utm}medium=badge\\&utm\\emph{campaign=pr-badge\\&utm}content=badge\n",
       "\n",
       "[docs-img]: https://img.shields.io/badge/docs-stable-blue.svg [docs-url]: https://docs.juliaplots.org/stable/\n",
       "\n",
       "[![][gh-ci-img]][gh-ci-url] [![][pkgeval-img]][pkgeval-url] \\href{https://julialang.zulipchat.com/#narrow/stream/236493-plots}{\\begin{figure}\n",
       "\\centering\n",
       "\\includegraphics{https://img.shields.io/badge/zulip-join_chat-brightgreen.svg}\n",
       "\\caption{project chat}\n",
       "\\end{figure}\n",
       "} [![][docs-img]][docs-url] \\href{https://codecov.io/gh/JuliaPlots/Plots.jl}{\\begin{figure}\n",
       "\\centering\n",
       "\\includegraphics{https://codecov.io/gh/JuliaPlots/Plots.jl/branch/master/graph/badge.svg}\n",
       "\\caption{Codecov}\n",
       "\\end{figure}\n",
       "} \\href{https://pkgs.genieframework.com?packages=Plots}{\\begin{figure}\n",
       "\\centering\n",
       "\\includegraphics{https://shields.io/endpoint?url=https://pkgs.genieframework.com/api/v1/badge/Plots}\n",
       "\\caption{Plots Downloads}\n",
       "\\end{figure}\n",
       "}\n",
       "\n",
       "\\href{https://doi.org/10.5281/zenodo.4725317}{\\begin{figure}\n",
       "\\centering\n",
       "\\includegraphics{https://zenodo.org/badge/DOI/10.5281/zenodo.4725317.svg}\n",
       "\\caption{DOI}\n",
       "\\end{figure}\n",
       "} This is the DOI for all Versions, please follow the link to get the DOI for a specific version.\n",
       "\n",
       "\\paragraph{Created by Tom Breloff (@tbreloff)}\n",
       "\\paragraph{Maintained by the \\href{https://github.com/orgs/JuliaPlots/people}{JuliaPlots members}}\n",
       "Plots is a plotting API and toolset.  My goals with the package are:\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item \\textbf{Powerful}.  Do more with less.  Complex visualizations become easy.\n",
       "\n",
       "\n",
       "\\item \\textbf{Intuitive}.  Stop reading so much documentation.  Commands should \"just work\".\n",
       "\n",
       "\n",
       "\\item \\textbf{Concise}.  Less code means fewer mistakes and more efficient development/analysis.\n",
       "\n",
       "\n",
       "\\item \\textbf{Flexible}.  Produce your favorite plots from your favorite package, but quicker and simpler.\n",
       "\n",
       "\n",
       "\\item \\textbf{Consistent}.  Don't commit to one graphics package, use the same code everywhere.\n",
       "\n",
       "\n",
       "\\item \\textbf{Lightweight}.  Very few dependencies.\n",
       "\n",
       "\n",
       "\\item \\textbf{Smart}. Attempts to figure out what you \\textbf{want} it to do... not just what you \\textbf{tell} it.\n",
       "\n",
       "\\end{itemize}\n"
      ],
      "text/markdown": [
       "No docstring found for module `Plots`.\n",
       "\n",
       "# Exported names\n",
       "\n",
       "`@animate`, `@colorant_str`, `@gif`, `@layout`, `@recipe`, `@series`, `@shorthands`, `@userplot`, `ABGR`, `ADIN99`, `ADIN99d`, `ADIN99o`, `AGray`, `AGray32`, `AHSI`, `AHSL`, `AHSV`, `ALCHab`, `ALCHuv`, `ALMS`, `ALab`, `ALuv`, `ARGB`, `ARGB32`, `AXYZ`, `AYCbCr`, `AYIQ`, `AbstractAGray`, `AbstractARGB`, `AbstractBackend`, `AbstractGray`, `AbstractGrayA`, `AbstractLayout`, `AbstractPlot`, `AbstractRGB`, `AbstractRGBA`, `AlphaColor`, `Animation`, `AxyY`, `BGR`, `BGRA`, `BezierCurve`, `CIE1931JV_CMF`, `CIE1931J_CMF`, `CIE1931_CMF`, `CIE1964_CMF`, `CIE2006_10_CMF`, `CIE2006_2_CMF`, `Color`, `Color3`, `ColorAlpha`, `ColorGradient`, `ColorPalette`, `ColorTypes`, `Colorant`, `ColorantNormed`, `Colors`, `DE_2000`, `DE_94`, `DE_AB`, `DE_BFD`, `DE_CMC`, `DE_DIN99`, `DE_DIN99d`, `DE_DIN99o`, `DE_JPC79`, `DIN99`, `DIN99A`, `DIN99d`, `DIN99dA`, `DIN99o`, `DIN99oA`, `Formatted`, `Fractional`, `GR`, `Gray`, `Gray24`, `GrayA`, `HSB`, `HSI`, `HSIA`, `HSL`, `HSLA`, `HSV`, `HSVA`, `KW`, `LCHab`, `LCHabA`, `LCHuv`, `LCHuvA`, `LMS`, `LMSA`, `Lab`, `LabA`, `Luv`, `LuvA`, `MSC`, `OHLC`, `PlotThemes`, `PlotUtils`, `RGB`, `RGB1`, `RGB24`, `RGB4`, `RGBA`, `RGBX`, `RecipeData`, `RecipesBase`, `Segments`, `Shape`, `Surface`, `Transparent3`, `TransparentColor`, `TransparentGray`, `TransparentRGB`, `XRGB`, `XYZ`, `XYZA`, `YCbCr`, `YCbCrA`, `YIQ`, `YIQA`, `adapted_grid`, `add_theme`, `aliases`, `alpha`, `alphacolor`, `animate`, `annotate!`, `areaplot`, `areaplot!`, `arrow`, `attr!`, `backend`, `backend_name`, `backend_object`, `backends`, `bar`, `bar!`, `barh`, `barh!`, `barhist`, `barhist!`, `base_color_type`, `base_colorant_type`, `bbox`, `blue`, `boxplot`, `boxplot!`, `brush`, `ccolor`, `center`, `cgrad`, `chroma`, `cie_color_match`, `closeall`, `color`, `color_list`, `color_type`, `coloralpha`, `colordiff`, `colormap`, `colormatch`, `comp1`, `comp2`, `comp3`, `comp4`, `comp5`, `contour`, `contour!`, `contour3d`, `contour3d!`, `contourf`, `contourf!`, `coords`, `current`, `curve_points`, `curves`, `curves!`, `cvec`, `default`, `default_cgrad`, `density`, `density!`, `deuteranopic`, `distinguishable_colors`, `diverging_palette`, `font`, `frame`, `gamutmax`, `gamutmin`, `gaston`, `get_color_palette`, `gif`, `gr`, `gray`, `green`, `grid`, `gui`, `hdf5`, `heatmap`, `heatmap!`, `hex`, `hexbin`, `hexbin!`, `histogram`, `histogram!`, `histogram2d`, `histogram2d!`, `hline`, `hline!`, `hspan`, `hspan!`, `hue`, `inline`, `inspectdr`, `invisible`, `isdark`, `iter_segments`, `lens!`, `mapc`, `mapreducec`, `mesh3d`, `mesh3d!`, `mov`, `mp4`, `normalize_hue`, `ohlc`, `ohlc!`, `optimize_datetime_ticks`, `optimize_ticks`, `palette`, `parametric_colorant`, `path3d`, `path3d!`, `pgfplots`, `pgfplotsx`, `pie`, `pie!`, `plot`, `plot!`, `plot3d`, `plot3d!`, `plot_color`, `plotarea`, `plotattr`, `plotly`, `plotlyjs`, `plots_heatmap`, `plots_heatmap!`, `png`, `portfoliocomposition`, `portfoliocomposition!`, `protanopic`, `pyplot`, `quiver`, `quiver!`, `red`, `reducec`, `resetfontsizes`, `rgb_string`, `rgba_string`, `rotate`, `rotate!`, `savefig`, `scalefontsize`, `scalefontsizes`, `scatter`, `scatter!`, `scatter3d`, `scatter3d!`, `scatterhist`, `scatterhist!`, `sequential_palette`, `set_theme`, `shape_coords`, `showtheme`, `showtheme!`, `spy`, `spy!`, `stephist`, `stephist!`, `sticks`, `sticks!`, `stroke`, `surface`, `surface!`, `test_examples`, `text`, `theme`, `theme_palette`, `title!`, `translate`, `translate!`, `tritanopic`, `twinx`, `unicodeplots`, `violin`, `violin!`, `vline`, `vline!`, `vspan`, `vspan!`, `webm`, `weighted_color_mean`, `whitebalance`, `wireframe`, `wireframe!`, `with`, `wrap`, `xaxis!`, `xflip!`, `xgrid!`, `xlabel!`, `xlims`, `xlims!`, `xticks`, `xticks!`, `xyY`, `xyYA`, `yaxis!`, `yflip!`, `ygrid!`, `ylabel!`, `ylims`, `ylims!`, `yticks`, `yticks!`, `zlims`, `zlims!`, `zscale`, `zticks`\n",
       "\n",
       "# Displaying contents of readme found at `D:\\TongYuan\\.julia\\packages\\Plots\\mnkn4\\README.md`\n",
       "\n",
       "# Plots\n",
       "\n",
       "[gh-ci-img]: https://github.com/JuliaPlots/Plots.jl/workflows/ci/badge.svg?branch=master [gh-ci-url]: https://github.com/JuliaPlots/Plots.jl/actions?query=workflow%3Aci\n",
       "\n",
       "[pkgeval-img]: https://juliaci.github.io/NanosoldierReports/pkgeval*badges/P/Plots.svg [pkgeval-url]: https://juliaci.github.io/NanosoldierReports/pkgeval*badges/report.html\n",
       "\n",
       "[gitter-img]: https://badges.gitter.im/tbreloff/Plots.jl.svg [gitter-url]: https://gitter.im/tbreloff/Plots.jl?utm*source=badge&utm*medium=badge&utm*campaign=pr-badge&utm*content=badge\n",
       "\n",
       "[docs-img]: https://img.shields.io/badge/docs-stable-blue.svg [docs-url]: https://docs.juliaplots.org/stable/\n",
       "\n",
       "[![][gh-ci-img]][gh-ci-url] [![][pkgeval-img]][pkgeval-url] [![project chat](https://img.shields.io/badge/zulip-join_chat-brightgreen.svg)](https://julialang.zulipchat.com/#narrow/stream/236493-plots) [![][docs-img]][docs-url] [![Codecov](https://codecov.io/gh/JuliaPlots/Plots.jl/branch/master/graph/badge.svg)](https://codecov.io/gh/JuliaPlots/Plots.jl) [![Plots Downloads](https://shields.io/endpoint?url=https://pkgs.genieframework.com/api/v1/badge/Plots)](https://pkgs.genieframework.com?packages=Plots)\n",
       "\n",
       "[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.4725317.svg)](https://doi.org/10.5281/zenodo.4725317) This is the DOI for all Versions, please follow the link to get the DOI for a specific version.\n",
       "\n",
       "#### Created by Tom Breloff (@tbreloff)\n",
       "\n",
       "#### Maintained by the [JuliaPlots members](https://github.com/orgs/JuliaPlots/people)\n",
       "\n",
       "Plots is a plotting API and toolset.  My goals with the package are:\n",
       "\n",
       "  * **Powerful**.  Do more with less.  Complex visualizations become easy.\n",
       "  * **Intuitive**.  Stop reading so much documentation.  Commands should \"just work\".\n",
       "  * **Concise**.  Less code means fewer mistakes and more efficient development/analysis.\n",
       "  * **Flexible**.  Produce your favorite plots from your favorite package, but quicker and simpler.\n",
       "  * **Consistent**.  Don't commit to one graphics package, use the same code everywhere.\n",
       "  * **Lightweight**.  Very few dependencies.\n",
       "  * **Smart**. Attempts to figure out what you **want** it to do... not just what you **tell** it.\n"
      ],
      "text/plain": [
       "  No docstring found for module \u001b[36mPlots\u001b[39m.\n",
       "\n",
       "\u001b[1m  Exported names\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "  \u001b[36m@animate\u001b[39m, \u001b[36m@colorant_str\u001b[39m, \u001b[36m@gif\u001b[39m, \u001b[36m@layout\u001b[39m, \u001b[36m@recipe\u001b[39m, \u001b[36m@series\u001b[39m, \u001b[36m@shorthands\u001b[39m,\n",
       "  \u001b[36m@userplot\u001b[39m, \u001b[36mABGR\u001b[39m, \u001b[36mADIN99\u001b[39m, \u001b[36mADIN99d\u001b[39m, \u001b[36mADIN99o\u001b[39m, \u001b[36mAGray\u001b[39m, \u001b[36mAGray32\u001b[39m, \u001b[36mAHSI\u001b[39m, \u001b[36mAHSL\u001b[39m, \u001b[36mAHSV\u001b[39m,\n",
       "  \u001b[36mALCHab\u001b[39m, \u001b[36mALCHuv\u001b[39m, \u001b[36mALMS\u001b[39m, \u001b[36mALab\u001b[39m, \u001b[36mALuv\u001b[39m, \u001b[36mARGB\u001b[39m, \u001b[36mARGB32\u001b[39m, \u001b[36mAXYZ\u001b[39m, \u001b[36mAYCbCr\u001b[39m, \u001b[36mAYIQ\u001b[39m,\n",
       "  \u001b[36mAbstractAGray\u001b[39m, \u001b[36mAbstractARGB\u001b[39m, \u001b[36mAbstractBackend\u001b[39m, \u001b[36mAbstractGray\u001b[39m, \u001b[36mAbstractGrayA\u001b[39m,\n",
       "  \u001b[36mAbstractLayout\u001b[39m, \u001b[36mAbstractPlot\u001b[39m, \u001b[36mAbstractRGB\u001b[39m, \u001b[36mAbstractRGBA\u001b[39m, \u001b[36mAlphaColor\u001b[39m,\n",
       "  \u001b[36mAnimation\u001b[39m, \u001b[36mAxyY\u001b[39m, \u001b[36mBGR\u001b[39m, \u001b[36mBGRA\u001b[39m, \u001b[36mBezierCurve\u001b[39m, \u001b[36mCIE1931JV_CMF\u001b[39m, \u001b[36mCIE1931J_CMF\u001b[39m,\n",
       "  \u001b[36mCIE1931_CMF\u001b[39m, \u001b[36mCIE1964_CMF\u001b[39m, \u001b[36mCIE2006_10_CMF\u001b[39m, \u001b[36mCIE2006_2_CMF\u001b[39m, \u001b[36mColor\u001b[39m, \u001b[36mColor3\u001b[39m,\n",
       "  \u001b[36mColorAlpha\u001b[39m, \u001b[36mColorGradient\u001b[39m, \u001b[36mColorPalette\u001b[39m, \u001b[36mColorTypes\u001b[39m, \u001b[36mColorant\u001b[39m,\n",
       "  \u001b[36mColorantNormed\u001b[39m, \u001b[36mColors\u001b[39m, \u001b[36mDE_2000\u001b[39m, \u001b[36mDE_94\u001b[39m, \u001b[36mDE_AB\u001b[39m, \u001b[36mDE_BFD\u001b[39m, \u001b[36mDE_CMC\u001b[39m, \u001b[36mDE_DIN99\u001b[39m,\n",
       "  \u001b[36mDE_DIN99d\u001b[39m, \u001b[36mDE_DIN99o\u001b[39m, \u001b[36mDE_JPC79\u001b[39m, \u001b[36mDIN99\u001b[39m, \u001b[36mDIN99A\u001b[39m, \u001b[36mDIN99d\u001b[39m, \u001b[36mDIN99dA\u001b[39m, \u001b[36mDIN99o\u001b[39m,\n",
       "  \u001b[36mDIN99oA\u001b[39m, \u001b[36mFormatted\u001b[39m, \u001b[36mFractional\u001b[39m, \u001b[36mGR\u001b[39m, \u001b[36mGray\u001b[39m, \u001b[36mGray24\u001b[39m, \u001b[36mGrayA\u001b[39m, \u001b[36mHSB\u001b[39m, \u001b[36mHSI\u001b[39m, \u001b[36mHSIA\u001b[39m,\n",
       "  \u001b[36mHSL\u001b[39m, \u001b[36mHSLA\u001b[39m, \u001b[36mHSV\u001b[39m, \u001b[36mHSVA\u001b[39m, \u001b[36mKW\u001b[39m, \u001b[36mLCHab\u001b[39m, \u001b[36mLCHabA\u001b[39m, \u001b[36mLCHuv\u001b[39m, \u001b[36mLCHuvA\u001b[39m, \u001b[36mLMS\u001b[39m, \u001b[36mLMSA\u001b[39m, \u001b[36mLab\u001b[39m,\n",
       "  \u001b[36mLabA\u001b[39m, \u001b[36mLuv\u001b[39m, \u001b[36mLuvA\u001b[39m, \u001b[36mMSC\u001b[39m, \u001b[36mOHLC\u001b[39m, \u001b[36mPlotThemes\u001b[39m, \u001b[36mPlotUtils\u001b[39m, \u001b[36mRGB\u001b[39m, \u001b[36mRGB1\u001b[39m, \u001b[36mRGB24\u001b[39m, \u001b[36mRGB4\u001b[39m,\n",
       "  \u001b[36mRGBA\u001b[39m, \u001b[36mRGBX\u001b[39m, \u001b[36mRecipeData\u001b[39m, \u001b[36mRecipesBase\u001b[39m, \u001b[36mSegments\u001b[39m, \u001b[36mShape\u001b[39m, \u001b[36mSurface\u001b[39m, \u001b[36mTransparent3\u001b[39m,\n",
       "  \u001b[36mTransparentColor\u001b[39m, \u001b[36mTransparentGray\u001b[39m, \u001b[36mTransparentRGB\u001b[39m, \u001b[36mXRGB\u001b[39m, \u001b[36mXYZ\u001b[39m, \u001b[36mXYZA\u001b[39m, \u001b[36mYCbCr\u001b[39m,\n",
       "  \u001b[36mYCbCrA\u001b[39m, \u001b[36mYIQ\u001b[39m, \u001b[36mYIQA\u001b[39m, \u001b[36madapted_grid\u001b[39m, \u001b[36madd_theme\u001b[39m, \u001b[36maliases\u001b[39m, \u001b[36malpha\u001b[39m, \u001b[36malphacolor\u001b[39m,\n",
       "  \u001b[36manimate\u001b[39m, \u001b[36mannotate!\u001b[39m, \u001b[36mareaplot\u001b[39m, \u001b[36mareaplot!\u001b[39m, \u001b[36marrow\u001b[39m, \u001b[36mattr!\u001b[39m, \u001b[36mbackend\u001b[39m,\n",
       "  \u001b[36mbackend_name\u001b[39m, \u001b[36mbackend_object\u001b[39m, \u001b[36mbackends\u001b[39m, \u001b[36mbar\u001b[39m, \u001b[36mbar!\u001b[39m, \u001b[36mbarh\u001b[39m, \u001b[36mbarh!\u001b[39m, \u001b[36mbarhist\u001b[39m,\n",
       "  \u001b[36mbarhist!\u001b[39m, \u001b[36mbase_color_type\u001b[39m, \u001b[36mbase_colorant_type\u001b[39m, \u001b[36mbbox\u001b[39m, \u001b[36mblue\u001b[39m, \u001b[36mboxplot\u001b[39m,\n",
       "  \u001b[36mboxplot!\u001b[39m, \u001b[36mbrush\u001b[39m, \u001b[36mccolor\u001b[39m, \u001b[36mcenter\u001b[39m, \u001b[36mcgrad\u001b[39m, \u001b[36mchroma\u001b[39m, \u001b[36mcie_color_match\u001b[39m, \u001b[36mcloseall\u001b[39m,\n",
       "  \u001b[36mcolor\u001b[39m, \u001b[36mcolor_list\u001b[39m, \u001b[36mcolor_type\u001b[39m, \u001b[36mcoloralpha\u001b[39m, \u001b[36mcolordiff\u001b[39m, \u001b[36mcolormap\u001b[39m, \u001b[36mcolormatch\u001b[39m,\n",
       "  \u001b[36mcomp1\u001b[39m, \u001b[36mcomp2\u001b[39m, \u001b[36mcomp3\u001b[39m, \u001b[36mcomp4\u001b[39m, \u001b[36mcomp5\u001b[39m, \u001b[36mcontour\u001b[39m, \u001b[36mcontour!\u001b[39m, \u001b[36mcontour3d\u001b[39m, \u001b[36mcontour3d!\u001b[39m,\n",
       "  \u001b[36mcontourf\u001b[39m, \u001b[36mcontourf!\u001b[39m, \u001b[36mcoords\u001b[39m, \u001b[36mcurrent\u001b[39m, \u001b[36mcurve_points\u001b[39m, \u001b[36mcurves\u001b[39m, \u001b[36mcurves!\u001b[39m, \u001b[36mcvec\u001b[39m,\n",
       "  \u001b[36mdefault\u001b[39m, \u001b[36mdefault_cgrad\u001b[39m, \u001b[36mdensity\u001b[39m, \u001b[36mdensity!\u001b[39m, \u001b[36mdeuteranopic\u001b[39m,\n",
       "  \u001b[36mdistinguishable_colors\u001b[39m, \u001b[36mdiverging_palette\u001b[39m, \u001b[36mfont\u001b[39m, \u001b[36mframe\u001b[39m, \u001b[36mgamutmax\u001b[39m, \u001b[36mgamutmin\u001b[39m,\n",
       "  \u001b[36mgaston\u001b[39m, \u001b[36mget_color_palette\u001b[39m, \u001b[36mgif\u001b[39m, \u001b[36mgr\u001b[39m, \u001b[36mgray\u001b[39m, \u001b[36mgreen\u001b[39m, \u001b[36mgrid\u001b[39m, \u001b[36mgui\u001b[39m, \u001b[36mhdf5\u001b[39m, \u001b[36mheatmap\u001b[39m,\n",
       "  \u001b[36mheatmap!\u001b[39m, \u001b[36mhex\u001b[39m, \u001b[36mhexbin\u001b[39m, \u001b[36mhexbin!\u001b[39m, \u001b[36mhistogram\u001b[39m, \u001b[36mhistogram!\u001b[39m, \u001b[36mhistogram2d\u001b[39m,\n",
       "  \u001b[36mhistogram2d!\u001b[39m, \u001b[36mhline\u001b[39m, \u001b[36mhline!\u001b[39m, \u001b[36mhspan\u001b[39m, \u001b[36mhspan!\u001b[39m, \u001b[36mhue\u001b[39m, \u001b[36minline\u001b[39m, \u001b[36minspectdr\u001b[39m,\n",
       "  \u001b[36minvisible\u001b[39m, \u001b[36misdark\u001b[39m, \u001b[36miter_segments\u001b[39m, \u001b[36mlens!\u001b[39m, \u001b[36mmapc\u001b[39m, \u001b[36mmapreducec\u001b[39m, \u001b[36mmesh3d\u001b[39m, \u001b[36mmesh3d!\u001b[39m,\n",
       "  \u001b[36mmov\u001b[39m, \u001b[36mmp4\u001b[39m, \u001b[36mnormalize_hue\u001b[39m, \u001b[36mohlc\u001b[39m, \u001b[36mohlc!\u001b[39m, \u001b[36moptimize_datetime_ticks\u001b[39m,\n",
       "  \u001b[36moptimize_ticks\u001b[39m, \u001b[36mpalette\u001b[39m, \u001b[36mparametric_colorant\u001b[39m, \u001b[36mpath3d\u001b[39m, \u001b[36mpath3d!\u001b[39m, \u001b[36mpgfplots\u001b[39m,\n",
       "  \u001b[36mpgfplotsx\u001b[39m, \u001b[36mpie\u001b[39m, \u001b[36mpie!\u001b[39m, \u001b[36mplot\u001b[39m, \u001b[36mplot!\u001b[39m, \u001b[36mplot3d\u001b[39m, \u001b[36mplot3d!\u001b[39m, \u001b[36mplot_color\u001b[39m, \u001b[36mplotarea\u001b[39m,\n",
       "  \u001b[36mplotattr\u001b[39m, \u001b[36mplotly\u001b[39m, \u001b[36mplotlyjs\u001b[39m, \u001b[36mplots_heatmap\u001b[39m, \u001b[36mplots_heatmap!\u001b[39m, \u001b[36mpng\u001b[39m,\n",
       "  \u001b[36mportfoliocomposition\u001b[39m, \u001b[36mportfoliocomposition!\u001b[39m, \u001b[36mprotanopic\u001b[39m, \u001b[36mpyplot\u001b[39m, \u001b[36mquiver\u001b[39m,\n",
       "  \u001b[36mquiver!\u001b[39m, \u001b[36mred\u001b[39m, \u001b[36mreducec\u001b[39m, \u001b[36mresetfontsizes\u001b[39m, \u001b[36mrgb_string\u001b[39m, \u001b[36mrgba_string\u001b[39m, \u001b[36mrotate\u001b[39m,\n",
       "  \u001b[36mrotate!\u001b[39m, \u001b[36msavefig\u001b[39m, \u001b[36mscalefontsize\u001b[39m, \u001b[36mscalefontsizes\u001b[39m, \u001b[36mscatter\u001b[39m, \u001b[36mscatter!\u001b[39m,\n",
       "  \u001b[36mscatter3d\u001b[39m, \u001b[36mscatter3d!\u001b[39m, \u001b[36mscatterhist\u001b[39m, \u001b[36mscatterhist!\u001b[39m, \u001b[36msequential_palette\u001b[39m,\n",
       "  \u001b[36mset_theme\u001b[39m, \u001b[36mshape_coords\u001b[39m, \u001b[36mshowtheme\u001b[39m, \u001b[36mshowtheme!\u001b[39m, \u001b[36mspy\u001b[39m, \u001b[36mspy!\u001b[39m, \u001b[36mstephist\u001b[39m,\n",
       "  \u001b[36mstephist!\u001b[39m, \u001b[36msticks\u001b[39m, \u001b[36msticks!\u001b[39m, \u001b[36mstroke\u001b[39m, \u001b[36msurface\u001b[39m, \u001b[36msurface!\u001b[39m, \u001b[36mtest_examples\u001b[39m, \u001b[36mtext\u001b[39m,\n",
       "  \u001b[36mtheme\u001b[39m, \u001b[36mtheme_palette\u001b[39m, \u001b[36mtitle!\u001b[39m, \u001b[36mtranslate\u001b[39m, \u001b[36mtranslate!\u001b[39m, \u001b[36mtritanopic\u001b[39m, \u001b[36mtwinx\u001b[39m,\n",
       "  \u001b[36municodeplots\u001b[39m, \u001b[36mviolin\u001b[39m, \u001b[36mviolin!\u001b[39m, \u001b[36mvline\u001b[39m, \u001b[36mvline!\u001b[39m, \u001b[36mvspan\u001b[39m, \u001b[36mvspan!\u001b[39m, \u001b[36mwebm\u001b[39m,\n",
       "  \u001b[36mweighted_color_mean\u001b[39m, \u001b[36mwhitebalance\u001b[39m, \u001b[36mwireframe\u001b[39m, \u001b[36mwireframe!\u001b[39m, \u001b[36mwith\u001b[39m, \u001b[36mwrap\u001b[39m,\n",
       "  \u001b[36mxaxis!\u001b[39m, \u001b[36mxflip!\u001b[39m, \u001b[36mxgrid!\u001b[39m, \u001b[36mxlabel!\u001b[39m, \u001b[36mxlims\u001b[39m, \u001b[36mxlims!\u001b[39m, \u001b[36mxticks\u001b[39m, \u001b[36mxticks!\u001b[39m, \u001b[36mxyY\u001b[39m, \u001b[36mxyYA\u001b[39m,\n",
       "  \u001b[36myaxis!\u001b[39m, \u001b[36myflip!\u001b[39m, \u001b[36mygrid!\u001b[39m, \u001b[36mylabel!\u001b[39m, \u001b[36mylims\u001b[39m, \u001b[36mylims!\u001b[39m, \u001b[36myticks\u001b[39m, \u001b[36myticks!\u001b[39m, \u001b[36mzlims\u001b[39m,\n",
       "  \u001b[36mzlims!\u001b[39m, \u001b[36mzscale\u001b[39m, \u001b[36mzticks\u001b[39m\n",
       "\n",
       "\u001b[1m  Displaying contents of readme found at\u001b[22m\n",
       "\u001b[1m \u001b[36mD:\\TongYuan\\.julia\\packages\\Plots\\mnkn4\\README.md\u001b[39m\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "\u001b[1m  Plots\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "  [gh-ci-img]:\n",
       "  https://github.com/JuliaPlots/Plots.jl/workflows/ci/badge.svg?branch=master\n",
       "  [gh-ci-url]:\n",
       "  https://github.com/JuliaPlots/Plots.jl/actions?query=workflow%3Aci\n",
       "\n",
       "  [pkgeval-img]:\n",
       "  https://juliaci.github.io/NanosoldierReports/pkgeval\u001b[4mbadges/P/Plots.svg\n",
       "  [pkgeval-url]:\n",
       "  https://juliaci.github.io/NanosoldierReports/pkgeval\u001b[24mbadges/report.html\n",
       "\n",
       "  [gitter-img]: https://badges.gitter.im/tbreloff/Plots.jl.svg [gitter-url]:\n",
       "  https://gitter.im/tbreloff/Plots.jl?utm\u001b[4msource=badge&utm\u001b[24mmedium=badge&utm\u001b[4mcampaign=pr-badge&utm\u001b[24mcontent=badge\n",
       "\n",
       "  [docs-img]: https://img.shields.io/badge/docs-stable-blue.svg [docs-url]:\n",
       "  https://docs.juliaplots.org/stable/\n",
       "\n",
       "  [![][gh-ci-img]][gh-ci-url] [![][pkgeval-img]][pkgeval-url] (Image: project\n",
       "  chat) (https://julialang.zulipchat.com/#narrow/stream/236493-plots)\n",
       "  [![][docs-img]][docs-url] (Image: Codecov)\n",
       "  (https://codecov.io/gh/JuliaPlots/Plots.jl) (Image: Plots Downloads)\n",
       "  (https://pkgs.genieframework.com?packages=Plots)\n",
       "\n",
       "  (Image: DOI) (https://doi.org/10.5281/zenodo.4725317) This is the DOI for\n",
       "  all Versions, please follow the link to get the DOI for a specific version.\n",
       "\n",
       "\u001b[1m  Created by Tom Breloff (@tbreloff)\u001b[22m\n",
       "\u001b[1m  ------------------------------------\u001b[22m\n",
       "\n",
       "\u001b[1m  Maintained by the JuliaPlots members\u001b[22m\n",
       "\u001b[1m (https://github.com/orgs/JuliaPlots/people)\u001b[22m\n",
       "\u001b[1m  ---------------------------------------------\u001b[22m\n",
       "\n",
       "  Plots is a plotting API and toolset. My goals with the package are:\n",
       "\n",
       "    •  \u001b[1mPowerful\u001b[22m. Do more with less. Complex visualizations become easy.\n",
       "\n",
       "    •  \u001b[1mIntuitive\u001b[22m. Stop reading so much documentation. Commands should\n",
       "       \"just work\".\n",
       "\n",
       "    •  \u001b[1mConcise\u001b[22m. Less code means fewer mistakes and more efficient\n",
       "       development/analysis.\n",
       "\n",
       "    •  \u001b[1mFlexible\u001b[22m. Produce your favorite plots from your favorite package,\n",
       "       but quicker and simpler.\n",
       "\n",
       "    •  \u001b[1mConsistent\u001b[22m. Don't commit to one graphics package, use the same\n",
       "       code everywhere.\n",
       "\n",
       "    •  \u001b[1mLightweight\u001b[22m. Very few dependencies.\n",
       "\n",
       "    •  \u001b[1mSmart\u001b[22m. Attempts to figure out what you \u001b[1mwant\u001b[22m it to do... not just\n",
       "       what you \u001b[1mtell\u001b[22m it."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T14:53:38.165000+08:00",
     "start_time": "2022-06-08T06:53:24.683Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1mp\u001b[22m\u001b[0m\u001b[1ml\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mt\u001b[22m \u001b[0m\u001b[1mp\u001b[22m\u001b[0m\u001b[1ml\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mt\u001b[22m! \u001b[0m\u001b[1mp\u001b[22m\u001b[0m\u001b[1ml\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mt\u001b[22mly \u001b[0m\u001b[1mp\u001b[22m\u001b[0m\u001b[1ml\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mt\u001b[22m3d \u001b[0m\u001b[1mP\u001b[22m\u001b[0m\u001b[1ml\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mt\u001b[22ms \u001b[0m\u001b[1mp\u001b[22m\u001b[0m\u001b[1ml\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mt\u001b[22m3d! \u001b[0m\u001b[1mp\u001b[22m\u001b[0m\u001b[1ml\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mt\u001b[22mlyjs \u001b[0m\u001b[1mp\u001b[22m\u001b[0m\u001b[1ml\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mt\u001b[22mattr \u001b[0m\u001b[1mp\u001b[22m\u001b[0m\u001b[1ml\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mt\u001b[22marea\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "The main plot command. Use \\texttt{plot} to create a new plot object, and \\texttt{plot!} to add to an existing one:\n",
       "\n",
       "\\begin{verbatim}\n",
       "    plot(args...; kw...)                  # creates a new plot window, and sets it to be the current\n",
       "    plot!(args...; kw...)                 # adds to the `current`\n",
       "    plot!(plotobj, args...; kw...)        # adds to the plot `plotobj`\n",
       "\\end{verbatim}\n",
       "There are lots of ways to pass in data, and lots of keyword arguments... just try it and it will likely work as expected. When you pass in matrices, it splits by columns. To see the list of available attributes, use the \\texttt{plotattr(attr)} function, where \\texttt{attr} is the symbol \\texttt{:Series}, \\texttt{:Subplot}, \\texttt{:Plot}, or \\texttt{:Axis}. Pass any attribute to \\texttt{plotattr} as a String to look up its docstring, e.g., \\texttt{plotattr(\"seriestype\")}.\n",
       "\n"
      ],
      "text/markdown": [
       "The main plot command. Use `plot` to create a new plot object, and `plot!` to add to an existing one:\n",
       "\n",
       "```\n",
       "    plot(args...; kw...)                  # creates a new plot window, and sets it to be the current\n",
       "    plot!(args...; kw...)                 # adds to the `current`\n",
       "    plot!(plotobj, args...; kw...)        # adds to the plot `plotobj`\n",
       "```\n",
       "\n",
       "There are lots of ways to pass in data, and lots of keyword arguments... just try it and it will likely work as expected. When you pass in matrices, it splits by columns. To see the list of available attributes, use the `plotattr(attr)` function, where `attr` is the symbol `:Series`, `:Subplot`, `:Plot`, or `:Axis`. Pass any attribute to `plotattr` as a String to look up its docstring, e.g., `plotattr(\"seriestype\")`.\n"
      ],
      "text/plain": [
       "  The main plot command. Use \u001b[36mplot\u001b[39m to create a new plot object, and \u001b[36mplot!\u001b[39m to\n",
       "  add to an existing one:\n",
       "\n",
       "\u001b[36m      plot(args...; kw...)                  # creates a new plot window, and sets it to be the current\u001b[39m\n",
       "\u001b[36m      plot!(args...; kw...)                 # adds to the `current`\u001b[39m\n",
       "\u001b[36m      plot!(plotobj, args...; kw...)        # adds to the plot `plotobj`\u001b[39m\n",
       "\n",
       "  There are lots of ways to pass in data, and lots of keyword arguments...\n",
       "  just try it and it will likely work as expected. When you pass in matrices,\n",
       "  it splits by columns. To see the list of available attributes, use the\n",
       "  \u001b[36mplotattr(attr)\u001b[39m function, where \u001b[36mattr\u001b[39m is the symbol \u001b[36m:Series\u001b[39m, \u001b[36m:Subplot\u001b[39m, \u001b[36m:Plot\u001b[39m,\n",
       "  or \u001b[36m:Axis\u001b[39m. Pass any attribute to \u001b[36mplotattr\u001b[39m as a String to look up its\n",
       "  docstring, e.g., \u001b[36mplotattr(\"seriestype\")\u001b[39m."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T14:53:38.183000+08:00",
     "start_time": "2022-06-08T06:53:24.684Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mc\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1mr\u001b[22m \u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mc\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1mr\u001b[22m! \u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mc\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1mr\u001b[22m3d \u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mc\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1mr\u001b[22mmat \u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mc\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1mr\u001b[22m3d! \u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mc\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1mr\u001b[22mmatm \u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mc\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1mr\u001b[22mhist\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "scatter(x,y)\n",
       "scatter!(x,y)\n",
       "\\end{verbatim}\n",
       "Make a scatter plot of y vs x.\n",
       "\n",
       "\\section{Examples}\n",
       "\\begin{verbatim}\n",
       "julia> scatter([1,2,3],[4,5,6],markersize=[3,4,5],markercolor=[:red,:green,:blue])\n",
       "julia> scatter([(1,4),(2,5),(3,6)])\n",
       "\\end{verbatim}\n"
      ],
      "text/markdown": [
       "```\n",
       "scatter(x,y)\n",
       "scatter!(x,y)\n",
       "```\n",
       "\n",
       "Make a scatter plot of y vs x.\n",
       "\n",
       "# Examples\n",
       "\n",
       "```julia-repl\n",
       "julia> scatter([1,2,3],[4,5,6],markersize=[3,4,5],markercolor=[:red,:green,:blue])\n",
       "julia> scatter([(1,4),(2,5),(3,6)])\n",
       "```\n"
      ],
      "text/plain": [
       "\u001b[36m  scatter(x,y)\u001b[39m\n",
       "\u001b[36m  scatter!(x,y)\u001b[39m\n",
       "\n",
       "  Make a scatter plot of y vs x.\n",
       "\n",
       "\u001b[1m  Examples\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "\u001b[36m  julia> scatter([1,2,3],[4,5,6],markersize=[3,4,5],markercolor=[:red,:green,:blue])\u001b[39m\n",
       "\u001b[36m  julia> scatter([(1,4),(2,5),(3,6)])\u001b[39m"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?scatter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-05T17:11:23.286000+08:00",
     "start_time": "2022-07-05T09:11:22.309Z"
    }
   },
   "outputs": [],
   "source": [
    "using MLJ  #机器学习库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T16:09:42.015000+08:00",
     "start_time": "2022-06-13T08:09:41.656Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1mM\u001b[22m\u001b[0m\u001b[1mL\u001b[22m\u001b[0m\u001b[1mJ\u001b[22m \u001b[0m\u001b[1mM\u001b[22m\u001b[0m\u001b[1mL\u001b[22m\u001b[0m\u001b[1mJ\u001b[22mOpenML \u001b[0m\u001b[1mM\u001b[22m\u001b[0m\u001b[1mL\u001b[22m\u001b[0m\u001b[1mJ\u001b[22m_VERSION \u001b[0m\u001b[1mM\u001b[22m\u001b[0m\u001b[1mL\u001b[22m\u001b[0m\u001b[1mJ\u001b[22mIteration l2_\u001b[0m\u001b[1mm\u001b[22margin_\u001b[0m\u001b[1ml\u001b[22moss dwd_\u001b[0m\u001b[1mm\u001b[22margin_\u001b[0m\u001b[1ml\u001b[22moss\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "No docstring found for module \\texttt{MLJ}.\n",
       "\n",
       "\\section{Exported names}\n",
       "\\texttt{@constant}, \\texttt{@from\\_network}, \\texttt{@iload}, \\texttt{@load}, \\texttt{@load\\_ames}, \\texttt{@load\\_boston}, \\texttt{@load\\_crabs}, \\texttt{@load\\_iris}, \\texttt{@load\\_reduced\\_ames}, \\texttt{@more}, \\texttt{@node}, \\texttt{@pipeline}, \\texttt{AUC}, \\texttt{AbstractNode}, \\texttt{Accuracy}, \\texttt{Annotator}, \\texttt{AnnotatorComposite}, \\texttt{AnnotatorSurrogate}, \\texttt{AreaUnderCurve}, \\texttt{BAC}, \\texttt{BACC}, \\texttt{BalancedAccuracy}, \\texttt{Binary}, \\texttt{BinaryThresholdPredictor}, \\texttt{BrierLoss}, \\texttt{BrierScore}, \\texttt{CPU1}, \\texttt{CPUProcesses}, \\texttt{CPUThreads}, \\texttt{CV}, \\texttt{Callback}, \\texttt{ColorImage}, \\texttt{Composite}, \\texttt{ConfusionMatrix}, \\texttt{ConstantClassifier}, \\texttt{ConstantRegressor}, \\texttt{Continuous}, \\texttt{ContinuousEncoder}, \\texttt{Count}, \\texttt{CrossEntropy}, \\texttt{CycleLearningRate}, \\texttt{DWDMarginLoss}, \\texttt{Data}, \\texttt{Deterministic}, \\texttt{DeterministicComposite}, \\texttt{DeterministicNetwork}, \\texttt{DeterministicSupervisedDetector}, \\texttt{DeterministicSupervisedDetectorComposite}, \\texttt{DeterministicSupervisedDetectorSurrogate}, \\texttt{DeterministicSurrogate}, \\texttt{DeterministicUnsupervisedDetector}, \\texttt{DeterministicUnsupervisedDetectorComposite}, \\texttt{DeterministicUnsupervisedDetectorSurrogate}, \\texttt{Disjunction}, \\texttt{EnsembleModel}, \\texttt{Error}, \\texttt{ExpLoss}, \\texttt{Explicit}, \\texttt{FDR}, \\texttt{FNR}, \\texttt{FPR}, \\texttt{FScore}, \\texttt{FalseDiscoveryRate}, \\texttt{FalseNegative}, \\texttt{FalseNegativeRate}, \\texttt{FalsePositive}, \\texttt{FalsePositiveRate}, \\texttt{FeatureSelector}, \\texttt{FillImputer}, \\texttt{Finite}, \\texttt{Found}, \\texttt{GL}, \\texttt{GrayImage}, \\texttt{Grid}, \\texttt{HANDLE\\_GIVEN\\_ID}, \\texttt{Holdout}, \\texttt{HuberLoss}, \\texttt{Image}, \\texttt{Infinite}, \\texttt{Info}, \\texttt{Interval}, \\texttt{IntervalComposite}, \\texttt{IntervalSurrogate}, \\texttt{InvalidValue}, \\texttt{IteratedModel}, \\texttt{IterationControl}, \\texttt{JointProbabilistic}, \\texttt{JointProbabilisticComposite}, \\texttt{JointProbabilisticSurrogate}, \\texttt{Kappa}, \\texttt{Known}, \\texttt{L1EpsilonInsLoss}, \\texttt{L1HingeLoss}, \\texttt{L2EpsilonInsLoss}, \\texttt{L2HingeLoss}, \\texttt{L2MarginLoss}, \\texttt{LPDistLoss}, \\texttt{LPLoss}, \\texttt{LatinHypercube}, \\texttt{LogCosh}, \\texttt{LogCoshLoss}, \\texttt{LogLoss}, \\texttt{LogScore}, \\texttt{LogitDistLoss}, \\texttt{LogitMarginLoss}, \\texttt{MAE}, \\texttt{MAPE}, \\texttt{MAV}, \\texttt{MCC}, \\texttt{MCR}, \\texttt{MFDR}, \\texttt{MFNR}, \\texttt{MFPR}, \\texttt{MLJIteration}, \\texttt{MLJOpenML}, \\texttt{MLJ\\_VERSION}, \\texttt{MNPV}, \\texttt{MPPV}, \\texttt{MTNR}, \\texttt{MTPR}, \\texttt{Machine}, \\texttt{MatthewsCorrelation}, \\texttt{MeanAbsoluteError}, \\texttt{MeanAbsoluteProportionalError}, \\texttt{MisclassificationRate}, \\texttt{ModifiedHuberLoss}, \\texttt{Multiclass}, \\texttt{MulticlassFScore}, \\texttt{MulticlassFalseDiscoveryRate}, \\texttt{MulticlassFalseNegative}, \\texttt{MulticlassFalseNegativeRate}, \\texttt{MulticlassFalsePositive}, \\texttt{MulticlassFalsePositiveRate}, \\texttt{MulticlassNegativePredictiveValue}, \\texttt{MulticlassPrecision}, \\texttt{MulticlassRecall}, \\texttt{MulticlassSpecificity}, \\texttt{MulticlassTrueNegative}, \\texttt{MulticlassTrueNegativeRate}, \\texttt{MulticlassTruePositive}, \\texttt{MulticlassTruePositiveRate}, \\texttt{NPV}, \\texttt{NegativePredictiveValue}, \\texttt{Never}, \\texttt{Node}, \\texttt{NotANumber}, \\texttt{NumberLimit}, \\texttt{NumberSinceBest}, \\texttt{OneHotEncoder}, \\texttt{OpenML}, \\texttt{OrderedFactor}, \\texttt{PPV}, \\texttt{PQ}, \\texttt{Patience}, \\texttt{PerceptronLoss}, \\texttt{PeriodicLoss}, \\texttt{Pipeline}, \\texttt{Precision}, \\texttt{Probabilistic}, \\texttt{ProbabilisticComposite}, \\texttt{ProbabilisticNetwork}, \\texttt{ProbabilisticSupervisedDetector}, \\texttt{ProbabilisticSupervisedDetectorComposite}, \\texttt{ProbabilisticSupervisedDetectorSurrogate}, \\texttt{ProbabilisticSurrogate}, \\texttt{ProbabilisticUnsupervisedDetector}, \\texttt{ProbabilisticUnsupervisedDetectorComposite}, \\texttt{ProbabilisticUnsupervisedDetectorSurrogate}, \\texttt{QuantileLoss}, \\texttt{RMS}, \\texttt{RMSL}, \\texttt{RMSLP}, \\texttt{RMSP}, \\texttt{RMSPV}, \\texttt{RSQ}, \\texttt{RSquared}, \\texttt{RandomSearch}, \\texttt{Recall}, \\texttt{Resampler}, \\texttt{ResamplingStrategy}, \\texttt{RootMeanSquaredError}, \\texttt{RootMeanSquaredLogError}, \\texttt{RootMeanSquaredLogProportionalError}, \\texttt{RootMeanSquaredProportionalError}, \\texttt{Save}, \\texttt{Scientific}, \\texttt{SigmoidLoss}, \\texttt{SmoothedL1HingeLoss}, \\texttt{Specificity}, \\texttt{SphericalScore}, \\texttt{Stack}, \\texttt{Standardizer}, \\texttt{Static}, \\texttt{StaticComposite}, \\texttt{StaticSurrogate}, \\texttt{Step}, \\texttt{StratifiedCV}, \\texttt{Supervised}, \\texttt{SupervisedAnnotator}, \\texttt{SupervisedAnnotatorComposite}, \\texttt{SupervisedAnnotatorSurrogate}, \\texttt{SupervisedComposite}, \\texttt{SupervisedDetector}, \\texttt{SupervisedDetectorComposite}, \\texttt{SupervisedDetectorSurrogate}, \\texttt{SupervisedSurrogate}, \\texttt{Surrogate}, \\texttt{TNR}, \\texttt{TPR}, \\texttt{Table}, \\texttt{Textual}, \\texttt{Threshold}, \\texttt{TimeLimit}, \\texttt{TimeSeriesCV}, \\texttt{TransformedTargetModel}, \\texttt{TrueNegative}, \\texttt{TrueNegativeRate}, \\texttt{TruePositive}, \\texttt{TruePositiveRate}, \\texttt{TunedModel}, \\texttt{UnivariateBoxCoxTransformer}, \\texttt{UnivariateDiscretizer}, \\texttt{UnivariateFinite}, \\texttt{UnivariateStandardizer}, \\texttt{UnivariateTimeTypeToContinuous}, \\texttt{Unknown}, \\texttt{Unsupervised}, \\texttt{UnsupervisedAnnotator}, \\texttt{UnsupervisedAnnotatorComposite}, \\texttt{UnsupervisedAnnotatorSurrogate}, \\texttt{UnsupervisedComposite}, \\texttt{UnsupervisedDetector}, \\texttt{UnsupervisedDetectorComposite}, \\texttt{UnsupervisedDetectorSurrogate}, \\texttt{UnsupervisedNetwork}, \\texttt{UnsupervisedSurrogate}, \\texttt{Warmup}, \\texttt{Warn}, \\texttt{WithEvaluationDo}, \\texttt{WithFittedParamsDo}, \\texttt{WithIterationsDo}, \\texttt{WithLossDo}, \\texttt{WithMachineDo}, \\texttt{WithModelDo}, \\texttt{WithNumberDo}, \\texttt{WithReportDo}, \\texttt{WithTrainingLossesDo}, \\texttt{ZeroOneLoss}, \\texttt{accuracy}, \\texttt{aggregate}, \\texttt{aggregation}, \\texttt{anonymize!}, \\texttt{area\\_under\\_curve}, \\texttt{auc}, \\texttt{autotype}, \\texttt{bac}, \\texttt{bacc}, \\texttt{balanced\\_accuracy}, \\texttt{brier\\_loss}, \\texttt{brier\\_score}, \\texttt{categorical}, \\texttt{classes}, \\texttt{coerce}, \\texttt{coerce!}, \\texttt{color\\_off}, \\texttt{color\\_on}, \\texttt{complement}, \\texttt{confmat}, \\texttt{confusion\\_matrix}, \\texttt{corestrict}, \\texttt{cross\\_entropy}, \\texttt{decoder}, \\texttt{deep\\_properties}, \\texttt{default\\_measure}, \\texttt{default\\_resource}, \\texttt{distribution\\_type}, \\texttt{docstring}, \\texttt{dwd\\_margin\\_loss}, \\texttt{elscitype}, \\texttt{evaluate}, \\texttt{evaluate!}, \\texttt{exp\\_loss}, \\texttt{f1score}, \\texttt{fallout}, \\texttt{false\\_discovery\\_rate}, \\texttt{false\\_negative}, \\texttt{false\\_negative\\_rate}, \\texttt{false\\_positive}, \\texttt{false\\_positive\\_rate}, \\texttt{falsediscovery\\_rate}, \\texttt{falsenegative}, \\texttt{falsenegative\\_rate}, \\texttt{falsepositive}, \\texttt{falsepositive\\_rate}, \\texttt{fdr}, \\texttt{fit!}, \\texttt{fit\\_data\\_scitype}, \\texttt{fit\\_only!}, \\texttt{fitresults}, \\texttt{fitted\\_params}, \\texttt{fnr}, \\texttt{fpr}, \\texttt{freeze!}, \\texttt{hit\\_rate}, \\texttt{huber\\_loss}, \\texttt{human\\_name}, \\texttt{hyperparameter\\_types}, \\texttt{hyperparameters}, \\texttt{info}, \\texttt{input\\_scitype}, \\texttt{int}, \\texttt{inverse\\_transform}, \\texttt{inverse\\_transform\\_scitype}, \\texttt{is\\_feature\\_dependent}, \\texttt{is\\_pure\\_julia}, \\texttt{is\\_supervised}, \\texttt{is\\_wrapper}, \\texttt{iteration\\_parameter}, \\texttt{iterator}, \\texttt{kappa}, \\texttt{l1}, \\texttt{l1\\_epsilon\\_ins\\_loss}, \\texttt{l1\\_hinge\\_loss}, \\texttt{l2}, \\texttt{l2\\_epsilon\\_ins\\_loss}, \\texttt{l2\\_hinge\\_loss}, \\texttt{l2\\_margin\\_loss}, \\texttt{learning\\_curve}, \\texttt{learning\\_curve!}, \\texttt{levels}, \\texttt{levels!}, \\texttt{load}, \\texttt{load\\_ames}, \\texttt{load\\_boston}, \\texttt{load\\_crabs}, \\texttt{load\\_iris}, \\texttt{load\\_path}, \\texttt{load\\_reduced\\_ames}, \\texttt{localmodels}, \\texttt{log\\_cosh}, \\texttt{log\\_cosh\\_loss}, \\texttt{log\\_loss}, \\texttt{log\\_score}, \\texttt{logit\\_dist\\_loss}, \\texttt{logit\\_margin\\_loss}, \\texttt{logpdf}, \\texttt{lp\\_dist\\_loss}, \\texttt{machine}, \\texttt{machines}, \\texttt{macro\\_avg}, \\texttt{macro\\_f1score}, \\texttt{mae}, \\texttt{make\\_blobs}, \\texttt{make\\_circles}, \\texttt{make\\_moons}, \\texttt{make\\_regression}, \\texttt{mape}, \\texttt{matching}, \\texttt{matthews\\_correlation}, \\texttt{mav}, \\texttt{mcc}, \\texttt{mcr}, \\texttt{mean}, \\texttt{mean\\_absolute\\_error}, \\texttt{mean\\_absolute\\_value}, \\texttt{measures}, \\texttt{median}, \\texttt{micro\\_avg}, \\texttt{micro\\_f1score}, \\texttt{misclassification\\_rate}, \\texttt{miss\\_rate}, \\texttt{mode}, \\texttt{models}, \\texttt{modified\\_huber\\_loss}, \\texttt{multiclass\\_f1score}, \\texttt{multiclass\\_fallout}, \\texttt{multiclass\\_false\\_discovery\\_rate}, \\texttt{multiclass\\_false\\_negative}, \\texttt{multiclass\\_false\\_negative\\_rate}, \\texttt{multiclass\\_false\\_positive}, \\texttt{multiclass\\_false\\_positive\\_rate}, \\texttt{multiclass\\_falsediscovery\\_rate}, \\texttt{multiclass\\_falsenegative}, \\texttt{multiclass\\_falsenegative\\_rate}, \\texttt{multiclass\\_falsepositive}, \\texttt{multiclass\\_falsepositive\\_rate}, \\texttt{multiclass\\_fdr}, \\texttt{multiclass\\_fnr}, \\texttt{multiclass\\_fpr}, \\texttt{multiclass\\_hit\\_rate}, \\texttt{multiclass\\_miss\\_rate}, \\texttt{multiclass\\_negative\\_predictive\\_value}, \\texttt{multiclass\\_negativepredictive\\_value}, \\texttt{multiclass\\_npv}, \\texttt{multiclass\\_positive\\_predictive\\_value}, \\texttt{multiclass\\_positivepredictive\\_value}, \\texttt{multiclass\\_ppv}, \\texttt{multiclass\\_precision}, \\texttt{multiclass\\_recall}, \\texttt{multiclass\\_selectivity}, \\texttt{multiclass\\_sensitivity}, \\texttt{multiclass\\_specificity}, \\texttt{multiclass\\_tnr}, \\texttt{multiclass\\_tpr}, \\texttt{multiclass\\_true\\_negative}, \\texttt{multiclass\\_true\\_negative\\_rate}, \\texttt{multiclass\\_true\\_positive}, \\texttt{multiclass\\_true\\_positive\\_rate}, \\texttt{multiclass\\_truenegative}, \\texttt{multiclass\\_truenegative\\_rate}, \\texttt{multiclass\\_truepositive}, \\texttt{multiclass\\_truepositive\\_rate}, \\texttt{negative\\_predictive\\_value}, \\texttt{negativepredictive\\_value}, \\texttt{no\\_avg}, \\texttt{node}, \\texttt{nonmissing}, \\texttt{npv}, \\texttt{nrows}, \\texttt{orientation}, \\texttt{origins}, \\texttt{output\\_scitype}, \\texttt{package\\_license}, \\texttt{package\\_name}, \\texttt{package\\_url}, \\texttt{package\\_uuid}, \\texttt{params}, \\texttt{partition}, \\texttt{pdf}, \\texttt{perceptron\\_loss}, \\texttt{periodic\\_loss}, \\texttt{positive\\_predictive\\_value}, \\texttt{positivepredictive\\_value}, \\texttt{ppv}, \\texttt{precision}, \\texttt{predict}, \\texttt{predict\\_joint}, \\texttt{predict\\_mean}, \\texttt{predict\\_median}, \\texttt{predict\\_mode}, \\texttt{predict\\_scitype}, \\texttt{prediction\\_type}, \\texttt{pretty}, \\texttt{quantile\\_loss}, \\texttt{rebind!}, \\texttt{recall}, \\texttt{report}, \\texttt{reports\\_each\\_observation}, \\texttt{restrict}, \\texttt{return!}, \\texttt{rms}, \\texttt{rmse}, \\texttt{rmsl}, \\texttt{rmsle}, \\texttt{rmslp1}, \\texttt{rmsp}, \\texttt{roc}, \\texttt{roc\\_curve}, \\texttt{root\\_mean\\_squared\\_error}, \\texttt{root\\_mean\\_squared\\_log\\_error}, \\texttt{rsq}, \\texttt{rsquared}, \\texttt{sampler}, \\texttt{schema}, \\texttt{scitype}, \\texttt{scitype\\_union}, \\texttt{selectcols}, \\texttt{selectivity}, \\texttt{selectrows}, \\texttt{sensitivity}, \\texttt{shuffle}, \\texttt{shuffle!}, \\texttt{sigmoid\\_loss}, \\texttt{skipinvalid}, \\texttt{smoothed\\_l1\\_hinge\\_loss}, \\texttt{source}, \\texttt{sources}, \\texttt{specificity}, \\texttt{spherical\\_score}, \\texttt{std}, \\texttt{support}, \\texttt{supports\\_class\\_weights}, \\texttt{supports\\_training\\_losses}, \\texttt{supports\\_weights}, \\texttt{table}, \\texttt{target\\_scitype}, \\texttt{thaw!}, \\texttt{tnr}, \\texttt{tpr}, \\texttt{training\\_losses}, \\texttt{trait}, \\texttt{transform}, \\texttt{transform\\_scitype}, \\texttt{true\\_negative}, \\texttt{true\\_negative\\_rate}, \\texttt{true\\_positive}, \\texttt{true\\_positive\\_rate}, \\texttt{truenegative}, \\texttt{truenegative\\_rate}, \\texttt{truepositive}, \\texttt{truepositive\\_rate}, \\texttt{unpack}, \\texttt{value}, \\texttt{zero\\_one\\_loss}\n",
       "\n",
       "\\section{Displaying contents of readme found at \\texttt{D:{\\textbackslash}TongYuan{\\textbackslash}.julia{\\textbackslash}packages{\\textbackslash}MLJ{\\textbackslash}hw6oD{\\textbackslash}README.md}}\n",
       "<div align=\"center\">     <img src=\"material/MLJLogo2.svg\" alt=\"MLJ\" width=\"200\"> </div>\n",
       "\n",
       "<h2 align=\"center\">A Machine Learning Framework for Julia <p align=\"center\">   <a href=\"https://github.com/alan-turing-institute/MLJ.jl/actions\">     <img src=\"https://github.com/alan-turing-institute/MLJ.jl/workflows/CI/badge.svg\"          alt=\"Build Status\">   </a>   <a href=\"https://alan-turing-institute.github.io/MLJ.jl/dev/\">     <img src=\"https://img.shields.io/badge/docs-stable-blue.svg\"          alt=\"Documentation\">   </a>   <a href=\"https://opensource.org/licenses/MIT\">     <img src=\"https://img.shields.io/badge/License-MIT-yelllow\"        alt=\"bibtex\">   </a>   <a href=\"BIBLIOGRAPHY.md\">     <img src=\"https://img.shields.io/badge/cite-BibTeX-blue\"        alt=\"bibtex\">   </a>\n",
       "\n",
       "</p> </h2>\n",
       "\n",
       "MLJ (Machine Learning in Julia) is a toolbox written in Julia providing a common interface and meta-algorithms for selecting, tuning, evaluating, composing and comparing over \\href{https://alan-turing-institute.github.io/MLJ.jl/dev/list_of_supported_models/}{160 machine learning models} written in Julia and other languages.\n",
       "\n",
       "\\textbf{New to MLJ?} Start \\href{https://alan-turing-institute.github.io/MLJ.jl/dev/}{here}.\n",
       "\n",
       "\\textbf{Integrating an existing machine learning model into the MLJ framework?} Start \\href{https://alan-turing-institute.github.io/MLJ.jl/dev/quick_start_guide_to_adding_models/}{here}.\n",
       "\n",
       "MLJ was initially created as a Tools, Practices and Systems project at the \\href{https://www.turing.ac.uk/}{Alan Turing Institute} in 2019. Current funding is provided by a \\href{https://www.mbie.govt.nz/science-and-technology/science-and-innovation/funding-information-and-opportunities/investment-funds/strategic-science-investment-fund/ssif-funded-programmes/university-of-auckland/}{New Zealand Strategic Science Investment Fund} awarded to the University of Auckland.\n",
       "\n",
       "MLJ been developed with the support of the following organizations:\n",
       "\n",
       "<div align=\"center\">     <img src=\"material/Turing\\emph{logo.png\" width = 100/>     <img src=\"material/UoA}logo.png\" width = 100/>     <img src=\"material/IQVIA\\_logo.png\" width = 100/>     <img src=\"material/warwick.png\" width = 100/>     <img src=\"material/julia.png\" width = 100/> </div>\n",
       "\n",
       "\\subsubsection{The MLJ Universe}\n",
       "The functionality of MLJ is distributed over a number of repositories illustrated in the dependency chart below. These repositories live at the \\href{https://github.com/JuliaAI}{JuliaAI} umbrella organization.\n",
       "\n",
       "<div align=\"center\">     <img src=\"material/MLJ\\_stack.svg\" alt=\"Dependency Chart\"> </div>\n",
       "\n",
       "\\emph{Dependency chart for MLJ repositories. Repositories with dashed connections do not currently exist but are planned/proposed.}\n",
       "\n",
       "<br> <p align=\"center\"> <a href=\"CONTRIBUTING.md\">Contributing</a> \\&nbsp;•\\&nbsp;  <a href=\"ORGANIZATION.md\">Code Organization</a> \\&nbsp;•\\&nbsp; <a href=\"ROADMAP.md\">Road Map</a>  </br>\n",
       "\n",
       "\\paragraph{Contributors}\n",
       "\\emph{Core design}: A. Blaom, F. Kiraly, S. Vollmer\n",
       "\n",
       "\\emph{Lead contributor}: A. Blaom\n",
       "\n",
       "\\emph{Active maintainers}: A. Blaom, S. Okon, T. Lienart, D. Aluthge\n",
       "\n"
      ],
      "text/markdown": [
       "No docstring found for module `MLJ`.\n",
       "\n",
       "# Exported names\n",
       "\n",
       "`@constant`, `@from_network`, `@iload`, `@load`, `@load_ames`, `@load_boston`, `@load_crabs`, `@load_iris`, `@load_reduced_ames`, `@more`, `@node`, `@pipeline`, `AUC`, `AbstractNode`, `Accuracy`, `Annotator`, `AnnotatorComposite`, `AnnotatorSurrogate`, `AreaUnderCurve`, `BAC`, `BACC`, `BalancedAccuracy`, `Binary`, `BinaryThresholdPredictor`, `BrierLoss`, `BrierScore`, `CPU1`, `CPUProcesses`, `CPUThreads`, `CV`, `Callback`, `ColorImage`, `Composite`, `ConfusionMatrix`, `ConstantClassifier`, `ConstantRegressor`, `Continuous`, `ContinuousEncoder`, `Count`, `CrossEntropy`, `CycleLearningRate`, `DWDMarginLoss`, `Data`, `Deterministic`, `DeterministicComposite`, `DeterministicNetwork`, `DeterministicSupervisedDetector`, `DeterministicSupervisedDetectorComposite`, `DeterministicSupervisedDetectorSurrogate`, `DeterministicSurrogate`, `DeterministicUnsupervisedDetector`, `DeterministicUnsupervisedDetectorComposite`, `DeterministicUnsupervisedDetectorSurrogate`, `Disjunction`, `EnsembleModel`, `Error`, `ExpLoss`, `Explicit`, `FDR`, `FNR`, `FPR`, `FScore`, `FalseDiscoveryRate`, `FalseNegative`, `FalseNegativeRate`, `FalsePositive`, `FalsePositiveRate`, `FeatureSelector`, `FillImputer`, `Finite`, `Found`, `GL`, `GrayImage`, `Grid`, `HANDLE_GIVEN_ID`, `Holdout`, `HuberLoss`, `Image`, `Infinite`, `Info`, `Interval`, `IntervalComposite`, `IntervalSurrogate`, `InvalidValue`, `IteratedModel`, `IterationControl`, `JointProbabilistic`, `JointProbabilisticComposite`, `JointProbabilisticSurrogate`, `Kappa`, `Known`, `L1EpsilonInsLoss`, `L1HingeLoss`, `L2EpsilonInsLoss`, `L2HingeLoss`, `L2MarginLoss`, `LPDistLoss`, `LPLoss`, `LatinHypercube`, `LogCosh`, `LogCoshLoss`, `LogLoss`, `LogScore`, `LogitDistLoss`, `LogitMarginLoss`, `MAE`, `MAPE`, `MAV`, `MCC`, `MCR`, `MFDR`, `MFNR`, `MFPR`, `MLJIteration`, `MLJOpenML`, `MLJ_VERSION`, `MNPV`, `MPPV`, `MTNR`, `MTPR`, `Machine`, `MatthewsCorrelation`, `MeanAbsoluteError`, `MeanAbsoluteProportionalError`, `MisclassificationRate`, `ModifiedHuberLoss`, `Multiclass`, `MulticlassFScore`, `MulticlassFalseDiscoveryRate`, `MulticlassFalseNegative`, `MulticlassFalseNegativeRate`, `MulticlassFalsePositive`, `MulticlassFalsePositiveRate`, `MulticlassNegativePredictiveValue`, `MulticlassPrecision`, `MulticlassRecall`, `MulticlassSpecificity`, `MulticlassTrueNegative`, `MulticlassTrueNegativeRate`, `MulticlassTruePositive`, `MulticlassTruePositiveRate`, `NPV`, `NegativePredictiveValue`, `Never`, `Node`, `NotANumber`, `NumberLimit`, `NumberSinceBest`, `OneHotEncoder`, `OpenML`, `OrderedFactor`, `PPV`, `PQ`, `Patience`, `PerceptronLoss`, `PeriodicLoss`, `Pipeline`, `Precision`, `Probabilistic`, `ProbabilisticComposite`, `ProbabilisticNetwork`, `ProbabilisticSupervisedDetector`, `ProbabilisticSupervisedDetectorComposite`, `ProbabilisticSupervisedDetectorSurrogate`, `ProbabilisticSurrogate`, `ProbabilisticUnsupervisedDetector`, `ProbabilisticUnsupervisedDetectorComposite`, `ProbabilisticUnsupervisedDetectorSurrogate`, `QuantileLoss`, `RMS`, `RMSL`, `RMSLP`, `RMSP`, `RMSPV`, `RSQ`, `RSquared`, `RandomSearch`, `Recall`, `Resampler`, `ResamplingStrategy`, `RootMeanSquaredError`, `RootMeanSquaredLogError`, `RootMeanSquaredLogProportionalError`, `RootMeanSquaredProportionalError`, `Save`, `Scientific`, `SigmoidLoss`, `SmoothedL1HingeLoss`, `Specificity`, `SphericalScore`, `Stack`, `Standardizer`, `Static`, `StaticComposite`, `StaticSurrogate`, `Step`, `StratifiedCV`, `Supervised`, `SupervisedAnnotator`, `SupervisedAnnotatorComposite`, `SupervisedAnnotatorSurrogate`, `SupervisedComposite`, `SupervisedDetector`, `SupervisedDetectorComposite`, `SupervisedDetectorSurrogate`, `SupervisedSurrogate`, `Surrogate`, `TNR`, `TPR`, `Table`, `Textual`, `Threshold`, `TimeLimit`, `TimeSeriesCV`, `TransformedTargetModel`, `TrueNegative`, `TrueNegativeRate`, `TruePositive`, `TruePositiveRate`, `TunedModel`, `UnivariateBoxCoxTransformer`, `UnivariateDiscretizer`, `UnivariateFinite`, `UnivariateStandardizer`, `UnivariateTimeTypeToContinuous`, `Unknown`, `Unsupervised`, `UnsupervisedAnnotator`, `UnsupervisedAnnotatorComposite`, `UnsupervisedAnnotatorSurrogate`, `UnsupervisedComposite`, `UnsupervisedDetector`, `UnsupervisedDetectorComposite`, `UnsupervisedDetectorSurrogate`, `UnsupervisedNetwork`, `UnsupervisedSurrogate`, `Warmup`, `Warn`, `WithEvaluationDo`, `WithFittedParamsDo`, `WithIterationsDo`, `WithLossDo`, `WithMachineDo`, `WithModelDo`, `WithNumberDo`, `WithReportDo`, `WithTrainingLossesDo`, `ZeroOneLoss`, `accuracy`, `aggregate`, `aggregation`, `anonymize!`, `area_under_curve`, `auc`, `autotype`, `bac`, `bacc`, `balanced_accuracy`, `brier_loss`, `brier_score`, `categorical`, `classes`, `coerce`, `coerce!`, `color_off`, `color_on`, `complement`, `confmat`, `confusion_matrix`, `corestrict`, `cross_entropy`, `decoder`, `deep_properties`, `default_measure`, `default_resource`, `distribution_type`, `docstring`, `dwd_margin_loss`, `elscitype`, `evaluate`, `evaluate!`, `exp_loss`, `f1score`, `fallout`, `false_discovery_rate`, `false_negative`, `false_negative_rate`, `false_positive`, `false_positive_rate`, `falsediscovery_rate`, `falsenegative`, `falsenegative_rate`, `falsepositive`, `falsepositive_rate`, `fdr`, `fit!`, `fit_data_scitype`, `fit_only!`, `fitresults`, `fitted_params`, `fnr`, `fpr`, `freeze!`, `hit_rate`, `huber_loss`, `human_name`, `hyperparameter_types`, `hyperparameters`, `info`, `input_scitype`, `int`, `inverse_transform`, `inverse_transform_scitype`, `is_feature_dependent`, `is_pure_julia`, `is_supervised`, `is_wrapper`, `iteration_parameter`, `iterator`, `kappa`, `l1`, `l1_epsilon_ins_loss`, `l1_hinge_loss`, `l2`, `l2_epsilon_ins_loss`, `l2_hinge_loss`, `l2_margin_loss`, `learning_curve`, `learning_curve!`, `levels`, `levels!`, `load`, `load_ames`, `load_boston`, `load_crabs`, `load_iris`, `load_path`, `load_reduced_ames`, `localmodels`, `log_cosh`, `log_cosh_loss`, `log_loss`, `log_score`, `logit_dist_loss`, `logit_margin_loss`, `logpdf`, `lp_dist_loss`, `machine`, `machines`, `macro_avg`, `macro_f1score`, `mae`, `make_blobs`, `make_circles`, `make_moons`, `make_regression`, `mape`, `matching`, `matthews_correlation`, `mav`, `mcc`, `mcr`, `mean`, `mean_absolute_error`, `mean_absolute_value`, `measures`, `median`, `micro_avg`, `micro_f1score`, `misclassification_rate`, `miss_rate`, `mode`, `models`, `modified_huber_loss`, `multiclass_f1score`, `multiclass_fallout`, `multiclass_false_discovery_rate`, `multiclass_false_negative`, `multiclass_false_negative_rate`, `multiclass_false_positive`, `multiclass_false_positive_rate`, `multiclass_falsediscovery_rate`, `multiclass_falsenegative`, `multiclass_falsenegative_rate`, `multiclass_falsepositive`, `multiclass_falsepositive_rate`, `multiclass_fdr`, `multiclass_fnr`, `multiclass_fpr`, `multiclass_hit_rate`, `multiclass_miss_rate`, `multiclass_negative_predictive_value`, `multiclass_negativepredictive_value`, `multiclass_npv`, `multiclass_positive_predictive_value`, `multiclass_positivepredictive_value`, `multiclass_ppv`, `multiclass_precision`, `multiclass_recall`, `multiclass_selectivity`, `multiclass_sensitivity`, `multiclass_specificity`, `multiclass_tnr`, `multiclass_tpr`, `multiclass_true_negative`, `multiclass_true_negative_rate`, `multiclass_true_positive`, `multiclass_true_positive_rate`, `multiclass_truenegative`, `multiclass_truenegative_rate`, `multiclass_truepositive`, `multiclass_truepositive_rate`, `negative_predictive_value`, `negativepredictive_value`, `no_avg`, `node`, `nonmissing`, `npv`, `nrows`, `orientation`, `origins`, `output_scitype`, `package_license`, `package_name`, `package_url`, `package_uuid`, `params`, `partition`, `pdf`, `perceptron_loss`, `periodic_loss`, `positive_predictive_value`, `positivepredictive_value`, `ppv`, `precision`, `predict`, `predict_joint`, `predict_mean`, `predict_median`, `predict_mode`, `predict_scitype`, `prediction_type`, `pretty`, `quantile_loss`, `rebind!`, `recall`, `report`, `reports_each_observation`, `restrict`, `return!`, `rms`, `rmse`, `rmsl`, `rmsle`, `rmslp1`, `rmsp`, `roc`, `roc_curve`, `root_mean_squared_error`, `root_mean_squared_log_error`, `rsq`, `rsquared`, `sampler`, `schema`, `scitype`, `scitype_union`, `selectcols`, `selectivity`, `selectrows`, `sensitivity`, `shuffle`, `shuffle!`, `sigmoid_loss`, `skipinvalid`, `smoothed_l1_hinge_loss`, `source`, `sources`, `specificity`, `spherical_score`, `std`, `support`, `supports_class_weights`, `supports_training_losses`, `supports_weights`, `table`, `target_scitype`, `thaw!`, `tnr`, `tpr`, `training_losses`, `trait`, `transform`, `transform_scitype`, `true_negative`, `true_negative_rate`, `true_positive`, `true_positive_rate`, `truenegative`, `truenegative_rate`, `truepositive`, `truepositive_rate`, `unpack`, `value`, `zero_one_loss`\n",
       "\n",
       "# Displaying contents of readme found at `D:\\TongYuan\\.julia\\packages\\MLJ\\hw6oD\\README.md`\n",
       "\n",
       "<div align=\"center\">     <img src=\"material/MLJLogo2.svg\" alt=\"MLJ\" width=\"200\"> </div>\n",
       "\n",
       "<h2 align=\"center\">A Machine Learning Framework for Julia <p align=\"center\">   <a href=\"https://github.com/alan-turing-institute/MLJ.jl/actions\">     <img src=\"https://github.com/alan-turing-institute/MLJ.jl/workflows/CI/badge.svg\"          alt=\"Build Status\">   </a>   <a href=\"https://alan-turing-institute.github.io/MLJ.jl/dev/\">     <img src=\"https://img.shields.io/badge/docs-stable-blue.svg\"          alt=\"Documentation\">   </a>   <a href=\"https://opensource.org/licenses/MIT\">     <img src=\"https://img.shields.io/badge/License-MIT-yelllow\"        alt=\"bibtex\">   </a>   <a href=\"BIBLIOGRAPHY.md\">     <img src=\"https://img.shields.io/badge/cite-BibTeX-blue\"        alt=\"bibtex\">   </a>\n",
       "\n",
       "</p> </h2>\n",
       "\n",
       "MLJ (Machine Learning in Julia) is a toolbox written in Julia providing a common interface and meta-algorithms for selecting, tuning, evaluating, composing and comparing over [160 machine learning models](https://alan-turing-institute.github.io/MLJ.jl/dev/list_of_supported_models/) written in Julia and other languages.\n",
       "\n",
       "**New to MLJ?** Start [here](https://alan-turing-institute.github.io/MLJ.jl/dev/).\n",
       "\n",
       "**Integrating an existing machine learning model into the MLJ framework?** Start [here](https://alan-turing-institute.github.io/MLJ.jl/dev/quick_start_guide_to_adding_models/).\n",
       "\n",
       "MLJ was initially created as a Tools, Practices and Systems project at the [Alan Turing Institute](https://www.turing.ac.uk/) in 2019. Current funding is provided by a [New Zealand Strategic Science Investment Fund](https://www.mbie.govt.nz/science-and-technology/science-and-innovation/funding-information-and-opportunities/investment-funds/strategic-science-investment-fund/ssif-funded-programmes/university-of-auckland/) awarded to the University of Auckland.\n",
       "\n",
       "MLJ been developed with the support of the following organizations:\n",
       "\n",
       "<div align=\"center\">     <img src=\"material/Turing*logo.png\" width = 100/>     <img src=\"material/UoA*logo.png\" width = 100/>     <img src=\"material/IQVIA_logo.png\" width = 100/>     <img src=\"material/warwick.png\" width = 100/>     <img src=\"material/julia.png\" width = 100/> </div>\n",
       "\n",
       "### The MLJ Universe\n",
       "\n",
       "The functionality of MLJ is distributed over a number of repositories illustrated in the dependency chart below. These repositories live at the [JuliaAI](https://github.com/JuliaAI) umbrella organization.\n",
       "\n",
       "<div align=\"center\">     <img src=\"material/MLJ_stack.svg\" alt=\"Dependency Chart\"> </div>\n",
       "\n",
       "*Dependency chart for MLJ repositories. Repositories with dashed connections do not currently exist but are planned/proposed.*\n",
       "\n",
       "<br> <p align=\"center\"> <a href=\"CONTRIBUTING.md\">Contributing</a> &nbsp;•&nbsp;  <a href=\"ORGANIZATION.md\">Code Organization</a> &nbsp;•&nbsp; <a href=\"ROADMAP.md\">Road Map</a>  </br>\n",
       "\n",
       "#### Contributors\n",
       "\n",
       "*Core design*: A. Blaom, F. Kiraly, S. Vollmer\n",
       "\n",
       "*Lead contributor*: A. Blaom\n",
       "\n",
       "*Active maintainers*: A. Blaom, S. Okon, T. Lienart, D. Aluthge\n"
      ],
      "text/plain": [
       "  No docstring found for module \u001b[36mMLJ\u001b[39m.\n",
       "\n",
       "\u001b[1m  Exported names\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "  \u001b[36m@constant\u001b[39m, \u001b[36m@from_network\u001b[39m, \u001b[36m@iload\u001b[39m, \u001b[36m@load\u001b[39m, \u001b[36m@load_ames\u001b[39m, \u001b[36m@load_boston\u001b[39m,\n",
       "  \u001b[36m@load_crabs\u001b[39m, \u001b[36m@load_iris\u001b[39m, \u001b[36m@load_reduced_ames\u001b[39m, \u001b[36m@more\u001b[39m, \u001b[36m@node\u001b[39m, \u001b[36m@pipeline\u001b[39m, \u001b[36mAUC\u001b[39m,\n",
       "  \u001b[36mAbstractNode\u001b[39m, \u001b[36mAccuracy\u001b[39m, \u001b[36mAnnotator\u001b[39m, \u001b[36mAnnotatorComposite\u001b[39m, \u001b[36mAnnotatorSurrogate\u001b[39m,\n",
       "  \u001b[36mAreaUnderCurve\u001b[39m, \u001b[36mBAC\u001b[39m, \u001b[36mBACC\u001b[39m, \u001b[36mBalancedAccuracy\u001b[39m, \u001b[36mBinary\u001b[39m,\n",
       "  \u001b[36mBinaryThresholdPredictor\u001b[39m, \u001b[36mBrierLoss\u001b[39m, \u001b[36mBrierScore\u001b[39m, \u001b[36mCPU1\u001b[39m, \u001b[36mCPUProcesses\u001b[39m,\n",
       "  \u001b[36mCPUThreads\u001b[39m, \u001b[36mCV\u001b[39m, \u001b[36mCallback\u001b[39m, \u001b[36mColorImage\u001b[39m, \u001b[36mComposite\u001b[39m, \u001b[36mConfusionMatrix\u001b[39m,\n",
       "  \u001b[36mConstantClassifier\u001b[39m, \u001b[36mConstantRegressor\u001b[39m, \u001b[36mContinuous\u001b[39m, \u001b[36mContinuousEncoder\u001b[39m, \u001b[36mCount\u001b[39m,\n",
       "  \u001b[36mCrossEntropy\u001b[39m, \u001b[36mCycleLearningRate\u001b[39m, \u001b[36mDWDMarginLoss\u001b[39m, \u001b[36mData\u001b[39m, \u001b[36mDeterministic\u001b[39m,\n",
       "  \u001b[36mDeterministicComposite\u001b[39m, \u001b[36mDeterministicNetwork\u001b[39m,\n",
       "  \u001b[36mDeterministicSupervisedDetector\u001b[39m, \u001b[36mDeterministicSupervisedDetectorComposite\u001b[39m,\n",
       "  \u001b[36mDeterministicSupervisedDetectorSurrogate\u001b[39m, \u001b[36mDeterministicSurrogate\u001b[39m,\n",
       "  \u001b[36mDeterministicUnsupervisedDetector\u001b[39m,\n",
       "  \u001b[36mDeterministicUnsupervisedDetectorComposite\u001b[39m,\n",
       "  \u001b[36mDeterministicUnsupervisedDetectorSurrogate\u001b[39m, \u001b[36mDisjunction\u001b[39m, \u001b[36mEnsembleModel\u001b[39m,\n",
       "  \u001b[36mError\u001b[39m, \u001b[36mExpLoss\u001b[39m, \u001b[36mExplicit\u001b[39m, \u001b[36mFDR\u001b[39m, \u001b[36mFNR\u001b[39m, \u001b[36mFPR\u001b[39m, \u001b[36mFScore\u001b[39m, \u001b[36mFalseDiscoveryRate\u001b[39m,\n",
       "  \u001b[36mFalseNegative\u001b[39m, \u001b[36mFalseNegativeRate\u001b[39m, \u001b[36mFalsePositive\u001b[39m, \u001b[36mFalsePositiveRate\u001b[39m,\n",
       "  \u001b[36mFeatureSelector\u001b[39m, \u001b[36mFillImputer\u001b[39m, \u001b[36mFinite\u001b[39m, \u001b[36mFound\u001b[39m, \u001b[36mGL\u001b[39m, \u001b[36mGrayImage\u001b[39m, \u001b[36mGrid\u001b[39m,\n",
       "  \u001b[36mHANDLE_GIVEN_ID\u001b[39m, \u001b[36mHoldout\u001b[39m, \u001b[36mHuberLoss\u001b[39m, \u001b[36mImage\u001b[39m, \u001b[36mInfinite\u001b[39m, \u001b[36mInfo\u001b[39m, \u001b[36mInterval\u001b[39m,\n",
       "  \u001b[36mIntervalComposite\u001b[39m, \u001b[36mIntervalSurrogate\u001b[39m, \u001b[36mInvalidValue\u001b[39m, \u001b[36mIteratedModel\u001b[39m,\n",
       "  \u001b[36mIterationControl\u001b[39m, \u001b[36mJointProbabilistic\u001b[39m, \u001b[36mJointProbabilisticComposite\u001b[39m,\n",
       "  \u001b[36mJointProbabilisticSurrogate\u001b[39m, \u001b[36mKappa\u001b[39m, \u001b[36mKnown\u001b[39m, \u001b[36mL1EpsilonInsLoss\u001b[39m, \u001b[36mL1HingeLoss\u001b[39m,\n",
       "  \u001b[36mL2EpsilonInsLoss\u001b[39m, \u001b[36mL2HingeLoss\u001b[39m, \u001b[36mL2MarginLoss\u001b[39m, \u001b[36mLPDistLoss\u001b[39m, \u001b[36mLPLoss\u001b[39m,\n",
       "  \u001b[36mLatinHypercube\u001b[39m, \u001b[36mLogCosh\u001b[39m, \u001b[36mLogCoshLoss\u001b[39m, \u001b[36mLogLoss\u001b[39m, \u001b[36mLogScore\u001b[39m, \u001b[36mLogitDistLoss\u001b[39m,\n",
       "  \u001b[36mLogitMarginLoss\u001b[39m, \u001b[36mMAE\u001b[39m, \u001b[36mMAPE\u001b[39m, \u001b[36mMAV\u001b[39m, \u001b[36mMCC\u001b[39m, \u001b[36mMCR\u001b[39m, \u001b[36mMFDR\u001b[39m, \u001b[36mMFNR\u001b[39m, \u001b[36mMFPR\u001b[39m, \u001b[36mMLJIteration\u001b[39m,\n",
       "  \u001b[36mMLJOpenML\u001b[39m, \u001b[36mMLJ_VERSION\u001b[39m, \u001b[36mMNPV\u001b[39m, \u001b[36mMPPV\u001b[39m, \u001b[36mMTNR\u001b[39m, \u001b[36mMTPR\u001b[39m, \u001b[36mMachine\u001b[39m,\n",
       "  \u001b[36mMatthewsCorrelation\u001b[39m, \u001b[36mMeanAbsoluteError\u001b[39m, \u001b[36mMeanAbsoluteProportionalError\u001b[39m,\n",
       "  \u001b[36mMisclassificationRate\u001b[39m, \u001b[36mModifiedHuberLoss\u001b[39m, \u001b[36mMulticlass\u001b[39m, \u001b[36mMulticlassFScore\u001b[39m,\n",
       "  \u001b[36mMulticlassFalseDiscoveryRate\u001b[39m, \u001b[36mMulticlassFalseNegative\u001b[39m,\n",
       "  \u001b[36mMulticlassFalseNegativeRate\u001b[39m, \u001b[36mMulticlassFalsePositive\u001b[39m,\n",
       "  \u001b[36mMulticlassFalsePositiveRate\u001b[39m, \u001b[36mMulticlassNegativePredictiveValue\u001b[39m,\n",
       "  \u001b[36mMulticlassPrecision\u001b[39m, \u001b[36mMulticlassRecall\u001b[39m, \u001b[36mMulticlassSpecificity\u001b[39m,\n",
       "  \u001b[36mMulticlassTrueNegative\u001b[39m, \u001b[36mMulticlassTrueNegativeRate\u001b[39m, \u001b[36mMulticlassTruePositive\u001b[39m,\n",
       "  \u001b[36mMulticlassTruePositiveRate\u001b[39m, \u001b[36mNPV\u001b[39m, \u001b[36mNegativePredictiveValue\u001b[39m, \u001b[36mNever\u001b[39m, \u001b[36mNode\u001b[39m,\n",
       "  \u001b[36mNotANumber\u001b[39m, \u001b[36mNumberLimit\u001b[39m, \u001b[36mNumberSinceBest\u001b[39m, \u001b[36mOneHotEncoder\u001b[39m, \u001b[36mOpenML\u001b[39m,\n",
       "  \u001b[36mOrderedFactor\u001b[39m, \u001b[36mPPV\u001b[39m, \u001b[36mPQ\u001b[39m, \u001b[36mPatience\u001b[39m, \u001b[36mPerceptronLoss\u001b[39m, \u001b[36mPeriodicLoss\u001b[39m, \u001b[36mPipeline\u001b[39m,\n",
       "  \u001b[36mPrecision\u001b[39m, \u001b[36mProbabilistic\u001b[39m, \u001b[36mProbabilisticComposite\u001b[39m, \u001b[36mProbabilisticNetwork\u001b[39m,\n",
       "  \u001b[36mProbabilisticSupervisedDetector\u001b[39m, \u001b[36mProbabilisticSupervisedDetectorComposite\u001b[39m,\n",
       "  \u001b[36mProbabilisticSupervisedDetectorSurrogate\u001b[39m, \u001b[36mProbabilisticSurrogate\u001b[39m,\n",
       "  \u001b[36mProbabilisticUnsupervisedDetector\u001b[39m,\n",
       "  \u001b[36mProbabilisticUnsupervisedDetectorComposite\u001b[39m,\n",
       "  \u001b[36mProbabilisticUnsupervisedDetectorSurrogate\u001b[39m, \u001b[36mQuantileLoss\u001b[39m, \u001b[36mRMS\u001b[39m, \u001b[36mRMSL\u001b[39m, \u001b[36mRMSLP\u001b[39m,\n",
       "  \u001b[36mRMSP\u001b[39m, \u001b[36mRMSPV\u001b[39m, \u001b[36mRSQ\u001b[39m, \u001b[36mRSquared\u001b[39m, \u001b[36mRandomSearch\u001b[39m, \u001b[36mRecall\u001b[39m, \u001b[36mResampler\u001b[39m,\n",
       "  \u001b[36mResamplingStrategy\u001b[39m, \u001b[36mRootMeanSquaredError\u001b[39m, \u001b[36mRootMeanSquaredLogError\u001b[39m,\n",
       "  \u001b[36mRootMeanSquaredLogProportionalError\u001b[39m, \u001b[36mRootMeanSquaredProportionalError\u001b[39m, \u001b[36mSave\u001b[39m,\n",
       "  \u001b[36mScientific\u001b[39m, \u001b[36mSigmoidLoss\u001b[39m, \u001b[36mSmoothedL1HingeLoss\u001b[39m, \u001b[36mSpecificity\u001b[39m, \u001b[36mSphericalScore\u001b[39m,\n",
       "  \u001b[36mStack\u001b[39m, \u001b[36mStandardizer\u001b[39m, \u001b[36mStatic\u001b[39m, \u001b[36mStaticComposite\u001b[39m, \u001b[36mStaticSurrogate\u001b[39m, \u001b[36mStep\u001b[39m,\n",
       "  \u001b[36mStratifiedCV\u001b[39m, \u001b[36mSupervised\u001b[39m, \u001b[36mSupervisedAnnotator\u001b[39m, \u001b[36mSupervisedAnnotatorComposite\u001b[39m,\n",
       "  \u001b[36mSupervisedAnnotatorSurrogate\u001b[39m, \u001b[36mSupervisedComposite\u001b[39m, \u001b[36mSupervisedDetector\u001b[39m,\n",
       "  \u001b[36mSupervisedDetectorComposite\u001b[39m, \u001b[36mSupervisedDetectorSurrogate\u001b[39m,\n",
       "  \u001b[36mSupervisedSurrogate\u001b[39m, \u001b[36mSurrogate\u001b[39m, \u001b[36mTNR\u001b[39m, \u001b[36mTPR\u001b[39m, \u001b[36mTable\u001b[39m, \u001b[36mTextual\u001b[39m, \u001b[36mThreshold\u001b[39m,\n",
       "  \u001b[36mTimeLimit\u001b[39m, \u001b[36mTimeSeriesCV\u001b[39m, \u001b[36mTransformedTargetModel\u001b[39m, \u001b[36mTrueNegative\u001b[39m,\n",
       "  \u001b[36mTrueNegativeRate\u001b[39m, \u001b[36mTruePositive\u001b[39m, \u001b[36mTruePositiveRate\u001b[39m, \u001b[36mTunedModel\u001b[39m,\n",
       "  \u001b[36mUnivariateBoxCoxTransformer\u001b[39m, \u001b[36mUnivariateDiscretizer\u001b[39m, \u001b[36mUnivariateFinite\u001b[39m,\n",
       "  \u001b[36mUnivariateStandardizer\u001b[39m, \u001b[36mUnivariateTimeTypeToContinuous\u001b[39m, \u001b[36mUnknown\u001b[39m,\n",
       "  \u001b[36mUnsupervised\u001b[39m, \u001b[36mUnsupervisedAnnotator\u001b[39m, \u001b[36mUnsupervisedAnnotatorComposite\u001b[39m,\n",
       "  \u001b[36mUnsupervisedAnnotatorSurrogate\u001b[39m, \u001b[36mUnsupervisedComposite\u001b[39m, \u001b[36mUnsupervisedDetector\u001b[39m,\n",
       "  \u001b[36mUnsupervisedDetectorComposite\u001b[39m, \u001b[36mUnsupervisedDetectorSurrogate\u001b[39m,\n",
       "  \u001b[36mUnsupervisedNetwork\u001b[39m, \u001b[36mUnsupervisedSurrogate\u001b[39m, \u001b[36mWarmup\u001b[39m, \u001b[36mWarn\u001b[39m, \u001b[36mWithEvaluationDo\u001b[39m,\n",
       "  \u001b[36mWithFittedParamsDo\u001b[39m, \u001b[36mWithIterationsDo\u001b[39m, \u001b[36mWithLossDo\u001b[39m, \u001b[36mWithMachineDo\u001b[39m,\n",
       "  \u001b[36mWithModelDo\u001b[39m, \u001b[36mWithNumberDo\u001b[39m, \u001b[36mWithReportDo\u001b[39m, \u001b[36mWithTrainingLossesDo\u001b[39m, \u001b[36mZeroOneLoss\u001b[39m,\n",
       "  \u001b[36maccuracy\u001b[39m, \u001b[36maggregate\u001b[39m, \u001b[36maggregation\u001b[39m, \u001b[36manonymize!\u001b[39m, \u001b[36marea_under_curve\u001b[39m, \u001b[36mauc\u001b[39m,\n",
       "  \u001b[36mautotype\u001b[39m, \u001b[36mbac\u001b[39m, \u001b[36mbacc\u001b[39m, \u001b[36mbalanced_accuracy\u001b[39m, \u001b[36mbrier_loss\u001b[39m, \u001b[36mbrier_score\u001b[39m,\n",
       "  \u001b[36mcategorical\u001b[39m, \u001b[36mclasses\u001b[39m, \u001b[36mcoerce\u001b[39m, \u001b[36mcoerce!\u001b[39m, \u001b[36mcolor_off\u001b[39m, \u001b[36mcolor_on\u001b[39m, \u001b[36mcomplement\u001b[39m,\n",
       "  \u001b[36mconfmat\u001b[39m, \u001b[36mconfusion_matrix\u001b[39m, \u001b[36mcorestrict\u001b[39m, \u001b[36mcross_entropy\u001b[39m, \u001b[36mdecoder\u001b[39m,\n",
       "  \u001b[36mdeep_properties\u001b[39m, \u001b[36mdefault_measure\u001b[39m, \u001b[36mdefault_resource\u001b[39m, \u001b[36mdistribution_type\u001b[39m,\n",
       "  \u001b[36mdocstring\u001b[39m, \u001b[36mdwd_margin_loss\u001b[39m, \u001b[36melscitype\u001b[39m, \u001b[36mevaluate\u001b[39m, \u001b[36mevaluate!\u001b[39m, \u001b[36mexp_loss\u001b[39m,\n",
       "  \u001b[36mf1score\u001b[39m, \u001b[36mfallout\u001b[39m, \u001b[36mfalse_discovery_rate\u001b[39m, \u001b[36mfalse_negative\u001b[39m, \u001b[36mfalse_negative_rate\u001b[39m,\n",
       "  \u001b[36mfalse_positive\u001b[39m, \u001b[36mfalse_positive_rate\u001b[39m, \u001b[36mfalsediscovery_rate\u001b[39m, \u001b[36mfalsenegative\u001b[39m,\n",
       "  \u001b[36mfalsenegative_rate\u001b[39m, \u001b[36mfalsepositive\u001b[39m, \u001b[36mfalsepositive_rate\u001b[39m, \u001b[36mfdr\u001b[39m, \u001b[36mfit!\u001b[39m,\n",
       "  \u001b[36mfit_data_scitype\u001b[39m, \u001b[36mfit_only!\u001b[39m, \u001b[36mfitresults\u001b[39m, \u001b[36mfitted_params\u001b[39m, \u001b[36mfnr\u001b[39m, \u001b[36mfpr\u001b[39m, \u001b[36mfreeze!\u001b[39m,\n",
       "  \u001b[36mhit_rate\u001b[39m, \u001b[36mhuber_loss\u001b[39m, \u001b[36mhuman_name\u001b[39m, \u001b[36mhyperparameter_types\u001b[39m, \u001b[36mhyperparameters\u001b[39m,\n",
       "  \u001b[36minfo\u001b[39m, \u001b[36minput_scitype\u001b[39m, \u001b[36mint\u001b[39m, \u001b[36minverse_transform\u001b[39m, \u001b[36minverse_transform_scitype\u001b[39m,\n",
       "  \u001b[36mis_feature_dependent\u001b[39m, \u001b[36mis_pure_julia\u001b[39m, \u001b[36mis_supervised\u001b[39m, \u001b[36mis_wrapper\u001b[39m,\n",
       "  \u001b[36miteration_parameter\u001b[39m, \u001b[36miterator\u001b[39m, \u001b[36mkappa\u001b[39m, \u001b[36ml1\u001b[39m, \u001b[36ml1_epsilon_ins_loss\u001b[39m,\n",
       "  \u001b[36ml1_hinge_loss\u001b[39m, \u001b[36ml2\u001b[39m, \u001b[36ml2_epsilon_ins_loss\u001b[39m, \u001b[36ml2_hinge_loss\u001b[39m, \u001b[36ml2_margin_loss\u001b[39m,\n",
       "  \u001b[36mlearning_curve\u001b[39m, \u001b[36mlearning_curve!\u001b[39m, \u001b[36mlevels\u001b[39m, \u001b[36mlevels!\u001b[39m, \u001b[36mload\u001b[39m, \u001b[36mload_ames\u001b[39m,\n",
       "  \u001b[36mload_boston\u001b[39m, \u001b[36mload_crabs\u001b[39m, \u001b[36mload_iris\u001b[39m, \u001b[36mload_path\u001b[39m, \u001b[36mload_reduced_ames\u001b[39m,\n",
       "  \u001b[36mlocalmodels\u001b[39m, \u001b[36mlog_cosh\u001b[39m, \u001b[36mlog_cosh_loss\u001b[39m, \u001b[36mlog_loss\u001b[39m, \u001b[36mlog_score\u001b[39m, \u001b[36mlogit_dist_loss\u001b[39m,\n",
       "  \u001b[36mlogit_margin_loss\u001b[39m, \u001b[36mlogpdf\u001b[39m, \u001b[36mlp_dist_loss\u001b[39m, \u001b[36mmachine\u001b[39m, \u001b[36mmachines\u001b[39m, \u001b[36mmacro_avg\u001b[39m,\n",
       "  \u001b[36mmacro_f1score\u001b[39m, \u001b[36mmae\u001b[39m, \u001b[36mmake_blobs\u001b[39m, \u001b[36mmake_circles\u001b[39m, \u001b[36mmake_moons\u001b[39m, \u001b[36mmake_regression\u001b[39m,\n",
       "  \u001b[36mmape\u001b[39m, \u001b[36mmatching\u001b[39m, \u001b[36mmatthews_correlation\u001b[39m, \u001b[36mmav\u001b[39m, \u001b[36mmcc\u001b[39m, \u001b[36mmcr\u001b[39m, \u001b[36mmean\u001b[39m,\n",
       "  \u001b[36mmean_absolute_error\u001b[39m, \u001b[36mmean_absolute_value\u001b[39m, \u001b[36mmeasures\u001b[39m, \u001b[36mmedian\u001b[39m, \u001b[36mmicro_avg\u001b[39m,\n",
       "  \u001b[36mmicro_f1score\u001b[39m, \u001b[36mmisclassification_rate\u001b[39m, \u001b[36mmiss_rate\u001b[39m, \u001b[36mmode\u001b[39m, \u001b[36mmodels\u001b[39m,\n",
       "  \u001b[36mmodified_huber_loss\u001b[39m, \u001b[36mmulticlass_f1score\u001b[39m, \u001b[36mmulticlass_fallout\u001b[39m,\n",
       "  \u001b[36mmulticlass_false_discovery_rate\u001b[39m, \u001b[36mmulticlass_false_negative\u001b[39m,\n",
       "  \u001b[36mmulticlass_false_negative_rate\u001b[39m, \u001b[36mmulticlass_false_positive\u001b[39m,\n",
       "  \u001b[36mmulticlass_false_positive_rate\u001b[39m, \u001b[36mmulticlass_falsediscovery_rate\u001b[39m,\n",
       "  \u001b[36mmulticlass_falsenegative\u001b[39m, \u001b[36mmulticlass_falsenegative_rate\u001b[39m,\n",
       "  \u001b[36mmulticlass_falsepositive\u001b[39m, \u001b[36mmulticlass_falsepositive_rate\u001b[39m, \u001b[36mmulticlass_fdr\u001b[39m,\n",
       "  \u001b[36mmulticlass_fnr\u001b[39m, \u001b[36mmulticlass_fpr\u001b[39m, \u001b[36mmulticlass_hit_rate\u001b[39m, \u001b[36mmulticlass_miss_rate\u001b[39m,\n",
       "  \u001b[36mmulticlass_negative_predictive_value\u001b[39m, \u001b[36mmulticlass_negativepredictive_value\u001b[39m,\n",
       "  \u001b[36mmulticlass_npv\u001b[39m, \u001b[36mmulticlass_positive_predictive_value\u001b[39m,\n",
       "  \u001b[36mmulticlass_positivepredictive_value\u001b[39m, \u001b[36mmulticlass_ppv\u001b[39m, \u001b[36mmulticlass_precision\u001b[39m,\n",
       "  \u001b[36mmulticlass_recall\u001b[39m, \u001b[36mmulticlass_selectivity\u001b[39m, \u001b[36mmulticlass_sensitivity\u001b[39m,\n",
       "  \u001b[36mmulticlass_specificity\u001b[39m, \u001b[36mmulticlass_tnr\u001b[39m, \u001b[36mmulticlass_tpr\u001b[39m,\n",
       "  \u001b[36mmulticlass_true_negative\u001b[39m, \u001b[36mmulticlass_true_negative_rate\u001b[39m,\n",
       "  \u001b[36mmulticlass_true_positive\u001b[39m, \u001b[36mmulticlass_true_positive_rate\u001b[39m,\n",
       "  \u001b[36mmulticlass_truenegative\u001b[39m, \u001b[36mmulticlass_truenegative_rate\u001b[39m,\n",
       "  \u001b[36mmulticlass_truepositive\u001b[39m, \u001b[36mmulticlass_truepositive_rate\u001b[39m,\n",
       "  \u001b[36mnegative_predictive_value\u001b[39m, \u001b[36mnegativepredictive_value\u001b[39m, \u001b[36mno_avg\u001b[39m, \u001b[36mnode\u001b[39m,\n",
       "  \u001b[36mnonmissing\u001b[39m, \u001b[36mnpv\u001b[39m, \u001b[36mnrows\u001b[39m, \u001b[36morientation\u001b[39m, \u001b[36morigins\u001b[39m, \u001b[36moutput_scitype\u001b[39m,\n",
       "  \u001b[36mpackage_license\u001b[39m, \u001b[36mpackage_name\u001b[39m, \u001b[36mpackage_url\u001b[39m, \u001b[36mpackage_uuid\u001b[39m, \u001b[36mparams\u001b[39m, \u001b[36mpartition\u001b[39m,\n",
       "  \u001b[36mpdf\u001b[39m, \u001b[36mperceptron_loss\u001b[39m, \u001b[36mperiodic_loss\u001b[39m, \u001b[36mpositive_predictive_value\u001b[39m,\n",
       "  \u001b[36mpositivepredictive_value\u001b[39m, \u001b[36mppv\u001b[39m, \u001b[36mprecision\u001b[39m, \u001b[36mpredict\u001b[39m, \u001b[36mpredict_joint\u001b[39m,\n",
       "  \u001b[36mpredict_mean\u001b[39m, \u001b[36mpredict_median\u001b[39m, \u001b[36mpredict_mode\u001b[39m, \u001b[36mpredict_scitype\u001b[39m,\n",
       "  \u001b[36mprediction_type\u001b[39m, \u001b[36mpretty\u001b[39m, \u001b[36mquantile_loss\u001b[39m, \u001b[36mrebind!\u001b[39m, \u001b[36mrecall\u001b[39m, \u001b[36mreport\u001b[39m,\n",
       "  \u001b[36mreports_each_observation\u001b[39m, \u001b[36mrestrict\u001b[39m, \u001b[36mreturn!\u001b[39m, \u001b[36mrms\u001b[39m, \u001b[36mrmse\u001b[39m, \u001b[36mrmsl\u001b[39m, \u001b[36mrmsle\u001b[39m, \u001b[36mrmslp1\u001b[39m,\n",
       "  \u001b[36mrmsp\u001b[39m, \u001b[36mroc\u001b[39m, \u001b[36mroc_curve\u001b[39m, \u001b[36mroot_mean_squared_error\u001b[39m, \u001b[36mroot_mean_squared_log_error\u001b[39m,\n",
       "  \u001b[36mrsq\u001b[39m, \u001b[36mrsquared\u001b[39m, \u001b[36msampler\u001b[39m, \u001b[36mschema\u001b[39m, \u001b[36mscitype\u001b[39m, \u001b[36mscitype_union\u001b[39m, \u001b[36mselectcols\u001b[39m,\n",
       "  \u001b[36mselectivity\u001b[39m, \u001b[36mselectrows\u001b[39m, \u001b[36msensitivity\u001b[39m, \u001b[36mshuffle\u001b[39m, \u001b[36mshuffle!\u001b[39m, \u001b[36msigmoid_loss\u001b[39m,\n",
       "  \u001b[36mskipinvalid\u001b[39m, \u001b[36msmoothed_l1_hinge_loss\u001b[39m, \u001b[36msource\u001b[39m, \u001b[36msources\u001b[39m, \u001b[36mspecificity\u001b[39m,\n",
       "  \u001b[36mspherical_score\u001b[39m, \u001b[36mstd\u001b[39m, \u001b[36msupport\u001b[39m, \u001b[36msupports_class_weights\u001b[39m,\n",
       "  \u001b[36msupports_training_losses\u001b[39m, \u001b[36msupports_weights\u001b[39m, \u001b[36mtable\u001b[39m, \u001b[36mtarget_scitype\u001b[39m, \u001b[36mthaw!\u001b[39m,\n",
       "  \u001b[36mtnr\u001b[39m, \u001b[36mtpr\u001b[39m, \u001b[36mtraining_losses\u001b[39m, \u001b[36mtrait\u001b[39m, \u001b[36mtransform\u001b[39m, \u001b[36mtransform_scitype\u001b[39m,\n",
       "  \u001b[36mtrue_negative\u001b[39m, \u001b[36mtrue_negative_rate\u001b[39m, \u001b[36mtrue_positive\u001b[39m, \u001b[36mtrue_positive_rate\u001b[39m,\n",
       "  \u001b[36mtruenegative\u001b[39m, \u001b[36mtruenegative_rate\u001b[39m, \u001b[36mtruepositive\u001b[39m, \u001b[36mtruepositive_rate\u001b[39m, \u001b[36munpack\u001b[39m,\n",
       "  \u001b[36mvalue\u001b[39m, \u001b[36mzero_one_loss\u001b[39m\n",
       "\n",
       "\u001b[1m  Displaying contents of readme found at\u001b[22m\n",
       "\u001b[1m \u001b[36mD:\\TongYuan\\.julia\\packages\\MLJ\\hw6oD\\README.md\u001b[39m\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "  <div align=\"center\"> <img src=\"material/MLJLogo2.svg\" alt=\"MLJ\" width=\"200\">\n",
       "  </div>\n",
       "\n",
       "  <h2 align=\"center\">A Machine Learning Framework for Julia <p align=\"center\">\n",
       "  <a href=\"https://github.com/alan-turing-institute/MLJ.jl/actions\"> <img\n",
       "  src=\"https://github.com/alan-turing-institute/MLJ.jl/workflows/CI/badge.svg\"\n",
       "  alt=\"Build Status\"> </a> <a\n",
       "  href=\"https://alan-turing-institute.github.io/MLJ.jl/dev/\"> <img\n",
       "  src=\"https://img.shields.io/badge/docs-stable-blue.svg\" alt=\"Documentation\">\n",
       "  </a> <a href=\"https://opensource.org/licenses/MIT\"> <img\n",
       "  src=\"https://img.shields.io/badge/License-MIT-yelllow\" alt=\"bibtex\"> </a> <a\n",
       "  href=\"BIBLIOGRAPHY.md\"> <img\n",
       "  src=\"https://img.shields.io/badge/cite-BibTeX-blue\" alt=\"bibtex\"> </a>\n",
       "\n",
       "  </p> </h2>\n",
       "\n",
       "  MLJ (Machine Learning in Julia) is a toolbox written in Julia providing a\n",
       "  common interface and meta-algorithms for selecting, tuning, evaluating,\n",
       "  composing and comparing over 160 machine learning models\n",
       "  (https://alan-turing-institute.github.io/MLJ.jl/dev/list_of_supported_models/)\n",
       "  written in Julia and other languages.\n",
       "\n",
       "  \u001b[1mNew to MLJ?\u001b[22m Start here\n",
       "  (https://alan-turing-institute.github.io/MLJ.jl/dev/).\n",
       "\n",
       "  \u001b[1mIntegrating an existing machine learning model into the MLJ framework?\u001b[22m Start\n",
       "  here\n",
       "  (https://alan-turing-institute.github.io/MLJ.jl/dev/quick_start_guide_to_adding_models/).\n",
       "\n",
       "  MLJ was initially created as a Tools, Practices and Systems project at the\n",
       "  Alan Turing Institute (https://www.turing.ac.uk/) in 2019. Current funding\n",
       "  is provided by a New Zealand Strategic Science Investment Fund\n",
       "  (https://www.mbie.govt.nz/science-and-technology/science-and-innovation/funding-information-and-opportunities/investment-funds/strategic-science-investment-fund/ssif-funded-programmes/university-of-auckland/)\n",
       "  awarded to the University of Auckland.\n",
       "\n",
       "  MLJ been developed with the support of the following organizations:\n",
       "\n",
       "  <div align=\"center\"> <img src=\"material/Turing\u001b[4mlogo.png\" width = 100/> <img\n",
       "  src=\"material/UoA\u001b[24mlogo.png\" width = 100/> <img src=\"material/IQVIA_logo.png\"\n",
       "  width = 100/> <img src=\"material/warwick.png\" width = 100/> <img\n",
       "  src=\"material/julia.png\" width = 100/> </div>\n",
       "\n",
       "\u001b[1m  The MLJ Universe\u001b[22m\n",
       "\u001b[1m  ––––––––––––––––––\u001b[22m\n",
       "\n",
       "  The functionality of MLJ is distributed over a number of repositories\n",
       "  illustrated in the dependency chart below. These repositories live at the\n",
       "  JuliaAI (https://github.com/JuliaAI) umbrella organization.\n",
       "\n",
       "  <div align=\"center\"> <img src=\"material/MLJ_stack.svg\" alt=\"Dependency\n",
       "  Chart\"> </div>\n",
       "\n",
       "  \u001b[4mDependency chart for MLJ repositories. Repositories with dashed connections\n",
       "  do not currently exist but are planned/proposed.\u001b[24m\n",
       "\n",
       "  <br> <p align=\"center\"> <a href=\"CONTRIBUTING.md\">Contributing</a>\n",
       "  &nbsp;•&nbsp; <a href=\"ORGANIZATION.md\">Code Organization</a> &nbsp;•&nbsp;\n",
       "  <a href=\"ROADMAP.md\">Road Map</a> </br>\n",
       "\n",
       "\u001b[1m  Contributors\u001b[22m\n",
       "\u001b[1m  --------------\u001b[22m\n",
       "\n",
       "  \u001b[4mCore design\u001b[24m: A. Blaom, F. Kiraly, S. Vollmer\n",
       "\n",
       "  \u001b[4mLead contributor\u001b[24m: A. Blaom\n",
       "\n",
       "  \u001b[4mActive maintainers\u001b[24m: A. Blaom, S. Okon, T. Lienart, D. Aluthge"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?MLJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-22T10:48:54.093000+08:00",
     "start_time": "2022-07-22T02:48:54.085Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1mp\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1mr\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "partition(X, fractions...;\n",
       "          shuffle=nothing,\n",
       "          rng=Random.GLOBAL_RNG,\n",
       "          stratify=nothing,\n",
       "          multi=false)\n",
       "\\end{verbatim}\n",
       "Splits the vector, matrix or table \\texttt{X} into a tuple of objects of the same type, whose vertical concatenation is \\texttt{X}. The number of rows in each component of the return value is determined by the corresponding \\texttt{fractions} of \\texttt{length(nrows(X))}, where valid fractions are floats between 0 and 1 whose sum is less than one. The last fraction is not provided, as it is inferred from the preceding ones.\n",
       "\n",
       "For \"synchronized\" partitioning of multiple objects, use the \\texttt{multi=true} option described below.\n",
       "\n",
       "\\begin{verbatim}\n",
       "julia> partition(1:1000, 0.8)\n",
       "([1,...,800], [801,...,1000])\n",
       "\n",
       "julia> partition(1:1000, 0.2, 0.7)\n",
       "([1,...,200], [201,...,900], [901,...,1000])\n",
       "\n",
       "julia> partition(reshape(1:10, 5, 2), 0.2, 0.4)\n",
       "([1 6], [2 7; 3 8], [4 9; 5 10])\n",
       "\n",
       "X, y = make_blobs() # a table and vector\n",
       "Xtrain, Xtest = partition(X, 0.8, stratify=y)\n",
       "\n",
       "(Xtrain, Xtest), (ytrain, ytest) = partition((X, y), 0.8, rng=123, multi=true)\n",
       "\\end{verbatim}\n",
       "\\subsection{Keywords}\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{shuffle=nothing}: if set to \\texttt{true}, shuffles the rows before taking fractions.\n",
       "\n",
       "\n",
       "\\item \\texttt{rng=Random.GLOBAL\\_RNG}: specifies the random number generator to be used, can be an integer seed. If specified, and \\texttt{shuffle === nothing} is interpreted as true.\n",
       "\n",
       "\n",
       "\\item \\texttt{stratify=nothing}: if a vector is specified, the partition will match the stratification of the given vector. In that case, \\texttt{shuffle} cannot be \\texttt{false}.\n",
       "\n",
       "\n",
       "\\item \\texttt{multi=false}: if \\texttt{true} then \\texttt{X} is expected to be a \\texttt{tuple} of objects sharing a common length, which are each partitioned separately using the same specified \\texttt{fractions} \\emph{and} the same row shuffling. Returns a tuple of partitions (a tuple of tuples).\n",
       "\n",
       "\\end{itemize}\n"
      ],
      "text/markdown": [
       "```\n",
       "partition(X, fractions...;\n",
       "          shuffle=nothing,\n",
       "          rng=Random.GLOBAL_RNG,\n",
       "          stratify=nothing,\n",
       "          multi=false)\n",
       "```\n",
       "\n",
       "Splits the vector, matrix or table `X` into a tuple of objects of the same type, whose vertical concatenation is `X`. The number of rows in each component of the return value is determined by the corresponding `fractions` of `length(nrows(X))`, where valid fractions are floats between 0 and 1 whose sum is less than one. The last fraction is not provided, as it is inferred from the preceding ones.\n",
       "\n",
       "For \"synchronized\" partitioning of multiple objects, use the `multi=true` option described below.\n",
       "\n",
       "```\n",
       "julia> partition(1:1000, 0.8)\n",
       "([1,...,800], [801,...,1000])\n",
       "\n",
       "julia> partition(1:1000, 0.2, 0.7)\n",
       "([1,...,200], [201,...,900], [901,...,1000])\n",
       "\n",
       "julia> partition(reshape(1:10, 5, 2), 0.2, 0.4)\n",
       "([1 6], [2 7; 3 8], [4 9; 5 10])\n",
       "\n",
       "X, y = make_blobs() # a table and vector\n",
       "Xtrain, Xtest = partition(X, 0.8, stratify=y)\n",
       "\n",
       "(Xtrain, Xtest), (ytrain, ytest) = partition((X, y), 0.8, rng=123, multi=true)\n",
       "```\n",
       "\n",
       "## Keywords\n",
       "\n",
       "  * `shuffle=nothing`: if set to `true`, shuffles the rows before taking fractions.\n",
       "  * `rng=Random.GLOBAL_RNG`: specifies the random number generator to be used, can be an integer seed. If specified, and `shuffle === nothing` is interpreted as true.\n",
       "  * `stratify=nothing`: if a vector is specified, the partition will match the stratification of the given vector. In that case, `shuffle` cannot be `false`.\n",
       "  * `multi=false`: if `true` then `X` is expected to be a `tuple` of objects sharing a common length, which are each partitioned separately using the same specified `fractions` *and* the same row shuffling. Returns a tuple of partitions (a tuple of tuples).\n"
      ],
      "text/plain": [
       "\u001b[36m  partition(X, fractions...;\u001b[39m\n",
       "\u001b[36m            shuffle=nothing,\u001b[39m\n",
       "\u001b[36m            rng=Random.GLOBAL_RNG,\u001b[39m\n",
       "\u001b[36m            stratify=nothing,\u001b[39m\n",
       "\u001b[36m            multi=false)\u001b[39m\n",
       "\n",
       "  Splits the vector, matrix or table \u001b[36mX\u001b[39m into a tuple of objects of the same\n",
       "  type, whose vertical concatenation is \u001b[36mX\u001b[39m. The number of rows in each\n",
       "  component of the return value is determined by the corresponding \u001b[36mfractions\u001b[39m\n",
       "  of \u001b[36mlength(nrows(X))\u001b[39m, where valid fractions are floats between 0 and 1 whose\n",
       "  sum is less than one. The last fraction is not provided, as it is inferred\n",
       "  from the preceding ones.\n",
       "\n",
       "  For \"synchronized\" partitioning of multiple objects, use the \u001b[36mmulti=true\u001b[39m\n",
       "  option described below.\n",
       "\n",
       "\u001b[36m  julia> partition(1:1000, 0.8)\u001b[39m\n",
       "\u001b[36m  ([1,...,800], [801,...,1000])\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> partition(1:1000, 0.2, 0.7)\u001b[39m\n",
       "\u001b[36m  ([1,...,200], [201,...,900], [901,...,1000])\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> partition(reshape(1:10, 5, 2), 0.2, 0.4)\u001b[39m\n",
       "\u001b[36m  ([1 6], [2 7; 3 8], [4 9; 5 10])\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  X, y = make_blobs() # a table and vector\u001b[39m\n",
       "\u001b[36m  Xtrain, Xtest = partition(X, 0.8, stratify=y)\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  (Xtrain, Xtest), (ytrain, ytest) = partition((X, y), 0.8, rng=123, multi=true)\u001b[39m\n",
       "\n",
       "\u001b[1m  Keywords\u001b[22m\n",
       "\u001b[1m  ==========\u001b[22m\n",
       "\n",
       "    •  \u001b[36mshuffle=nothing\u001b[39m: if set to \u001b[36mtrue\u001b[39m, shuffles the rows before taking\n",
       "       fractions.\n",
       "\n",
       "    •  \u001b[36mrng=Random.GLOBAL_RNG\u001b[39m: specifies the random number generator to be\n",
       "       used, can be an integer seed. If specified, and \u001b[36mshuffle ===\n",
       "       nothing\u001b[39m is interpreted as true.\n",
       "\n",
       "    •  \u001b[36mstratify=nothing\u001b[39m: if a vector is specified, the partition will\n",
       "       match the stratification of the given vector. In that case,\n",
       "       \u001b[36mshuffle\u001b[39m cannot be \u001b[36mfalse\u001b[39m.\n",
       "\n",
       "    •  \u001b[36mmulti=false\u001b[39m: if \u001b[36mtrue\u001b[39m then \u001b[36mX\u001b[39m is expected to be a \u001b[36mtuple\u001b[39m of objects\n",
       "       sharing a common length, which are each partitioned separately\n",
       "       using the same specified \u001b[36mfractions\u001b[39m \u001b[4mand\u001b[24m the same row shuffling.\n",
       "       Returns a tuple of partitions (a tuple of tuples)."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T16:10:02.480000+08:00",
     "start_time": "2022-06-13T08:09:46.116Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "187-element Vector{NamedTuple{(:name, :package_name, :is_supervised, :abstract_type, :deep_properties, :docstring, :fit_data_scitype, :human_name, :hyperparameter_ranges, :hyperparameter_types, :hyperparameters, :implemented_methods, :inverse_transform_scitype, :is_pure_julia, :is_wrapper, :iteration_parameter, :load_path, :package_license, :package_url, :package_uuid, :predict_scitype, :prediction_type, :supports_class_weights, :supports_online, :supports_training_losses, :supports_weights, :transform_scitype, :input_scitype, :target_scitype, :output_scitype)}}:\n",
       " (name = ABODDetector, package_name = OutlierDetectionNeighbors, ... )\n",
       " (name = ABODDetector, package_name = OutlierDetectionPython, ... )\n",
       " (name = AEDetector, package_name = OutlierDetectionNetworks, ... )\n",
       " (name = ARDRegressor, package_name = ScikitLearn, ... )\n",
       " (name = AdaBoostClassifier, package_name = ScikitLearn, ... )\n",
       " (name = AdaBoostRegressor, package_name = ScikitLearn, ... )\n",
       " (name = AdaBoostStumpClassifier, package_name = DecisionTree, ... )\n",
       " (name = AffinityPropagation, package_name = ScikitLearn, ... )\n",
       " (name = AgglomerativeClustering, package_name = ScikitLearn, ... )\n",
       " (name = BM25Transformer, package_name = MLJText, ... )\n",
       " (name = BaggingClassifier, package_name = ScikitLearn, ... )\n",
       " (name = BaggingRegressor, package_name = ScikitLearn, ... )\n",
       " (name = BayesianLDA, package_name = MultivariateStats, ... )\n",
       " ⋮\n",
       " (name = SubspaceLDA, package_name = MultivariateStats, ... )\n",
       " (name = TSVDTransformer, package_name = TSVD, ... )\n",
       " (name = TfidfTransformer, package_name = MLJText, ... )\n",
       " (name = TheilSenRegressor, package_name = ScikitLearn, ... )\n",
       " (name = UnivariateBoxCoxTransformer, package_name = MLJModels, ... )\n",
       " (name = UnivariateDiscretizer, package_name = MLJModels, ... )\n",
       " (name = UnivariateFillImputer, package_name = MLJModels, ... )\n",
       " (name = UnivariateStandardizer, package_name = MLJModels, ... )\n",
       " (name = UnivariateTimeTypeToContinuous, package_name = MLJModels, ... )\n",
       " (name = XGBoostClassifier, package_name = XGBoost, ... )\n",
       " (name = XGBoostCount, package_name = XGBoost, ... )\n",
       " (name = XGBoostRegressor, package_name = XGBoost, ... )"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T16:10:07.352000+08:00",
     "start_time": "2022-06-13T08:10:07.318Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLJ库包含的机器学习方法种类数：187\n"
     ]
    }
   ],
   "source": [
    "println(\"MLJ库包含的机器学习方法种类数：\",length(models()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T16:10:19.827000+08:00",
     "start_time": "2022-06-13T08:10:19.637Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "writelines (generic function with 1 method)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function writelines(lines, filename) #写入数据\n",
    "    open(filename, \"w\") do io\n",
    "        for line in lines\n",
    "            println(io, line)\n",
    "        end\n",
    "    end\n",
    "    return\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-13T16:10:25.319000+08:00",
     "start_time": "2022-06-13T08:10:21.228Z"
    }
   },
   "outputs": [],
   "source": [
    "x = models()\n",
    "fh = \"C://Users//TR//Desktop//MLJ库.md\"\n",
    "writelines(x,fh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVMLinearRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T16:33:26.942000+08:00",
     "start_time": "2022-06-08T08:33:19.131Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLJScikitLearnInterface.SVMLinearRegressor"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@load SVMLinearRegressor verbosity=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T14:54:04.267000+08:00",
     "start_time": "2022-06-08T06:53:24.699Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "No documentation found.\n",
       "\n",
       "\\section{Summary}\n",
       "\\begin{verbatim}\n",
       "mutable struct MLJScikitLearnInterface.SVMLinearRegressor\n",
       "\\end{verbatim}\n",
       "\\section{Fields}\n",
       "\\begin{verbatim}\n",
       "epsilon           :: Float64\n",
       "tol               :: Float64\n",
       "C                 :: Float64\n",
       "loss              :: String\n",
       "fit_intercept     :: Bool\n",
       "intercept_scaling :: Float64\n",
       "dual              :: Bool\n",
       "random_state      :: Any\n",
       "max_iter          :: Int64\n",
       "\\end{verbatim}\n",
       "\\section{Supertype Hierarchy}\n",
       "\\begin{verbatim}\n",
       "MLJScikitLearnInterface.SVMLinearRegressor <: Deterministic <: Supervised <: MLJModelInterface.Model <: MLJModelInterface.MLJType <: Any\n",
       "\\end{verbatim}\n"
      ],
      "text/markdown": [
       "No documentation found.\n",
       "\n",
       "# Summary\n",
       "\n",
       "```\n",
       "mutable struct MLJScikitLearnInterface.SVMLinearRegressor\n",
       "```\n",
       "\n",
       "# Fields\n",
       "\n",
       "```\n",
       "epsilon           :: Float64\n",
       "tol               :: Float64\n",
       "C                 :: Float64\n",
       "loss              :: String\n",
       "fit_intercept     :: Bool\n",
       "intercept_scaling :: Float64\n",
       "dual              :: Bool\n",
       "random_state      :: Any\n",
       "max_iter          :: Int64\n",
       "```\n",
       "\n",
       "# Supertype Hierarchy\n",
       "\n",
       "```\n",
       "MLJScikitLearnInterface.SVMLinearRegressor <: Deterministic <: Supervised <: MLJModelInterface.Model <: MLJModelInterface.MLJType <: Any\n",
       "```\n"
      ],
      "text/plain": [
       "  No documentation found.\n",
       "\n",
       "\u001b[1m  Summary\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "\u001b[36m  mutable struct MLJScikitLearnInterface.SVMLinearRegressor\u001b[39m\n",
       "\n",
       "\u001b[1m  Fields\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "\u001b[36m  epsilon           :: Float64\u001b[39m\n",
       "\u001b[36m  tol               :: Float64\u001b[39m\n",
       "\u001b[36m  C                 :: Float64\u001b[39m\n",
       "\u001b[36m  loss              :: String\u001b[39m\n",
       "\u001b[36m  fit_intercept     :: Bool\u001b[39m\n",
       "\u001b[36m  intercept_scaling :: Float64\u001b[39m\n",
       "\u001b[36m  dual              :: Bool\u001b[39m\n",
       "\u001b[36m  random_state      :: Any\u001b[39m\n",
       "\u001b[36m  max_iter          :: Int64\u001b[39m\n",
       "\n",
       "\u001b[1m  Supertype Hierarchy\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "\u001b[36m  MLJScikitLearnInterface.SVMLinearRegressor <: Deterministic <: Supervised <: MLJModelInterface.Model <: MLJModelInterface.MLJType <: Any\u001b[39m"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?MLJScikitLearnInterface.SVMLinearRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T16:33:32.222000+08:00",
     "start_time": "2022-06-08T08:33:31.699Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVMLinearRegressor(\n",
       "    epsilon = 0.0,\n",
       "    tol = 0.0001,\n",
       "    C = 1.0,\n",
       "    loss = \"epsilon_insensitive\",\n",
       "    fit_intercept = true,\n",
       "    intercept_scaling = 1.0,\n",
       "    dual = true,\n",
       "    random_state = nothing,\n",
       "    max_iter = 1000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm = MLJScikitLearnInterface.SVMLinearRegressor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T14:54:06.084000+08:00",
     "start_time": "2022-06-08T06:53:24.700Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Precompiling MLJDecisionTreeInterface [c6f25543-311c-4c74-83dc-3ea6d1015661]\n",
      "└ @ Base loading.jl:1423\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLJDecisionTreeInterface.DecisionTreeRegressor"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@load DecisionTreeRegressor pkg = DecisionTree verbosity=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T14:54:07.342000+08:00",
     "start_time": "2022-06-08T06:53:24.701Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Precompiling MLJClusteringInterface [d354fa79-ed1c-40d4-88ef-b8c7bd1568af]\n",
      "└ @ Base loading.jl:1423\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLJClusteringInterface.KMeans"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@load  KMeans pkg=Clustering verbosity=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FactorAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T14:54:07.764000+08:00",
     "start_time": "2022-06-08T06:53:24.702Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLJMultivariateStatsInterface.FactorAnalysis"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@load FactorAnalysis verbosity=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T14:54:07.764000+08:00",
     "start_time": "2022-06-08T06:53:24.704Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "FactorAnalysis(; kwargs...)\n",
       "\\end{verbatim}\n",
       "Probabilistic principal component analysis\n",
       "\n",
       "\\section{Keyword Parameters}\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{method::Symbol=:cm}: Method to use to solve the problem, one of \\texttt{:ml}, \\texttt{:em}, \\texttt{:bayes}.\n",
       "\n",
       "\n",
       "\\item \\texttt{maxoutdim::Int=0}: Maximum number of output dimensions, uses max(no\\emph{of}features - 1, 1)   if 0 (default).\n",
       "\n",
       "\n",
       "\\item \\texttt{maxiter::Int=1000}: Maximum number of iterations.\n",
       "\n",
       "\n",
       "\\item \\texttt{tol::Real=1e-6}: Convergence tolerance.\n",
       "\n",
       "\n",
       "\\item \\texttt{eta::Real=tol}: Variance lower bound\n",
       "\n",
       "\n",
       "\\item \\texttt{mean::Union\\{Nothing, Real, Vector\\{Float64\\}\\}=nothing}: If set to nothing(default)   centering will be computed and applied, if set to \\texttt{0} no   centering(assumed pre-centered), if a vector is passed, the centering is done with   that vector.\n",
       "\n",
       "\\end{itemize}\n"
      ],
      "text/markdown": [
       "```\n",
       "FactorAnalysis(; kwargs...)\n",
       "```\n",
       "\n",
       "Probabilistic principal component analysis\n",
       "\n",
       "# Keyword Parameters\n",
       "\n",
       "  * `method::Symbol=:cm`: Method to use to solve the problem, one of `:ml`, `:em`, `:bayes`.\n",
       "  * `maxoutdim::Int=0`: Maximum number of output dimensions, uses max(no*of*features - 1, 1)   if 0 (default).\n",
       "  * `maxiter::Int=1000`: Maximum number of iterations.\n",
       "  * `tol::Real=1e-6`: Convergence tolerance.\n",
       "  * `eta::Real=tol`: Variance lower bound\n",
       "  * `mean::Union{Nothing, Real, Vector{Float64}}=nothing`: If set to nothing(default)   centering will be computed and applied, if set to `0` no   centering(assumed pre-centered), if a vector is passed, the centering is done with   that vector.\n"
      ],
      "text/plain": [
       "\u001b[36m  FactorAnalysis(; kwargs...)\u001b[39m\n",
       "\n",
       "  Probabilistic principal component analysis\n",
       "\n",
       "\u001b[1m  Keyword Parameters\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "    •  \u001b[36mmethod::Symbol=:cm\u001b[39m: Method to use to solve the problem, one of\n",
       "       \u001b[36m:ml\u001b[39m, \u001b[36m:em\u001b[39m, \u001b[36m:bayes\u001b[39m.\n",
       "\n",
       "    •  \u001b[36mmaxoutdim::Int=0\u001b[39m: Maximum number of output dimensions, uses\n",
       "       max(no\u001b[4mof\u001b[24mfeatures - 1, 1) if 0 (default).\n",
       "\n",
       "    •  \u001b[36mmaxiter::Int=1000\u001b[39m: Maximum number of iterations.\n",
       "\n",
       "    •  \u001b[36mtol::Real=1e-6\u001b[39m: Convergence tolerance.\n",
       "\n",
       "    •  \u001b[36meta::Real=tol\u001b[39m: Variance lower bound\n",
       "\n",
       "    •  \u001b[36mmean::Union{Nothing, Real, Vector{Float64}}=nothing\u001b[39m: If set to\n",
       "       nothing(default) centering will be computed and applied, if set to\n",
       "       \u001b[36m0\u001b[39m no centering(assumed pre-centered), if a vector is passed, the\n",
       "       centering is done with that vector."
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?MLJMultivariateStatsInterface.FactorAnalysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T14:54:07.764000+08:00",
     "start_time": "2022-06-08T06:53:24.705Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLJScikitLearnInterface.AgglomerativeClustering"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using MLJ\n",
    "@load AgglomerativeClustering verbosity=0 #静默模式开启  层次聚类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T14:54:07.764000+08:00",
     "start_time": "2022-06-08T06:53:24.706Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "AgglomerativeClustering\n",
       "\\end{verbatim}\n",
       "A model type for constructing a agglomerative clustering, based on \\href{https://github.com/cstjean/ScikitLearn.jl}{ScikitLearn.jl}, and implementing the MLJ model interface.\n",
       "\n",
       "From MLJ, the type can be imported using\n",
       "\n",
       "\\begin{verbatim}\n",
       "AgglomerativeClustering = @load AgglomerativeClustering pkg=ScikitLearn\n",
       "\\end{verbatim}\n",
       "Do \\texttt{model = AgglomerativeClustering()} to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in \\texttt{AgglomerativeClustering(n\\_clusters=...)}.\n",
       "\n",
       "Recursively merges the pair of clusters that minimally increases a given linkage distance. Note: there is no \\texttt{predict} or \\texttt{transform}. Instead, inspect the \\texttt{fitted\\_params}.\n",
       "\n"
      ],
      "text/markdown": [
       "```\n",
       "AgglomerativeClustering\n",
       "```\n",
       "\n",
       "A model type for constructing a agglomerative clustering, based on [ScikitLearn.jl](https://github.com/cstjean/ScikitLearn.jl), and implementing the MLJ model interface.\n",
       "\n",
       "From MLJ, the type can be imported using\n",
       "\n",
       "```\n",
       "AgglomerativeClustering = @load AgglomerativeClustering pkg=ScikitLearn\n",
       "```\n",
       "\n",
       "Do `model = AgglomerativeClustering()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `AgglomerativeClustering(n_clusters=...)`.\n",
       "\n",
       "Recursively merges the pair of clusters that minimally increases a given linkage distance. Note: there is no `predict` or `transform`. Instead, inspect the `fitted_params`.\n"
      ],
      "text/plain": [
       "\u001b[36m  AgglomerativeClustering\u001b[39m\n",
       "\n",
       "  A model type for constructing a agglomerative clustering, based on\n",
       "  ScikitLearn.jl (https://github.com/cstjean/ScikitLearn.jl), and implementing\n",
       "  the MLJ model interface.\n",
       "\n",
       "  From MLJ, the type can be imported using\n",
       "\n",
       "\u001b[36m  AgglomerativeClustering = @load AgglomerativeClustering pkg=ScikitLearn\u001b[39m\n",
       "\n",
       "  Do \u001b[36mmodel = AgglomerativeClustering()\u001b[39m to construct an instance with default\n",
       "  hyper-parameters. Provide keyword arguments to override hyper-parameter\n",
       "  defaults, as in \u001b[36mAgglomerativeClustering(n_clusters=...)\u001b[39m.\n",
       "\n",
       "  Recursively merges the pair of clusters that minimally increases a given\n",
       "  linkage distance. Note: there is no \u001b[36mpredict\u001b[39m or \u001b[36mtransform\u001b[39m. Instead, inspect\n",
       "  the \u001b[36mfitted_params\u001b[39m."
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?MLJScikitLearnInterface.AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KMedoids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T14:54:07.779000+08:00",
     "start_time": "2022-06-08T06:53:24.708Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLJClusteringInterface.KMedoids"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using MLJ\n",
    "@load KMedoids pkg = Clustering verbosity=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T14:54:07.779000+08:00",
     "start_time": "2022-06-08T06:53:24.709Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "KMedoids(; kwargs...)\n",
       "\n",
       "K-Medoids algorithm: find K centroids corresponding to K clusters in the data. Unlike K-Means, the centroids are found among data points themselves.\n",
       "\n",
       "\\begin{verbatim}\n",
       "## Keywords\n",
       "\n",
       "* `k=3`     : number of centroids\n",
       "* `metric`  : distance metric to use\n",
       "\\end{verbatim}\n",
       "See also the  \\href{http://juliastats.github.io/Clustering.jl/latest/kmedoids.html}{package documentation}.\n",
       "\n"
      ],
      "text/markdown": [
       "KMedoids(; kwargs...)\n",
       "\n",
       "K-Medoids algorithm: find K centroids corresponding to K clusters in the data. Unlike K-Means, the centroids are found among data points themselves.\n",
       "\n",
       "```\n",
       "## Keywords\n",
       "\n",
       "* `k=3`     : number of centroids\n",
       "* `metric`  : distance metric to use\n",
       "```\n",
       "\n",
       "See also the  [package documentation](http://juliastats.github.io/Clustering.jl/latest/kmedoids.html).\n"
      ],
      "text/plain": [
       "  KMedoids(; kwargs...)\n",
       "\n",
       "  K-Medoids algorithm: find K centroids corresponding to K clusters in the\n",
       "  data. Unlike K-Means, the centroids are found among data points themselves.\n",
       "\n",
       "\u001b[36m  ## Keywords\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  * `k=3`     : number of centroids\u001b[39m\n",
       "\u001b[36m  * `metric`  : distance metric to use\u001b[39m\n",
       "\n",
       "  See also the package documentation\n",
       "  (http://juliastats.github.io/Clustering.jl/latest/kmedoids.html)."
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?MLJClusteringInterface.KMedoids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T14:54:07.779000+08:00",
     "start_time": "2022-06-08T06:53:24.711Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLJScikitLearnInterface.DBSCAN"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using MLJ\n",
    "@load DBSCAN pkg = ScikitLearn verbosity=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T14:54:07.779000+08:00",
     "start_time": "2022-06-08T06:53:24.712Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "DBSCAN\n",
       "\\end{verbatim}\n",
       "A model type for constructing a dbscan, based on \\href{https://github.com/cstjean/ScikitLearn.jl}{ScikitLearn.jl}, and implementing the MLJ model interface.\n",
       "\n",
       "From MLJ, the type can be imported using\n",
       "\n",
       "\\begin{verbatim}\n",
       "DBSCAN = @load DBSCAN pkg=ScikitLearn\n",
       "\\end{verbatim}\n",
       "Do \\texttt{model = DBSCAN()} to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in \\texttt{DBSCAN(eps=...)}.\n",
       "\n",
       "Density-Based Spatial Clustering of Applications with Noise. Finds core samples of high density and expands clusters from them. Good for data which contains clusters of similar density.\n",
       "\n"
      ],
      "text/markdown": [
       "```\n",
       "DBSCAN\n",
       "```\n",
       "\n",
       "A model type for constructing a dbscan, based on [ScikitLearn.jl](https://github.com/cstjean/ScikitLearn.jl), and implementing the MLJ model interface.\n",
       "\n",
       "From MLJ, the type can be imported using\n",
       "\n",
       "```\n",
       "DBSCAN = @load DBSCAN pkg=ScikitLearn\n",
       "```\n",
       "\n",
       "Do `model = DBSCAN()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `DBSCAN(eps=...)`.\n",
       "\n",
       "Density-Based Spatial Clustering of Applications with Noise. Finds core samples of high density and expands clusters from them. Good for data which contains clusters of similar density.\n"
      ],
      "text/plain": [
       "\u001b[36m  DBSCAN\u001b[39m\n",
       "\n",
       "  A model type for constructing a dbscan, based on ScikitLearn.jl\n",
       "  (https://github.com/cstjean/ScikitLearn.jl), and implementing the MLJ model\n",
       "  interface.\n",
       "\n",
       "  From MLJ, the type can be imported using\n",
       "\n",
       "\u001b[36m  DBSCAN = @load DBSCAN pkg=ScikitLearn\u001b[39m\n",
       "\n",
       "  Do \u001b[36mmodel = DBSCAN()\u001b[39m to construct an instance with default hyper-parameters.\n",
       "  Provide keyword arguments to override hyper-parameter defaults, as in\n",
       "  \u001b[36mDBSCAN(eps=...)\u001b[39m.\n",
       "\n",
       "  Density-Based Spatial Clustering of Applications with Noise. Finds core\n",
       "  samples of high density and expands clusters from them. Good for data which\n",
       "  contains clusters of similar density."
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?MLJScikitLearnInterface.DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SpectralClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T14:54:07.795000+08:00",
     "start_time": "2022-06-08T06:53:24.713Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLJScikitLearnInterface.SpectralClustering"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using MLJ\n",
    "@load SpectralClustering pkg = ScikitLearn verbosity=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T14:54:07.795000+08:00",
     "start_time": "2022-06-08T06:53:24.714Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "SpectralClustering\n",
       "\\end{verbatim}\n",
       "A model type for constructing a spectral clustering, based on \\href{https://github.com/cstjean/ScikitLearn.jl}{ScikitLearn.jl}, and implementing the MLJ model interface.\n",
       "\n",
       "From MLJ, the type can be imported using\n",
       "\n",
       "\\begin{verbatim}\n",
       "SpectralClustering = @load SpectralClustering pkg=ScikitLearn\n",
       "\\end{verbatim}\n",
       "Do \\texttt{model = SpectralClustering()} to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in \\texttt{SpectralClustering(n\\_clusters=...)}.\n",
       "\n",
       "Apply clustering to a projection of the normalized Laplacian.  In practice spectral clustering is very useful when the structure of the individual clusters is highly non-convex or more generally when a measure of the center and spread of the cluster is not a suitable description of the complete cluster. For instance when clusters are nested circles on the 2D plane.\n",
       "\n"
      ],
      "text/markdown": [
       "```\n",
       "SpectralClustering\n",
       "```\n",
       "\n",
       "A model type for constructing a spectral clustering, based on [ScikitLearn.jl](https://github.com/cstjean/ScikitLearn.jl), and implementing the MLJ model interface.\n",
       "\n",
       "From MLJ, the type can be imported using\n",
       "\n",
       "```\n",
       "SpectralClustering = @load SpectralClustering pkg=ScikitLearn\n",
       "```\n",
       "\n",
       "Do `model = SpectralClustering()` to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in `SpectralClustering(n_clusters=...)`.\n",
       "\n",
       "Apply clustering to a projection of the normalized Laplacian.  In practice spectral clustering is very useful when the structure of the individual clusters is highly non-convex or more generally when a measure of the center and spread of the cluster is not a suitable description of the complete cluster. For instance when clusters are nested circles on the 2D plane.\n"
      ],
      "text/plain": [
       "\u001b[36m  SpectralClustering\u001b[39m\n",
       "\n",
       "  A model type for constructing a spectral clustering, based on ScikitLearn.jl\n",
       "  (https://github.com/cstjean/ScikitLearn.jl), and implementing the MLJ model\n",
       "  interface.\n",
       "\n",
       "  From MLJ, the type can be imported using\n",
       "\n",
       "\u001b[36m  SpectralClustering = @load SpectralClustering pkg=ScikitLearn\u001b[39m\n",
       "\n",
       "  Do \u001b[36mmodel = SpectralClustering()\u001b[39m to construct an instance with default\n",
       "  hyper-parameters. Provide keyword arguments to override hyper-parameter\n",
       "  defaults, as in \u001b[36mSpectralClustering(n_clusters=...)\u001b[39m.\n",
       "\n",
       "  Apply clustering to a projection of the normalized Laplacian. In practice\n",
       "  spectral clustering is very useful when the structure of the individual\n",
       "  clusters is highly non-convex or more generally when a measure of the center\n",
       "  and spread of the cluster is not a suitable description of the complete\n",
       "  cluster. For instance when clusters are nested circles on the 2D plane."
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?MLJScikitLearnInterface.SpectralClustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GMMClusterer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-23T14:16:48.708000+08:00",
     "start_time": "2022-09-23T06:16:40.693Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BetaML.GMM.GMMClusterer"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using MLJ\n",
    "@load GMMClusterer pkg = BetaML verbosity=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BetaML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-23T15:50:48.556000+08:00",
     "start_time": "2022-09-23T07:50:48.553Z"
    }
   },
   "outputs": [],
   "source": [
    "using BetaML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-23T15:50:50.225000+08:00",
     "start_time": "2022-09-23T07:50:50.217Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1mB\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1mM\u001b[22m\u001b[0m\u001b[1mL\u001b[22m \u001b[0m\u001b[1mB\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1mM\u001b[22m\u001b[0m\u001b[1mL\u001b[22mModel \u001b[0m\u001b[1mB\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1mM\u001b[22m\u001b[0m\u001b[1mL\u001b[22mRFImputer \u001b[0m\u001b[1mB\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1mM\u001b[22m\u001b[0m\u001b[1mL\u001b[22mOptionsSet \u001b[0m\u001b[1mB\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1mM\u001b[22m\u001b[0m\u001b[1mL\u001b[22mGMMImputer\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "BetaML\n",
       "\\end{verbatim}\n",
       "The Beta Machine Learning toolkit https://github.com/sylvaticus/BetaML.jl\n",
       "\n",
       "Licence is MIT\n",
       "\n",
       "For documentation, please look at the individual modules or online.\n",
       "\n",
       "While the code is organised in different sub-modules, all objects are re-exported at the BetaML root level, hence the functionality of this package can be accessed by simply \\texttt{using BetaML} and then employing the required function directly. \n",
       "\n"
      ],
      "text/markdown": [
       "```\n",
       "BetaML\n",
       "```\n",
       "\n",
       "The Beta Machine Learning toolkit https://github.com/sylvaticus/BetaML.jl\n",
       "\n",
       "Licence is MIT\n",
       "\n",
       "For documentation, please look at the individual modules or online.\n",
       "\n",
       "While the code is organised in different sub-modules, all objects are re-exported at the BetaML root level, hence the functionality of this package can be accessed by simply `using BetaML` and then employing the required function directly. \n"
      ],
      "text/plain": [
       "\u001b[36m  BetaML\u001b[39m\n",
       "\n",
       "  The Beta Machine Learning toolkit https://github.com/sylvaticus/BetaML.jl\n",
       "\n",
       "  Licence is MIT\n",
       "\n",
       "  For documentation, please look at the individual modules or online.\n",
       "\n",
       "  While the code is organised in different sub-modules, all objects are\n",
       "  re-exported at the BetaML root level, hence the functionality of this\n",
       "  package can be accessed by simply \u001b[36musing BetaML\u001b[39m and then employing the\n",
       "  required function directly."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?BetaML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLJBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-22T09:46:23.812000+08:00",
     "start_time": "2022-07-22T01:46:18.876Z"
    }
   },
   "outputs": [],
   "source": [
    "using MLJBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-01T14:56:00.390000+08:00",
     "start_time": "2022-07-01T06:55:58.115Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1mM\u001b[22m\u001b[0m\u001b[1mL\u001b[22m\u001b[0m\u001b[1mJ\u001b[22m\u001b[0m\u001b[1mB\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1me\u001b[22m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "No docstring found for module \\texttt{MLJBase}.\n",
       "\n",
       "\\section{Exported names}\n",
       "\\texttt{@bind}, \\texttt{@constant}, \\texttt{@from\\_network}, \\texttt{@load\\_ames}, \\texttt{@load\\_boston}, \\texttt{@load\\_crabs}, \\texttt{@load\\_iris}, \\texttt{@load\\_reduced\\_ames}, \\texttt{@load\\_smarket}, \\texttt{@load\\_sunspots}, \\texttt{@mlj\\_model}, \\texttt{@more}, \\texttt{@node}, \\texttt{@pipeline}, \\texttt{@tuple}, \\texttt{AUC}, \\texttt{AbstractNode}, \\texttt{Accuracy}, \\texttt{Annotator}, \\texttt{AnnotatorComposite}, \\texttt{AnnotatorSurrogate}, \\texttt{AreaUnderCurve}, \\texttt{BAC}, \\texttt{BACC}, \\texttt{BalancedAccuracy}, \\texttt{Binary}, \\texttt{BrierLoss}, \\texttt{BrierScore}, \\texttt{CPU1}, \\texttt{CPUProcesses}, \\texttt{CPUThreads}, \\texttt{CV}, \\texttt{CallableReturning}, \\texttt{ColorImage}, \\texttt{Composite}, \\texttt{ConfusionMatrix}, \\texttt{Continuous}, \\texttt{Count}, \\texttt{CrossEntropy}, \\texttt{DWDMarginLoss}, \\texttt{Deterministic}, \\texttt{DeterministicComposite}, \\texttt{DeterministicNetwork}, \\texttt{DeterministicPipeline}, \\texttt{DeterministicSupervisedDetector}, \\texttt{DeterministicSupervisedDetectorComposite}, \\texttt{DeterministicSupervisedDetectorSurrogate}, \\texttt{DeterministicSurrogate}, \\texttt{DeterministicUnsupervisedDetector}, \\texttt{DeterministicUnsupervisedDetectorComposite}, \\texttt{DeterministicUnsupervisedDetectorSurrogate}, \\texttt{ExpLoss}, \\texttt{FDR}, \\texttt{FNR}, \\texttt{FPR}, \\texttt{FScore}, \\texttt{FalseDiscoveryRate}, \\texttt{FalseNegative}, \\texttt{FalseNegativeRate}, \\texttt{FalsePositive}, \\texttt{FalsePositiveRate}, \\texttt{Finite}, \\texttt{GrayImage}, \\texttt{HANDLE\\_GIVEN\\_ID}, \\texttt{Holdout}, \\texttt{HuberLoss}, \\texttt{Image}, \\texttt{Infinite}, \\texttt{Interval}, \\texttt{IntervalComposite}, \\texttt{IntervalPipeline}, \\texttt{IntervalSurrogate}, \\texttt{JointProbabilistic}, \\texttt{JointProbabilisticComposite}, \\texttt{JointProbabilisticSurrogate}, \\texttt{Kappa}, \\texttt{Known}, \\texttt{L1EpsilonInsLoss}, \\texttt{L1HingeLoss}, \\texttt{L2EpsilonInsLoss}, \\texttt{L2HingeLoss}, \\texttt{L2MarginLoss}, \\texttt{LPDistLoss}, \\texttt{LPLoss}, \\texttt{LittleDict}, \\texttt{LogCosh}, \\texttt{LogCoshLoss}, \\texttt{LogLoss}, \\texttt{LogScore}, \\texttt{LogitDistLoss}, \\texttt{LogitMarginLoss}, \\texttt{MAE}, \\texttt{MAPE}, \\texttt{MAV}, \\texttt{MCC}, \\texttt{MCR}, \\texttt{MFDR}, \\texttt{MFNR}, \\texttt{MFPR}, \\texttt{MLJType}, \\texttt{MNPV}, \\texttt{MPPV}, \\texttt{MTNR}, \\texttt{MTPR}, \\texttt{Machine}, \\texttt{MatthewsCorrelation}, \\texttt{MeanAbsoluteError}, \\texttt{MeanAbsoluteProportionalError}, \\texttt{MisclassificationRate}, \\texttt{Model}, \\texttt{ModifiedHuberLoss}, \\texttt{Multiclass}, \\texttt{MulticlassFScore}, \\texttt{MulticlassFalseDiscoveryRate}, \\texttt{MulticlassFalseNegative}, \\texttt{MulticlassFalseNegativeRate}, \\texttt{MulticlassFalsePositive}, \\texttt{MulticlassFalsePositiveRate}, \\texttt{MulticlassNegativePredictiveValue}, \\texttt{MulticlassPrecision}, \\texttt{MulticlassRecall}, \\texttt{MulticlassSpecificity}, \\texttt{MulticlassTrueNegative}, \\texttt{MulticlassTrueNegativeRate}, \\texttt{MulticlassTruePositive}, \\texttt{MulticlassTruePositiveRate}, \\texttt{NPV}, \\texttt{NegativePredictiveValue}, \\texttt{Node}, \\texttt{NominalRange}, \\texttt{Not}, \\texttt{NumericRange}, \\texttt{OrderedFactor}, \\texttt{PPV}, \\texttt{ParamRange}, \\texttt{PerceptronLoss}, \\texttt{PerformanceEvaluation}, \\texttt{PeriodicLoss}, \\texttt{Pipeline}, \\texttt{Precision}, \\texttt{Probabilistic}, \\texttt{ProbabilisticComposite}, \\texttt{ProbabilisticNetwork}, \\texttt{ProbabilisticPipeline}, \\texttt{ProbabilisticSupervisedDetector}, \\texttt{ProbabilisticSupervisedDetectorComposite}, \\texttt{ProbabilisticSupervisedDetectorSurrogate}, \\texttt{ProbabilisticSurrogate}, \\texttt{ProbabilisticUnsupervisedDetector}, \\texttt{ProbabilisticUnsupervisedDetectorComposite}, \\texttt{ProbabilisticUnsupervisedDetectorSurrogate}, \\texttt{QuantileLoss}, \\texttt{RMS}, \\texttt{RMSL}, \\texttt{RMSLP}, \\texttt{RMSP}, \\texttt{RMSPV}, \\texttt{RSQ}, \\texttt{RSquared}, \\texttt{Recall}, \\texttt{Resampler}, \\texttt{ResamplingStrategy}, \\texttt{RootMeanSquaredError}, \\texttt{RootMeanSquaredLogError}, \\texttt{RootMeanSquaredLogProportionalError}, \\texttt{RootMeanSquaredProportionalError}, \\texttt{SigmoidLoss}, \\texttt{SmoothedL1HingeLoss}, \\texttt{Source}, \\texttt{Specificity}, \\texttt{SphericalScore}, \\texttt{Stack}, \\texttt{Static}, \\texttt{StaticComposite}, \\texttt{StaticPipeline}, \\texttt{StaticSurrogate}, \\texttt{StratifiedCV}, \\texttt{Supervised}, \\texttt{SupervisedAnnotator}, \\texttt{SupervisedAnnotatorComposite}, \\texttt{SupervisedAnnotatorSurrogate}, \\texttt{SupervisedComposite}, \\texttt{SupervisedDetector}, \\texttt{SupervisedDetectorComposite}, \\texttt{SupervisedDetectorSurrogate}, \\texttt{SupervisedSurrogate}, \\texttt{Surrogate}, \\texttt{TNR}, \\texttt{TPR}, \\texttt{Table}, \\texttt{Textual}, \\texttt{TimeSeriesCV}, \\texttt{TransformedTargetModel}, \\texttt{TrueNegative}, \\texttt{TrueNegativeRate}, \\texttt{TruePositive}, \\texttt{TruePositiveRate}, \\texttt{UnivariateFinite}, \\texttt{UnivariateFiniteArray}, \\texttt{UnivariateFiniteVector}, \\texttt{Unknown}, \\texttt{Unsupervised}, \\texttt{UnsupervisedAnnotator}, \\texttt{UnsupervisedAnnotatorComposite}, \\texttt{UnsupervisedAnnotatorSurrogate}, \\texttt{UnsupervisedComposite}, \\texttt{UnsupervisedDetector}, \\texttt{UnsupervisedDetectorComposite}, \\texttt{UnsupervisedDetectorSurrogate}, \\texttt{UnsupervisedNetwork}, \\texttt{UnsupervisedPipeline}, \\texttt{UnsupervisedSurrogate}, \\texttt{ZeroOneLoss}, \\texttt{abstract\\_type}, \\texttt{accuracy}, \\texttt{aggregate}, \\texttt{aggregation}, \\texttt{area\\_under\\_curve}, \\texttt{auc}, \\texttt{autotype}, \\texttt{bac}, \\texttt{bacc}, \\texttt{balanced\\_accuracy}, \\texttt{brier\\_loss}, \\texttt{brier\\_score}, \\texttt{categorical}, \\texttt{classes}, \\texttt{clean!}, \\texttt{coerce}, \\texttt{coerce!}, \\texttt{color\\_off}, \\texttt{color\\_on}, \\texttt{complement}, \\texttt{confmat}, \\texttt{confusion\\_matrix}, \\texttt{corestrict}, \\texttt{cross\\_entropy}, \\texttt{decoder}, \\texttt{deep\\_properties}, \\texttt{default\\_measure}, \\texttt{default\\_resource}, \\texttt{distribution\\_type}, \\texttt{docstring}, \\texttt{dwd\\_margin\\_loss}, \\texttt{elscitype}, \\texttt{evaluate}, \\texttt{evaluate!}, \\texttt{exp\\_loss}, \\texttt{f1score}, \\texttt{fallout}, \\texttt{false\\_discovery\\_rate}, \\texttt{false\\_negative}, \\texttt{false\\_negative\\_rate}, \\texttt{false\\_positive}, \\texttt{false\\_positive\\_rate}, \\texttt{falsediscovery\\_rate}, \\texttt{falsenegative}, \\texttt{falsenegative\\_rate}, \\texttt{falsepositive}, \\texttt{falsepositive\\_rate}, \\texttt{fdr}, \\texttt{fit}, \\texttt{fit!}, \\texttt{fit\\_data\\_scitype}, \\texttt{fit\\_only!}, \\texttt{fitted\\_params}, \\texttt{flat\\_values}, \\texttt{fnr}, \\texttt{fpr}, \\texttt{freeze!}, \\texttt{glb}, \\texttt{hit\\_rate}, \\texttt{huber\\_loss}, \\texttt{human\\_name}, \\texttt{hyperparameter\\_ranges}, \\texttt{hyperparameter\\_types}, \\texttt{hyperparameters}, \\texttt{implemented\\_methods}, \\texttt{info}, \\texttt{input\\_scitype}, \\texttt{int}, \\texttt{inverse\\_transform}, \\texttt{inverse\\_transform\\_scitype}, \\texttt{is\\_feature\\_dependent}, \\texttt{is\\_pure\\_julia}, \\texttt{is\\_same\\_except}, \\texttt{is\\_supervised}, \\texttt{is\\_wrapper}, \\texttt{iteration\\_parameter}, \\texttt{iterator}, \\texttt{kappa}, \\texttt{l1}, \\texttt{l1\\_epsilon\\_ins\\_loss}, \\texttt{l1\\_hinge\\_loss}, \\texttt{l2}, \\texttt{l2\\_epsilon\\_ins\\_loss}, \\texttt{l2\\_hinge\\_loss}, \\texttt{l2\\_margin\\_loss}, \\texttt{levels}, \\texttt{levels!}, \\texttt{load\\_ames}, \\texttt{load\\_boston}, \\texttt{load\\_crabs}, \\texttt{load\\_iris}, \\texttt{load\\_path}, \\texttt{load\\_reduced\\_ames}, \\texttt{load\\_smarket}, \\texttt{load\\_sunspots}, \\texttt{log\\_cosh}, \\texttt{log\\_cosh\\_loss}, \\texttt{log\\_loss}, \\texttt{log\\_score}, \\texttt{logit\\_dist\\_loss}, \\texttt{logit\\_margin\\_loss}, \\texttt{logpdf}, \\texttt{lp\\_dist\\_loss}, \\texttt{machine}, \\texttt{machines}, \\texttt{macro\\_avg}, \\texttt{macro\\_f1score}, \\texttt{mae}, \\texttt{make\\_blobs}, \\texttt{make\\_circles}, \\texttt{make\\_moons}, \\texttt{make\\_regression}, \\texttt{mape}, \\texttt{matrix}, \\texttt{matthews\\_correlation}, \\texttt{mav}, \\texttt{mcc}, \\texttt{mcr}, \\texttt{mean}, \\texttt{mean\\_absolute\\_error}, \\texttt{mean\\_absolute\\_value}, \\texttt{measures}, \\texttt{median}, \\texttt{metadata\\_measure}, \\texttt{metadata\\_model}, \\texttt{metadata\\_pkg}, \\texttt{micro\\_avg}, \\texttt{micro\\_f1score}, \\texttt{misclassification\\_rate}, \\texttt{miss\\_rate}, \\texttt{mode}, \\texttt{modified\\_huber\\_loss}, \\texttt{multiclass\\_f1score}, \\texttt{multiclass\\_fallout}, \\texttt{multiclass\\_false\\_discovery\\_rate}, \\texttt{multiclass\\_false\\_negative}, \\texttt{multiclass\\_false\\_negative\\_rate}, \\texttt{multiclass\\_false\\_positive}, \\texttt{multiclass\\_false\\_positive\\_rate}, \\texttt{multiclass\\_falsediscovery\\_rate}, \\texttt{multiclass\\_falsenegative}, \\texttt{multiclass\\_falsenegative\\_rate}, \\texttt{multiclass\\_falsepositive}, \\texttt{multiclass\\_falsepositive\\_rate}, \\texttt{multiclass\\_fdr}, \\texttt{multiclass\\_fnr}, \\texttt{multiclass\\_fpr}, \\texttt{multiclass\\_hit\\_rate}, \\texttt{multiclass\\_miss\\_rate}, \\texttt{multiclass\\_negative\\_predictive\\_value}, \\texttt{multiclass\\_negativepredictive\\_value}, \\texttt{multiclass\\_npv}, \\texttt{multiclass\\_positive\\_predictive\\_value}, \\texttt{multiclass\\_positivepredictive\\_value}, \\texttt{multiclass\\_ppv}, \\texttt{multiclass\\_precision}, \\texttt{multiclass\\_recall}, \\texttt{multiclass\\_selectivity}, \\texttt{multiclass\\_sensitivity}, \\texttt{multiclass\\_specificity}, \\texttt{multiclass\\_tnr}, \\texttt{multiclass\\_tpr}, \\texttt{multiclass\\_true\\_negative}, \\texttt{multiclass\\_true\\_negative\\_rate}, \\texttt{multiclass\\_true\\_positive}, \\texttt{multiclass\\_true\\_positive\\_rate}, \\texttt{multiclass\\_truenegative}, \\texttt{multiclass\\_truenegative\\_rate}, \\texttt{multiclass\\_truepositive}, \\texttt{multiclass\\_truepositive\\_rate}, \\texttt{name}, \\texttt{negative\\_predictive\\_value}, \\texttt{negativepredictive\\_value}, \\texttt{no\\_avg}, \\texttt{node}, \\texttt{nodes}, \\texttt{nonmissing}, \\texttt{npv}, \\texttt{nrows}, \\texttt{nrows\\_at\\_source}, \\texttt{orientation}, \\texttt{origins}, \\texttt{output\\_scitype}, \\texttt{package\\_license}, \\texttt{package\\_name}, \\texttt{package\\_url}, \\texttt{package\\_uuid}, \\texttt{params}, \\texttt{partition}, \\texttt{pdf}, \\texttt{perceptron\\_loss}, \\texttt{periodic\\_loss}, \\texttt{positive\\_predictive\\_value}, \\texttt{positivepredictive\\_value}, \\texttt{ppv}, \\texttt{precision}, \\texttt{predict}, \\texttt{predict\\_joint}, \\texttt{predict\\_mean}, \\texttt{predict\\_median}, \\texttt{predict\\_mode}, \\texttt{predict\\_scitype}, \\texttt{prediction\\_type}, \\texttt{pretty}, \\texttt{quantile\\_loss}, \\texttt{rebind!}, \\texttt{recall}, \\texttt{recursive\\_getproperty}, \\texttt{recursive\\_setproperty!}, \\texttt{report}, \\texttt{reports\\_each\\_observation}, \\texttt{restrict}, \\texttt{return!}, \\texttt{rms}, \\texttt{rmse}, \\texttt{rmsl}, \\texttt{rmsle}, \\texttt{rmslp1}, \\texttt{rmsp}, \\texttt{roc}, \\texttt{roc\\_curve}, \\texttt{root\\_mean\\_squared\\_error}, \\texttt{root\\_mean\\_squared\\_log\\_error}, \\texttt{rsq}, \\texttt{rsquared}, \\texttt{sampler}, \\texttt{scale}, \\texttt{schema}, \\texttt{scitype}, \\texttt{scitype\\_union}, \\texttt{select}, \\texttt{selectcols}, \\texttt{selectivity}, \\texttt{selectrows}, \\texttt{sensitivity}, \\texttt{shuffle}, \\texttt{shuffle!}, \\texttt{sigmoid\\_loss}, \\texttt{skipinvalid}, \\texttt{smoothed\\_l1\\_hinge\\_loss}, \\texttt{source}, \\texttt{sources}, \\texttt{specificity}, \\texttt{spherical\\_score}, \\texttt{std}, \\texttt{support}, \\texttt{supports\\_class\\_weights}, \\texttt{supports\\_online}, \\texttt{supports\\_training\\_losses}, \\texttt{supports\\_weights}, \\texttt{table}, \\texttt{target\\_scitype}, \\texttt{thaw!}, \\texttt{tnr}, \\texttt{tpr}, \\texttt{training\\_losses}, \\texttt{transform}, \\texttt{transform\\_scitype}, \\texttt{true\\_negative}, \\texttt{true\\_negative\\_rate}, \\texttt{true\\_positive}, \\texttt{true\\_positive\\_rate}, \\texttt{truenegative}, \\texttt{truenegative\\_rate}, \\texttt{truepositive}, \\texttt{truepositive\\_rate}, \\texttt{unpack}, \\texttt{unwind}, \\texttt{update}, \\texttt{update\\_data}, \\texttt{value}, \\texttt{zero\\_one\\_loss}\n",
       "\n",
       "\\section{Displaying contents of readme found at \\texttt{D:{\\textbackslash}TongYuan{\\textbackslash}.julia{\\textbackslash}packages{\\textbackslash}MLJBase{\\textbackslash}CMT6L{\\textbackslash}README.md}}\n",
       "\\subsection{MLJBase}\n",
       "Repository for developers that provides core functionality for the \\href{https://github.com/alan-turing-institute/MLJ.jl}{MLJ} machine learning framework.\n",
       "\n",
       "\\begin{tabular}\n",
       "{r | r | r | r}\n",
       "Branch & Julia & Build & Coverage \\\\\n",
       "\\hline\n",
       "\\texttt{master} & v1 & [![Continuous Integration (CPU)][gha-img-master]][gha-url] & [![Code Coverage][codecov-img-master]][codecov-url-master] \\\\\n",
       "\\texttt{dev} & v1 & [![Continuous Integration (CPU)][gha-img-dev]][gha-url] & [![Code Coverage][codecov-img-dev]][codecov-url-dev] \\\\\n",
       "\\end{tabular}\n",
       "[gha-img-master]: https://github.com/JuliaAI/MLJBase.jl/workflows/CI/badge.svg?branch=master \"Continuous Integration (CPU)\" [gha-img-dev]: https://github.com/JuliaAI/MLJBase.jl/workflows/CI/badge.svg?branch=dev \"Continuous Integration (CPU)\" [gha-url]: https://github.com/JuliaAI/MLJBase.jl/actions/workflows/ci.yml\n",
       "\n",
       "[codecov-img-master]: https://codecov.io/gh/JuliaAI/MLJBase.jl/branch/master/graphs/badge.svg?branch=master \"Code Coverage\" [codecov-img-dev]: https://codecov.io/gh/JuliaAI/MLJBase.jl/branch/dev/graphs/badge.svg?branch=dev \"Code Coverage\" [codecov-url-master]: https://codecov.io/github/JuliaAI/MLJBase.jl?branch=master [codecov-url-dev]: https://codecov.io/github/JuliaAI/MLJBase.jl?branch=dev\n",
       "\n",
       "\\href{https://juliaai.github.io/MLJBase.jl/stable/}{\\begin{figure}\n",
       "\\centering\n",
       "\\includegraphics{https://img.shields.io/badge/docs-stable-blue.svg}\n",
       "\\caption{Stable}\n",
       "\\end{figure}\n",
       "}\n",
       "\n",
       "\\href{https://github.com/alan-turing-institute/MLJ.jl}{MLJ} is a Julia framework for combining and tuning machine learning models. This repository provides core functionality for MLJ, including:\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item completing the functionality for methods defined \"minimally\" in MLJ's light-weight model interface \\href{https://github.com/JuliaAI/MLJModelInterface.jl}{MLJModelInterface} (/src/interface)\n",
       "\n",
       "\n",
       "\\item definition of \\textbf{machines} and their associated methods, such as \\texttt{fit!} and \\texttt{predict}/\\texttt{transform} (src/machines). Serialization of machines, however, now lives in \\href{https://github.com/JuliaAI/MLJSerialization.jl}{MLJSerialization}.\n",
       "\n",
       "\n",
       "\\item MLJ's \\textbf{model composition} interface, including \\textbf{learning networks}, \\textbf{pipelines}, \\textbf{stacks}, \\textbf{target transforms} (/src/composition)\n",
       "\n",
       "\n",
       "\\item basic utilities for \\textbf{manipulating datasets} and for \\textbf{synthesizing datasets} (src/data)\n",
       "\n",
       "\n",
       "\\item a \\href{https://alan-turing-institute.github.io/MLJ.jl/dev/evaluating_model_performance/#Custom-resampling-strategies-1}{small interface} for \\textbf{resampling strategies} and implementations, including \\texttt{CV()}, \\texttt{StratifiedCV} and \\texttt{Holdout} (src/resampling.jl)\n",
       "\n",
       "\n",
       "\\item methods for \\textbf{performance evaluation}, based on those resampling strategies (src/resampling.jl)\n",
       "\n",
       "\n",
       "\\item \\textbf{one-dimensional hyperparameter range types}, constructors and associated methods, for use with \\href{https://github.com/JuliaAI/MLJTuning.jl}{MLJTuning} (src/hyperparam)\n",
       "\n",
       "\n",
       "\\item a \\href{https://alan-turing-institute.github.io/MLJ.jl/dev/performance_measures/#Traits-and-custom-measures-1}{small interface} for \\textbf{performance measures} (losses and scores), implementation of about 60 such measures, including integration of the \\href{https://github.com/JuliaML/LossFunctions.jl}{LossFunctions.jl} library (src/measures). To be migrated into separate package in the near future. \n",
       "\n",
       "\\end{itemize}\n"
      ],
      "text/markdown": [
       "No docstring found for module `MLJBase`.\n",
       "\n",
       "# Exported names\n",
       "\n",
       "`@bind`, `@constant`, `@from_network`, `@load_ames`, `@load_boston`, `@load_crabs`, `@load_iris`, `@load_reduced_ames`, `@load_smarket`, `@load_sunspots`, `@mlj_model`, `@more`, `@node`, `@pipeline`, `@tuple`, `AUC`, `AbstractNode`, `Accuracy`, `Annotator`, `AnnotatorComposite`, `AnnotatorSurrogate`, `AreaUnderCurve`, `BAC`, `BACC`, `BalancedAccuracy`, `Binary`, `BrierLoss`, `BrierScore`, `CPU1`, `CPUProcesses`, `CPUThreads`, `CV`, `CallableReturning`, `ColorImage`, `Composite`, `ConfusionMatrix`, `Continuous`, `Count`, `CrossEntropy`, `DWDMarginLoss`, `Deterministic`, `DeterministicComposite`, `DeterministicNetwork`, `DeterministicPipeline`, `DeterministicSupervisedDetector`, `DeterministicSupervisedDetectorComposite`, `DeterministicSupervisedDetectorSurrogate`, `DeterministicSurrogate`, `DeterministicUnsupervisedDetector`, `DeterministicUnsupervisedDetectorComposite`, `DeterministicUnsupervisedDetectorSurrogate`, `ExpLoss`, `FDR`, `FNR`, `FPR`, `FScore`, `FalseDiscoveryRate`, `FalseNegative`, `FalseNegativeRate`, `FalsePositive`, `FalsePositiveRate`, `Finite`, `GrayImage`, `HANDLE_GIVEN_ID`, `Holdout`, `HuberLoss`, `Image`, `Infinite`, `Interval`, `IntervalComposite`, `IntervalPipeline`, `IntervalSurrogate`, `JointProbabilistic`, `JointProbabilisticComposite`, `JointProbabilisticSurrogate`, `Kappa`, `Known`, `L1EpsilonInsLoss`, `L1HingeLoss`, `L2EpsilonInsLoss`, `L2HingeLoss`, `L2MarginLoss`, `LPDistLoss`, `LPLoss`, `LittleDict`, `LogCosh`, `LogCoshLoss`, `LogLoss`, `LogScore`, `LogitDistLoss`, `LogitMarginLoss`, `MAE`, `MAPE`, `MAV`, `MCC`, `MCR`, `MFDR`, `MFNR`, `MFPR`, `MLJType`, `MNPV`, `MPPV`, `MTNR`, `MTPR`, `Machine`, `MatthewsCorrelation`, `MeanAbsoluteError`, `MeanAbsoluteProportionalError`, `MisclassificationRate`, `Model`, `ModifiedHuberLoss`, `Multiclass`, `MulticlassFScore`, `MulticlassFalseDiscoveryRate`, `MulticlassFalseNegative`, `MulticlassFalseNegativeRate`, `MulticlassFalsePositive`, `MulticlassFalsePositiveRate`, `MulticlassNegativePredictiveValue`, `MulticlassPrecision`, `MulticlassRecall`, `MulticlassSpecificity`, `MulticlassTrueNegative`, `MulticlassTrueNegativeRate`, `MulticlassTruePositive`, `MulticlassTruePositiveRate`, `NPV`, `NegativePredictiveValue`, `Node`, `NominalRange`, `Not`, `NumericRange`, `OrderedFactor`, `PPV`, `ParamRange`, `PerceptronLoss`, `PerformanceEvaluation`, `PeriodicLoss`, `Pipeline`, `Precision`, `Probabilistic`, `ProbabilisticComposite`, `ProbabilisticNetwork`, `ProbabilisticPipeline`, `ProbabilisticSupervisedDetector`, `ProbabilisticSupervisedDetectorComposite`, `ProbabilisticSupervisedDetectorSurrogate`, `ProbabilisticSurrogate`, `ProbabilisticUnsupervisedDetector`, `ProbabilisticUnsupervisedDetectorComposite`, `ProbabilisticUnsupervisedDetectorSurrogate`, `QuantileLoss`, `RMS`, `RMSL`, `RMSLP`, `RMSP`, `RMSPV`, `RSQ`, `RSquared`, `Recall`, `Resampler`, `ResamplingStrategy`, `RootMeanSquaredError`, `RootMeanSquaredLogError`, `RootMeanSquaredLogProportionalError`, `RootMeanSquaredProportionalError`, `SigmoidLoss`, `SmoothedL1HingeLoss`, `Source`, `Specificity`, `SphericalScore`, `Stack`, `Static`, `StaticComposite`, `StaticPipeline`, `StaticSurrogate`, `StratifiedCV`, `Supervised`, `SupervisedAnnotator`, `SupervisedAnnotatorComposite`, `SupervisedAnnotatorSurrogate`, `SupervisedComposite`, `SupervisedDetector`, `SupervisedDetectorComposite`, `SupervisedDetectorSurrogate`, `SupervisedSurrogate`, `Surrogate`, `TNR`, `TPR`, `Table`, `Textual`, `TimeSeriesCV`, `TransformedTargetModel`, `TrueNegative`, `TrueNegativeRate`, `TruePositive`, `TruePositiveRate`, `UnivariateFinite`, `UnivariateFiniteArray`, `UnivariateFiniteVector`, `Unknown`, `Unsupervised`, `UnsupervisedAnnotator`, `UnsupervisedAnnotatorComposite`, `UnsupervisedAnnotatorSurrogate`, `UnsupervisedComposite`, `UnsupervisedDetector`, `UnsupervisedDetectorComposite`, `UnsupervisedDetectorSurrogate`, `UnsupervisedNetwork`, `UnsupervisedPipeline`, `UnsupervisedSurrogate`, `ZeroOneLoss`, `abstract_type`, `accuracy`, `aggregate`, `aggregation`, `area_under_curve`, `auc`, `autotype`, `bac`, `bacc`, `balanced_accuracy`, `brier_loss`, `brier_score`, `categorical`, `classes`, `clean!`, `coerce`, `coerce!`, `color_off`, `color_on`, `complement`, `confmat`, `confusion_matrix`, `corestrict`, `cross_entropy`, `decoder`, `deep_properties`, `default_measure`, `default_resource`, `distribution_type`, `docstring`, `dwd_margin_loss`, `elscitype`, `evaluate`, `evaluate!`, `exp_loss`, `f1score`, `fallout`, `false_discovery_rate`, `false_negative`, `false_negative_rate`, `false_positive`, `false_positive_rate`, `falsediscovery_rate`, `falsenegative`, `falsenegative_rate`, `falsepositive`, `falsepositive_rate`, `fdr`, `fit`, `fit!`, `fit_data_scitype`, `fit_only!`, `fitted_params`, `flat_values`, `fnr`, `fpr`, `freeze!`, `glb`, `hit_rate`, `huber_loss`, `human_name`, `hyperparameter_ranges`, `hyperparameter_types`, `hyperparameters`, `implemented_methods`, `info`, `input_scitype`, `int`, `inverse_transform`, `inverse_transform_scitype`, `is_feature_dependent`, `is_pure_julia`, `is_same_except`, `is_supervised`, `is_wrapper`, `iteration_parameter`, `iterator`, `kappa`, `l1`, `l1_epsilon_ins_loss`, `l1_hinge_loss`, `l2`, `l2_epsilon_ins_loss`, `l2_hinge_loss`, `l2_margin_loss`, `levels`, `levels!`, `load_ames`, `load_boston`, `load_crabs`, `load_iris`, `load_path`, `load_reduced_ames`, `load_smarket`, `load_sunspots`, `log_cosh`, `log_cosh_loss`, `log_loss`, `log_score`, `logit_dist_loss`, `logit_margin_loss`, `logpdf`, `lp_dist_loss`, `machine`, `machines`, `macro_avg`, `macro_f1score`, `mae`, `make_blobs`, `make_circles`, `make_moons`, `make_regression`, `mape`, `matrix`, `matthews_correlation`, `mav`, `mcc`, `mcr`, `mean`, `mean_absolute_error`, `mean_absolute_value`, `measures`, `median`, `metadata_measure`, `metadata_model`, `metadata_pkg`, `micro_avg`, `micro_f1score`, `misclassification_rate`, `miss_rate`, `mode`, `modified_huber_loss`, `multiclass_f1score`, `multiclass_fallout`, `multiclass_false_discovery_rate`, `multiclass_false_negative`, `multiclass_false_negative_rate`, `multiclass_false_positive`, `multiclass_false_positive_rate`, `multiclass_falsediscovery_rate`, `multiclass_falsenegative`, `multiclass_falsenegative_rate`, `multiclass_falsepositive`, `multiclass_falsepositive_rate`, `multiclass_fdr`, `multiclass_fnr`, `multiclass_fpr`, `multiclass_hit_rate`, `multiclass_miss_rate`, `multiclass_negative_predictive_value`, `multiclass_negativepredictive_value`, `multiclass_npv`, `multiclass_positive_predictive_value`, `multiclass_positivepredictive_value`, `multiclass_ppv`, `multiclass_precision`, `multiclass_recall`, `multiclass_selectivity`, `multiclass_sensitivity`, `multiclass_specificity`, `multiclass_tnr`, `multiclass_tpr`, `multiclass_true_negative`, `multiclass_true_negative_rate`, `multiclass_true_positive`, `multiclass_true_positive_rate`, `multiclass_truenegative`, `multiclass_truenegative_rate`, `multiclass_truepositive`, `multiclass_truepositive_rate`, `name`, `negative_predictive_value`, `negativepredictive_value`, `no_avg`, `node`, `nodes`, `nonmissing`, `npv`, `nrows`, `nrows_at_source`, `orientation`, `origins`, `output_scitype`, `package_license`, `package_name`, `package_url`, `package_uuid`, `params`, `partition`, `pdf`, `perceptron_loss`, `periodic_loss`, `positive_predictive_value`, `positivepredictive_value`, `ppv`, `precision`, `predict`, `predict_joint`, `predict_mean`, `predict_median`, `predict_mode`, `predict_scitype`, `prediction_type`, `pretty`, `quantile_loss`, `rebind!`, `recall`, `recursive_getproperty`, `recursive_setproperty!`, `report`, `reports_each_observation`, `restrict`, `return!`, `rms`, `rmse`, `rmsl`, `rmsle`, `rmslp1`, `rmsp`, `roc`, `roc_curve`, `root_mean_squared_error`, `root_mean_squared_log_error`, `rsq`, `rsquared`, `sampler`, `scale`, `schema`, `scitype`, `scitype_union`, `select`, `selectcols`, `selectivity`, `selectrows`, `sensitivity`, `shuffle`, `shuffle!`, `sigmoid_loss`, `skipinvalid`, `smoothed_l1_hinge_loss`, `source`, `sources`, `specificity`, `spherical_score`, `std`, `support`, `supports_class_weights`, `supports_online`, `supports_training_losses`, `supports_weights`, `table`, `target_scitype`, `thaw!`, `tnr`, `tpr`, `training_losses`, `transform`, `transform_scitype`, `true_negative`, `true_negative_rate`, `true_positive`, `true_positive_rate`, `truenegative`, `truenegative_rate`, `truepositive`, `truepositive_rate`, `unpack`, `unwind`, `update`, `update_data`, `value`, `zero_one_loss`\n",
       "\n",
       "# Displaying contents of readme found at `D:\\TongYuan\\.julia\\packages\\MLJBase\\CMT6L\\README.md`\n",
       "\n",
       "## MLJBase\n",
       "\n",
       "Repository for developers that provides core functionality for the [MLJ](https://github.com/alan-turing-institute/MLJ.jl) machine learning framework.\n",
       "\n",
       "|   Branch | Julia |                                                      Build |                                                   Coverage |\n",
       "| --------:| -----:| ----------------------------------------------------------:| ----------------------------------------------------------:|\n",
       "| `master` |    v1 | [![Continuous Integration (CPU)][gha-img-master]][gha-url] | [![Code Coverage][codecov-img-master]][codecov-url-master] |\n",
       "|    `dev` |    v1 |    [![Continuous Integration (CPU)][gha-img-dev]][gha-url] |       [![Code Coverage][codecov-img-dev]][codecov-url-dev] |\n",
       "\n",
       "[gha-img-master]: https://github.com/JuliaAI/MLJBase.jl/workflows/CI/badge.svg?branch=master \"Continuous Integration (CPU)\" [gha-img-dev]: https://github.com/JuliaAI/MLJBase.jl/workflows/CI/badge.svg?branch=dev \"Continuous Integration (CPU)\" [gha-url]: https://github.com/JuliaAI/MLJBase.jl/actions/workflows/ci.yml\n",
       "\n",
       "[codecov-img-master]: https://codecov.io/gh/JuliaAI/MLJBase.jl/branch/master/graphs/badge.svg?branch=master \"Code Coverage\" [codecov-img-dev]: https://codecov.io/gh/JuliaAI/MLJBase.jl/branch/dev/graphs/badge.svg?branch=dev \"Code Coverage\" [codecov-url-master]: https://codecov.io/github/JuliaAI/MLJBase.jl?branch=master [codecov-url-dev]: https://codecov.io/github/JuliaAI/MLJBase.jl?branch=dev\n",
       "\n",
       "[![Stable](https://img.shields.io/badge/docs-stable-blue.svg)](https://juliaai.github.io/MLJBase.jl/stable/)\n",
       "\n",
       "[MLJ](https://github.com/alan-turing-institute/MLJ.jl) is a Julia framework for combining and tuning machine learning models. This repository provides core functionality for MLJ, including:\n",
       "\n",
       "  * completing the functionality for methods defined \"minimally\" in MLJ's light-weight model interface [MLJModelInterface](https://github.com/JuliaAI/MLJModelInterface.jl) (/src/interface)\n",
       "  * definition of **machines** and their associated methods, such as `fit!` and `predict`/`transform` (src/machines). Serialization of machines, however, now lives in [MLJSerialization](https://github.com/JuliaAI/MLJSerialization.jl).\n",
       "  * MLJ's **model composition** interface, including **learning networks**, **pipelines**, **stacks**, **target transforms** (/src/composition)\n",
       "  * basic utilities for **manipulating datasets** and for **synthesizing datasets** (src/data)\n",
       "  * a [small interface](https://alan-turing-institute.github.io/MLJ.jl/dev/evaluating_model_performance/#Custom-resampling-strategies-1) for **resampling strategies** and implementations, including `CV()`, `StratifiedCV` and `Holdout` (src/resampling.jl)\n",
       "  * methods for **performance evaluation**, based on those resampling strategies (src/resampling.jl)\n",
       "  * **one-dimensional hyperparameter range types**, constructors and associated methods, for use with [MLJTuning](https://github.com/JuliaAI/MLJTuning.jl) (src/hyperparam)\n",
       "  * a [small interface](https://alan-turing-institute.github.io/MLJ.jl/dev/performance_measures/#Traits-and-custom-measures-1) for **performance measures** (losses and scores), implementation of about 60 such measures, including integration of the [LossFunctions.jl](https://github.com/JuliaML/LossFunctions.jl) library (src/measures). To be migrated into separate package in the near future.\n"
      ],
      "text/plain": [
       "  No docstring found for module \u001b[36mMLJBase\u001b[39m.\n",
       "\n",
       "\u001b[1m  Exported names\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "  \u001b[36m@bind\u001b[39m, \u001b[36m@constant\u001b[39m, \u001b[36m@from_network\u001b[39m, \u001b[36m@load_ames\u001b[39m, \u001b[36m@load_boston\u001b[39m, \u001b[36m@load_crabs\u001b[39m,\n",
       "  \u001b[36m@load_iris\u001b[39m, \u001b[36m@load_reduced_ames\u001b[39m, \u001b[36m@load_smarket\u001b[39m, \u001b[36m@load_sunspots\u001b[39m, \u001b[36m@mlj_model\u001b[39m,\n",
       "  \u001b[36m@more\u001b[39m, \u001b[36m@node\u001b[39m, \u001b[36m@pipeline\u001b[39m, \u001b[36m@tuple\u001b[39m, \u001b[36mAUC\u001b[39m, \u001b[36mAbstractNode\u001b[39m, \u001b[36mAccuracy\u001b[39m, \u001b[36mAnnotator\u001b[39m,\n",
       "  \u001b[36mAnnotatorComposite\u001b[39m, \u001b[36mAnnotatorSurrogate\u001b[39m, \u001b[36mAreaUnderCurve\u001b[39m, \u001b[36mBAC\u001b[39m, \u001b[36mBACC\u001b[39m,\n",
       "  \u001b[36mBalancedAccuracy\u001b[39m, \u001b[36mBinary\u001b[39m, \u001b[36mBrierLoss\u001b[39m, \u001b[36mBrierScore\u001b[39m, \u001b[36mCPU1\u001b[39m, \u001b[36mCPUProcesses\u001b[39m,\n",
       "  \u001b[36mCPUThreads\u001b[39m, \u001b[36mCV\u001b[39m, \u001b[36mCallableReturning\u001b[39m, \u001b[36mColorImage\u001b[39m, \u001b[36mComposite\u001b[39m, \u001b[36mConfusionMatrix\u001b[39m,\n",
       "  \u001b[36mContinuous\u001b[39m, \u001b[36mCount\u001b[39m, \u001b[36mCrossEntropy\u001b[39m, \u001b[36mDWDMarginLoss\u001b[39m, \u001b[36mDeterministic\u001b[39m,\n",
       "  \u001b[36mDeterministicComposite\u001b[39m, \u001b[36mDeterministicNetwork\u001b[39m, \u001b[36mDeterministicPipeline\u001b[39m,\n",
       "  \u001b[36mDeterministicSupervisedDetector\u001b[39m, \u001b[36mDeterministicSupervisedDetectorComposite\u001b[39m,\n",
       "  \u001b[36mDeterministicSupervisedDetectorSurrogate\u001b[39m, \u001b[36mDeterministicSurrogate\u001b[39m,\n",
       "  \u001b[36mDeterministicUnsupervisedDetector\u001b[39m,\n",
       "  \u001b[36mDeterministicUnsupervisedDetectorComposite\u001b[39m,\n",
       "  \u001b[36mDeterministicUnsupervisedDetectorSurrogate\u001b[39m, \u001b[36mExpLoss\u001b[39m, \u001b[36mFDR\u001b[39m, \u001b[36mFNR\u001b[39m, \u001b[36mFPR\u001b[39m, \u001b[36mFScore\u001b[39m,\n",
       "  \u001b[36mFalseDiscoveryRate\u001b[39m, \u001b[36mFalseNegative\u001b[39m, \u001b[36mFalseNegativeRate\u001b[39m, \u001b[36mFalsePositive\u001b[39m,\n",
       "  \u001b[36mFalsePositiveRate\u001b[39m, \u001b[36mFinite\u001b[39m, \u001b[36mGrayImage\u001b[39m, \u001b[36mHANDLE_GIVEN_ID\u001b[39m, \u001b[36mHoldout\u001b[39m, \u001b[36mHuberLoss\u001b[39m,\n",
       "  \u001b[36mImage\u001b[39m, \u001b[36mInfinite\u001b[39m, \u001b[36mInterval\u001b[39m, \u001b[36mIntervalComposite\u001b[39m, \u001b[36mIntervalPipeline\u001b[39m,\n",
       "  \u001b[36mIntervalSurrogate\u001b[39m, \u001b[36mJointProbabilistic\u001b[39m, \u001b[36mJointProbabilisticComposite\u001b[39m,\n",
       "  \u001b[36mJointProbabilisticSurrogate\u001b[39m, \u001b[36mKappa\u001b[39m, \u001b[36mKnown\u001b[39m, \u001b[36mL1EpsilonInsLoss\u001b[39m, \u001b[36mL1HingeLoss\u001b[39m,\n",
       "  \u001b[36mL2EpsilonInsLoss\u001b[39m, \u001b[36mL2HingeLoss\u001b[39m, \u001b[36mL2MarginLoss\u001b[39m, \u001b[36mLPDistLoss\u001b[39m, \u001b[36mLPLoss\u001b[39m, \u001b[36mLittleDict\u001b[39m,\n",
       "  \u001b[36mLogCosh\u001b[39m, \u001b[36mLogCoshLoss\u001b[39m, \u001b[36mLogLoss\u001b[39m, \u001b[36mLogScore\u001b[39m, \u001b[36mLogitDistLoss\u001b[39m, \u001b[36mLogitMarginLoss\u001b[39m,\n",
       "  \u001b[36mMAE\u001b[39m, \u001b[36mMAPE\u001b[39m, \u001b[36mMAV\u001b[39m, \u001b[36mMCC\u001b[39m, \u001b[36mMCR\u001b[39m, \u001b[36mMFDR\u001b[39m, \u001b[36mMFNR\u001b[39m, \u001b[36mMFPR\u001b[39m, \u001b[36mMLJType\u001b[39m, \u001b[36mMNPV\u001b[39m, \u001b[36mMPPV\u001b[39m, \u001b[36mMTNR\u001b[39m, \u001b[36mMTPR\u001b[39m,\n",
       "  \u001b[36mMachine\u001b[39m, \u001b[36mMatthewsCorrelation\u001b[39m, \u001b[36mMeanAbsoluteError\u001b[39m,\n",
       "  \u001b[36mMeanAbsoluteProportionalError\u001b[39m, \u001b[36mMisclassificationRate\u001b[39m, \u001b[36mModel\u001b[39m,\n",
       "  \u001b[36mModifiedHuberLoss\u001b[39m, \u001b[36mMulticlass\u001b[39m, \u001b[36mMulticlassFScore\u001b[39m,\n",
       "  \u001b[36mMulticlassFalseDiscoveryRate\u001b[39m, \u001b[36mMulticlassFalseNegative\u001b[39m,\n",
       "  \u001b[36mMulticlassFalseNegativeRate\u001b[39m, \u001b[36mMulticlassFalsePositive\u001b[39m,\n",
       "  \u001b[36mMulticlassFalsePositiveRate\u001b[39m, \u001b[36mMulticlassNegativePredictiveValue\u001b[39m,\n",
       "  \u001b[36mMulticlassPrecision\u001b[39m, \u001b[36mMulticlassRecall\u001b[39m, \u001b[36mMulticlassSpecificity\u001b[39m,\n",
       "  \u001b[36mMulticlassTrueNegative\u001b[39m, \u001b[36mMulticlassTrueNegativeRate\u001b[39m, \u001b[36mMulticlassTruePositive\u001b[39m,\n",
       "  \u001b[36mMulticlassTruePositiveRate\u001b[39m, \u001b[36mNPV\u001b[39m, \u001b[36mNegativePredictiveValue\u001b[39m, \u001b[36mNode\u001b[39m,\n",
       "  \u001b[36mNominalRange\u001b[39m, \u001b[36mNot\u001b[39m, \u001b[36mNumericRange\u001b[39m, \u001b[36mOrderedFactor\u001b[39m, \u001b[36mPPV\u001b[39m, \u001b[36mParamRange\u001b[39m,\n",
       "  \u001b[36mPerceptronLoss\u001b[39m, \u001b[36mPerformanceEvaluation\u001b[39m, \u001b[36mPeriodicLoss\u001b[39m, \u001b[36mPipeline\u001b[39m, \u001b[36mPrecision\u001b[39m,\n",
       "  \u001b[36mProbabilistic\u001b[39m, \u001b[36mProbabilisticComposite\u001b[39m, \u001b[36mProbabilisticNetwork\u001b[39m,\n",
       "  \u001b[36mProbabilisticPipeline\u001b[39m, \u001b[36mProbabilisticSupervisedDetector\u001b[39m,\n",
       "  \u001b[36mProbabilisticSupervisedDetectorComposite\u001b[39m,\n",
       "  \u001b[36mProbabilisticSupervisedDetectorSurrogate\u001b[39m, \u001b[36mProbabilisticSurrogate\u001b[39m,\n",
       "  \u001b[36mProbabilisticUnsupervisedDetector\u001b[39m,\n",
       "  \u001b[36mProbabilisticUnsupervisedDetectorComposite\u001b[39m,\n",
       "  \u001b[36mProbabilisticUnsupervisedDetectorSurrogate\u001b[39m, \u001b[36mQuantileLoss\u001b[39m, \u001b[36mRMS\u001b[39m, \u001b[36mRMSL\u001b[39m, \u001b[36mRMSLP\u001b[39m,\n",
       "  \u001b[36mRMSP\u001b[39m, \u001b[36mRMSPV\u001b[39m, \u001b[36mRSQ\u001b[39m, \u001b[36mRSquared\u001b[39m, \u001b[36mRecall\u001b[39m, \u001b[36mResampler\u001b[39m, \u001b[36mResamplingStrategy\u001b[39m,\n",
       "  \u001b[36mRootMeanSquaredError\u001b[39m, \u001b[36mRootMeanSquaredLogError\u001b[39m,\n",
       "  \u001b[36mRootMeanSquaredLogProportionalError\u001b[39m, \u001b[36mRootMeanSquaredProportionalError\u001b[39m,\n",
       "  \u001b[36mSigmoidLoss\u001b[39m, \u001b[36mSmoothedL1HingeLoss\u001b[39m, \u001b[36mSource\u001b[39m, \u001b[36mSpecificity\u001b[39m, \u001b[36mSphericalScore\u001b[39m,\n",
       "  \u001b[36mStack\u001b[39m, \u001b[36mStatic\u001b[39m, \u001b[36mStaticComposite\u001b[39m, \u001b[36mStaticPipeline\u001b[39m, \u001b[36mStaticSurrogate\u001b[39m,\n",
       "  \u001b[36mStratifiedCV\u001b[39m, \u001b[36mSupervised\u001b[39m, \u001b[36mSupervisedAnnotator\u001b[39m, \u001b[36mSupervisedAnnotatorComposite\u001b[39m,\n",
       "  \u001b[36mSupervisedAnnotatorSurrogate\u001b[39m, \u001b[36mSupervisedComposite\u001b[39m, \u001b[36mSupervisedDetector\u001b[39m,\n",
       "  \u001b[36mSupervisedDetectorComposite\u001b[39m, \u001b[36mSupervisedDetectorSurrogate\u001b[39m,\n",
       "  \u001b[36mSupervisedSurrogate\u001b[39m, \u001b[36mSurrogate\u001b[39m, \u001b[36mTNR\u001b[39m, \u001b[36mTPR\u001b[39m, \u001b[36mTable\u001b[39m, \u001b[36mTextual\u001b[39m, \u001b[36mTimeSeriesCV\u001b[39m,\n",
       "  \u001b[36mTransformedTargetModel\u001b[39m, \u001b[36mTrueNegative\u001b[39m, \u001b[36mTrueNegativeRate\u001b[39m, \u001b[36mTruePositive\u001b[39m,\n",
       "  \u001b[36mTruePositiveRate\u001b[39m, \u001b[36mUnivariateFinite\u001b[39m, \u001b[36mUnivariateFiniteArray\u001b[39m,\n",
       "  \u001b[36mUnivariateFiniteVector\u001b[39m, \u001b[36mUnknown\u001b[39m, \u001b[36mUnsupervised\u001b[39m, \u001b[36mUnsupervisedAnnotator\u001b[39m,\n",
       "  \u001b[36mUnsupervisedAnnotatorComposite\u001b[39m, \u001b[36mUnsupervisedAnnotatorSurrogate\u001b[39m,\n",
       "  \u001b[36mUnsupervisedComposite\u001b[39m, \u001b[36mUnsupervisedDetector\u001b[39m, \u001b[36mUnsupervisedDetectorComposite\u001b[39m,\n",
       "  \u001b[36mUnsupervisedDetectorSurrogate\u001b[39m, \u001b[36mUnsupervisedNetwork\u001b[39m, \u001b[36mUnsupervisedPipeline\u001b[39m,\n",
       "  \u001b[36mUnsupervisedSurrogate\u001b[39m, \u001b[36mZeroOneLoss\u001b[39m, \u001b[36mabstract_type\u001b[39m, \u001b[36maccuracy\u001b[39m, \u001b[36maggregate\u001b[39m,\n",
       "  \u001b[36maggregation\u001b[39m, \u001b[36marea_under_curve\u001b[39m, \u001b[36mauc\u001b[39m, \u001b[36mautotype\u001b[39m, \u001b[36mbac\u001b[39m, \u001b[36mbacc\u001b[39m, \u001b[36mbalanced_accuracy\u001b[39m,\n",
       "  \u001b[36mbrier_loss\u001b[39m, \u001b[36mbrier_score\u001b[39m, \u001b[36mcategorical\u001b[39m, \u001b[36mclasses\u001b[39m, \u001b[36mclean!\u001b[39m, \u001b[36mcoerce\u001b[39m, \u001b[36mcoerce!\u001b[39m,\n",
       "  \u001b[36mcolor_off\u001b[39m, \u001b[36mcolor_on\u001b[39m, \u001b[36mcomplement\u001b[39m, \u001b[36mconfmat\u001b[39m, \u001b[36mconfusion_matrix\u001b[39m, \u001b[36mcorestrict\u001b[39m,\n",
       "  \u001b[36mcross_entropy\u001b[39m, \u001b[36mdecoder\u001b[39m, \u001b[36mdeep_properties\u001b[39m, \u001b[36mdefault_measure\u001b[39m, \u001b[36mdefault_resource\u001b[39m,\n",
       "  \u001b[36mdistribution_type\u001b[39m, \u001b[36mdocstring\u001b[39m, \u001b[36mdwd_margin_loss\u001b[39m, \u001b[36melscitype\u001b[39m, \u001b[36mevaluate\u001b[39m,\n",
       "  \u001b[36mevaluate!\u001b[39m, \u001b[36mexp_loss\u001b[39m, \u001b[36mf1score\u001b[39m, \u001b[36mfallout\u001b[39m, \u001b[36mfalse_discovery_rate\u001b[39m, \u001b[36mfalse_negative\u001b[39m,\n",
       "  \u001b[36mfalse_negative_rate\u001b[39m, \u001b[36mfalse_positive\u001b[39m, \u001b[36mfalse_positive_rate\u001b[39m,\n",
       "  \u001b[36mfalsediscovery_rate\u001b[39m, \u001b[36mfalsenegative\u001b[39m, \u001b[36mfalsenegative_rate\u001b[39m, \u001b[36mfalsepositive\u001b[39m,\n",
       "  \u001b[36mfalsepositive_rate\u001b[39m, \u001b[36mfdr\u001b[39m, \u001b[36mfit\u001b[39m, \u001b[36mfit!\u001b[39m, \u001b[36mfit_data_scitype\u001b[39m, \u001b[36mfit_only!\u001b[39m,\n",
       "  \u001b[36mfitted_params\u001b[39m, \u001b[36mflat_values\u001b[39m, \u001b[36mfnr\u001b[39m, \u001b[36mfpr\u001b[39m, \u001b[36mfreeze!\u001b[39m, \u001b[36mglb\u001b[39m, \u001b[36mhit_rate\u001b[39m, \u001b[36mhuber_loss\u001b[39m,\n",
       "  \u001b[36mhuman_name\u001b[39m, \u001b[36mhyperparameter_ranges\u001b[39m, \u001b[36mhyperparameter_types\u001b[39m, \u001b[36mhyperparameters\u001b[39m,\n",
       "  \u001b[36mimplemented_methods\u001b[39m, \u001b[36minfo\u001b[39m, \u001b[36minput_scitype\u001b[39m, \u001b[36mint\u001b[39m, \u001b[36minverse_transform\u001b[39m,\n",
       "  \u001b[36minverse_transform_scitype\u001b[39m, \u001b[36mis_feature_dependent\u001b[39m, \u001b[36mis_pure_julia\u001b[39m,\n",
       "  \u001b[36mis_same_except\u001b[39m, \u001b[36mis_supervised\u001b[39m, \u001b[36mis_wrapper\u001b[39m, \u001b[36miteration_parameter\u001b[39m, \u001b[36miterator\u001b[39m,\n",
       "  \u001b[36mkappa\u001b[39m, \u001b[36ml1\u001b[39m, \u001b[36ml1_epsilon_ins_loss\u001b[39m, \u001b[36ml1_hinge_loss\u001b[39m, \u001b[36ml2\u001b[39m, \u001b[36ml2_epsilon_ins_loss\u001b[39m,\n",
       "  \u001b[36ml2_hinge_loss\u001b[39m, \u001b[36ml2_margin_loss\u001b[39m, \u001b[36mlevels\u001b[39m, \u001b[36mlevels!\u001b[39m, \u001b[36mload_ames\u001b[39m, \u001b[36mload_boston\u001b[39m,\n",
       "  \u001b[36mload_crabs\u001b[39m, \u001b[36mload_iris\u001b[39m, \u001b[36mload_path\u001b[39m, \u001b[36mload_reduced_ames\u001b[39m, \u001b[36mload_smarket\u001b[39m,\n",
       "  \u001b[36mload_sunspots\u001b[39m, \u001b[36mlog_cosh\u001b[39m, \u001b[36mlog_cosh_loss\u001b[39m, \u001b[36mlog_loss\u001b[39m, \u001b[36mlog_score\u001b[39m,\n",
       "  \u001b[36mlogit_dist_loss\u001b[39m, \u001b[36mlogit_margin_loss\u001b[39m, \u001b[36mlogpdf\u001b[39m, \u001b[36mlp_dist_loss\u001b[39m, \u001b[36mmachine\u001b[39m, \u001b[36mmachines\u001b[39m,\n",
       "  \u001b[36mmacro_avg\u001b[39m, \u001b[36mmacro_f1score\u001b[39m, \u001b[36mmae\u001b[39m, \u001b[36mmake_blobs\u001b[39m, \u001b[36mmake_circles\u001b[39m, \u001b[36mmake_moons\u001b[39m,\n",
       "  \u001b[36mmake_regression\u001b[39m, \u001b[36mmape\u001b[39m, \u001b[36mmatrix\u001b[39m, \u001b[36mmatthews_correlation\u001b[39m, \u001b[36mmav\u001b[39m, \u001b[36mmcc\u001b[39m, \u001b[36mmcr\u001b[39m, \u001b[36mmean\u001b[39m,\n",
       "  \u001b[36mmean_absolute_error\u001b[39m, \u001b[36mmean_absolute_value\u001b[39m, \u001b[36mmeasures\u001b[39m, \u001b[36mmedian\u001b[39m,\n",
       "  \u001b[36mmetadata_measure\u001b[39m, \u001b[36mmetadata_model\u001b[39m, \u001b[36mmetadata_pkg\u001b[39m, \u001b[36mmicro_avg\u001b[39m, \u001b[36mmicro_f1score\u001b[39m,\n",
       "  \u001b[36mmisclassification_rate\u001b[39m, \u001b[36mmiss_rate\u001b[39m, \u001b[36mmode\u001b[39m, \u001b[36mmodified_huber_loss\u001b[39m,\n",
       "  \u001b[36mmulticlass_f1score\u001b[39m, \u001b[36mmulticlass_fallout\u001b[39m, \u001b[36mmulticlass_false_discovery_rate\u001b[39m,\n",
       "  \u001b[36mmulticlass_false_negative\u001b[39m, \u001b[36mmulticlass_false_negative_rate\u001b[39m,\n",
       "  \u001b[36mmulticlass_false_positive\u001b[39m, \u001b[36mmulticlass_false_positive_rate\u001b[39m,\n",
       "  \u001b[36mmulticlass_falsediscovery_rate\u001b[39m, \u001b[36mmulticlass_falsenegative\u001b[39m,\n",
       "  \u001b[36mmulticlass_falsenegative_rate\u001b[39m, \u001b[36mmulticlass_falsepositive\u001b[39m,\n",
       "  \u001b[36mmulticlass_falsepositive_rate\u001b[39m, \u001b[36mmulticlass_fdr\u001b[39m, \u001b[36mmulticlass_fnr\u001b[39m,\n",
       "  \u001b[36mmulticlass_fpr\u001b[39m, \u001b[36mmulticlass_hit_rate\u001b[39m, \u001b[36mmulticlass_miss_rate\u001b[39m,\n",
       "  \u001b[36mmulticlass_negative_predictive_value\u001b[39m, \u001b[36mmulticlass_negativepredictive_value\u001b[39m,\n",
       "  \u001b[36mmulticlass_npv\u001b[39m, \u001b[36mmulticlass_positive_predictive_value\u001b[39m,\n",
       "  \u001b[36mmulticlass_positivepredictive_value\u001b[39m, \u001b[36mmulticlass_ppv\u001b[39m, \u001b[36mmulticlass_precision\u001b[39m,\n",
       "  \u001b[36mmulticlass_recall\u001b[39m, \u001b[36mmulticlass_selectivity\u001b[39m, \u001b[36mmulticlass_sensitivity\u001b[39m,\n",
       "  \u001b[36mmulticlass_specificity\u001b[39m, \u001b[36mmulticlass_tnr\u001b[39m, \u001b[36mmulticlass_tpr\u001b[39m,\n",
       "  \u001b[36mmulticlass_true_negative\u001b[39m, \u001b[36mmulticlass_true_negative_rate\u001b[39m,\n",
       "  \u001b[36mmulticlass_true_positive\u001b[39m, \u001b[36mmulticlass_true_positive_rate\u001b[39m,\n",
       "  \u001b[36mmulticlass_truenegative\u001b[39m, \u001b[36mmulticlass_truenegative_rate\u001b[39m,\n",
       "  \u001b[36mmulticlass_truepositive\u001b[39m, \u001b[36mmulticlass_truepositive_rate\u001b[39m, \u001b[36mname\u001b[39m,\n",
       "  \u001b[36mnegative_predictive_value\u001b[39m, \u001b[36mnegativepredictive_value\u001b[39m, \u001b[36mno_avg\u001b[39m, \u001b[36mnode\u001b[39m, \u001b[36mnodes\u001b[39m,\n",
       "  \u001b[36mnonmissing\u001b[39m, \u001b[36mnpv\u001b[39m, \u001b[36mnrows\u001b[39m, \u001b[36mnrows_at_source\u001b[39m, \u001b[36morientation\u001b[39m, \u001b[36morigins\u001b[39m,\n",
       "  \u001b[36moutput_scitype\u001b[39m, \u001b[36mpackage_license\u001b[39m, \u001b[36mpackage_name\u001b[39m, \u001b[36mpackage_url\u001b[39m, \u001b[36mpackage_uuid\u001b[39m,\n",
       "  \u001b[36mparams\u001b[39m, \u001b[36mpartition\u001b[39m, \u001b[36mpdf\u001b[39m, \u001b[36mperceptron_loss\u001b[39m, \u001b[36mperiodic_loss\u001b[39m,\n",
       "  \u001b[36mpositive_predictive_value\u001b[39m, \u001b[36mpositivepredictive_value\u001b[39m, \u001b[36mppv\u001b[39m, \u001b[36mprecision\u001b[39m,\n",
       "  \u001b[36mpredict\u001b[39m, \u001b[36mpredict_joint\u001b[39m, \u001b[36mpredict_mean\u001b[39m, \u001b[36mpredict_median\u001b[39m, \u001b[36mpredict_mode\u001b[39m,\n",
       "  \u001b[36mpredict_scitype\u001b[39m, \u001b[36mprediction_type\u001b[39m, \u001b[36mpretty\u001b[39m, \u001b[36mquantile_loss\u001b[39m, \u001b[36mrebind!\u001b[39m, \u001b[36mrecall\u001b[39m,\n",
       "  \u001b[36mrecursive_getproperty\u001b[39m, \u001b[36mrecursive_setproperty!\u001b[39m, \u001b[36mreport\u001b[39m,\n",
       "  \u001b[36mreports_each_observation\u001b[39m, \u001b[36mrestrict\u001b[39m, \u001b[36mreturn!\u001b[39m, \u001b[36mrms\u001b[39m, \u001b[36mrmse\u001b[39m, \u001b[36mrmsl\u001b[39m, \u001b[36mrmsle\u001b[39m, \u001b[36mrmslp1\u001b[39m,\n",
       "  \u001b[36mrmsp\u001b[39m, \u001b[36mroc\u001b[39m, \u001b[36mroc_curve\u001b[39m, \u001b[36mroot_mean_squared_error\u001b[39m, \u001b[36mroot_mean_squared_log_error\u001b[39m,\n",
       "  \u001b[36mrsq\u001b[39m, \u001b[36mrsquared\u001b[39m, \u001b[36msampler\u001b[39m, \u001b[36mscale\u001b[39m, \u001b[36mschema\u001b[39m, \u001b[36mscitype\u001b[39m, \u001b[36mscitype_union\u001b[39m, \u001b[36mselect\u001b[39m,\n",
       "  \u001b[36mselectcols\u001b[39m, \u001b[36mselectivity\u001b[39m, \u001b[36mselectrows\u001b[39m, \u001b[36msensitivity\u001b[39m, \u001b[36mshuffle\u001b[39m, \u001b[36mshuffle!\u001b[39m,\n",
       "  \u001b[36msigmoid_loss\u001b[39m, \u001b[36mskipinvalid\u001b[39m, \u001b[36msmoothed_l1_hinge_loss\u001b[39m, \u001b[36msource\u001b[39m, \u001b[36msources\u001b[39m,\n",
       "  \u001b[36mspecificity\u001b[39m, \u001b[36mspherical_score\u001b[39m, \u001b[36mstd\u001b[39m, \u001b[36msupport\u001b[39m, \u001b[36msupports_class_weights\u001b[39m,\n",
       "  \u001b[36msupports_online\u001b[39m, \u001b[36msupports_training_losses\u001b[39m, \u001b[36msupports_weights\u001b[39m, \u001b[36mtable\u001b[39m,\n",
       "  \u001b[36mtarget_scitype\u001b[39m, \u001b[36mthaw!\u001b[39m, \u001b[36mtnr\u001b[39m, \u001b[36mtpr\u001b[39m, \u001b[36mtraining_losses\u001b[39m, \u001b[36mtransform\u001b[39m,\n",
       "  \u001b[36mtransform_scitype\u001b[39m, \u001b[36mtrue_negative\u001b[39m, \u001b[36mtrue_negative_rate\u001b[39m, \u001b[36mtrue_positive\u001b[39m,\n",
       "  \u001b[36mtrue_positive_rate\u001b[39m, \u001b[36mtruenegative\u001b[39m, \u001b[36mtruenegative_rate\u001b[39m, \u001b[36mtruepositive\u001b[39m,\n",
       "  \u001b[36mtruepositive_rate\u001b[39m, \u001b[36munpack\u001b[39m, \u001b[36munwind\u001b[39m, \u001b[36mupdate\u001b[39m, \u001b[36mupdate_data\u001b[39m, \u001b[36mvalue\u001b[39m, \u001b[36mzero_one_loss\u001b[39m\n",
       "\n",
       "\u001b[1m  Displaying contents of readme found at\u001b[22m\n",
       "\u001b[1m \u001b[36mD:\\TongYuan\\.julia\\packages\\MLJBase\\CMT6L\\README.md\u001b[39m\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "\u001b[1m  MLJBase\u001b[22m\n",
       "\u001b[1m  =========\u001b[22m\n",
       "\n",
       "  Repository for developers that provides core functionality for the MLJ\n",
       "  (https://github.com/alan-turing-institute/MLJ.jl) machine learning\n",
       "  framework.\n",
       "\n",
       "  Branch Julia                                                      Build                                                   Coverage\n",
       "  –––––– ––––– –––––––––––––––––––––––––––––––––––––––––––––––––––––––––– ––––––––––––––––––––––––––––––––––––––––––––––––––––––––––\n",
       "  \u001b[36mmaster\u001b[39m    v1 [![Continuous Integration (CPU)][gha-img-master]][gha-url] [![Code Coverage][codecov-img-master]][codecov-url-master]\n",
       "     \u001b[36mdev\u001b[39m    v1    [![Continuous Integration (CPU)][gha-img-dev]][gha-url]       [![Code Coverage][codecov-img-dev]][codecov-url-dev]\n",
       "\n",
       "  [gha-img-master]:\n",
       "  https://github.com/JuliaAI/MLJBase.jl/workflows/CI/badge.svg?branch=master\n",
       "  \"Continuous Integration (CPU)\" [gha-img-dev]:\n",
       "  https://github.com/JuliaAI/MLJBase.jl/workflows/CI/badge.svg?branch=dev\n",
       "  \"Continuous Integration (CPU)\" [gha-url]:\n",
       "  https://github.com/JuliaAI/MLJBase.jl/actions/workflows/ci.yml\n",
       "\n",
       "  [codecov-img-master]:\n",
       "  https://codecov.io/gh/JuliaAI/MLJBase.jl/branch/master/graphs/badge.svg?branch=master\n",
       "  \"Code Coverage\" [codecov-img-dev]:\n",
       "  https://codecov.io/gh/JuliaAI/MLJBase.jl/branch/dev/graphs/badge.svg?branch=dev\n",
       "  \"Code Coverage\" [codecov-url-master]:\n",
       "  https://codecov.io/github/JuliaAI/MLJBase.jl?branch=master\n",
       "  [codecov-url-dev]: https://codecov.io/github/JuliaAI/MLJBase.jl?branch=dev\n",
       "\n",
       "  (Image: Stable) (https://juliaai.github.io/MLJBase.jl/stable/)\n",
       "\n",
       "  MLJ (https://github.com/alan-turing-institute/MLJ.jl) is a Julia framework\n",
       "  for combining and tuning machine learning models. This repository provides\n",
       "  core functionality for MLJ, including:\n",
       "\n",
       "    •  completing the functionality for methods defined \"minimally\" in\n",
       "       MLJ's light-weight model interface MLJModelInterface\n",
       "       (https://github.com/JuliaAI/MLJModelInterface.jl) (/src/interface)\n",
       "\n",
       "    •  definition of \u001b[1mmachines\u001b[22m and their associated methods, such as \u001b[36mfit!\u001b[39m\n",
       "       and \u001b[36mpredict\u001b[39m/\u001b[36mtransform\u001b[39m (src/machines). Serialization of machines,\n",
       "       however, now lives in MLJSerialization\n",
       "       (https://github.com/JuliaAI/MLJSerialization.jl).\n",
       "\n",
       "    •  MLJ's \u001b[1mmodel composition\u001b[22m interface, including \u001b[1mlearning networks\u001b[22m,\n",
       "       \u001b[1mpipelines\u001b[22m, \u001b[1mstacks\u001b[22m, \u001b[1mtarget transforms\u001b[22m (/src/composition)\n",
       "\n",
       "    •  basic utilities for \u001b[1mmanipulating datasets\u001b[22m and for \u001b[1msynthesizing\n",
       "       datasets\u001b[22m (src/data)\n",
       "\n",
       "    •  a small interface\n",
       "       (https://alan-turing-institute.github.io/MLJ.jl/dev/evaluating_model_performance/#Custom-resampling-strategies-1)\n",
       "       for \u001b[1mresampling strategies\u001b[22m and implementations, including \u001b[36mCV()\u001b[39m,\n",
       "       \u001b[36mStratifiedCV\u001b[39m and \u001b[36mHoldout\u001b[39m (src/resampling.jl)\n",
       "\n",
       "    •  methods for \u001b[1mperformance evaluation\u001b[22m, based on those resampling\n",
       "       strategies (src/resampling.jl)\n",
       "\n",
       "    •  \u001b[1mone-dimensional hyperparameter range types\u001b[22m, constructors and\n",
       "       associated methods, for use with MLJTuning\n",
       "       (https://github.com/JuliaAI/MLJTuning.jl) (src/hyperparam)\n",
       "\n",
       "    •  a small interface\n",
       "       (https://alan-turing-institute.github.io/MLJ.jl/dev/performance_measures/#Traits-and-custom-measures-1)\n",
       "       for \u001b[1mperformance measures\u001b[22m (losses and scores), implementation of\n",
       "       about 60 such measures, including integration of the\n",
       "       LossFunctions.jl (https://github.com/JuliaML/LossFunctions.jl)\n",
       "       library (src/measures). To be migrated into separate package in\n",
       "       the near future."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?MLJBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-22T09:47:25.905000+08:00",
     "start_time": "2022-07-22T01:47:25.641Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1mc\u001b[22mross_\u001b[0m\u001b[1mv\u001b[22malidate \u001b[0m\u001b[1mC\u001b[22m\u001b[0m\u001b[1mV\u001b[22m \u001b[0m\u001b[1mC\u001b[22m\u001b[0m\u001b[1mv\u001b[22moid LOO\u001b[0m\u001b[1mC\u001b[22m\u001b[0m\u001b[1mV\u001b[22m \u001b[0m\u001b[1mc\u001b[22mo\u001b[0m\u001b[1mv\u001b[22m \u001b[0m\u001b[1mc\u001b[22mo\u001b[0m\u001b[1mv\u001b[22m2cor \u001b[0m\u001b[1mC\u001b[22mo\u001b[0m\u001b[1mv\u001b[22marianceEstimator v\u001b[0m\u001b[1mc\u001b[22mo\u001b[0m\u001b[1mv\u001b[22m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "cv = CV(; nfolds=6,  shuffle=nothing, rng=nothing)\n",
       "\\end{verbatim}\n",
       "Cross-validation resampling strategy, for use in \\texttt{evaluate!}, \\texttt{evaluate} and tuning.\n",
       "\n",
       "\\begin{verbatim}\n",
       "train_test_pairs(cv, rows)\n",
       "\\end{verbatim}\n",
       "Returns an \\texttt{nfolds}-length iterator of \\texttt{(train, test)} pairs of vectors (row indices), where each \\texttt{train} and \\texttt{test} is a sub-vector of \\texttt{rows}. The \\texttt{test} vectors are mutually exclusive and exhaust \\texttt{rows}. Each \\texttt{train} vector is the complement of the corresponding \\texttt{test} vector. With no row pre-shuffling, the order of \\texttt{rows} is preserved, in the sense that \\texttt{rows} coincides precisely with the concatenation of the \\texttt{test} vectors, in the order they are generated. The first \\texttt{r} test vectors have length \\texttt{n + 1}, where \\texttt{n, r = divrem(length(rows), nfolds)}, and the remaining test vectors have length \\texttt{n}.\n",
       "\n",
       "Pre-shuffling of \\texttt{rows} is controlled by \\texttt{rng} and \\texttt{shuffle}. If \\texttt{rng} is an integer, then the \\texttt{CV} keyword constructor resets it to \\texttt{MersenneTwister(rng)}. Otherwise some \\texttt{AbstractRNG} object is expected.\n",
       "\n",
       "If \\texttt{rng} is left unspecified, \\texttt{rng} is reset to \\texttt{Random.GLOBAL\\_RNG}, in which case rows are only pre-shuffled if \\texttt{shuffle=true} is explicitly specified.\n",
       "\n"
      ],
      "text/markdown": [
       "```\n",
       "cv = CV(; nfolds=6,  shuffle=nothing, rng=nothing)\n",
       "```\n",
       "\n",
       "Cross-validation resampling strategy, for use in `evaluate!`, `evaluate` and tuning.\n",
       "\n",
       "```\n",
       "train_test_pairs(cv, rows)\n",
       "```\n",
       "\n",
       "Returns an `nfolds`-length iterator of `(train, test)` pairs of vectors (row indices), where each `train` and `test` is a sub-vector of `rows`. The `test` vectors are mutually exclusive and exhaust `rows`. Each `train` vector is the complement of the corresponding `test` vector. With no row pre-shuffling, the order of `rows` is preserved, in the sense that `rows` coincides precisely with the concatenation of the `test` vectors, in the order they are generated. The first `r` test vectors have length `n + 1`, where `n, r = divrem(length(rows), nfolds)`, and the remaining test vectors have length `n`.\n",
       "\n",
       "Pre-shuffling of `rows` is controlled by `rng` and `shuffle`. If `rng` is an integer, then the `CV` keyword constructor resets it to `MersenneTwister(rng)`. Otherwise some `AbstractRNG` object is expected.\n",
       "\n",
       "If `rng` is left unspecified, `rng` is reset to `Random.GLOBAL_RNG`, in which case rows are only pre-shuffled if `shuffle=true` is explicitly specified.\n"
      ],
      "text/plain": [
       "\u001b[36m  cv = CV(; nfolds=6,  shuffle=nothing, rng=nothing)\u001b[39m\n",
       "\n",
       "  Cross-validation resampling strategy, for use in \u001b[36mevaluate!\u001b[39m, \u001b[36mevaluate\u001b[39m and\n",
       "  tuning.\n",
       "\n",
       "\u001b[36m  train_test_pairs(cv, rows)\u001b[39m\n",
       "\n",
       "  Returns an \u001b[36mnfolds\u001b[39m-length iterator of \u001b[36m(train, test)\u001b[39m pairs of vectors (row\n",
       "  indices), where each \u001b[36mtrain\u001b[39m and \u001b[36mtest\u001b[39m is a sub-vector of \u001b[36mrows\u001b[39m. The \u001b[36mtest\u001b[39m\n",
       "  vectors are mutually exclusive and exhaust \u001b[36mrows\u001b[39m. Each \u001b[36mtrain\u001b[39m vector is the\n",
       "  complement of the corresponding \u001b[36mtest\u001b[39m vector. With no row pre-shuffling, the\n",
       "  order of \u001b[36mrows\u001b[39m is preserved, in the sense that \u001b[36mrows\u001b[39m coincides precisely with\n",
       "  the concatenation of the \u001b[36mtest\u001b[39m vectors, in the order they are generated. The\n",
       "  first \u001b[36mr\u001b[39m test vectors have length \u001b[36mn + 1\u001b[39m, where \u001b[36mn, r = divrem(length(rows),\n",
       "  nfolds)\u001b[39m, and the remaining test vectors have length \u001b[36mn\u001b[39m.\n",
       "\n",
       "  Pre-shuffling of \u001b[36mrows\u001b[39m is controlled by \u001b[36mrng\u001b[39m and \u001b[36mshuffle\u001b[39m. If \u001b[36mrng\u001b[39m is an\n",
       "  integer, then the \u001b[36mCV\u001b[39m keyword constructor resets it to \u001b[36mMersenneTwister(rng)\u001b[39m.\n",
       "  Otherwise some \u001b[36mAbstractRNG\u001b[39m object is expected.\n",
       "\n",
       "  If \u001b[36mrng\u001b[39m is left unspecified, \u001b[36mrng\u001b[39m is reset to \u001b[36mRandom.GLOBAL_RNG\u001b[39m, in which case\n",
       "  rows are only pre-shuffled if \u001b[36mshuffle=true\u001b[39m is explicitly specified."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-22T09:53:24.234000+08:00",
     "start_time": "2022-07-22T01:53:24.233Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CV(\n",
       "    nfolds = 10,\n",
       "    shuffle = false,\n",
       "    rng = Random._GLOBAL_RNG())"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CV(nfolds=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T14:54:14.530000+08:00",
     "start_time": "2022-06-08T06:53:24.718Z"
    }
   },
   "outputs": [],
   "source": [
    "using Flux #Flux的核心是数值梯度，主要用于深度学习，[flʌks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T14:54:15.090000+08:00",
     "start_time": "2022-06-08T06:53:24.719Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1mF\u001b[22m\u001b[0m\u001b[1ml\u001b[22m\u001b[0m\u001b[1mu\u001b[22m\u001b[0m\u001b[1mx\u001b[22m \u001b[0m\u001b[1mf\u001b[22m\u001b[0m\u001b[1ml\u001b[22m\u001b[0m\u001b[1mu\u001b[22msh so\u001b[0m\u001b[1mf\u001b[22mtp\u001b[0m\u001b[1ml\u001b[22m\u001b[0m\u001b[1mu\u001b[22ms \u001b[0m\u001b[1mf\u001b[22ma\u001b[0m\u001b[1ml\u001b[22mlo\u001b[0m\u001b[1mu\u001b[22mt \u001b[0m\u001b[1mF\u001b[22mi\u001b[0m\u001b[1ml\u001b[22mlImp\u001b[0m\u001b[1mu\u001b[22mter \u001b[0m\u001b[1mf\u001b[22mie\u001b[0m\u001b[1ml\u001b[22mdco\u001b[0m\u001b[1mu\u001b[22mnt multiclass_\u001b[0m\u001b[1mf\u001b[22ma\u001b[0m\u001b[1ml\u001b[22mlo\u001b[0m\u001b[1mu\u001b[22mt\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "No docstring found for module \\texttt{Flux}.\n",
       "\n",
       "\\section{Exported names}\n",
       "\\texttt{ADADelta}, \\texttt{ADAGrad}, \\texttt{ADAM}, \\texttt{ADAMW}, \\texttt{AMSGrad}, \\texttt{AdaBelief}, \\texttt{AdaMax}, \\texttt{AdaptiveMaxPool}, \\texttt{AdaptiveMeanPool}, \\texttt{AlphaDropout}, \\texttt{BatchNorm}, \\texttt{Chain}, \\texttt{ClipNorm}, \\texttt{ClipValue}, \\texttt{Conv}, \\texttt{ConvDims}, \\texttt{ConvTranspose}, \\texttt{CrossCor}, \\texttt{Dense}, \\texttt{DenseConvDims}, \\texttt{DepthwiseConv}, \\texttt{DepthwiseConvDims}, \\texttt{Descent}, \\texttt{Dropout}, \\texttt{ExpDecay}, \\texttt{GRU}, \\texttt{GRUv3}, \\texttt{GlobalMaxPool}, \\texttt{GlobalMeanPool}, \\texttt{GroupNorm}, \\texttt{InstanceNorm}, \\texttt{InvDecay}, \\texttt{LSTM}, \\texttt{LayerNorm}, \\texttt{MaxPool}, \\texttt{Maxout}, \\texttt{MeanPool}, \\texttt{Momentum}, \\texttt{NADAM}, \\texttt{NNlib}, \\texttt{Nesterov}, \\texttt{OADAM}, \\texttt{Parallel}, \\texttt{PixelShuffle}, \\texttt{PoolDims}, \\texttt{RADAM}, \\texttt{RMSProp}, \\texttt{RNN}, \\texttt{SamePad}, \\texttt{SkipConnection}, \\texttt{Upsample}, \\texttt{WeightDecay}, \\texttt{batched\\_adjoint}, \\texttt{batched\\_mul}, \\texttt{batched\\_mul!}, \\texttt{batched\\_transpose}, \\texttt{batched\\_vec}, \\texttt{celu}, \\texttt{conv}, \\texttt{conv!}, \\texttt{conv\\_bias\\_act}, \\texttt{conv\\_bias\\_act!}, \\texttt{cpu}, \\texttt{depthwiseconv}, \\texttt{depthwiseconv!}, \\texttt{elu}, \\texttt{f32}, \\texttt{f64}, \\texttt{fmap}, \\texttt{frequencies}, \\texttt{gelu}, \\texttt{gpu}, \\texttt{gradient}, \\texttt{grid\\_sample}, \\texttt{hardsigmoid}, \\texttt{hardswish}, \\texttt{hardtanh}, \\texttt{hardσ}, \\texttt{leakyrelu}, \\texttt{lisht}, \\texttt{logcosh}, \\texttt{logsigmoid}, \\texttt{logsoftmax}, \\texttt{logsoftmax!}, \\texttt{logsumexp}, \\texttt{logσ}, \\texttt{maxpool}, \\texttt{maxpool!}, \\texttt{meanpool}, \\texttt{meanpool!}, \\texttt{mish}, \\texttt{pad\\_constant}, \\texttt{pad\\_reflect}, \\texttt{pad\\_repeat}, \\texttt{pad\\_zeros}, \\texttt{pixel\\_shuffle}, \\texttt{relu}, \\texttt{relu6}, \\texttt{rrelu}, \\texttt{selu}, \\texttt{sigmoid}, \\texttt{sigmoid\\_fast}, \\texttt{softmax}, \\texttt{softmax!}, \\texttt{softplus}, \\texttt{softshrink}, \\texttt{softsign}, \\texttt{swish}, \\texttt{tanh\\_fast}, \\texttt{tanhshrink}, \\texttt{testmode!}, \\texttt{thresholdrelu}, \\texttt{trainmode!}, \\texttt{trelu}, \\texttt{upsample\\_bilinear}, \\texttt{upsample\\_linear}, \\texttt{upsample\\_nearest}, \\texttt{upsample\\_trilinear}, \\texttt{σ}, \\texttt{∇conv\\_data}, \\texttt{∇conv\\_data!}, \\texttt{∇conv\\_filter}, \\texttt{∇conv\\_filter!}, \\texttt{∇depthwiseconv\\_data}, \\texttt{∇depthwiseconv\\_data!}, \\texttt{∇depthwiseconv\\_filter}, \\texttt{∇depthwiseconv\\_filter!}, \\texttt{∇grid\\_sample}, \\texttt{∇logsoftmax}, \\texttt{∇logsoftmax!}, \\texttt{∇maxpool}, \\texttt{∇maxpool!}, \\texttt{∇meanpool}, \\texttt{∇meanpool!}, \\texttt{∇softmax}, \\texttt{∇softmax!}, \\texttt{∇upsample\\_bilinear}, \\texttt{∇upsample\\_linear}, \\texttt{∇upsample\\_nearest}, \\texttt{∇upsample\\_trilinear}, \\texttt{⊠}\n",
       "\n",
       "\\section{Displaying contents of readme found at \\texttt{D:{\\textbackslash}TongYuan{\\textbackslash}.julia{\\textbackslash}packages{\\textbackslash}Flux{\\textbackslash}js6mP{\\textbackslash}README.md}}\n",
       "<p align=\"center\"> <img width=\"400px\" src=\"https://raw.githubusercontent.com/FluxML/fluxml.github.io/master/logo.png\"/> </p>\n",
       "\n",
       "[![][action-img]][action-url] \\href{https://fluxml.github.io/Flux.jl/stable/}{\\begin{figure}\n",
       "\\centering\n",
       "\\includegraphics{https://img.shields.io/badge/docs-stable-blue.svg}\n",
       "\\caption{}\n",
       "\\end{figure}\n",
       "} \\href{https://julialang.org/slack/}{\\begin{figure}\n",
       "\\centering\n",
       "\\includegraphics{https://img.shields.io/badge/chat-on%20slack-yellow.svg}\n",
       "\\caption{}\n",
       "\\end{figure}\n",
       "} \\href{https://doi.org/10.21105/joss.00602}{\\begin{figure}\n",
       "\\centering\n",
       "\\includegraphics{https://joss.theoj.org/papers/10.21105/joss.00602/status.svg}\n",
       "\\caption{DOI}\n",
       "\\end{figure}\n",
       "} \\href{https://github.com/SciML/ColPrac}{\\begin{figure}\n",
       "\\centering\n",
       "\\includegraphics{https://img.shields.io/badge/ColPrac-Contributor's%20Guide-blueviolet}\n",
       "\\caption{ColPrac: Contributor's Guide on Collaborative Practices for Community Packages}\n",
       "\\end{figure}\n",
       "} [![][codecov-img]][codecov-url]\n",
       "\n",
       "[action-img]: https://github.com/FluxML/Flux.jl/workflows/CI/badge.svg [action-url]: https://github.com/FluxML/Flux.jl/actions [codecov-img]: https://codecov.io/gh/FluxML/Flux.jl/branch/master/graph/badge.svg [codecov-url]: https://codecov.io/gh/FluxML/Flux.jl\n",
       "\n",
       "Flux is an elegant approach to machine learning. It's a 100\\% pure-Julia stack, and provides lightweight abstractions on top of Julia's native GPU and AD support. Flux makes the easy things easy while remaining fully hackable.\n",
       "\n",
       "\\begin{verbatim}\n",
       "] add Flux\n",
       "\\end{verbatim}\n",
       "See the \\href{https://fluxml.github.io/Flux.jl/}{documentation} or the \\href{https://github.com/FluxML/model-zoo/}{model zoo} for examples.\n",
       "\n",
       "If you use Flux in your research, please \\href{CITATION.bib}{cite} our work.\n",
       "\n"
      ],
      "text/markdown": [
       "No docstring found for module `Flux`.\n",
       "\n",
       "# Exported names\n",
       "\n",
       "`ADADelta`, `ADAGrad`, `ADAM`, `ADAMW`, `AMSGrad`, `AdaBelief`, `AdaMax`, `AdaptiveMaxPool`, `AdaptiveMeanPool`, `AlphaDropout`, `BatchNorm`, `Chain`, `ClipNorm`, `ClipValue`, `Conv`, `ConvDims`, `ConvTranspose`, `CrossCor`, `Dense`, `DenseConvDims`, `DepthwiseConv`, `DepthwiseConvDims`, `Descent`, `Dropout`, `ExpDecay`, `GRU`, `GRUv3`, `GlobalMaxPool`, `GlobalMeanPool`, `GroupNorm`, `InstanceNorm`, `InvDecay`, `LSTM`, `LayerNorm`, `MaxPool`, `Maxout`, `MeanPool`, `Momentum`, `NADAM`, `NNlib`, `Nesterov`, `OADAM`, `Parallel`, `PixelShuffle`, `PoolDims`, `RADAM`, `RMSProp`, `RNN`, `SamePad`, `SkipConnection`, `Upsample`, `WeightDecay`, `batched_adjoint`, `batched_mul`, `batched_mul!`, `batched_transpose`, `batched_vec`, `celu`, `conv`, `conv!`, `conv_bias_act`, `conv_bias_act!`, `cpu`, `depthwiseconv`, `depthwiseconv!`, `elu`, `f32`, `f64`, `fmap`, `frequencies`, `gelu`, `gpu`, `gradient`, `grid_sample`, `hardsigmoid`, `hardswish`, `hardtanh`, `hardσ`, `leakyrelu`, `lisht`, `logcosh`, `logsigmoid`, `logsoftmax`, `logsoftmax!`, `logsumexp`, `logσ`, `maxpool`, `maxpool!`, `meanpool`, `meanpool!`, `mish`, `pad_constant`, `pad_reflect`, `pad_repeat`, `pad_zeros`, `pixel_shuffle`, `relu`, `relu6`, `rrelu`, `selu`, `sigmoid`, `sigmoid_fast`, `softmax`, `softmax!`, `softplus`, `softshrink`, `softsign`, `swish`, `tanh_fast`, `tanhshrink`, `testmode!`, `thresholdrelu`, `trainmode!`, `trelu`, `upsample_bilinear`, `upsample_linear`, `upsample_nearest`, `upsample_trilinear`, `σ`, `∇conv_data`, `∇conv_data!`, `∇conv_filter`, `∇conv_filter!`, `∇depthwiseconv_data`, `∇depthwiseconv_data!`, `∇depthwiseconv_filter`, `∇depthwiseconv_filter!`, `∇grid_sample`, `∇logsoftmax`, `∇logsoftmax!`, `∇maxpool`, `∇maxpool!`, `∇meanpool`, `∇meanpool!`, `∇softmax`, `∇softmax!`, `∇upsample_bilinear`, `∇upsample_linear`, `∇upsample_nearest`, `∇upsample_trilinear`, `⊠`\n",
       "\n",
       "# Displaying contents of readme found at `D:\\TongYuan\\.julia\\packages\\Flux\\js6mP\\README.md`\n",
       "\n",
       "<p align=\"center\"> <img width=\"400px\" src=\"https://raw.githubusercontent.com/FluxML/fluxml.github.io/master/logo.png\"/> </p>\n",
       "\n",
       "[![][action-img]][action-url] [![](https://img.shields.io/badge/docs-stable-blue.svg)](https://fluxml.github.io/Flux.jl/stable/) [![](https://img.shields.io/badge/chat-on%20slack-yellow.svg)](https://julialang.org/slack/) [![DOI](https://joss.theoj.org/papers/10.21105/joss.00602/status.svg)](https://doi.org/10.21105/joss.00602) [![ColPrac: Contributor's Guide on Collaborative Practices for Community Packages](https://img.shields.io/badge/ColPrac-Contributor's%20Guide-blueviolet)](https://github.com/SciML/ColPrac) [![][codecov-img]][codecov-url]\n",
       "\n",
       "[action-img]: https://github.com/FluxML/Flux.jl/workflows/CI/badge.svg [action-url]: https://github.com/FluxML/Flux.jl/actions [codecov-img]: https://codecov.io/gh/FluxML/Flux.jl/branch/master/graph/badge.svg [codecov-url]: https://codecov.io/gh/FluxML/Flux.jl\n",
       "\n",
       "Flux is an elegant approach to machine learning. It's a 100% pure-Julia stack, and provides lightweight abstractions on top of Julia's native GPU and AD support. Flux makes the easy things easy while remaining fully hackable.\n",
       "\n",
       "```julia\n",
       "] add Flux\n",
       "```\n",
       "\n",
       "See the [documentation](https://fluxml.github.io/Flux.jl/) or the [model zoo](https://github.com/FluxML/model-zoo/) for examples.\n",
       "\n",
       "If you use Flux in your research, please [cite](CITATION.bib) our work.\n"
      ],
      "text/plain": [
       "  No docstring found for module \u001b[36mFlux\u001b[39m.\n",
       "\n",
       "\u001b[1m  Exported names\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "  \u001b[36mADADelta\u001b[39m, \u001b[36mADAGrad\u001b[39m, \u001b[36mADAM\u001b[39m, \u001b[36mADAMW\u001b[39m, \u001b[36mAMSGrad\u001b[39m, \u001b[36mAdaBelief\u001b[39m, \u001b[36mAdaMax\u001b[39m, \u001b[36mAdaptiveMaxPool\u001b[39m,\n",
       "  \u001b[36mAdaptiveMeanPool\u001b[39m, \u001b[36mAlphaDropout\u001b[39m, \u001b[36mBatchNorm\u001b[39m, \u001b[36mChain\u001b[39m, \u001b[36mClipNorm\u001b[39m, \u001b[36mClipValue\u001b[39m, \u001b[36mConv\u001b[39m,\n",
       "  \u001b[36mConvDims\u001b[39m, \u001b[36mConvTranspose\u001b[39m, \u001b[36mCrossCor\u001b[39m, \u001b[36mDense\u001b[39m, \u001b[36mDenseConvDims\u001b[39m, \u001b[36mDepthwiseConv\u001b[39m,\n",
       "  \u001b[36mDepthwiseConvDims\u001b[39m, \u001b[36mDescent\u001b[39m, \u001b[36mDropout\u001b[39m, \u001b[36mExpDecay\u001b[39m, \u001b[36mGRU\u001b[39m, \u001b[36mGRUv3\u001b[39m, \u001b[36mGlobalMaxPool\u001b[39m,\n",
       "  \u001b[36mGlobalMeanPool\u001b[39m, \u001b[36mGroupNorm\u001b[39m, \u001b[36mInstanceNorm\u001b[39m, \u001b[36mInvDecay\u001b[39m, \u001b[36mLSTM\u001b[39m, \u001b[36mLayerNorm\u001b[39m, \u001b[36mMaxPool\u001b[39m,\n",
       "  \u001b[36mMaxout\u001b[39m, \u001b[36mMeanPool\u001b[39m, \u001b[36mMomentum\u001b[39m, \u001b[36mNADAM\u001b[39m, \u001b[36mNNlib\u001b[39m, \u001b[36mNesterov\u001b[39m, \u001b[36mOADAM\u001b[39m, \u001b[36mParallel\u001b[39m,\n",
       "  \u001b[36mPixelShuffle\u001b[39m, \u001b[36mPoolDims\u001b[39m, \u001b[36mRADAM\u001b[39m, \u001b[36mRMSProp\u001b[39m, \u001b[36mRNN\u001b[39m, \u001b[36mSamePad\u001b[39m, \u001b[36mSkipConnection\u001b[39m,\n",
       "  \u001b[36mUpsample\u001b[39m, \u001b[36mWeightDecay\u001b[39m, \u001b[36mbatched_adjoint\u001b[39m, \u001b[36mbatched_mul\u001b[39m, \u001b[36mbatched_mul!\u001b[39m,\n",
       "  \u001b[36mbatched_transpose\u001b[39m, \u001b[36mbatched_vec\u001b[39m, \u001b[36mcelu\u001b[39m, \u001b[36mconv\u001b[39m, \u001b[36mconv!\u001b[39m, \u001b[36mconv_bias_act\u001b[39m,\n",
       "  \u001b[36mconv_bias_act!\u001b[39m, \u001b[36mcpu\u001b[39m, \u001b[36mdepthwiseconv\u001b[39m, \u001b[36mdepthwiseconv!\u001b[39m, \u001b[36melu\u001b[39m, \u001b[36mf32\u001b[39m, \u001b[36mf64\u001b[39m, \u001b[36mfmap\u001b[39m,\n",
       "  \u001b[36mfrequencies\u001b[39m, \u001b[36mgelu\u001b[39m, \u001b[36mgpu\u001b[39m, \u001b[36mgradient\u001b[39m, \u001b[36mgrid_sample\u001b[39m, \u001b[36mhardsigmoid\u001b[39m, \u001b[36mhardswish\u001b[39m,\n",
       "  \u001b[36mhardtanh\u001b[39m, \u001b[36mhardσ\u001b[39m, \u001b[36mleakyrelu\u001b[39m, \u001b[36mlisht\u001b[39m, \u001b[36mlogcosh\u001b[39m, \u001b[36mlogsigmoid\u001b[39m, \u001b[36mlogsoftmax\u001b[39m,\n",
       "  \u001b[36mlogsoftmax!\u001b[39m, \u001b[36mlogsumexp\u001b[39m, \u001b[36mlogσ\u001b[39m, \u001b[36mmaxpool\u001b[39m, \u001b[36mmaxpool!\u001b[39m, \u001b[36mmeanpool\u001b[39m, \u001b[36mmeanpool!\u001b[39m, \u001b[36mmish\u001b[39m,\n",
       "  \u001b[36mpad_constant\u001b[39m, \u001b[36mpad_reflect\u001b[39m, \u001b[36mpad_repeat\u001b[39m, \u001b[36mpad_zeros\u001b[39m, \u001b[36mpixel_shuffle\u001b[39m, \u001b[36mrelu\u001b[39m,\n",
       "  \u001b[36mrelu6\u001b[39m, \u001b[36mrrelu\u001b[39m, \u001b[36mselu\u001b[39m, \u001b[36msigmoid\u001b[39m, \u001b[36msigmoid_fast\u001b[39m, \u001b[36msoftmax\u001b[39m, \u001b[36msoftmax!\u001b[39m, \u001b[36msoftplus\u001b[39m,\n",
       "  \u001b[36msoftshrink\u001b[39m, \u001b[36msoftsign\u001b[39m, \u001b[36mswish\u001b[39m, \u001b[36mtanh_fast\u001b[39m, \u001b[36mtanhshrink\u001b[39m, \u001b[36mtestmode!\u001b[39m,\n",
       "  \u001b[36mthresholdrelu\u001b[39m, \u001b[36mtrainmode!\u001b[39m, \u001b[36mtrelu\u001b[39m, \u001b[36mupsample_bilinear\u001b[39m, \u001b[36mupsample_linear\u001b[39m,\n",
       "  \u001b[36mupsample_nearest\u001b[39m, \u001b[36mupsample_trilinear\u001b[39m, \u001b[36mσ\u001b[39m, \u001b[36m∇conv_data\u001b[39m, \u001b[36m∇conv_data!\u001b[39m,\n",
       "  \u001b[36m∇conv_filter\u001b[39m, \u001b[36m∇conv_filter!\u001b[39m, \u001b[36m∇depthwiseconv_data\u001b[39m, \u001b[36m∇depthwiseconv_data!\u001b[39m,\n",
       "  \u001b[36m∇depthwiseconv_filter\u001b[39m, \u001b[36m∇depthwiseconv_filter!\u001b[39m, \u001b[36m∇grid_sample\u001b[39m, \u001b[36m∇logsoftmax\u001b[39m,\n",
       "  \u001b[36m∇logsoftmax!\u001b[39m, \u001b[36m∇maxpool\u001b[39m, \u001b[36m∇maxpool!\u001b[39m, \u001b[36m∇meanpool\u001b[39m, \u001b[36m∇meanpool!\u001b[39m, \u001b[36m∇softmax\u001b[39m,\n",
       "  \u001b[36m∇softmax!\u001b[39m, \u001b[36m∇upsample_bilinear\u001b[39m, \u001b[36m∇upsample_linear\u001b[39m, \u001b[36m∇upsample_nearest\u001b[39m,\n",
       "  \u001b[36m∇upsample_trilinear\u001b[39m, \u001b[36m⊠\u001b[39m\n",
       "\n",
       "\u001b[1m  Displaying contents of readme found at\u001b[22m\n",
       "\u001b[1m \u001b[36mD:\\TongYuan\\.julia\\packages\\Flux\\js6mP\\README.md\u001b[39m\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "  <p align=\"center\"> <img width=\"400px\"\n",
       "  src=\"https://raw.githubusercontent.com/FluxML/fluxml.github.io/master/logo.png\"/>\n",
       "  </p>\n",
       "\n",
       "  [![][action-img]][action-url] (Image: )\n",
       "  (https://fluxml.github.io/Flux.jl/stable/) (Image: )\n",
       "  (https://julialang.org/slack/) (Image: DOI)\n",
       "  (https://doi.org/10.21105/joss.00602) (Image: ColPrac: Contributor's Guide\n",
       "  on Collaborative Practices for Community Packages)\n",
       "  (https://github.com/SciML/ColPrac) [![][codecov-img]][codecov-url]\n",
       "\n",
       "  [action-img]: https://github.com/FluxML/Flux.jl/workflows/CI/badge.svg\n",
       "  [action-url]: https://github.com/FluxML/Flux.jl/actions [codecov-img]:\n",
       "  https://codecov.io/gh/FluxML/Flux.jl/branch/master/graph/badge.svg\n",
       "  [codecov-url]: https://codecov.io/gh/FluxML/Flux.jl\n",
       "\n",
       "  Flux is an elegant approach to machine learning. It's a 100% pure-Julia\n",
       "  stack, and provides lightweight abstractions on top of Julia's native GPU\n",
       "  and AD support. Flux makes the easy things easy while remaining fully\n",
       "  hackable.\n",
       "\n",
       "\u001b[36m  ] add Flux\u001b[39m\n",
       "\n",
       "  See the documentation (https://fluxml.github.io/Flux.jl/) or the model zoo\n",
       "  (https://github.com/FluxML/model-zoo/) for examples.\n",
       "\n",
       "  If you use Flux in your research, please cite (CITATION.bib) our work."
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?Flux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-22T09:32:44.980000+08:00",
     "start_time": "2022-07-22T01:32:44.978Z"
    }
   },
   "outputs": [],
   "source": [
    "using MLBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-22T09:32:46.974000+08:00",
     "start_time": "2022-07-22T01:32:45.848Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1mM\u001b[22m\u001b[0m\u001b[1mL\u001b[22m\u001b[0m\u001b[1mB\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1me\u001b[22m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "No docstring found for module \\texttt{MLBase}.\n",
       "\n",
       "\\section{Exported names}\n",
       "\\texttt{AbstractDataTransform}, \\texttt{AbstractHistogram}, \\texttt{AbstractWeights}, \\texttt{AnalyticWeights}, \\texttt{CoefTable}, \\texttt{ConvergenceException}, \\texttt{CovarianceEstimator}, \\texttt{CronbachAlpha}, \\texttt{CrossValGenerator}, \\texttt{ECDF}, \\texttt{Forward}, \\texttt{FrequencyWeights}, \\texttt{Histogram}, \\texttt{Kfold}, \\texttt{L1dist}, \\texttt{L2dist}, \\texttt{LOOCV}, \\texttt{LabelMap}, \\texttt{Linfdist}, \\texttt{ProbabilityWeights}, \\texttt{ROCNums}, \\texttt{RandomSub}, \\texttt{RegressionModel}, \\texttt{Reverse}, \\texttt{SimpleCovariance}, \\texttt{Standardize}, \\texttt{StatisticalModel}, \\texttt{StatsBase}, \\texttt{StratifiedKfold}, \\texttt{StratifiedRandomSub}, \\texttt{UnitRangeTransform}, \\texttt{UnitWeights}, \\texttt{Weights}, \\texttt{ZScoreTransform}, \\texttt{addcounts!}, \\texttt{adjr2}, \\texttt{adjr²}, \\texttt{aic}, \\texttt{aicc}, \\texttt{autocor}, \\texttt{autocor!}, \\texttt{autocov}, \\texttt{autocov!}, \\texttt{aweights}, \\texttt{bic}, \\texttt{classify}, \\texttt{classify!}, \\texttt{classify\\_withscore}, \\texttt{classify\\_withscores}, \\texttt{classify\\_withscores!}, \\texttt{coef}, \\texttt{coefnames}, \\texttt{coeftable}, \\texttt{competerank}, \\texttt{confint}, \\texttt{confusmat}, \\texttt{cooksdistance}, \\texttt{cor}, \\texttt{cor2cov}, \\texttt{corkendall}, \\texttt{correctrate}, \\texttt{corspearman}, \\texttt{counteq}, \\texttt{counthits}, \\texttt{countmap}, \\texttt{countne}, \\texttt{counts}, \\texttt{cov}, \\texttt{cov2cor}, \\texttt{cronbachalpha}, \\texttt{cross\\_validate}, \\texttt{crosscor}, \\texttt{crosscor!}, \\texttt{crosscov}, \\texttt{crosscov!}, \\texttt{crossentropy}, \\texttt{crossmodelmatrix}, \\texttt{denserank}, \\texttt{describe}, \\texttt{deviance}, \\texttt{dof}, \\texttt{dof\\_residual}, \\texttt{ecdf}, \\texttt{entropy}, \\texttt{errorrate}, \\texttt{eweights}, \\texttt{f1score}, \\texttt{false\\_negative}, \\texttt{false\\_negative\\_rate}, \\texttt{false\\_positive}, \\texttt{false\\_positive\\_rate}, \\texttt{fit}, \\texttt{fit!}, \\texttt{fitted}, \\texttt{fweights}, \\texttt{genmean}, \\texttt{genvar}, \\texttt{geomean}, \\texttt{gkldiv}, \\texttt{gridtune}, \\texttt{groupindices}, \\texttt{harmmean}, \\texttt{hitrate}, \\texttt{hitrates}, \\texttt{indexmap}, \\texttt{indicatormat}, \\texttt{informationmatrix}, \\texttt{inverse\\_rle}, \\texttt{iqr}, \\texttt{isfitted}, \\texttt{islinear}, \\texttt{kldivergence}, \\texttt{kurtosis}, \\texttt{labeldecode}, \\texttt{labelencode}, \\texttt{labelmap}, \\texttt{levelsmap}, \\texttt{leverage}, \\texttt{loglikelihood}, \\texttt{mad}, \\texttt{mad!}, \\texttt{maxad}, \\texttt{mean}, \\texttt{mean!}, \\texttt{mean\\_and\\_cov}, \\texttt{mean\\_and\\_std}, \\texttt{mean\\_and\\_var}, \\texttt{meanad}, \\texttt{meanresponse}, \\texttt{median}, \\texttt{median!}, \\texttt{middle}, \\texttt{midpoints}, \\texttt{mode}, \\texttt{model\\_response}, \\texttt{modelmatrix}, \\texttt{modes}, \\texttt{moment}, \\texttt{msd}, \\texttt{mss}, \\texttt{nobs}, \\texttt{norepeats}, \\texttt{nquantile}, \\texttt{nulldeviance}, \\texttt{nullloglikelihood}, \\texttt{ordinalrank}, \\texttt{pacf}, \\texttt{pacf!}, \\texttt{pairwise}, \\texttt{pairwise!}, \\texttt{partialcor}, \\texttt{percentile}, \\texttt{percentilerank}, \\texttt{precision}, \\texttt{predict}, \\texttt{predict!}, \\texttt{proportionmap}, \\texttt{proportions}, \\texttt{psnr}, \\texttt{pweights}, \\texttt{quantile}, \\texttt{quantile!}, \\texttt{quantilerank}, \\texttt{r2}, \\texttt{recall}, \\texttt{renyientropy}, \\texttt{repeach}, \\texttt{repeachcol}, \\texttt{repeachrow}, \\texttt{residuals}, \\texttt{response}, \\texttt{responsename}, \\texttt{rle}, \\texttt{rmsd}, \\texttt{roc}, \\texttt{rss}, \\texttt{r²}, \\texttt{sample}, \\texttt{sample!}, \\texttt{samplepair}, \\texttt{scattermat}, \\texttt{scattermat\\_zm}, \\texttt{scattermatm}, \\texttt{score}, \\texttt{sem}, \\texttt{skewness}, \\texttt{span}, \\texttt{sqL2dist}, \\texttt{standardize}, \\texttt{standardize!}, \\texttt{std}, \\texttt{stderror}, \\texttt{sum}, \\texttt{summarystats}, \\texttt{tiedrank}, \\texttt{totalvar}, \\texttt{transform}, \\texttt{trim}, \\texttt{trim!}, \\texttt{trimvar}, \\texttt{true\\_negative}, \\texttt{true\\_negative\\_rate}, \\texttt{true\\_positive}, \\texttt{true\\_positive\\_rate}, \\texttt{uweights}, \\texttt{values}, \\texttt{var}, \\texttt{variation}, \\texttt{vcov}, \\texttt{weights}, \\texttt{winsor}, \\texttt{winsor!}, \\texttt{wmedian}, \\texttt{wquantile}, \\texttt{wsample}, \\texttt{wsample!}, \\texttt{wsum}, \\texttt{wsum!}, \\texttt{zscore}, \\texttt{zscore!}\n",
       "\n",
       "\\section{Displaying contents of readme found at \\texttt{D:{\\textbackslash}TongYuan{\\textbackslash}.julia{\\textbackslash}packages{\\textbackslash}MLBase{\\textbackslash}SAM9e{\\textbackslash}README.md}}\n",
       "\\section{MLBase.jl}\n",
       "Swiss knife for machine learning.\n",
       "\n",
       "\\href{https://github.com/JuliaStats/MLBase.jl/actions?query=workflow%3ACI+branch%3Amaster}{\\begin{figure}\n",
       "\\centering\n",
       "\\includegraphics{https://github.com/JuliaStats/MLBase.jl/workflows/CI/badge.svg}\n",
       "\\caption{Build Status}\n",
       "\\end{figure}\n",
       "} \\href{https://coveralls.io/github/JuliaStats/MLBase.jl?branch=master}{\\begin{figure}\n",
       "\\centering\n",
       "\\includegraphics{https://coveralls.io/repos/github/JuliaStats/MLBase.jl/badge.svg?branch=master}\n",
       "\\caption{Coveralls}\n",
       "\\end{figure}\n",
       "} \\href{http://mlbasejl.readthedocs.io/en/latest/?badge=latest}{\\begin{figure}\n",
       "\\centering\n",
       "\\includegraphics{https://readthedocs.org/projects/mlbasejl/badge/?version=latest}\n",
       "\\caption{Documentation Status}\n",
       "\\end{figure}\n",
       "}\n",
       "\n",
       "This package does not implement specific machine learning algorithms. Instead, it provides a collection of useful tools to support machine learning programs, including:\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item Data manipulation \\& preprocessing\n",
       "\n",
       "\n",
       "\\item Score-based classification\n",
       "\n",
       "\n",
       "\\item Performance evaluation (\\emph{e.g.} evaluating ROC)\n",
       "\n",
       "\n",
       "\\item Cross validation\n",
       "\n",
       "\n",
       "\\item Model tuning (\\emph{i.e.} search best settings of parameters)\n",
       "\n",
       "\\end{itemize}\n",
       "\\textbf{Notes:} This package depends on \\href{https://github.com/JuliaStats/StatsBase.jl}{StatsBase} and reexports all names therefrom.\n",
       "\n",
       "\\subsubsection{Resources}\n",
       "\\begin{itemize}\n",
       "\\item \\textbf{Documentation:} \\href{http://mlbasejl.readthedocs.org/en/latest/}{http://mlbasejl.readthedocs.org/en/latest/}\n",
       "\n",
       "\n",
       "\\item \\textbf{Release Notes:} \\href{https://github.com/JuliaStats/MLBase.jl/blob/master/NEWS.md}{https://github.com/JuliaStats/MLBase.jl/blob/master/NEWS.md}\n",
       "\n",
       "\\end{itemize}\n"
      ],
      "text/markdown": [
       "No docstring found for module `MLBase`.\n",
       "\n",
       "# Exported names\n",
       "\n",
       "`AbstractDataTransform`, `AbstractHistogram`, `AbstractWeights`, `AnalyticWeights`, `CoefTable`, `ConvergenceException`, `CovarianceEstimator`, `CronbachAlpha`, `CrossValGenerator`, `ECDF`, `Forward`, `FrequencyWeights`, `Histogram`, `Kfold`, `L1dist`, `L2dist`, `LOOCV`, `LabelMap`, `Linfdist`, `ProbabilityWeights`, `ROCNums`, `RandomSub`, `RegressionModel`, `Reverse`, `SimpleCovariance`, `Standardize`, `StatisticalModel`, `StatsBase`, `StratifiedKfold`, `StratifiedRandomSub`, `UnitRangeTransform`, `UnitWeights`, `Weights`, `ZScoreTransform`, `addcounts!`, `adjr2`, `adjr²`, `aic`, `aicc`, `autocor`, `autocor!`, `autocov`, `autocov!`, `aweights`, `bic`, `classify`, `classify!`, `classify_withscore`, `classify_withscores`, `classify_withscores!`, `coef`, `coefnames`, `coeftable`, `competerank`, `confint`, `confusmat`, `cooksdistance`, `cor`, `cor2cov`, `corkendall`, `correctrate`, `corspearman`, `counteq`, `counthits`, `countmap`, `countne`, `counts`, `cov`, `cov2cor`, `cronbachalpha`, `cross_validate`, `crosscor`, `crosscor!`, `crosscov`, `crosscov!`, `crossentropy`, `crossmodelmatrix`, `denserank`, `describe`, `deviance`, `dof`, `dof_residual`, `ecdf`, `entropy`, `errorrate`, `eweights`, `f1score`, `false_negative`, `false_negative_rate`, `false_positive`, `false_positive_rate`, `fit`, `fit!`, `fitted`, `fweights`, `genmean`, `genvar`, `geomean`, `gkldiv`, `gridtune`, `groupindices`, `harmmean`, `hitrate`, `hitrates`, `indexmap`, `indicatormat`, `informationmatrix`, `inverse_rle`, `iqr`, `isfitted`, `islinear`, `kldivergence`, `kurtosis`, `labeldecode`, `labelencode`, `labelmap`, `levelsmap`, `leverage`, `loglikelihood`, `mad`, `mad!`, `maxad`, `mean`, `mean!`, `mean_and_cov`, `mean_and_std`, `mean_and_var`, `meanad`, `meanresponse`, `median`, `median!`, `middle`, `midpoints`, `mode`, `model_response`, `modelmatrix`, `modes`, `moment`, `msd`, `mss`, `nobs`, `norepeats`, `nquantile`, `nulldeviance`, `nullloglikelihood`, `ordinalrank`, `pacf`, `pacf!`, `pairwise`, `pairwise!`, `partialcor`, `percentile`, `percentilerank`, `precision`, `predict`, `predict!`, `proportionmap`, `proportions`, `psnr`, `pweights`, `quantile`, `quantile!`, `quantilerank`, `r2`, `recall`, `renyientropy`, `repeach`, `repeachcol`, `repeachrow`, `residuals`, `response`, `responsename`, `rle`, `rmsd`, `roc`, `rss`, `r²`, `sample`, `sample!`, `samplepair`, `scattermat`, `scattermat_zm`, `scattermatm`, `score`, `sem`, `skewness`, `span`, `sqL2dist`, `standardize`, `standardize!`, `std`, `stderror`, `sum`, `summarystats`, `tiedrank`, `totalvar`, `transform`, `trim`, `trim!`, `trimvar`, `true_negative`, `true_negative_rate`, `true_positive`, `true_positive_rate`, `uweights`, `values`, `var`, `variation`, `vcov`, `weights`, `winsor`, `winsor!`, `wmedian`, `wquantile`, `wsample`, `wsample!`, `wsum`, `wsum!`, `zscore`, `zscore!`\n",
       "\n",
       "# Displaying contents of readme found at `D:\\TongYuan\\.julia\\packages\\MLBase\\SAM9e\\README.md`\n",
       "\n",
       "# MLBase.jl\n",
       "\n",
       "Swiss knife for machine learning.\n",
       "\n",
       "[![Build Status](https://github.com/JuliaStats/MLBase.jl/workflows/CI/badge.svg)](https://github.com/JuliaStats/MLBase.jl/actions?query=workflow%3ACI+branch%3Amaster) [![Coveralls](https://coveralls.io/repos/github/JuliaStats/MLBase.jl/badge.svg?branch=master)](https://coveralls.io/github/JuliaStats/MLBase.jl?branch=master) [![Documentation Status](https://readthedocs.org/projects/mlbasejl/badge/?version=latest)](http://mlbasejl.readthedocs.io/en/latest/?badge=latest)\n",
       "\n",
       "This package does not implement specific machine learning algorithms. Instead, it provides a collection of useful tools to support machine learning programs, including:\n",
       "\n",
       "  * Data manipulation & preprocessing\n",
       "  * Score-based classification\n",
       "  * Performance evaluation (*e.g.* evaluating ROC)\n",
       "  * Cross validation\n",
       "  * Model tuning (*i.e.* search best settings of parameters)\n",
       "\n",
       "**Notes:** This package depends on [StatsBase](https://github.com/JuliaStats/StatsBase.jl) and reexports all names therefrom.\n",
       "\n",
       "### Resources\n",
       "\n",
       "  * **Documentation:** [http://mlbasejl.readthedocs.org/en/latest/](http://mlbasejl.readthedocs.org/en/latest/)\n",
       "  * **Release Notes:** [https://github.com/JuliaStats/MLBase.jl/blob/master/NEWS.md](https://github.com/JuliaStats/MLBase.jl/blob/master/NEWS.md)\n"
      ],
      "text/plain": [
       "  No docstring found for module \u001b[36mMLBase\u001b[39m.\n",
       "\n",
       "\u001b[1m  Exported names\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "  \u001b[36mAbstractDataTransform\u001b[39m, \u001b[36mAbstractHistogram\u001b[39m, \u001b[36mAbstractWeights\u001b[39m, \u001b[36mAnalyticWeights\u001b[39m,\n",
       "  \u001b[36mCoefTable\u001b[39m, \u001b[36mConvergenceException\u001b[39m, \u001b[36mCovarianceEstimator\u001b[39m, \u001b[36mCronbachAlpha\u001b[39m,\n",
       "  \u001b[36mCrossValGenerator\u001b[39m, \u001b[36mECDF\u001b[39m, \u001b[36mForward\u001b[39m, \u001b[36mFrequencyWeights\u001b[39m, \u001b[36mHistogram\u001b[39m, \u001b[36mKfold\u001b[39m,\n",
       "  \u001b[36mL1dist\u001b[39m, \u001b[36mL2dist\u001b[39m, \u001b[36mLOOCV\u001b[39m, \u001b[36mLabelMap\u001b[39m, \u001b[36mLinfdist\u001b[39m, \u001b[36mProbabilityWeights\u001b[39m, \u001b[36mROCNums\u001b[39m,\n",
       "  \u001b[36mRandomSub\u001b[39m, \u001b[36mRegressionModel\u001b[39m, \u001b[36mReverse\u001b[39m, \u001b[36mSimpleCovariance\u001b[39m, \u001b[36mStandardize\u001b[39m,\n",
       "  \u001b[36mStatisticalModel\u001b[39m, \u001b[36mStatsBase\u001b[39m, \u001b[36mStratifiedKfold\u001b[39m, \u001b[36mStratifiedRandomSub\u001b[39m,\n",
       "  \u001b[36mUnitRangeTransform\u001b[39m, \u001b[36mUnitWeights\u001b[39m, \u001b[36mWeights\u001b[39m, \u001b[36mZScoreTransform\u001b[39m, \u001b[36maddcounts!\u001b[39m,\n",
       "  \u001b[36madjr2\u001b[39m, \u001b[36madjr²\u001b[39m, \u001b[36maic\u001b[39m, \u001b[36maicc\u001b[39m, \u001b[36mautocor\u001b[39m, \u001b[36mautocor!\u001b[39m, \u001b[36mautocov\u001b[39m, \u001b[36mautocov!\u001b[39m, \u001b[36maweights\u001b[39m,\n",
       "  \u001b[36mbic\u001b[39m, \u001b[36mclassify\u001b[39m, \u001b[36mclassify!\u001b[39m, \u001b[36mclassify_withscore\u001b[39m, \u001b[36mclassify_withscores\u001b[39m,\n",
       "  \u001b[36mclassify_withscores!\u001b[39m, \u001b[36mcoef\u001b[39m, \u001b[36mcoefnames\u001b[39m, \u001b[36mcoeftable\u001b[39m, \u001b[36mcompeterank\u001b[39m, \u001b[36mconfint\u001b[39m,\n",
       "  \u001b[36mconfusmat\u001b[39m, \u001b[36mcooksdistance\u001b[39m, \u001b[36mcor\u001b[39m, \u001b[36mcor2cov\u001b[39m, \u001b[36mcorkendall\u001b[39m, \u001b[36mcorrectrate\u001b[39m,\n",
       "  \u001b[36mcorspearman\u001b[39m, \u001b[36mcounteq\u001b[39m, \u001b[36mcounthits\u001b[39m, \u001b[36mcountmap\u001b[39m, \u001b[36mcountne\u001b[39m, \u001b[36mcounts\u001b[39m, \u001b[36mcov\u001b[39m, \u001b[36mcov2cor\u001b[39m,\n",
       "  \u001b[36mcronbachalpha\u001b[39m, \u001b[36mcross_validate\u001b[39m, \u001b[36mcrosscor\u001b[39m, \u001b[36mcrosscor!\u001b[39m, \u001b[36mcrosscov\u001b[39m, \u001b[36mcrosscov!\u001b[39m,\n",
       "  \u001b[36mcrossentropy\u001b[39m, \u001b[36mcrossmodelmatrix\u001b[39m, \u001b[36mdenserank\u001b[39m, \u001b[36mdescribe\u001b[39m, \u001b[36mdeviance\u001b[39m, \u001b[36mdof\u001b[39m,\n",
       "  \u001b[36mdof_residual\u001b[39m, \u001b[36mecdf\u001b[39m, \u001b[36mentropy\u001b[39m, \u001b[36merrorrate\u001b[39m, \u001b[36meweights\u001b[39m, \u001b[36mf1score\u001b[39m, \u001b[36mfalse_negative\u001b[39m,\n",
       "  \u001b[36mfalse_negative_rate\u001b[39m, \u001b[36mfalse_positive\u001b[39m, \u001b[36mfalse_positive_rate\u001b[39m, \u001b[36mfit\u001b[39m, \u001b[36mfit!\u001b[39m, \u001b[36mfitted\u001b[39m,\n",
       "  \u001b[36mfweights\u001b[39m, \u001b[36mgenmean\u001b[39m, \u001b[36mgenvar\u001b[39m, \u001b[36mgeomean\u001b[39m, \u001b[36mgkldiv\u001b[39m, \u001b[36mgridtune\u001b[39m, \u001b[36mgroupindices\u001b[39m,\n",
       "  \u001b[36mharmmean\u001b[39m, \u001b[36mhitrate\u001b[39m, \u001b[36mhitrates\u001b[39m, \u001b[36mindexmap\u001b[39m, \u001b[36mindicatormat\u001b[39m, \u001b[36minformationmatrix\u001b[39m,\n",
       "  \u001b[36minverse_rle\u001b[39m, \u001b[36miqr\u001b[39m, \u001b[36misfitted\u001b[39m, \u001b[36mislinear\u001b[39m, \u001b[36mkldivergence\u001b[39m, \u001b[36mkurtosis\u001b[39m, \u001b[36mlabeldecode\u001b[39m,\n",
       "  \u001b[36mlabelencode\u001b[39m, \u001b[36mlabelmap\u001b[39m, \u001b[36mlevelsmap\u001b[39m, \u001b[36mleverage\u001b[39m, \u001b[36mloglikelihood\u001b[39m, \u001b[36mmad\u001b[39m, \u001b[36mmad!\u001b[39m, \u001b[36mmaxad\u001b[39m,\n",
       "  \u001b[36mmean\u001b[39m, \u001b[36mmean!\u001b[39m, \u001b[36mmean_and_cov\u001b[39m, \u001b[36mmean_and_std\u001b[39m, \u001b[36mmean_and_var\u001b[39m, \u001b[36mmeanad\u001b[39m, \u001b[36mmeanresponse\u001b[39m,\n",
       "  \u001b[36mmedian\u001b[39m, \u001b[36mmedian!\u001b[39m, \u001b[36mmiddle\u001b[39m, \u001b[36mmidpoints\u001b[39m, \u001b[36mmode\u001b[39m, \u001b[36mmodel_response\u001b[39m, \u001b[36mmodelmatrix\u001b[39m,\n",
       "  \u001b[36mmodes\u001b[39m, \u001b[36mmoment\u001b[39m, \u001b[36mmsd\u001b[39m, \u001b[36mmss\u001b[39m, \u001b[36mnobs\u001b[39m, \u001b[36mnorepeats\u001b[39m, \u001b[36mnquantile\u001b[39m, \u001b[36mnulldeviance\u001b[39m,\n",
       "  \u001b[36mnullloglikelihood\u001b[39m, \u001b[36mordinalrank\u001b[39m, \u001b[36mpacf\u001b[39m, \u001b[36mpacf!\u001b[39m, \u001b[36mpairwise\u001b[39m, \u001b[36mpairwise!\u001b[39m,\n",
       "  \u001b[36mpartialcor\u001b[39m, \u001b[36mpercentile\u001b[39m, \u001b[36mpercentilerank\u001b[39m, \u001b[36mprecision\u001b[39m, \u001b[36mpredict\u001b[39m, \u001b[36mpredict!\u001b[39m,\n",
       "  \u001b[36mproportionmap\u001b[39m, \u001b[36mproportions\u001b[39m, \u001b[36mpsnr\u001b[39m, \u001b[36mpweights\u001b[39m, \u001b[36mquantile\u001b[39m, \u001b[36mquantile!\u001b[39m,\n",
       "  \u001b[36mquantilerank\u001b[39m, \u001b[36mr2\u001b[39m, \u001b[36mrecall\u001b[39m, \u001b[36mrenyientropy\u001b[39m, \u001b[36mrepeach\u001b[39m, \u001b[36mrepeachcol\u001b[39m, \u001b[36mrepeachrow\u001b[39m,\n",
       "  \u001b[36mresiduals\u001b[39m, \u001b[36mresponse\u001b[39m, \u001b[36mresponsename\u001b[39m, \u001b[36mrle\u001b[39m, \u001b[36mrmsd\u001b[39m, \u001b[36mroc\u001b[39m, \u001b[36mrss\u001b[39m, \u001b[36mr²\u001b[39m, \u001b[36msample\u001b[39m, \u001b[36msample!\u001b[39m,\n",
       "  \u001b[36msamplepair\u001b[39m, \u001b[36mscattermat\u001b[39m, \u001b[36mscattermat_zm\u001b[39m, \u001b[36mscattermatm\u001b[39m, \u001b[36mscore\u001b[39m, \u001b[36msem\u001b[39m, \u001b[36mskewness\u001b[39m,\n",
       "  \u001b[36mspan\u001b[39m, \u001b[36msqL2dist\u001b[39m, \u001b[36mstandardize\u001b[39m, \u001b[36mstandardize!\u001b[39m, \u001b[36mstd\u001b[39m, \u001b[36mstderror\u001b[39m, \u001b[36msum\u001b[39m, \u001b[36msummarystats\u001b[39m,\n",
       "  \u001b[36mtiedrank\u001b[39m, \u001b[36mtotalvar\u001b[39m, \u001b[36mtransform\u001b[39m, \u001b[36mtrim\u001b[39m, \u001b[36mtrim!\u001b[39m, \u001b[36mtrimvar\u001b[39m, \u001b[36mtrue_negative\u001b[39m,\n",
       "  \u001b[36mtrue_negative_rate\u001b[39m, \u001b[36mtrue_positive\u001b[39m, \u001b[36mtrue_positive_rate\u001b[39m, \u001b[36muweights\u001b[39m, \u001b[36mvalues\u001b[39m,\n",
       "  \u001b[36mvar\u001b[39m, \u001b[36mvariation\u001b[39m, \u001b[36mvcov\u001b[39m, \u001b[36mweights\u001b[39m, \u001b[36mwinsor\u001b[39m, \u001b[36mwinsor!\u001b[39m, \u001b[36mwmedian\u001b[39m, \u001b[36mwquantile\u001b[39m, \u001b[36mwsample\u001b[39m,\n",
       "  \u001b[36mwsample!\u001b[39m, \u001b[36mwsum\u001b[39m, \u001b[36mwsum!\u001b[39m, \u001b[36mzscore\u001b[39m, \u001b[36mzscore!\u001b[39m\n",
       "\n",
       "\u001b[1m  Displaying contents of readme found at\u001b[22m\n",
       "\u001b[1m \u001b[36mD:\\TongYuan\\.julia\\packages\\MLBase\\SAM9e\\README.md\u001b[39m\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "\u001b[1m  MLBase.jl\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "  Swiss knife for machine learning.\n",
       "\n",
       "  (Image: Build Status)\n",
       "  (https://github.com/JuliaStats/MLBase.jl/actions?query=workflow%3ACI+branch%3Amaster)\n",
       "  (Image: Coveralls)\n",
       "  (https://coveralls.io/github/JuliaStats/MLBase.jl?branch=master) (Image:\n",
       "  Documentation Status)\n",
       "  (http://mlbasejl.readthedocs.io/en/latest/?badge=latest)\n",
       "\n",
       "  This package does not implement specific machine learning algorithms.\n",
       "  Instead, it provides a collection of useful tools to support machine\n",
       "  learning programs, including:\n",
       "\n",
       "    •  Data manipulation & preprocessing\n",
       "\n",
       "    •  Score-based classification\n",
       "\n",
       "    •  Performance evaluation (\u001b[4me.g.\u001b[24m evaluating ROC)\n",
       "\n",
       "    •  Cross validation\n",
       "\n",
       "    •  Model tuning (\u001b[4mi.e.\u001b[24m search best settings of parameters)\n",
       "\n",
       "  \u001b[1mNotes:\u001b[22m This package depends on StatsBase\n",
       "  (https://github.com/JuliaStats/StatsBase.jl) and reexports all names\n",
       "  therefrom.\n",
       "\n",
       "\u001b[1m  Resources\u001b[22m\n",
       "\u001b[1m  –––––––––––\u001b[22m\n",
       "\n",
       "    •  \u001b[1mDocumentation:\u001b[22m http://mlbasejl.readthedocs.org/en/latest/\n",
       "       (http://mlbasejl.readthedocs.org/en/latest/)\n",
       "\n",
       "    •  \u001b[1mRelease Notes:\u001b[22m\n",
       "       https://github.com/JuliaStats/MLBase.jl/blob/master/NEWS.md\n",
       "       (https://github.com/JuliaStats/MLBase.jl/blob/master/NEWS.md)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?MLBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-22T10:11:35.290000+08:00",
     "start_time": "2022-07-22T02:11:35.042Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1mC\u001b[22m\u001b[0m\u001b[1mr\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mV\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1ml\u001b[22m\u001b[0m\u001b[1mG\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1mr\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mr\u001b[22m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "No documentation found.\n",
       "\n",
       "\\section{Summary}\n",
       "\\begin{verbatim}\n",
       "abstract type CrossValGenerator\n",
       "\\end{verbatim}\n",
       "\\section{Subtypes}\n",
       "\\begin{verbatim}\n",
       "Kfold\n",
       "LOOCV\n",
       "RandomSub\n",
       "StratifiedKfold\n",
       "StratifiedRandomSub\n",
       "\\end{verbatim}\n"
      ],
      "text/markdown": [
       "No documentation found.\n",
       "\n",
       "# Summary\n",
       "\n",
       "```\n",
       "abstract type CrossValGenerator\n",
       "```\n",
       "\n",
       "# Subtypes\n",
       "\n",
       "```\n",
       "Kfold\n",
       "LOOCV\n",
       "RandomSub\n",
       "StratifiedKfold\n",
       "StratifiedRandomSub\n",
       "```\n"
      ],
      "text/plain": [
       "  No documentation found.\n",
       "\n",
       "\u001b[1m  Summary\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "\u001b[36m  abstract type CrossValGenerator\u001b[39m\n",
       "\n",
       "\u001b[1m  Subtypes\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "\u001b[36m  Kfold\u001b[39m\n",
       "\u001b[36m  LOOCV\u001b[39m\n",
       "\u001b[36m  RandomSub\u001b[39m\n",
       "\u001b[36m  StratifiedKfold\u001b[39m\n",
       "\u001b[36m  StratifiedRandomSub\u001b[39m"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?CrossValGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HMMBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-05T17:16:51.772000+08:00",
     "start_time": "2022-07-05T09:16:51.770Z"
    }
   },
   "outputs": [],
   "source": [
    "using HMMBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-01T09:07:07.620000+08:00",
     "start_time": "2022-07-01T01:07:06.669Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1mH\u001b[22m\u001b[0m\u001b[1mM\u001b[22m\u001b[0m\u001b[1mM\u001b[22m\u001b[0m\u001b[1mB\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1me\u001b[22m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "Hidden Markov Models for Julia.\n",
       "\n",
       "\\href{https://maxmouchet.github.io/HMMBase.jl/stable/}{Documentation}.   \\href{https://github.com/maxmouchet/HMMBase.jl/issues}{Issues}.\n",
       "\n"
      ],
      "text/markdown": [
       "Hidden Markov Models for Julia.\n",
       "\n",
       "[Documentation](https://maxmouchet.github.io/HMMBase.jl/stable/).   [Issues](https://github.com/maxmouchet/HMMBase.jl/issues).\n"
      ],
      "text/plain": [
       "  Hidden Markov Models for Julia.\n",
       "\n",
       "  Documentation (https://maxmouchet.github.io/HMMBase.jl/stable/). Issues\n",
       "  (https://github.com/maxmouchet/HMMBase.jl/issues)."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?HMMBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-30T15:30:18.573000+08:00",
     "start_time": "2022-06-30T07:30:17.762Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30-element Vector{Symbol}:\n",
       " :AbstractHMM\n",
       " :HMM\n",
       " :HMMBase\n",
       " :backward\n",
       " :compute_transition_matrix\n",
       " :copy\n",
       " :fit_mle\n",
       " :forward\n",
       " :forward_backward\n",
       " :gettransmat\n",
       " :istransmat\n",
       " :likelihoods\n",
       " :log_likelihoods\n",
       " ⋮\n",
       " :messages_forwards_log\n",
       " :n_parameters\n",
       " :nparams\n",
       " :permute\n",
       " :posteriors\n",
       " :rand\n",
       " :rand_transition_matrix\n",
       " :randtransmat\n",
       " :remapseq\n",
       " :size\n",
       " :statdists\n",
       " :viterbi"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names(HMMBase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-01T11:27:15.804000+08:00",
     "start_time": "2022-07-01T03:27:15.801Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "fit_mle(D, x)\n",
       "\\end{verbatim}\n",
       "Fit a distribution of type \\texttt{D} to a given data set \\texttt{x}.\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item For univariate distribution, x can be an array of arbitrary size.\n",
       "\n",
       "\n",
       "\\item For multivariate distribution, x should be a matrix, where each column is a sample.\n",
       "\n",
       "\\end{itemize}\n",
       "\\rule{\\textwidth}{1pt}\n",
       "\\begin{verbatim}\n",
       "fit_mle(D, x, w)\n",
       "\\end{verbatim}\n",
       "Fit a distribution of type \\texttt{D} to a weighted data set \\texttt{x}, with weights given by \\texttt{w}.\n",
       "\n",
       "Here, \\texttt{w} should be an array with length \\texttt{n}, where \\texttt{n} is the number of samples contained in \\texttt{x}.\n",
       "\n",
       "\\rule{\\textwidth}{1pt}\n",
       "\\begin{verbatim}\n",
       "fit_mle(::Type{<:Beta}, x::AbstractArray{T})\n",
       "\\end{verbatim}\n",
       "Maximum Likelihood Estimate of \\texttt{Beta} Distribution via Newton's Method\n",
       "\n",
       "\\rule{\\textwidth}{1pt}\n",
       "\\begin{verbatim}\n",
       "fit_mle(::Type{<:Weibull}, x::AbstractArray{<:Real}; \n",
       "alpha0::Real = 1, maxiter::Int = 1000, tol::Real = 1e-16)\n",
       "\\end{verbatim}\n",
       "Compute the maximum likelihood estimate of the \\href{@ref}{\\texttt{Weibull}} distribution with Newton's method.\n",
       "\n",
       "\\rule{\\textwidth}{1pt}\n",
       "\\begin{verbatim}\n",
       "fit_mle(hmm, observations; ...) -> AbstractHMM\n",
       "\\end{verbatim}\n",
       "Estimate the HMM parameters using the EM (Baum-Welch) algorithm, with \\texttt{hmm} as the initial state.\n",
       "\n",
       "\\textbf{Keyword Arguments}\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{display::Symbol = :none}: when to display convergence logs, can be set to \\texttt{:iter} or \\texttt{:final}.\n",
       "\n",
       "\n",
       "\\item \\texttt{init::Symbol = :none}: if set to \\texttt{:kmeans} the HMM parameters will be initialized using a K-means clustering.\n",
       "\n",
       "\n",
       "\\item \\texttt{maxiter::Integer = 100}: maximum number of iterations to perform.\n",
       "\n",
       "\n",
       "\\item \\texttt{tol::Integer = 1e-3}: stop the algorithm when the improvement in the log-likelihood is less than \\texttt{tol}.\n",
       "\n",
       "\\end{itemize}\n",
       "\\textbf{Output}\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{<:AbstractHMM}: a copy of the original HMM with the updated parameters.\n",
       "\n",
       "\\end{itemize}\n"
      ],
      "text/markdown": [
       "```\n",
       "fit_mle(D, x)\n",
       "```\n",
       "\n",
       "Fit a distribution of type `D` to a given data set `x`.\n",
       "\n",
       "  * For univariate distribution, x can be an array of arbitrary size.\n",
       "  * For multivariate distribution, x should be a matrix, where each column is a sample.\n",
       "\n",
       "---\n",
       "\n",
       "```\n",
       "fit_mle(D, x, w)\n",
       "```\n",
       "\n",
       "Fit a distribution of type `D` to a weighted data set `x`, with weights given by `w`.\n",
       "\n",
       "Here, `w` should be an array with length `n`, where `n` is the number of samples contained in `x`.\n",
       "\n",
       "---\n",
       "\n",
       "```\n",
       "fit_mle(::Type{<:Beta}, x::AbstractArray{T})\n",
       "```\n",
       "\n",
       "Maximum Likelihood Estimate of `Beta` Distribution via Newton's Method\n",
       "\n",
       "---\n",
       "\n",
       "```\n",
       "fit_mle(::Type{<:Weibull}, x::AbstractArray{<:Real}; \n",
       "alpha0::Real = 1, maxiter::Int = 1000, tol::Real = 1e-16)\n",
       "```\n",
       "\n",
       "Compute the maximum likelihood estimate of the [`Weibull`](@ref) distribution with Newton's method.\n",
       "\n",
       "---\n",
       "\n",
       "```\n",
       "fit_mle(hmm, observations; ...) -> AbstractHMM\n",
       "```\n",
       "\n",
       "Estimate the HMM parameters using the EM (Baum-Welch) algorithm, with `hmm` as the initial state.\n",
       "\n",
       "**Keyword Arguments**\n",
       "\n",
       "  * `display::Symbol = :none`: when to display convergence logs, can be set to `:iter` or `:final`.\n",
       "  * `init::Symbol = :none`: if set to `:kmeans` the HMM parameters will be initialized using a K-means clustering.\n",
       "  * `maxiter::Integer = 100`: maximum number of iterations to perform.\n",
       "  * `tol::Integer = 1e-3`: stop the algorithm when the improvement in the log-likelihood is less than `tol`.\n",
       "\n",
       "**Output**\n",
       "\n",
       "  * `<:AbstractHMM`: a copy of the original HMM with the updated parameters.\n"
      ],
      "text/plain": [
       "\u001b[36m  fit_mle(D, x)\u001b[39m\n",
       "\n",
       "  Fit a distribution of type \u001b[36mD\u001b[39m to a given data set \u001b[36mx\u001b[39m.\n",
       "\n",
       "    •  For univariate distribution, x can be an array of arbitrary size.\n",
       "\n",
       "    •  For multivariate distribution, x should be a matrix, where each\n",
       "       column is a sample.\n",
       "\n",
       "  ────────────────────────────────────────────────────────────────────────────\n",
       "\n",
       "\u001b[36m  fit_mle(D, x, w)\u001b[39m\n",
       "\n",
       "  Fit a distribution of type \u001b[36mD\u001b[39m to a weighted data set \u001b[36mx\u001b[39m, with weights given by\n",
       "  \u001b[36mw\u001b[39m.\n",
       "\n",
       "  Here, \u001b[36mw\u001b[39m should be an array with length \u001b[36mn\u001b[39m, where \u001b[36mn\u001b[39m is the number of samples\n",
       "  contained in \u001b[36mx\u001b[39m.\n",
       "\n",
       "  ────────────────────────────────────────────────────────────────────────────\n",
       "\n",
       "\u001b[36m  fit_mle(::Type{<:Beta}, x::AbstractArray{T})\u001b[39m\n",
       "\n",
       "  Maximum Likelihood Estimate of \u001b[36mBeta\u001b[39m Distribution via Newton's Method\n",
       "\n",
       "  ────────────────────────────────────────────────────────────────────────────\n",
       "\n",
       "\u001b[36m  fit_mle(::Type{<:Weibull}, x::AbstractArray{<:Real}; \u001b[39m\n",
       "\u001b[36m  alpha0::Real = 1, maxiter::Int = 1000, tol::Real = 1e-16)\u001b[39m\n",
       "\n",
       "  Compute the maximum likelihood estimate of the \u001b[36mWeibull\u001b[39m distribution with\n",
       "  Newton's method.\n",
       "\n",
       "  ────────────────────────────────────────────────────────────────────────────\n",
       "\n",
       "\u001b[36m  fit_mle(hmm, observations; ...) -> AbstractHMM\u001b[39m\n",
       "\n",
       "  Estimate the HMM parameters using the EM (Baum-Welch) algorithm, with \u001b[36mhmm\u001b[39m as\n",
       "  the initial state.\n",
       "\n",
       "  \u001b[1mKeyword Arguments\u001b[22m\n",
       "\n",
       "    •  \u001b[36mdisplay::Symbol = :none\u001b[39m: when to display convergence logs, can be\n",
       "       set to \u001b[36m:iter\u001b[39m or \u001b[36m:final\u001b[39m.\n",
       "\n",
       "    •  \u001b[36minit::Symbol = :none\u001b[39m: if set to \u001b[36m:kmeans\u001b[39m the HMM parameters will be\n",
       "       initialized using a K-means clustering.\n",
       "\n",
       "    •  \u001b[36mmaxiter::Integer = 100\u001b[39m: maximum number of iterations to perform.\n",
       "\n",
       "    •  \u001b[36mtol::Integer = 1e-3\u001b[39m: stop the algorithm when the improvement in\n",
       "       the log-likelihood is less than \u001b[36mtol\u001b[39m.\n",
       "\n",
       "  \u001b[1mOutput\u001b[22m\n",
       "\n",
       "    •  \u001b[36m<:AbstractHMM\u001b[39m: a copy of the original HMM with the updated\n",
       "       parameters."
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?HMMBase.fit_mle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T14:54:16.416000+08:00",
     "start_time": "2022-06-08T06:53:24.728Z"
    }
   },
   "outputs": [],
   "source": [
    "#x = names(HMMBase)\n",
    "#fh = \"C://Users//TR//Desktop//HMM库.md\"\n",
    "#writelines(x,fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-01T09:07:12.691000+08:00",
     "start_time": "2022-07-01T01:07:12.207Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "HMM([a, ]A, B) -> HMM\n",
       "\\end{verbatim}\n",
       "Build an HMM with transition matrix \\texttt{A} and observation distributions \\texttt{B}.   If the initial state distribution \\texttt{a} is not specified, a uniform distribution is assumed. \n",
       "\n",
       "Observations distributions can be of different types (for example \\texttt{Normal} and \\texttt{Exponential}),   but they must be of the same dimension.\n",
       "\n",
       "Alternatively, \\texttt{B} can be an emission matrix where \\texttt{B[i,j]} is the probability of observing symbol \\texttt{j} in state \\texttt{i}.\n",
       "\n",
       "\\textbf{Arguments}\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{a::AbstractVector\\{T\\}}: initial probabilities vector.\n",
       "\n",
       "\n",
       "\\item \\texttt{A::AbstractMatrix\\{T\\}}: transition matrix.\n",
       "\n",
       "\n",
       "\\item \\texttt{B::AbstractVector\\{<:Distribution\\{F\\}\\}}: observations distributions.\n",
       "\n",
       "\n",
       "\\item or \\texttt{B::AbstractMatrix}: emission matrix.\n",
       "\n",
       "\\end{itemize}\n",
       "\\textbf{Example}\n",
       "\n",
       "\\begin{verbatim}\n",
       "using Distributions, HMMBase\n",
       "# from distributions\n",
       "hmm = HMM([0.9 0.1; 0.1 0.9], [Normal(0,1), Normal(10,1)])\n",
       "# from an emission matrix\n",
       "hmm = HMM([0.9 0.1; 0.1 0.9], [0. 0.5 0.5; 0.25 0.25 0.5])\n",
       "\\end{verbatim}\n"
      ],
      "text/markdown": [
       "```\n",
       "HMM([a, ]A, B) -> HMM\n",
       "```\n",
       "\n",
       "Build an HMM with transition matrix `A` and observation distributions `B`.   If the initial state distribution `a` is not specified, a uniform distribution is assumed. \n",
       "\n",
       "Observations distributions can be of different types (for example `Normal` and `Exponential`),   but they must be of the same dimension.\n",
       "\n",
       "Alternatively, `B` can be an emission matrix where `B[i,j]` is the probability of observing symbol `j` in state `i`.\n",
       "\n",
       "**Arguments**\n",
       "\n",
       "  * `a::AbstractVector{T}`: initial probabilities vector.\n",
       "  * `A::AbstractMatrix{T}`: transition matrix.\n",
       "  * `B::AbstractVector{<:Distribution{F}}`: observations distributions.\n",
       "  * or `B::AbstractMatrix`: emission matrix.\n",
       "\n",
       "**Example**\n",
       "\n",
       "```julia\n",
       "using Distributions, HMMBase\n",
       "# from distributions\n",
       "hmm = HMM([0.9 0.1; 0.1 0.9], [Normal(0,1), Normal(10,1)])\n",
       "# from an emission matrix\n",
       "hmm = HMM([0.9 0.1; 0.1 0.9], [0. 0.5 0.5; 0.25 0.25 0.5])\n",
       "```\n"
      ],
      "text/plain": [
       "\u001b[36m  HMM([a, ]A, B) -> HMM\u001b[39m\n",
       "\n",
       "  Build an HMM with transition matrix \u001b[36mA\u001b[39m and observation distributions \u001b[36mB\u001b[39m. If\n",
       "  the initial state distribution \u001b[36ma\u001b[39m is not specified, a uniform distribution is\n",
       "  assumed.\n",
       "\n",
       "  Observations distributions can be of different types (for example \u001b[36mNormal\u001b[39m and\n",
       "  \u001b[36mExponential\u001b[39m), but they must be of the same dimension.\n",
       "\n",
       "  Alternatively, \u001b[36mB\u001b[39m can be an emission matrix where \u001b[36mB[i,j]\u001b[39m is the probability\n",
       "  of observing symbol \u001b[36mj\u001b[39m in state \u001b[36mi\u001b[39m.\n",
       "\n",
       "  \u001b[1mArguments\u001b[22m\n",
       "\n",
       "    •  \u001b[36ma::AbstractVector{T}\u001b[39m: initial probabilities vector.\n",
       "\n",
       "    •  \u001b[36mA::AbstractMatrix{T}\u001b[39m: transition matrix.\n",
       "\n",
       "    •  \u001b[36mB::AbstractVector{<:Distribution{F}}\u001b[39m: observations distributions.\n",
       "\n",
       "    •  or \u001b[36mB::AbstractMatrix\u001b[39m: emission matrix.\n",
       "\n",
       "  \u001b[1mExample\u001b[22m\n",
       "\n",
       "\u001b[36m  using Distributions, HMMBase\u001b[39m\n",
       "\u001b[36m  # from distributions\u001b[39m\n",
       "\u001b[36m  hmm = HMM([0.9 0.1; 0.1 0.9], [Normal(0,1), Normal(10,1)])\u001b[39m\n",
       "\u001b[36m  # from an emission matrix\u001b[39m\n",
       "\u001b[36m  hmm = HMM([0.9 0.1; 0.1 0.9], [0. 0.5 0.5; 0.25 0.25 0.5])\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?HMMBase.HMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-01T09:07:24.566000+08:00",
     "start_time": "2022-07-01T01:07:23.405Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HMM{Distributions.Univariate, Float64}([0.5, 0.5], [0.9 0.1; 0.1 0.9], Distributions.UnivariateDistribution[Distributions.Categorical{Float64, Vector{Float64}}(support=Base.OneTo(3), p=[0.0, 0.5, 0.5]), Distributions.Categorical{Float64, Vector{Float64}}(support=Base.OneTo(3), p=[0.25, 0.25, 0.5])])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hmm = HMM([0.9 0.1; 0.1 0.9], [0. 0.5 0.5; 0.25 0.25 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-01T09:45:13.616000+08:00",
     "start_time": "2022-07-01T01:45:12.615Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000-element Vector{Int64}:\n",
       " 1\n",
       " 1\n",
       " 2\n",
       " 2\n",
       " 2\n",
       " 2\n",
       " 2\n",
       " 2\n",
       " 2\n",
       " 2\n",
       " 2\n",
       " 2\n",
       " 2\n",
       " ⋮\n",
       " 2\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#预测问题\n",
    "using Distributions, HMMBase\n",
    "hmm = HMM([0.9 0.1; 0.1 0.9], [Normal(0,1), Normal(10,1)])\n",
    "y = rand(hmm, 1000)\n",
    "zv = viterbi(hmm, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T14:54:16.431000+08:00",
     "start_time": "2022-06-08T06:53:24.731Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "viterbi(a, A, LL) -> Vector\n",
       "\\end{verbatim}\n",
       "Find the most likely hidden state sequence, see \\href{https://en.wikipedia.org/wiki/Viterbi_algorithm}{Viterbi algorithm}.\n",
       "\n",
       "\\rule{\\textwidth}{1pt}\n",
       "\\begin{verbatim}\n",
       "viterbi(hmm, observations; robust) -> Vector\n",
       "\\end{verbatim}\n",
       "\\textbf{Example}\n",
       "\n",
       "\\begin{verbatim}\n",
       "using Distributions, HMMBase\n",
       "hmm = HMM([0.9 0.1; 0.1 0.9], [Normal(0,1), Normal(10,1)])\n",
       "y = rand(hmm, 1000)\n",
       "zv = viterbi(hmm, y)\n",
       "\\end{verbatim}\n"
      ],
      "text/markdown": [
       "```\n",
       "viterbi(a, A, LL) -> Vector\n",
       "```\n",
       "\n",
       "Find the most likely hidden state sequence, see [Viterbi algorithm](https://en.wikipedia.org/wiki/Viterbi_algorithm).\n",
       "\n",
       "---\n",
       "\n",
       "```\n",
       "viterbi(hmm, observations; robust) -> Vector\n",
       "```\n",
       "\n",
       "**Example**\n",
       "\n",
       "```julia\n",
       "using Distributions, HMMBase\n",
       "hmm = HMM([0.9 0.1; 0.1 0.9], [Normal(0,1), Normal(10,1)])\n",
       "y = rand(hmm, 1000)\n",
       "zv = viterbi(hmm, y)\n",
       "```\n"
      ],
      "text/plain": [
       "\u001b[36m  viterbi(a, A, LL) -> Vector\u001b[39m\n",
       "\n",
       "  Find the most likely hidden state sequence, see Viterbi algorithm\n",
       "  (https://en.wikipedia.org/wiki/Viterbi_algorithm).\n",
       "\n",
       "  ────────────────────────────────────────────────────────────────────────────\n",
       "\n",
       "\u001b[36m  viterbi(hmm, observations; robust) -> Vector\u001b[39m\n",
       "\n",
       "  \u001b[1mExample\u001b[22m\n",
       "\n",
       "\u001b[36m  using Distributions, HMMBase\u001b[39m\n",
       "\u001b[36m  hmm = HMM([0.9 0.1; 0.1 0.9], [Normal(0,1), Normal(10,1)])\u001b[39m\n",
       "\u001b[36m  y = rand(hmm, 1000)\u001b[39m\n",
       "\u001b[36m  zv = viterbi(hmm, y)\u001b[39m"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?HMMBase.viterbi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-01T10:10:35.342000+08:00",
     "start_time": "2022-07-01T02:10:35.328Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "loglikelihoods(hmm, observations; robust) -> Matrix\n",
       "\\end{verbatim}\n",
       "Return the log-likelihood per-state and per-observation.\n",
       "\n",
       "\\textbf{Output}\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{Matrix\\{Float64\\}}: log-likelihoods matrix (\\texttt{T x K}).\n",
       "\n",
       "\\end{itemize}\n",
       "\\textbf{Example}\n",
       "\n",
       "\\begin{verbatim}\n",
       "using Distributions, HMMBase\n",
       "hmm = HMM([0.9 0.1; 0.1 0.9], [Normal(0,1), Normal(10,1)])\n",
       "y = rand(hmm, 1000)\n",
       "LL = likelihoods(hmm, y)\n",
       "\\end{verbatim}\n"
      ],
      "text/markdown": [
       "```\n",
       "loglikelihoods(hmm, observations; robust) -> Matrix\n",
       "```\n",
       "\n",
       "Return the log-likelihood per-state and per-observation.\n",
       "\n",
       "**Output**\n",
       "\n",
       "  * `Matrix{Float64}`: log-likelihoods matrix (`T x K`).\n",
       "\n",
       "**Example**\n",
       "\n",
       "```julia\n",
       "using Distributions, HMMBase\n",
       "hmm = HMM([0.9 0.1; 0.1 0.9], [Normal(0,1), Normal(10,1)])\n",
       "y = rand(hmm, 1000)\n",
       "LL = likelihoods(hmm, y)\n",
       "```\n"
      ],
      "text/plain": [
       "\u001b[36m  loglikelihoods(hmm, observations; robust) -> Matrix\u001b[39m\n",
       "\n",
       "  Return the log-likelihood per-state and per-observation.\n",
       "\n",
       "  \u001b[1mOutput\u001b[22m\n",
       "\n",
       "    •  \u001b[36mMatrix{Float64}\u001b[39m: log-likelihoods matrix (\u001b[36mT x K\u001b[39m).\n",
       "\n",
       "  \u001b[1mExample\u001b[22m\n",
       "\n",
       "\u001b[36m  using Distributions, HMMBase\u001b[39m\n",
       "\u001b[36m  hmm = HMM([0.9 0.1; 0.1 0.9], [Normal(0,1), Normal(10,1)])\u001b[39m\n",
       "\u001b[36m  y = rand(hmm, 1000)\u001b[39m\n",
       "\u001b[36m  LL = likelihoods(hmm, y)\u001b[39m"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?HMMBase.loglikelihoods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-01T10:09:58.726000+08:00",
     "start_time": "2022-07-01T02:09:58.721Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000×2 Matrix{Float64}:\n",
       " -67.7543     -2.13825\n",
       " -41.6018     -1.39885\n",
       " -63.9071     -1.66792\n",
       " -47.0982     -0.994857\n",
       "  -2.08833   -36.7952\n",
       "  -0.922152  -51.7239\n",
       "  -0.922335  -51.7466\n",
       "  -0.966499  -54.0507\n",
       "  -1.33978   -60.5141\n",
       "  -0.939436  -52.9641\n",
       "  -1.0343    -55.8377\n",
       "  -0.971081  -47.7417\n",
       "  -1.28731   -42.704\n",
       "   ⋮         \n",
       "  -0.978029  -47.5403\n",
       "  -1.0445    -56.0558\n",
       "  -2.88687   -72.7259\n",
       "  -1.57229   -40.1412\n",
       "  -1.02413   -46.4374\n",
       "  -1.08212   -56.7949\n",
       "  -1.27509   -42.8353\n",
       "  -1.30841   -60.1342\n",
       "  -1.87195   -65.6778\n",
       "  -0.921316  -51.6109\n",
       "  -1.2196    -58.9741\n",
       "  -1.62801   -63.5366"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#学习问题\n",
    "using Distributions, HMMBase\n",
    "hmm = HMM([0.9 0.1; 0.1 0.9], [Normal(0,1), Normal(10,1)])\n",
    "y = rand(hmm, 1000)\n",
    "LL = loglikelihoods(hmm, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-01T10:28:46.845000+08:00",
     "start_time": "2022-07-01T02:28:46.820Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "posteriors(α, β) -> Vector\n",
       "\\end{verbatim}\n",
       "Compute posterior probabilities from \\texttt{α} and \\texttt{β}.\n",
       "\n",
       "\\textbf{Arguments}\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{α::AbstractVector}: forward probabilities.\n",
       "\n",
       "\n",
       "\\item \\texttt{β::AbstractVector}: backward probabilities.\n",
       "\n",
       "\\end{itemize}\n",
       "\\rule{\\textwidth}{1pt}\n",
       "\\begin{verbatim}\n",
       "posteriors(a, A, LL; kwargs...) -> Vector\n",
       "\\end{verbatim}\n",
       "Compute posterior probabilities using samples likelihoods.\n",
       "\n",
       "\\rule{\\textwidth}{1pt}\n",
       "\\begin{verbatim}\n",
       "posteriors(hmm, observations; robust) -> Vector\n",
       "\\end{verbatim}\n",
       "Compute posterior probabilities using samples likelihoods.\n",
       "\n",
       "\\textbf{Example}\n",
       "\n",
       "\\begin{verbatim}\n",
       "using Distributions, HMMBase\n",
       "hmm = HMM([0.9 0.1; 0.1 0.9], [Normal(0,1), Normal(10,1)])\n",
       "y = rand(hmm, 1000)\n",
       "γ = posteriors(hmm, y)\n",
       "\\end{verbatim}\n"
      ],
      "text/markdown": [
       "```\n",
       "posteriors(α, β) -> Vector\n",
       "```\n",
       "\n",
       "Compute posterior probabilities from `α` and `β`.\n",
       "\n",
       "**Arguments**\n",
       "\n",
       "  * `α::AbstractVector`: forward probabilities.\n",
       "  * `β::AbstractVector`: backward probabilities.\n",
       "\n",
       "---\n",
       "\n",
       "```\n",
       "posteriors(a, A, LL; kwargs...) -> Vector\n",
       "```\n",
       "\n",
       "Compute posterior probabilities using samples likelihoods.\n",
       "\n",
       "---\n",
       "\n",
       "```\n",
       "posteriors(hmm, observations; robust) -> Vector\n",
       "```\n",
       "\n",
       "Compute posterior probabilities using samples likelihoods.\n",
       "\n",
       "**Example**\n",
       "\n",
       "```julia\n",
       "using Distributions, HMMBase\n",
       "hmm = HMM([0.9 0.1; 0.1 0.9], [Normal(0,1), Normal(10,1)])\n",
       "y = rand(hmm, 1000)\n",
       "γ = posteriors(hmm, y)\n",
       "```\n"
      ],
      "text/plain": [
       "\u001b[36m  posteriors(α, β) -> Vector\u001b[39m\n",
       "\n",
       "  Compute posterior probabilities from \u001b[36mα\u001b[39m and \u001b[36mβ\u001b[39m.\n",
       "\n",
       "  \u001b[1mArguments\u001b[22m\n",
       "\n",
       "    •  \u001b[36mα::AbstractVector\u001b[39m: forward probabilities.\n",
       "\n",
       "    •  \u001b[36mβ::AbstractVector\u001b[39m: backward probabilities.\n",
       "\n",
       "  ────────────────────────────────────────────────────────────────────────────\n",
       "\n",
       "\u001b[36m  posteriors(a, A, LL; kwargs...) -> Vector\u001b[39m\n",
       "\n",
       "  Compute posterior probabilities using samples likelihoods.\n",
       "\n",
       "  ────────────────────────────────────────────────────────────────────────────\n",
       "\n",
       "\u001b[36m  posteriors(hmm, observations; robust) -> Vector\u001b[39m\n",
       "\n",
       "  Compute posterior probabilities using samples likelihoods.\n",
       "\n",
       "  \u001b[1mExample\u001b[22m\n",
       "\n",
       "\u001b[36m  using Distributions, HMMBase\u001b[39m\n",
       "\u001b[36m  hmm = HMM([0.9 0.1; 0.1 0.9], [Normal(0,1), Normal(10,1)])\u001b[39m\n",
       "\u001b[36m  y = rand(hmm, 1000)\u001b[39m\n",
       "\u001b[36m  γ = posteriors(hmm, y)\u001b[39m"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?HMMBase.posteriors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-01T10:34:30.867000+08:00",
     "start_time": "2022-07-01T02:34:30.699Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000×2 Matrix{Float64}:\n",
       " 1.37488e-25  1.0\n",
       " 7.23292e-32  1.0\n",
       " 1.10828e-18  1.0\n",
       " 1.46636e-15  1.0\n",
       " 1.0          1.28639e-18\n",
       " 1.0          3.15619e-24\n",
       " 1.0          2.69479e-23\n",
       " 1.0          5.11238e-18\n",
       " 1.0          1.77088e-27\n",
       " 1.0          6.88613e-24\n",
       " 1.0          1.39823e-20\n",
       " 1.0          1.24936e-24\n",
       " 1.0          2.47539e-25\n",
       " ⋮            \n",
       " 7.95542e-20  1.0\n",
       " 8.85816e-23  1.0\n",
       " 4.27447e-21  1.0\n",
       " 2.36838e-20  1.0\n",
       " 2.54239e-23  1.0\n",
       " 4.1151e-25   1.0\n",
       " 9.62757e-32  1.0\n",
       " 1.47415e-20  1.0\n",
       " 1.0          2.16126e-27\n",
       " 1.0          2.55439e-20\n",
       " 1.0          2.91914e-20\n",
       " 1.0          1.83575e-23"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Distributions, HMMBase\n",
    "hmm = HMM([0.9 0.1; 0.1 0.9], [Normal(0,1), Normal(10,1)])\n",
    "y = rand(hmm, 1000)\n",
    "γ = posteriors(hmm, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-05T17:17:20.083000+08:00",
     "start_time": "2022-07-05T09:17:20.081Z"
    }
   },
   "outputs": [],
   "source": [
    "using NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T14:54:16.791000+08:00",
     "start_time": "2022-06-08T06:53:24.735Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1mN\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1mr\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1mN\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1mg\u001b[22m\u001b[0m\u001b[1mh\u001b[22m\u001b[0m\u001b[1mb\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mr\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "No docstring found for module \\texttt{NearestNeighbors}.\n",
       "\n",
       "\\section{Exported names}\n",
       "\\texttt{BallTree}, \\texttt{BruteTree}, \\texttt{Chebyshev}, \\texttt{Cityblock}, \\texttt{DataFreeTree}, \\texttt{Euclidean}, \\texttt{Hamming}, \\texttt{KDTree}, \\texttt{Minkowski}, \\texttt{NNTree}, \\texttt{WeightedCityblock}, \\texttt{WeightedEuclidean}, \\texttt{WeightedMinkowski}, \\texttt{injectdata}, \\texttt{inrange}, \\texttt{inrangecount}, \\texttt{knn}, \\texttt{nn}\n",
       "\n",
       "\\section{Displaying contents of readme found at \\texttt{D:{\\textbackslash}TongYuan{\\textbackslash}.julia{\\textbackslash}packages{\\textbackslash}NearestNeighbors{\\textbackslash}YCcEC{\\textbackslash}README.md}}\n",
       "\\section{NearestNeighbors.jl}\n",
       "\\href{https://github.com/KristofferC/NearestNeighbors.jl/actions?query=workflows/CI}{\\begin{figure}\n",
       "\\centering\n",
       "\\includegraphics{https://github.com/KristofferC/NearestNeighbors.jl/workflows/CI/badge.svg}\n",
       "\\caption{Build Status}\n",
       "\\end{figure}\n",
       "}  \\href{https://codecov.io/github/KristofferC/NearestNeighbors.jl?branch=master}{\\begin{figure}\n",
       "\\centering\n",
       "\\includegraphics{https://codecov.io/github/KristofferC/NearestNeighbors.jl/coverage.svg?branch=master}\n",
       "\\caption{codecov.io}\n",
       "\\end{figure}\n",
       "} \\href{https://zenodo.org/badge/latestdoi/45321556}{\\begin{figure}\n",
       "\\centering\n",
       "\\includegraphics{https://zenodo.org/badge/45321556.svg}\n",
       "\\caption{DOI}\n",
       "\\end{figure}\n",
       "}\n",
       "\n",
       "\\texttt{NearestNeighbors.jl} is a package written in Julia to perform high performance nearest neighbor searches in  arbitrarily high dimensions.\n",
       "\n",
       "\\rule{\\textwidth}{1pt}\n",
       "\\subsection{Creating a tree}\n",
       "There are currently three types of trees available:\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{BruteTree}: Not actually a tree. It linearly searches all points in a brute force fashion. Works with any \\texttt{Metric}.\n",
       "\n",
       "\n",
       "\\item \\texttt{KDTree}: In a kd tree the points are recursively split into groups using hyper-planes.\n",
       "\n",
       "\\end{itemize}\n",
       "Therefore a \\texttt{KDTree} only works with axis aligned metrics which are: \\texttt{Euclidean}, \\texttt{Chebyshev}, \\texttt{Minkowski} and \\texttt{Cityblock}.\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{BallTree}: Points are recursively split into groups bounded by hyper-spheres. Works with any \\texttt{Metric}.\n",
       "\n",
       "\\end{itemize}\n",
       "These trees are created with the following syntax:\n",
       "\n",
       "\\begin{verbatim}\n",
       "BruteTree(data, metric; leafsize, reorder)\n",
       "KDTree(data, metric; leafsize, reorder)\n",
       "BallTree(data, metric; leafsize, reorder)\n",
       "\\end{verbatim}\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{data}: The data, i.e., the points to build up the tree from. It can either be \n",
       "\n",
       "\\begin{itemize}\n",
       "\\item a matrix of size \\texttt{nd × np} with the points to insert in the tree where \\texttt{nd} is the dimensionality of the points and \\texttt{np} is the number of points\n",
       "\n",
       "\n",
       "\\item a vector of vectors with fixed dimensionality, \\texttt{nd}, which must be part of the type. Specifically, \\texttt{data} should be a \\texttt{Vector\\{V\\}}, where \\texttt{V} is itself a subtype of an \\texttt{AbstractVector} and such that \\texttt{eltype(V)} and \\texttt{length(V)} are defined.   (For example, with 3D points, \\texttt{V = SVector\\{3, Float64\\}} works because \\texttt{eltype(V) = Float64} and \\texttt{length(V) = 3} are defined in \\texttt{V}.)\n",
       "\n",
       "\\end{itemize}\n",
       "\n",
       "\\item \\texttt{metric}: The metric to use, defaults to \\texttt{Euclidean}. This is one of the \\texttt{Metric} types defined in the \\texttt{Distances.jl} packages. It is possible to define your own metrics by simply creating new types that are subtypes of \\texttt{Metric}.\n",
       "\n",
       "\n",
       "\\item \\texttt{leafsize} (keyword argument): Determines at what number of points to stop splitting the tree further. There is a trade-off between traversing the tree and having to evaluate the metric function for increasing number of points.\n",
       "\n",
       "\n",
       "\\item \\texttt{reorder} (keyword argument): While building the tree this will put points close in distance close in memory since this helps with cache locality. In this case, a copy of the original data will be made so that the original data is left unmodified. This can have a significant impact on performance and is by default set to \\texttt{true}.\n",
       "\n",
       "\\end{itemize}\n",
       "All trees in \\texttt{NearestNeighbors.jl} are static which means that points can not be added or removed from an already created tree.\n",
       "\n",
       "Here are a few examples of creating trees:\n",
       "\n",
       "\\begin{verbatim}\n",
       "using NearestNeighbors\n",
       "data = rand(3, 10^4)\n",
       "\n",
       "# Create trees\n",
       "kdtree = KDTree(data; leafsize = 10)\n",
       "balltree = BallTree(data, Minkowski(3.5); reorder = false)\n",
       "brutetree = BruteTree(data)\n",
       "\\end{verbatim}\n",
       "\\subsection{k Nearest Neighbor (kNN) searches}\n",
       "A kNN search finds the \\texttt{k} nearest neighbors to given point(s). This is done with the method:\n",
       "\n",
       "\\begin{verbatim}\n",
       "knn(tree, points, k, sortres = false, skip = always_false) -> idxs, dists\n",
       "\\end{verbatim}\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{tree}: The tree instance\n",
       "\n",
       "\n",
       "\\item \\texttt{points}: A vector or matrix of points to find the \\texttt{k} nearest neighbors to. If \\texttt{points} is a vector of numbers then this represents a single point, if \\texttt{points} is a matrix then the \\texttt{k} nearest neighbors to each point (column) will be computed. \\texttt{points} can also be a vector of other vectors where each element in the outer vector is considered a point.\n",
       "\n",
       "\n",
       "\\item \\texttt{sortres} (optional): Determines if the results should be sorted before returning.\n",
       "\n",
       "\\end{itemize}\n",
       "In this case the results will be sorted in order of increasing distance to the point.\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{skip} (optional): A predicate to determine if a given point should be skipped, for\n",
       "\n",
       "\\end{itemize}\n",
       "example if iterating over points and a point has already been visited.\n",
       "\n",
       "It is generally better for performance to query once with a large number of points than to query multiple times with one point per query.\n",
       "\n",
       "As a convenience, if you only want the closest nearest neighbor, you can call \\texttt{nn} instead for a cleaner result:\n",
       "\n",
       "\\begin{verbatim}\n",
       "nn(tree, points, skip = always_false) -> idxs, dists\n",
       "\\end{verbatim}\n",
       "Some examples:\n",
       "\n",
       "\\begin{verbatim}\n",
       "using NearestNeighbors\n",
       "data = rand(3, 10^4)\n",
       "k = 3\n",
       "point = rand(3)\n",
       "\n",
       "kdtree = KDTree(data)\n",
       "idxs, dists = knn(kdtree, point, k, true)\n",
       "\n",
       "idxs\n",
       "# 3-element Array{Int64,1}:\n",
       "#  4683\n",
       "#  6119\n",
       "#  3278\n",
       "\n",
       "dists\n",
       "# 3-element Array{Float64,1}:\n",
       "#  0.039032201026256215\n",
       "#  0.04134193711411951\n",
       "#  0.042974090446474184\n",
       "\n",
       "# Multiple points\n",
       "points = rand(3, 4);\n",
       "\n",
       "idxs, dists = knn(kdtree, points, k, true);\n",
       "\n",
       "idxs\n",
       "# 4-element Array{Array{Int64,1},1}:\n",
       "#  [3330, 4072, 2696]\n",
       "#  [1825, 7799, 8358]\n",
       "#  [3497, 2169, 3737]\n",
       "#  [1845, 9796, 2908]\n",
       "\n",
       "# dists\n",
       "# 4-element Array{Array{Float64,1},1}:\n",
       "#  [0.0298932, 0.0327349, 0.0365979]\n",
       "#  [0.0348751, 0.0498355, 0.0506802]\n",
       "#  [0.0318547, 0.037291, 0.0421208]\n",
       "#  [0.03321, 0.0360935, 0.0411951]\n",
       "\n",
       "# Static vectors\n",
       "v = @SVector[0.5, 0.3, 0.2];\n",
       "\n",
       "idxs, dists = knn(kdtree, v, k, true);\n",
       "\n",
       "idxs\n",
       "# 3-element Array{Int64,1}:\n",
       "#   842\n",
       "#  3075\n",
       "#  3046\n",
       "\n",
       "dists\n",
       "# 3-element Array{Float64,1}:\n",
       "#  0.04178677766255837\n",
       "#  0.04556078331418939\n",
       "#  0.049967238112417205\n",
       "\\end{verbatim}\n",
       "\\subsection{Range searches}\n",
       "A range search finds all neighbors within the range \\texttt{r} of given point(s). This is done with the method:\n",
       "\n",
       "\\begin{verbatim}\n",
       "inrange(tree, points, r, sortres = false) -> idxs\n",
       "\\end{verbatim}\n",
       "Note that for performance reasons the distances are not returned. The arguments to \\texttt{inrange} are the same as for \\texttt{knn} except that \\texttt{sortres} just sorts the returned index vector.\n",
       "\n",
       "An example:\n",
       "\n",
       "\\begin{verbatim}\n",
       "using NearestNeighbors\n",
       "data = rand(3, 10^4)\n",
       "r = 0.05\n",
       "point = rand(3)\n",
       "\n",
       "balltree = BallTree(data)\n",
       "idxs = inrange(balltree, point, r, true)\n",
       "\n",
       "# 4-element Array{Int64,1}:\n",
       "#  317\n",
       "#  983\n",
       "# 4577\n",
       "# 8675\n",
       "\n",
       "neighborscount = inrangecount(balltree, point, r, true) # if you were just interested in the number of points, this function will count them without allocating arrays for the indexes\n",
       "\\end{verbatim}\n",
       "\\subsection{Using on-disk data sets}\n",
       "By default, the trees store a copy of the \\texttt{data} provided during construction. This is problematic in case you want to work on data sets which are larger than available memory, thus wanting to \\texttt{mmap} the data or want to store the data in a different place, outside the tree.\n",
       "\n",
       "\\texttt{DataFreeTree} can be used to strip a constructed tree of its data field and re-link it with that data at a later stage. An example of using a large on-disk data set looks like this:\n",
       "\n",
       "\\begin{verbatim}\n",
       "using Mmap\n",
       "ndim = 2; ndata = 10_000_000_000\n",
       "data = Mmap.mmap(datafilename, Matrix{Float32}, (ndim, ndata))\n",
       "data[:] = rand(Float32, ndim, ndata)  # create example data\n",
       "dftree = DataFreeTree(KDTree, data)\n",
       "\\end{verbatim}\n",
       "\\texttt{dftree} now only stores the indexing data structures. It can be passed around, saved and reloaded independently of \\texttt{data}.\n",
       "\n",
       "To perform look-ups, \\texttt{dftree} is re-linked to the underlying data:\n",
       "\n",
       "\\begin{verbatim}\n",
       "tree = injectdata(dftree, data)  # yields a KDTree\n",
       "knn(tree, data[:,1], 3)  # perform operations as usual\n",
       "\\end{verbatim}\n",
       "\\subsection{Author}\n",
       "Kristoffer Carlsson -  @KristofferC - kristoffer.carlsson@chalmers.se\n",
       "\n"
      ],
      "text/markdown": [
       "No docstring found for module `NearestNeighbors`.\n",
       "\n",
       "# Exported names\n",
       "\n",
       "`BallTree`, `BruteTree`, `Chebyshev`, `Cityblock`, `DataFreeTree`, `Euclidean`, `Hamming`, `KDTree`, `Minkowski`, `NNTree`, `WeightedCityblock`, `WeightedEuclidean`, `WeightedMinkowski`, `injectdata`, `inrange`, `inrangecount`, `knn`, `nn`\n",
       "\n",
       "# Displaying contents of readme found at `D:\\TongYuan\\.julia\\packages\\NearestNeighbors\\YCcEC\\README.md`\n",
       "\n",
       "# NearestNeighbors.jl\n",
       "\n",
       "[![Build Status](https://github.com/KristofferC/NearestNeighbors.jl/workflows/CI/badge.svg)](https://github.com/KristofferC/NearestNeighbors.jl/actions?query=workflows/CI)  [![codecov.io](https://codecov.io/github/KristofferC/NearestNeighbors.jl/coverage.svg?branch=master)](https://codecov.io/github/KristofferC/NearestNeighbors.jl?branch=master) [![DOI](https://zenodo.org/badge/45321556.svg)](https://zenodo.org/badge/latestdoi/45321556)\n",
       "\n",
       "`NearestNeighbors.jl` is a package written in Julia to perform high performance nearest neighbor searches in  arbitrarily high dimensions.\n",
       "\n",
       "---\n",
       "\n",
       "## Creating a tree\n",
       "\n",
       "There are currently three types of trees available:\n",
       "\n",
       "  * `BruteTree`: Not actually a tree. It linearly searches all points in a brute force fashion. Works with any `Metric`.\n",
       "  * `KDTree`: In a kd tree the points are recursively split into groups using hyper-planes.\n",
       "\n",
       "Therefore a `KDTree` only works with axis aligned metrics which are: `Euclidean`, `Chebyshev`, `Minkowski` and `Cityblock`.\n",
       "\n",
       "  * `BallTree`: Points are recursively split into groups bounded by hyper-spheres. Works with any `Metric`.\n",
       "\n",
       "These trees are created with the following syntax:\n",
       "\n",
       "```jl\n",
       "BruteTree(data, metric; leafsize, reorder)\n",
       "KDTree(data, metric; leafsize, reorder)\n",
       "BallTree(data, metric; leafsize, reorder)\n",
       "```\n",
       "\n",
       "  * `data`: The data, i.e., the points to build up the tree from. It can either be \n",
       "\n",
       "      * a matrix of size `nd × np` with the points to insert in the tree where `nd` is the dimensionality of the points and `np` is the number of points\n",
       "      * a vector of vectors with fixed dimensionality, `nd`, which must be part of the type. Specifically, `data` should be a `Vector{V}`, where `V` is itself a subtype of an `AbstractVector` and such that `eltype(V)` and `length(V)` are defined.   (For example, with 3D points, `V = SVector{3, Float64}` works because `eltype(V) = Float64` and `length(V) = 3` are defined in `V`.)\n",
       "  * `metric`: The metric to use, defaults to `Euclidean`. This is one of the `Metric` types defined in the `Distances.jl` packages. It is possible to define your own metrics by simply creating new types that are subtypes of `Metric`.\n",
       "  * `leafsize` (keyword argument): Determines at what number of points to stop splitting the tree further. There is a trade-off between traversing the tree and having to evaluate the metric function for increasing number of points.\n",
       "  * `reorder` (keyword argument): While building the tree this will put points close in distance close in memory since this helps with cache locality. In this case, a copy of the original data will be made so that the original data is left unmodified. This can have a significant impact on performance and is by default set to `true`.\n",
       "\n",
       "All trees in `NearestNeighbors.jl` are static which means that points can not be added or removed from an already created tree.\n",
       "\n",
       "Here are a few examples of creating trees:\n",
       "\n",
       "```jl\n",
       "using NearestNeighbors\n",
       "data = rand(3, 10^4)\n",
       "\n",
       "# Create trees\n",
       "kdtree = KDTree(data; leafsize = 10)\n",
       "balltree = BallTree(data, Minkowski(3.5); reorder = false)\n",
       "brutetree = BruteTree(data)\n",
       "```\n",
       "\n",
       "## k Nearest Neighbor (kNN) searches\n",
       "\n",
       "A kNN search finds the `k` nearest neighbors to given point(s). This is done with the method:\n",
       "\n",
       "```jl\n",
       "knn(tree, points, k, sortres = false, skip = always_false) -> idxs, dists\n",
       "```\n",
       "\n",
       "  * `tree`: The tree instance\n",
       "  * `points`: A vector or matrix of points to find the `k` nearest neighbors to. If `points` is a vector of numbers then this represents a single point, if `points` is a matrix then the `k` nearest neighbors to each point (column) will be computed. `points` can also be a vector of other vectors where each element in the outer vector is considered a point.\n",
       "  * `sortres` (optional): Determines if the results should be sorted before returning.\n",
       "\n",
       "In this case the results will be sorted in order of increasing distance to the point.\n",
       "\n",
       "  * `skip` (optional): A predicate to determine if a given point should be skipped, for\n",
       "\n",
       "example if iterating over points and a point has already been visited.\n",
       "\n",
       "It is generally better for performance to query once with a large number of points than to query multiple times with one point per query.\n",
       "\n",
       "As a convenience, if you only want the closest nearest neighbor, you can call `nn` instead for a cleaner result:\n",
       "\n",
       "```jl\n",
       "nn(tree, points, skip = always_false) -> idxs, dists\n",
       "```\n",
       "\n",
       "Some examples:\n",
       "\n",
       "```jl\n",
       "using NearestNeighbors\n",
       "data = rand(3, 10^4)\n",
       "k = 3\n",
       "point = rand(3)\n",
       "\n",
       "kdtree = KDTree(data)\n",
       "idxs, dists = knn(kdtree, point, k, true)\n",
       "\n",
       "idxs\n",
       "# 3-element Array{Int64,1}:\n",
       "#  4683\n",
       "#  6119\n",
       "#  3278\n",
       "\n",
       "dists\n",
       "# 3-element Array{Float64,1}:\n",
       "#  0.039032201026256215\n",
       "#  0.04134193711411951\n",
       "#  0.042974090446474184\n",
       "\n",
       "# Multiple points\n",
       "points = rand(3, 4);\n",
       "\n",
       "idxs, dists = knn(kdtree, points, k, true);\n",
       "\n",
       "idxs\n",
       "# 4-element Array{Array{Int64,1},1}:\n",
       "#  [3330, 4072, 2696]\n",
       "#  [1825, 7799, 8358]\n",
       "#  [3497, 2169, 3737]\n",
       "#  [1845, 9796, 2908]\n",
       "\n",
       "# dists\n",
       "# 4-element Array{Array{Float64,1},1}:\n",
       "#  [0.0298932, 0.0327349, 0.0365979]\n",
       "#  [0.0348751, 0.0498355, 0.0506802]\n",
       "#  [0.0318547, 0.037291, 0.0421208]\n",
       "#  [0.03321, 0.0360935, 0.0411951]\n",
       "\n",
       "# Static vectors\n",
       "v = @SVector[0.5, 0.3, 0.2];\n",
       "\n",
       "idxs, dists = knn(kdtree, v, k, true);\n",
       "\n",
       "idxs\n",
       "# 3-element Array{Int64,1}:\n",
       "#   842\n",
       "#  3075\n",
       "#  3046\n",
       "\n",
       "dists\n",
       "# 3-element Array{Float64,1}:\n",
       "#  0.04178677766255837\n",
       "#  0.04556078331418939\n",
       "#  0.049967238112417205\n",
       "```\n",
       "\n",
       "## Range searches\n",
       "\n",
       "A range search finds all neighbors within the range `r` of given point(s). This is done with the method:\n",
       "\n",
       "```jl\n",
       "inrange(tree, points, r, sortres = false) -> idxs\n",
       "```\n",
       "\n",
       "Note that for performance reasons the distances are not returned. The arguments to `inrange` are the same as for `knn` except that `sortres` just sorts the returned index vector.\n",
       "\n",
       "An example:\n",
       "\n",
       "```jl\n",
       "using NearestNeighbors\n",
       "data = rand(3, 10^4)\n",
       "r = 0.05\n",
       "point = rand(3)\n",
       "\n",
       "balltree = BallTree(data)\n",
       "idxs = inrange(balltree, point, r, true)\n",
       "\n",
       "# 4-element Array{Int64,1}:\n",
       "#  317\n",
       "#  983\n",
       "# 4577\n",
       "# 8675\n",
       "\n",
       "neighborscount = inrangecount(balltree, point, r, true) # if you were just interested in the number of points, this function will count them without allocating arrays for the indexes\n",
       "```\n",
       "\n",
       "## Using on-disk data sets\n",
       "\n",
       "By default, the trees store a copy of the `data` provided during construction. This is problematic in case you want to work on data sets which are larger than available memory, thus wanting to `mmap` the data or want to store the data in a different place, outside the tree.\n",
       "\n",
       "`DataFreeTree` can be used to strip a constructed tree of its data field and re-link it with that data at a later stage. An example of using a large on-disk data set looks like this:\n",
       "\n",
       "```jl\n",
       "using Mmap\n",
       "ndim = 2; ndata = 10_000_000_000\n",
       "data = Mmap.mmap(datafilename, Matrix{Float32}, (ndim, ndata))\n",
       "data[:] = rand(Float32, ndim, ndata)  # create example data\n",
       "dftree = DataFreeTree(KDTree, data)\n",
       "```\n",
       "\n",
       "`dftree` now only stores the indexing data structures. It can be passed around, saved and reloaded independently of `data`.\n",
       "\n",
       "To perform look-ups, `dftree` is re-linked to the underlying data:\n",
       "\n",
       "```jl\n",
       "tree = injectdata(dftree, data)  # yields a KDTree\n",
       "knn(tree, data[:,1], 3)  # perform operations as usual\n",
       "```\n",
       "\n",
       "## Author\n",
       "\n",
       "Kristoffer Carlsson -  @KristofferC - kristoffer.carlsson@chalmers.se\n"
      ],
      "text/plain": [
       "  No docstring found for module \u001b[36mNearestNeighbors\u001b[39m.\n",
       "\n",
       "\u001b[1m  Exported names\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "  \u001b[36mBallTree\u001b[39m, \u001b[36mBruteTree\u001b[39m, \u001b[36mChebyshev\u001b[39m, \u001b[36mCityblock\u001b[39m, \u001b[36mDataFreeTree\u001b[39m, \u001b[36mEuclidean\u001b[39m, \u001b[36mHamming\u001b[39m,\n",
       "  \u001b[36mKDTree\u001b[39m, \u001b[36mMinkowski\u001b[39m, \u001b[36mNNTree\u001b[39m, \u001b[36mWeightedCityblock\u001b[39m, \u001b[36mWeightedEuclidean\u001b[39m,\n",
       "  \u001b[36mWeightedMinkowski\u001b[39m, \u001b[36minjectdata\u001b[39m, \u001b[36minrange\u001b[39m, \u001b[36minrangecount\u001b[39m, \u001b[36mknn\u001b[39m, \u001b[36mnn\u001b[39m\n",
       "\n",
       "\u001b[1m  Displaying contents of readme found at\u001b[22m\n",
       "\u001b[1m \u001b[36mD:\\TongYuan\\.julia\\packages\\NearestNeighbors\\YCcEC\\README.md\u001b[39m\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "\u001b[1m  NearestNeighbors.jl\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "  (Image: Build Status)\n",
       "  (https://github.com/KristofferC/NearestNeighbors.jl/actions?query=workflows/CI)\n",
       "  (Image: codecov.io)\n",
       "  (https://codecov.io/github/KristofferC/NearestNeighbors.jl?branch=master)\n",
       "  (Image: DOI) (https://zenodo.org/badge/latestdoi/45321556)\n",
       "\n",
       "  \u001b[36mNearestNeighbors.jl\u001b[39m is a package written in Julia to perform high\n",
       "  performance nearest neighbor searches in arbitrarily high dimensions.\n",
       "\n",
       "  ────────────────────────────────────────────────────────────────────────────\n",
       "\n",
       "\u001b[1m  Creating a tree\u001b[22m\n",
       "\u001b[1m  =================\u001b[22m\n",
       "\n",
       "  There are currently three types of trees available:\n",
       "\n",
       "    •  \u001b[36mBruteTree\u001b[39m: Not actually a tree. It linearly searches all points in\n",
       "       a brute force fashion. Works with any \u001b[36mMetric\u001b[39m.\n",
       "\n",
       "    •  \u001b[36mKDTree\u001b[39m: In a kd tree the points are recursively split into groups\n",
       "       using hyper-planes.\n",
       "\n",
       "  Therefore a \u001b[36mKDTree\u001b[39m only works with axis aligned metrics which are:\n",
       "  \u001b[36mEuclidean\u001b[39m, \u001b[36mChebyshev\u001b[39m, \u001b[36mMinkowski\u001b[39m and \u001b[36mCityblock\u001b[39m.\n",
       "\n",
       "    •  \u001b[36mBallTree\u001b[39m: Points are recursively split into groups bounded by\n",
       "       hyper-spheres. Works with any \u001b[36mMetric\u001b[39m.\n",
       "\n",
       "  These trees are created with the following syntax:\n",
       "\n",
       "\u001b[36m  BruteTree(data, metric; leafsize, reorder)\u001b[39m\n",
       "\u001b[36m  KDTree(data, metric; leafsize, reorder)\u001b[39m\n",
       "\u001b[36m  BallTree(data, metric; leafsize, reorder)\u001b[39m\n",
       "\n",
       "    •  \u001b[36mdata\u001b[39m: The data, i.e., the points to build up the tree from. It can\n",
       "       either be\n",
       "       • a matrix of size \u001b[36mnd × np\u001b[39m with the points to insert in\n",
       "       the tree where \u001b[36mnd\u001b[39m is the dimensionality of the points\n",
       "       and \u001b[36mnp\u001b[39m is the number of points\n",
       "       • a vector of vectors with fixed dimensionality, \u001b[36mnd\u001b[39m, which\n",
       "       must be part of the type. Specifically, \u001b[36mdata\u001b[39m should be a\n",
       "       \u001b[36mVector{V}\u001b[39m, where \u001b[36mV\u001b[39m is itself a subtype of an\n",
       "       \u001b[36mAbstractVector\u001b[39m and such that \u001b[36meltype(V)\u001b[39m and \u001b[36mlength(V)\u001b[39m are\n",
       "       defined. (For example, with 3D points, \u001b[36mV = SVector{3,\n",
       "       Float64}\u001b[39m works because \u001b[36meltype(V) = Float64\u001b[39m and \u001b[36mlength(V)\n",
       "       = 3\u001b[39m are defined in \u001b[36mV\u001b[39m.)\n",
       "\n",
       "    •  \u001b[36mmetric\u001b[39m: The metric to use, defaults to \u001b[36mEuclidean\u001b[39m. This is one of\n",
       "       the \u001b[36mMetric\u001b[39m types defined in the \u001b[36mDistances.jl\u001b[39m packages. It is\n",
       "       possible to define your own metrics by simply creating new types\n",
       "       that are subtypes of \u001b[36mMetric\u001b[39m.\n",
       "\n",
       "    •  \u001b[36mleafsize\u001b[39m (keyword argument): Determines at what number of points\n",
       "       to stop splitting the tree further. There is a trade-off between\n",
       "       traversing the tree and having to evaluate the metric function for\n",
       "       increasing number of points.\n",
       "\n",
       "    •  \u001b[36mreorder\u001b[39m (keyword argument): While building the tree this will put\n",
       "       points close in distance close in memory since this helps with\n",
       "       cache locality. In this case, a copy of the original data will be\n",
       "       made so that the original data is left unmodified. This can have a\n",
       "       significant impact on performance and is by default set to \u001b[36mtrue\u001b[39m.\n",
       "\n",
       "  All trees in \u001b[36mNearestNeighbors.jl\u001b[39m are static which means that points can not\n",
       "  be added or removed from an already created tree.\n",
       "\n",
       "  Here are a few examples of creating trees:\n",
       "\n",
       "\u001b[36m  using NearestNeighbors\u001b[39m\n",
       "\u001b[36m  data = rand(3, 10^4)\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  # Create trees\u001b[39m\n",
       "\u001b[36m  kdtree = KDTree(data; leafsize = 10)\u001b[39m\n",
       "\u001b[36m  balltree = BallTree(data, Minkowski(3.5); reorder = false)\u001b[39m\n",
       "\u001b[36m  brutetree = BruteTree(data)\u001b[39m\n",
       "\n",
       "\u001b[1m  k Nearest Neighbor (kNN) searches\u001b[22m\n",
       "\u001b[1m  ===================================\u001b[22m\n",
       "\n",
       "  A kNN search finds the \u001b[36mk\u001b[39m nearest neighbors to given point(s). This is done\n",
       "  with the method:\n",
       "\n",
       "\u001b[36m  knn(tree, points, k, sortres = false, skip = always_false) -> idxs, dists\u001b[39m\n",
       "\n",
       "    •  \u001b[36mtree\u001b[39m: The tree instance\n",
       "\n",
       "    •  \u001b[36mpoints\u001b[39m: A vector or matrix of points to find the \u001b[36mk\u001b[39m nearest\n",
       "       neighbors to. If \u001b[36mpoints\u001b[39m is a vector of numbers then this\n",
       "       represents a single point, if \u001b[36mpoints\u001b[39m is a matrix then the \u001b[36mk\u001b[39m\n",
       "       nearest neighbors to each point (column) will be computed. \u001b[36mpoints\u001b[39m\n",
       "       can also be a vector of other vectors where each element in the\n",
       "       outer vector is considered a point.\n",
       "\n",
       "    •  \u001b[36msortres\u001b[39m (optional): Determines if the results should be sorted\n",
       "       before returning.\n",
       "\n",
       "  In this case the results will be sorted in order of increasing distance to\n",
       "  the point.\n",
       "\n",
       "    •  \u001b[36mskip\u001b[39m (optional): A predicate to determine if a given point should\n",
       "       be skipped, for\n",
       "\n",
       "  example if iterating over points and a point has already been visited.\n",
       "\n",
       "  It is generally better for performance to query once with a large number of\n",
       "  points than to query multiple times with one point per query.\n",
       "\n",
       "  As a convenience, if you only want the closest nearest neighbor, you can\n",
       "  call \u001b[36mnn\u001b[39m instead for a cleaner result:\n",
       "\n",
       "\u001b[36m  nn(tree, points, skip = always_false) -> idxs, dists\u001b[39m\n",
       "\n",
       "  Some examples:\n",
       "\n",
       "\u001b[36m  using NearestNeighbors\u001b[39m\n",
       "\u001b[36m  data = rand(3, 10^4)\u001b[39m\n",
       "\u001b[36m  k = 3\u001b[39m\n",
       "\u001b[36m  point = rand(3)\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  kdtree = KDTree(data)\u001b[39m\n",
       "\u001b[36m  idxs, dists = knn(kdtree, point, k, true)\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  idxs\u001b[39m\n",
       "\u001b[36m  # 3-element Array{Int64,1}:\u001b[39m\n",
       "\u001b[36m  #  4683\u001b[39m\n",
       "\u001b[36m  #  6119\u001b[39m\n",
       "\u001b[36m  #  3278\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  dists\u001b[39m\n",
       "\u001b[36m  # 3-element Array{Float64,1}:\u001b[39m\n",
       "\u001b[36m  #  0.039032201026256215\u001b[39m\n",
       "\u001b[36m  #  0.04134193711411951\u001b[39m\n",
       "\u001b[36m  #  0.042974090446474184\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  # Multiple points\u001b[39m\n",
       "\u001b[36m  points = rand(3, 4);\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  idxs, dists = knn(kdtree, points, k, true);\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  idxs\u001b[39m\n",
       "\u001b[36m  # 4-element Array{Array{Int64,1},1}:\u001b[39m\n",
       "\u001b[36m  #  [3330, 4072, 2696]\u001b[39m\n",
       "\u001b[36m  #  [1825, 7799, 8358]\u001b[39m\n",
       "\u001b[36m  #  [3497, 2169, 3737]\u001b[39m\n",
       "\u001b[36m  #  [1845, 9796, 2908]\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  # dists\u001b[39m\n",
       "\u001b[36m  # 4-element Array{Array{Float64,1},1}:\u001b[39m\n",
       "\u001b[36m  #  [0.0298932, 0.0327349, 0.0365979]\u001b[39m\n",
       "\u001b[36m  #  [0.0348751, 0.0498355, 0.0506802]\u001b[39m\n",
       "\u001b[36m  #  [0.0318547, 0.037291, 0.0421208]\u001b[39m\n",
       "\u001b[36m  #  [0.03321, 0.0360935, 0.0411951]\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  # Static vectors\u001b[39m\n",
       "\u001b[36m  v = @SVector[0.5, 0.3, 0.2];\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  idxs, dists = knn(kdtree, v, k, true);\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  idxs\u001b[39m\n",
       "\u001b[36m  # 3-element Array{Int64,1}:\u001b[39m\n",
       "\u001b[36m  #   842\u001b[39m\n",
       "\u001b[36m  #  3075\u001b[39m\n",
       "\u001b[36m  #  3046\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  dists\u001b[39m\n",
       "\u001b[36m  # 3-element Array{Float64,1}:\u001b[39m\n",
       "\u001b[36m  #  0.04178677766255837\u001b[39m\n",
       "\u001b[36m  #  0.04556078331418939\u001b[39m\n",
       "\u001b[36m  #  0.049967238112417205\u001b[39m\n",
       "\n",
       "\u001b[1m  Range searches\u001b[22m\n",
       "\u001b[1m  ================\u001b[22m\n",
       "\n",
       "  A range search finds all neighbors within the range \u001b[36mr\u001b[39m of given point(s).\n",
       "  This is done with the method:\n",
       "\n",
       "\u001b[36m  inrange(tree, points, r, sortres = false) -> idxs\u001b[39m\n",
       "\n",
       "  Note that for performance reasons the distances are not returned. The\n",
       "  arguments to \u001b[36minrange\u001b[39m are the same as for \u001b[36mknn\u001b[39m except that \u001b[36msortres\u001b[39m just sorts\n",
       "  the returned index vector.\n",
       "\n",
       "  An example:\n",
       "\n",
       "\u001b[36m  using NearestNeighbors\u001b[39m\n",
       "\u001b[36m  data = rand(3, 10^4)\u001b[39m\n",
       "\u001b[36m  r = 0.05\u001b[39m\n",
       "\u001b[36m  point = rand(3)\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  balltree = BallTree(data)\u001b[39m\n",
       "\u001b[36m  idxs = inrange(balltree, point, r, true)\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  # 4-element Array{Int64,1}:\u001b[39m\n",
       "\u001b[36m  #  317\u001b[39m\n",
       "\u001b[36m  #  983\u001b[39m\n",
       "\u001b[36m  # 4577\u001b[39m\n",
       "\u001b[36m  # 8675\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  neighborscount = inrangecount(balltree, point, r, true) # if you were just interested in the number of points, this function will count them without allocating arrays for the indexes\u001b[39m\n",
       "\n",
       "\u001b[1m  Using on-disk data sets\u001b[22m\n",
       "\u001b[1m  =========================\u001b[22m\n",
       "\n",
       "  By default, the trees store a copy of the \u001b[36mdata\u001b[39m provided during construction.\n",
       "  This is problematic in case you want to work on data sets which are larger\n",
       "  than available memory, thus wanting to \u001b[36mmmap\u001b[39m the data or want to store the\n",
       "  data in a different place, outside the tree.\n",
       "\n",
       "  \u001b[36mDataFreeTree\u001b[39m can be used to strip a constructed tree of its data field and\n",
       "  re-link it with that data at a later stage. An example of using a large\n",
       "  on-disk data set looks like this:\n",
       "\n",
       "\u001b[36m  using Mmap\u001b[39m\n",
       "\u001b[36m  ndim = 2; ndata = 10_000_000_000\u001b[39m\n",
       "\u001b[36m  data = Mmap.mmap(datafilename, Matrix{Float32}, (ndim, ndata))\u001b[39m\n",
       "\u001b[36m  data[:] = rand(Float32, ndim, ndata)  # create example data\u001b[39m\n",
       "\u001b[36m  dftree = DataFreeTree(KDTree, data)\u001b[39m\n",
       "\n",
       "  \u001b[36mdftree\u001b[39m now only stores the indexing data structures. It can be passed\n",
       "  around, saved and reloaded independently of \u001b[36mdata\u001b[39m.\n",
       "\n",
       "  To perform look-ups, \u001b[36mdftree\u001b[39m is re-linked to the underlying data:\n",
       "\n",
       "\u001b[36m  tree = injectdata(dftree, data)  # yields a KDTree\u001b[39m\n",
       "\u001b[36m  knn(tree, data[:,1], 3)  # perform operations as usual\u001b[39m\n",
       "\n",
       "\u001b[1m  Author\u001b[22m\n",
       "\u001b[1m  ========\u001b[22m\n",
       "\n",
       "  Kristoffer Carlsson - @KristofferC - kristoffer.carlsson@chalmers.se"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T14:54:16.807000+08:00",
     "start_time": "2022-06-08T06:53:24.737Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "knn(tree::NNTree, points, k [, sortres=false]) -> indices, distances\n",
       "nn(tree:NNTree, points) -> indices, distances\n",
       "\\end{verbatim}\n",
       "Performs a lookup of the \\texttt{k} nearest neigbours to the \\texttt{points} from the data in the \\texttt{tree}. If \\texttt{sortres = true} the result is sorted such that the results are in the order of increasing distance to the point. \\texttt{skip} is an optional predicate to determine if a point that would be returned should be skipped based on its  index.\n",
       "\n"
      ],
      "text/markdown": [
       "```\n",
       "knn(tree::NNTree, points, k [, sortres=false]) -> indices, distances\n",
       "nn(tree:NNTree, points) -> indices, distances\n",
       "```\n",
       "\n",
       "Performs a lookup of the `k` nearest neigbours to the `points` from the data in the `tree`. If `sortres = true` the result is sorted such that the results are in the order of increasing distance to the point. `skip` is an optional predicate to determine if a point that would be returned should be skipped based on its  index.\n"
      ],
      "text/plain": [
       "\u001b[36m  knn(tree::NNTree, points, k [, sortres=false]) -> indices, distances\u001b[39m\n",
       "\u001b[36m  nn(tree:NNTree, points) -> indices, distances\u001b[39m\n",
       "\n",
       "  Performs a lookup of the \u001b[36mk\u001b[39m nearest neigbours to the \u001b[36mpoints\u001b[39m from the data in\n",
       "  the \u001b[36mtree\u001b[39m. If \u001b[36msortres = true\u001b[39m the result is sorted such that the results are\n",
       "  in the order of increasing distance to the point. \u001b[36mskip\u001b[39m is an optional\n",
       "  predicate to determine if a point that would be returned should be skipped\n",
       "  based on its index."
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?NearestNeighbors.knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T14:54:16.807000+08:00",
     "start_time": "2022-06-08T06:53:24.738Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "KDTree(data [, metric = Euclidean(); leafsize = 10, reorder = true]) -> kdtree\n",
       "\\end{verbatim}\n",
       "Creates a \\texttt{KDTree} from the data using the given \\texttt{metric} and \\texttt{leafsize}. The \\texttt{metric} must be a \\texttt{MinkowskiMetric}.\n",
       "\n"
      ],
      "text/markdown": [
       "```\n",
       "KDTree(data [, metric = Euclidean(); leafsize = 10, reorder = true]) -> kdtree\n",
       "```\n",
       "\n",
       "Creates a `KDTree` from the data using the given `metric` and `leafsize`. The `metric` must be a `MinkowskiMetric`.\n"
      ],
      "text/plain": [
       "\u001b[36m  KDTree(data [, metric = Euclidean(); leafsize = 10, reorder = true]) -> kdtree\u001b[39m\n",
       "\n",
       "  Creates a \u001b[36mKDTree\u001b[39m from the data using the given \u001b[36mmetric\u001b[39m and \u001b[36mleafsize\u001b[39m. The\n",
       "  \u001b[36mmetric\u001b[39m must be a \u001b[36mMinkowskiMetric\u001b[39m."
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?NearestNeighbors.KDTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-08T14:54:16.807000+08:00",
     "start_time": "2022-06-08T06:53:24.739Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "inrange(tree::NNTree, points, radius [, sortres=false]) -> indices\n",
       "\\end{verbatim}\n",
       "Find all the points in the tree which is closer than \\texttt{radius} to \\texttt{points}. If \\texttt{sortres = true} the resulting indices are sorted.\n",
       "\n"
      ],
      "text/markdown": [
       "```\n",
       "inrange(tree::NNTree, points, radius [, sortres=false]) -> indices\n",
       "```\n",
       "\n",
       "Find all the points in the tree which is closer than `radius` to `points`. If `sortres = true` the resulting indices are sorted.\n"
      ],
      "text/plain": [
       "\u001b[36m  inrange(tree::NNTree, points, radius [, sortres=false]) -> indices\u001b[39m\n",
       "\n",
       "  Find all the points in the tree which is closer than \u001b[36mradius\u001b[39m to \u001b[36mpoints\u001b[39m. If\n",
       "  \u001b[36msortres = true\u001b[39m the resulting indices are sorted."
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?NearestNeighbors.inrange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-14T14:30:15.114000+08:00",
     "start_time": "2022-07-14T06:30:15.112Z"
    }
   },
   "outputs": [],
   "source": [
    "using Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-14T14:30:17.175000+08:00",
     "start_time": "2022-07-14T06:30:17.062Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1mD\u001b[22m\u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1mr\u001b[22m\u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1mb\u001b[22m\u001b[0m\u001b[1mu\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1ms\u001b[22m \u001b[0m\u001b[1mD\u001b[22m\u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1mr\u001b[22m\u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1mb\u001b[22m\u001b[0m\u001b[1mu\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mn\u001b[22m Matrix\u001b[0m\u001b[1mD\u001b[22m\u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1mr\u001b[22m\u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1mb\u001b[22m\u001b[0m\u001b[1mu\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mn\u001b[22m NonMatrix\u001b[0m\u001b[1mD\u001b[22m\u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1mr\u001b[22m\u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1mb\u001b[22m\u001b[0m\u001b[1mu\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "A Julia package for probability distributions and associated functions.\n",
       "\n",
       "API overview (major features):\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{d = Dist(parameters...)} creates a distribution instance \\texttt{d} for some distribution \\texttt{Dist} (see choices below) with the specified \\texttt{parameters}\n",
       "\n",
       "\n",
       "\\item \\texttt{rand(d, sz)} samples from the distribution\n",
       "\n",
       "\n",
       "\\item \\texttt{pdf(d, x)} and \\texttt{logpdf(d, x)} compute the probability density or log-probability density of \\texttt{d} at \\texttt{x}\n",
       "\n",
       "\n",
       "\\item \\texttt{cdf(d, x)} and \\texttt{ccdf(d, x)} compute the (complementary) cumulative distribution function at \\texttt{x}\n",
       "\n",
       "\n",
       "\\item \\texttt{quantile(d, p)} is the inverse \\texttt{cdf} (see also \\texttt{cquantile})\n",
       "\n",
       "\n",
       "\\item \\texttt{mean(d)}, \\texttt{var(d)}, \\texttt{std(d)}, \\texttt{skewness(d)}, \\texttt{kurtosis(d)} compute moments of \\texttt{d}\n",
       "\n",
       "\n",
       "\\item \\texttt{fit(Dist, xs)} generates a distribution of type \\texttt{Dist} that best fits the samples in \\texttt{xs}\n",
       "\n",
       "\\end{itemize}\n",
       "These represent just a few of the operations supported by this package; users are encouraged to refer to the full documentation at https://JuliaStats.github.io/Distributions.jl/stable/ for further information.\n",
       "\n",
       "Supported distributions:\n",
       "\n",
       "\\begin{verbatim}\n",
       "Arcsine, Bernoulli, Beta, BetaBinomial, BetaPrime, Binomial, Biweight,\n",
       "Categorical, Cauchy, Censored, Chi, Chisq, Cosine, DiagNormal, DiagNormalCanon,\n",
       "Dirichlet, DiscreteUniform, DoubleExponential, EdgeworthMean,\n",
       "EdgeworthSum, EdgeworthZ, Erlang,\n",
       "Epanechnikov, Exponential, FDist, FisherNoncentralHypergeometric,\n",
       "Frechet, FullNormal, FullNormalCanon, Gamma, GeneralizedPareto,\n",
       "GeneralizedExtremeValue, Geometric, Gumbel, Hypergeometric,\n",
       "InverseWishart, InverseGamma, InverseGaussian, IsoNormal,\n",
       "IsoNormalCanon, Kolmogorov, KSDist, KSOneSided, Laplace, Levy, LKJ, LKJCholesky,\n",
       "Logistic, LogNormal, MatrixBeta, MatrixFDist, MatrixNormal,\n",
       "MatrixTDist, MixtureModel, Multinomial,\n",
       "MultivariateNormal, MvLogNormal, MvNormal, MvNormalCanon,\n",
       "MvNormalKnownCov, MvTDist, NegativeBinomial, NoncentralBeta, NoncentralChisq,\n",
       "NoncentralF, NoncentralHypergeometric, NoncentralT, Normal, NormalCanon,\n",
       "NormalInverseGaussian, Pareto, PGeneralizedGaussian, Poisson, PoissonBinomial,\n",
       "QQPair, Rayleigh, Rician, Skellam, Soliton, StudentizedRange, SymTriangularDist, TDist, TriangularDist,\n",
       "Triweight, Truncated, TruncatedNormal, Uniform, UnivariateGMM,\n",
       "VonMises, VonMisesFisher, WalleniusNoncentralHypergeometric, Weibull,\n",
       "Wishart, ZeroMeanIsoNormal, ZeroMeanIsoNormalCanon,\n",
       "ZeroMeanDiagNormal, ZeroMeanDiagNormalCanon, ZeroMeanFullNormal,\n",
       "ZeroMeanFullNormalCanon\n",
       "\\end{verbatim}\n"
      ],
      "text/markdown": [
       "A Julia package for probability distributions and associated functions.\n",
       "\n",
       "API overview (major features):\n",
       "\n",
       "  * `d = Dist(parameters...)` creates a distribution instance `d` for some distribution `Dist` (see choices below) with the specified `parameters`\n",
       "  * `rand(d, sz)` samples from the distribution\n",
       "  * `pdf(d, x)` and `logpdf(d, x)` compute the probability density or log-probability density of `d` at `x`\n",
       "  * `cdf(d, x)` and `ccdf(d, x)` compute the (complementary) cumulative distribution function at `x`\n",
       "  * `quantile(d, p)` is the inverse `cdf` (see also `cquantile`)\n",
       "  * `mean(d)`, `var(d)`, `std(d)`, `skewness(d)`, `kurtosis(d)` compute moments of `d`\n",
       "  * `fit(Dist, xs)` generates a distribution of type `Dist` that best fits the samples in `xs`\n",
       "\n",
       "These represent just a few of the operations supported by this package; users are encouraged to refer to the full documentation at https://JuliaStats.github.io/Distributions.jl/stable/ for further information.\n",
       "\n",
       "Supported distributions:\n",
       "\n",
       "```\n",
       "Arcsine, Bernoulli, Beta, BetaBinomial, BetaPrime, Binomial, Biweight,\n",
       "Categorical, Cauchy, Censored, Chi, Chisq, Cosine, DiagNormal, DiagNormalCanon,\n",
       "Dirichlet, DiscreteUniform, DoubleExponential, EdgeworthMean,\n",
       "EdgeworthSum, EdgeworthZ, Erlang,\n",
       "Epanechnikov, Exponential, FDist, FisherNoncentralHypergeometric,\n",
       "Frechet, FullNormal, FullNormalCanon, Gamma, GeneralizedPareto,\n",
       "GeneralizedExtremeValue, Geometric, Gumbel, Hypergeometric,\n",
       "InverseWishart, InverseGamma, InverseGaussian, IsoNormal,\n",
       "IsoNormalCanon, Kolmogorov, KSDist, KSOneSided, Laplace, Levy, LKJ, LKJCholesky,\n",
       "Logistic, LogNormal, MatrixBeta, MatrixFDist, MatrixNormal,\n",
       "MatrixTDist, MixtureModel, Multinomial,\n",
       "MultivariateNormal, MvLogNormal, MvNormal, MvNormalCanon,\n",
       "MvNormalKnownCov, MvTDist, NegativeBinomial, NoncentralBeta, NoncentralChisq,\n",
       "NoncentralF, NoncentralHypergeometric, NoncentralT, Normal, NormalCanon,\n",
       "NormalInverseGaussian, Pareto, PGeneralizedGaussian, Poisson, PoissonBinomial,\n",
       "QQPair, Rayleigh, Rician, Skellam, Soliton, StudentizedRange, SymTriangularDist, TDist, TriangularDist,\n",
       "Triweight, Truncated, TruncatedNormal, Uniform, UnivariateGMM,\n",
       "VonMises, VonMisesFisher, WalleniusNoncentralHypergeometric, Weibull,\n",
       "Wishart, ZeroMeanIsoNormal, ZeroMeanIsoNormalCanon,\n",
       "ZeroMeanDiagNormal, ZeroMeanDiagNormalCanon, ZeroMeanFullNormal,\n",
       "ZeroMeanFullNormalCanon\n",
       "```\n"
      ],
      "text/plain": [
       "  A Julia package for probability distributions and associated functions.\n",
       "\n",
       "  API overview (major features):\n",
       "\n",
       "    •  \u001b[36md = Dist(parameters...)\u001b[39m creates a distribution instance \u001b[36md\u001b[39m for some\n",
       "       distribution \u001b[36mDist\u001b[39m (see choices below) with the specified\n",
       "       \u001b[36mparameters\u001b[39m\n",
       "\n",
       "    •  \u001b[36mrand(d, sz)\u001b[39m samples from the distribution\n",
       "\n",
       "    •  \u001b[36mpdf(d, x)\u001b[39m and \u001b[36mlogpdf(d, x)\u001b[39m compute the probability density or\n",
       "       log-probability density of \u001b[36md\u001b[39m at \u001b[36mx\u001b[39m\n",
       "\n",
       "    •  \u001b[36mcdf(d, x)\u001b[39m and \u001b[36mccdf(d, x)\u001b[39m compute the (complementary) cumulative\n",
       "       distribution function at \u001b[36mx\u001b[39m\n",
       "\n",
       "    •  \u001b[36mquantile(d, p)\u001b[39m is the inverse \u001b[36mcdf\u001b[39m (see also \u001b[36mcquantile\u001b[39m)\n",
       "\n",
       "    •  \u001b[36mmean(d)\u001b[39m, \u001b[36mvar(d)\u001b[39m, \u001b[36mstd(d)\u001b[39m, \u001b[36mskewness(d)\u001b[39m, \u001b[36mkurtosis(d)\u001b[39m compute moments\n",
       "       of \u001b[36md\u001b[39m\n",
       "\n",
       "    •  \u001b[36mfit(Dist, xs)\u001b[39m generates a distribution of type \u001b[36mDist\u001b[39m that best fits\n",
       "       the samples in \u001b[36mxs\u001b[39m\n",
       "\n",
       "  These represent just a few of the operations supported by this package;\n",
       "  users are encouraged to refer to the full documentation at\n",
       "  https://JuliaStats.github.io/Distributions.jl/stable/ for further\n",
       "  information.\n",
       "\n",
       "  Supported distributions:\n",
       "\n",
       "\u001b[36m  Arcsine, Bernoulli, Beta, BetaBinomial, BetaPrime, Binomial, Biweight,\u001b[39m\n",
       "\u001b[36m  Categorical, Cauchy, Censored, Chi, Chisq, Cosine, DiagNormal, DiagNormalCanon,\u001b[39m\n",
       "\u001b[36m  Dirichlet, DiscreteUniform, DoubleExponential, EdgeworthMean,\u001b[39m\n",
       "\u001b[36m  EdgeworthSum, EdgeworthZ, Erlang,\u001b[39m\n",
       "\u001b[36m  Epanechnikov, Exponential, FDist, FisherNoncentralHypergeometric,\u001b[39m\n",
       "\u001b[36m  Frechet, FullNormal, FullNormalCanon, Gamma, GeneralizedPareto,\u001b[39m\n",
       "\u001b[36m  GeneralizedExtremeValue, Geometric, Gumbel, Hypergeometric,\u001b[39m\n",
       "\u001b[36m  InverseWishart, InverseGamma, InverseGaussian, IsoNormal,\u001b[39m\n",
       "\u001b[36m  IsoNormalCanon, Kolmogorov, KSDist, KSOneSided, Laplace, Levy, LKJ, LKJCholesky,\u001b[39m\n",
       "\u001b[36m  Logistic, LogNormal, MatrixBeta, MatrixFDist, MatrixNormal,\u001b[39m\n",
       "\u001b[36m  MatrixTDist, MixtureModel, Multinomial,\u001b[39m\n",
       "\u001b[36m  MultivariateNormal, MvLogNormal, MvNormal, MvNormalCanon,\u001b[39m\n",
       "\u001b[36m  MvNormalKnownCov, MvTDist, NegativeBinomial, NoncentralBeta, NoncentralChisq,\u001b[39m\n",
       "\u001b[36m  NoncentralF, NoncentralHypergeometric, NoncentralT, Normal, NormalCanon,\u001b[39m\n",
       "\u001b[36m  NormalInverseGaussian, Pareto, PGeneralizedGaussian, Poisson, PoissonBinomial,\u001b[39m\n",
       "\u001b[36m  QQPair, Rayleigh, Rician, Skellam, Soliton, StudentizedRange, SymTriangularDist, TDist, TriangularDist,\u001b[39m\n",
       "\u001b[36m  Triweight, Truncated, TruncatedNormal, Uniform, UnivariateGMM,\u001b[39m\n",
       "\u001b[36m  VonMises, VonMisesFisher, WalleniusNoncentralHypergeometric, Weibull,\u001b[39m\n",
       "\u001b[36m  Wishart, ZeroMeanIsoNormal, ZeroMeanIsoNormalCanon,\u001b[39m\n",
       "\u001b[36m  ZeroMeanDiagNormal, ZeroMeanDiagNormalCanon, ZeroMeanFullNormal,\u001b[39m\n",
       "\u001b[36m  ZeroMeanFullNormalCanon\u001b[39m"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MvNormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-14T17:47:20.750000+08:00",
     "start_time": "2022-07-14T09:47:20.747Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "MvNormal\n",
       "\\end{verbatim}\n",
       "Generally, users don't have to worry about these internal details.\n",
       "\n",
       "We provide a common constructor \\texttt{MvNormal}, which will construct a distribution of appropriate type depending on the input arguments.\n",
       "\n",
       "\\rule{\\textwidth}{1pt}\n",
       "\\begin{verbatim}\n",
       "MvNormal(μ::AbstractVector{<:Real}, Σ::AbstractMatrix{<:Real})\n",
       "\\end{verbatim}\n",
       "Construct a multivariate normal distribution with mean \\texttt{μ} and covariance matrix \\texttt{Σ}.\n",
       "\n",
       "\\rule{\\textwidth}{1pt}\n",
       "\\begin{verbatim}\n",
       "MvNormal(Σ::AbstractMatrix{<:Real})\n",
       "\\end{verbatim}\n",
       "Construct a multivariate normal distribution with zero mean and covariance matrix \\texttt{Σ}.\n",
       "\n"
      ],
      "text/markdown": [
       "```\n",
       "MvNormal\n",
       "```\n",
       "\n",
       "Generally, users don't have to worry about these internal details.\n",
       "\n",
       "We provide a common constructor `MvNormal`, which will construct a distribution of appropriate type depending on the input arguments.\n",
       "\n",
       "---\n",
       "\n",
       "```\n",
       "MvNormal(μ::AbstractVector{<:Real}, Σ::AbstractMatrix{<:Real})\n",
       "```\n",
       "\n",
       "Construct a multivariate normal distribution with mean `μ` and covariance matrix `Σ`.\n",
       "\n",
       "---\n",
       "\n",
       "```\n",
       "MvNormal(Σ::AbstractMatrix{<:Real})\n",
       "```\n",
       "\n",
       "Construct a multivariate normal distribution with zero mean and covariance matrix `Σ`.\n"
      ],
      "text/plain": [
       "\u001b[36m  MvNormal\u001b[39m\n",
       "\n",
       "  Generally, users don't have to worry about these internal details.\n",
       "\n",
       "  We provide a common constructor \u001b[36mMvNormal\u001b[39m, which will construct a\n",
       "  distribution of appropriate type depending on the input arguments.\n",
       "\n",
       "  ────────────────────────────────────────────────────────────────────────────\n",
       "\n",
       "\u001b[36m  MvNormal(μ::AbstractVector{<:Real}, Σ::AbstractMatrix{<:Real})\u001b[39m\n",
       "\n",
       "  Construct a multivariate normal distribution with mean \u001b[36mμ\u001b[39m and covariance\n",
       "  matrix \u001b[36mΣ\u001b[39m.\n",
       "\n",
       "  ────────────────────────────────────────────────────────────────────────────\n",
       "\n",
       "\u001b[36m  MvNormal(Σ::AbstractMatrix{<:Real})\u001b[39m\n",
       "\n",
       "  Construct a multivariate normal distribution with zero mean and covariance\n",
       "  matrix \u001b[36mΣ\u001b[39m."
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?Distributions.MvNormal #多元高斯分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-14T17:36:33.172000+08:00",
     "start_time": "2022-07-14T09:36:33.167Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gmdistribution (generic function with 1 method)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Distributions\n",
    "\n",
    "function gmdistribution(mu, sigma)\n",
    "    \n",
    "    size(mu, 1) #返回行数，代表高斯模型总数，2个代表2个高斯模型的组合\n",
    "    size(mu, 2) #返回列数，代表高斯模型变量数，1个即为单变量，多个为多变量\n",
    "\n",
    "    mui = Vector{Vector{Float64}}(undef, size(mu, 1)) #空均值向量\n",
    "    #将mu按行分组\n",
    "    for i=1:size(mu, 1)\n",
    "        mui[i] = mu[i, :]\n",
    "    end\n",
    "\n",
    "    size(sigma, 1) #返回行数，同mu\n",
    "    size(sigma, 2) #返回列数，同mu\n",
    "\n",
    "    sigmai = Vector{Vector{Float64}}(undef, size(sigma, 1)) #空方差向量\n",
    "    #将sigma按行分组\n",
    "    for j=1:size(sigma, 1)\n",
    "        sigmai[j] = sigma[j, :]\n",
    "    end\n",
    "\n",
    "    dn = Vector{DiagNormal}(undef, size(mu, 1)) #空高斯模型组合向量\n",
    "    #将高斯模型分组\n",
    "    for k=1:size(mu, 1)\n",
    "        dn[k] = MvNormal(mui[k], sigmai[k])\n",
    "    end\n",
    "    #返回高斯模型组合结果\n",
    "    return MixtureModel(dn)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-14T17:42:51.418000+08:00",
     "start_time": "2022-07-14T09:42:51.357Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MixtureModel{DiagNormal}(K = 3)\n",
       "components[1] (prior = 0.3333): DiagNormal(\n",
       "dim: 2\n",
       "μ: [1.0, 2.0]\n",
       "Σ: [4.0 0.0; 0.0 0.25]\n",
       ")\n",
       "\n",
       "components[2] (prior = 0.3333): DiagNormal(\n",
       "dim: 2\n",
       "μ: [-3.0, -5.0]\n",
       "Σ: [1.0 0.0; 0.0 1.0]\n",
       ")\n",
       "\n",
       "components[3] (prior = 0.3333): DiagNormal(\n",
       "dim: 2\n",
       "μ: [3.0, 2.0]\n",
       "Σ: [0.6400000000000001 0.0; 0.0 0.81]\n",
       ")\n",
       "\n"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu = [1 2;-3 -5;3 2]\n",
    "sigma = [2 0.5;1 1;0.8 0.9]\n",
    "gmdistribution(mu, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GaussianMixtures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-14T10:27:23.215000+08:00",
     "start_time": "2022-07-14T02:27:21.415Z"
    }
   },
   "outputs": [],
   "source": [
    "using GaussianMixtures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-14T10:27:26.652000+08:00",
     "start_time": "2022-07-14T02:27:25.445Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1mG\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1mu\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1mM\u001b[22m\u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1mx\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1mu\u001b[22m\u001b[0m\u001b[1mr\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "No docstring found for module \\texttt{GaussianMixtures}.\n",
       "\n",
       "\\section{Exported names}\n",
       "\\texttt{CSstats}, \\texttt{Cstats}, \\texttt{Data}, \\texttt{DataOrMatrix}, \\texttt{GMM}, \\texttt{GMMprior}, \\texttt{History}, \\texttt{VGMM}, \\texttt{avll}, \\texttt{covars}, \\texttt{dmap}, \\texttt{em!}, \\texttt{gmmposterior}, \\texttt{gmmsplit}, \\texttt{history}, \\texttt{kind}, \\texttt{llpg}, \\texttt{maxapost}, \\texttt{means}, \\texttt{nparams}, \\texttt{rand}, \\texttt{sanitycheck!}, \\texttt{setmem}, \\texttt{show}, \\texttt{stats}, \\texttt{vec}, \\texttt{weights}\n",
       "\n",
       "\\section{Displaying contents of readme found at \\texttt{D:{\\textbackslash}TongYuan{\\textbackslash}.julia{\\textbackslash}packages{\\textbackslash}GaussianMixtures{\\textbackslash}hU9VM{\\textbackslash}README.md}}\n",
       "\\section{GaussianMixtures}\n",
       "\\subsection{A Julia package for Gaussian Mixture Models (GMMs).}\n",
       "\\href{https://github.com/davidavdav/GaussianMixtures.jl/actions/workflows/test.yml}{\\begin{figure}\n",
       "\\centering\n",
       "\\includegraphics{https://github.com/davidavdav/GaussianMixtures.jl/actions/workflows/test.yml/badge.svg}\n",
       "\\caption{Unit test}\n",
       "\\end{figure}\n",
       "}\n",
       "\n",
       "This package contains support for Gaussian Mixture Models.  Basic training, likelihood calculation, model adaptation, and i/o are implemented.\n",
       "\n",
       "This Julia type is more specific than Dahua Lin's \\href{http://distributionsjl.readthedocs.org/en/latest/mixture.html}{MixtureModels}, in that it deals only with normal (multivariate) distributions (a.k.a Gaussians), but it does so more efficiently, hopefully.  We have support for switching between GMM and MixtureModels types. \n",
       "\n",
       "At this moment, we have implemented both diagonal covariance and full covariance GMMs, and full covariance variational Bayes GMMs.  \n",
       "\n",
       "In training the parameters of a GMM using the Expectation Maximization (EM) algorithm, the inner loop (computing the Baum-Welch statistics) can be executed efficiently using Julia's standard parallelization infrastructure, e.g., by using SGE.  We further support very large data (larger than will fit in the combined memory of the computing cluster) though \\href{https://github.com/davidavdav/BigData.jl}{BigData}, which has now been incorporated in this package. \n",
       "\n",
       "\\subsection{Install}\n",
       "\\begin{verbatim}\n",
       "Pkg.add(\"GaussianMixtures\")\n",
       "\\end{verbatim}\n",
       "\\subsection{Vector dimensions}\n",
       "Some remarks on the dimension.  There are three main indexing variables:\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item The Gaussian index \n",
       "\n",
       "\n",
       "\\item The data point\n",
       "\n",
       "\n",
       "\\item The feature dimension (for full covariance this adds to two dimensions)\n",
       "\n",
       "\\end{itemize}\n",
       "Often data is stored in 2D slices, and computations can be done efficiently as  matrix multiplications.  For this it is nice to have the data in standard row,column order.  However, we can't have these consistently over all three indexes. \n",
       "\n",
       "My approach is to have:\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item The data index (\\texttt{i}) always be a the first (row) index\n",
       "\n",
       "\n",
       "\\item The feature dimension index (\\texttt{k}) always to be a the second (column) index\n",
       "\n",
       "\n",
       "\\item The Gaussian index (\\texttt{j}) to be mixed, depending on how it is combined with either dimension above. \n",
       "\n",
       "\\end{itemize}\n",
       "The consequence is that \"data points run down\" in a matrix, just like records do in a DataFrame.  Hence, statistics per feature dimension occur consecutive in memory which may be advantageous for caching efficiency.  On the other hand, features belonging to the same data point are separated in memory, which probably is not according to the way they are generated, and does not extend to streamlined implementation.  The choice in which direction the data must run is an almost philosophical problem that I haven't come to a final conclusion about.  \n",
       "\n",
       "Please note that the choice for \"data points run down\" is the opposite of the convention used in \\href{https://github.com/JuliaStats/Distributions.jl}{Distributions.jl}, so if you convert \\texttt{GMM}s to \\texttt{MixtureModel}s in order to benefit from the methods provided for these distributions, you need to transpose the data.  \n",
       "\n",
       "\\subsection{Type}\n",
       "A simplified version of the type definition for the Gaussian Mixture Model is\n",
       "\n",
       "\\begin{verbatim}\n",
       "type GMM\n",
       "    n::Int                         # number of Gaussians\n",
       "    d::Int                         # dimension of Gaussian\n",
       "    w::Vector                      # weights: n\n",
       "    μ::Array                       # means: n x d\n",
       "    Σ::Union(Array, Vector{Array}) # diagonal covariances n x d, or Vector n of d x d full covariances\n",
       "    hist::Array{History}           # history of this GMM\n",
       "end\n",
       "\\end{verbatim}\n",
       "Currently, the variable \\texttt{Σ} is heterogeneous both in form and interpretation, depending on whether it represents full covariance or diagonal covariance matrices.  \n",
       "\n",
       "\\begin{itemize}\n",
       "\\item full covariance: \\texttt{Σ} is represented by a \\texttt{Vector} of \\texttt{chol(inv(Σ), :U)}\n",
       "\n",
       "\n",
       "\\item diagonal covariance: \\texttt{Σ} is formed by vertically stacking row-vectors of \\texttt{diag(Σ)}\n",
       "\n",
       "\\end{itemize}\n",
       "\\subsection{Constructors}\n",
       "\\begin{verbatim}\n",
       "GMM(weights::Vector, μ::Array,  Σ::Array, hist::Vector)\n",
       "\\end{verbatim}\n",
       "This is the basic outer constructor for GMM.  Here we have\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{weights} a Vector of length \\texttt{n} with the weights of the mixtures\n",
       "\n",
       "\n",
       "\\item \\texttt{μ} a matrix of \\texttt{n} by \\texttt{d} means of the Gaussians\n",
       "\n",
       "\n",
       "\\item \\texttt{Σ} either a matrix of \\texttt{n} by \\texttt{d} variances of diagonal Gaussians, or a vector of \\texttt{n} \\texttt{Triangular} matrices of \\texttt{d} by \\texttt{d}, representing the Cholesky decomposition of the full covariance\n",
       "\n",
       "\n",
       "\\item \\texttt{hist} a vector of \\texttt{History} objects describing how the GMM was obtained. (The type \\texttt{History} simply contains a time \\texttt{t} and a comment string \\texttt{s})\n",
       "\n",
       "\\end{itemize}\n",
       "\\begin{verbatim}\n",
       "GMM(x::Matrix; kind=:diag)\n",
       "GMM(x::Vector)\n",
       "\\end{verbatim}\n",
       "Create a GMM with 1 mixture, i.e., a multivariate Gaussian, and initialize with mean an variance of the data in \\texttt{x}.  The data in \\texttt{x} must be a \\texttt{nx} x \\texttt{d} Matrix, where \\texttt{nx} is the number of data points, or a Vector of length \\texttt{nx}. \n",
       "\n",
       "\\begin{verbatim}\n",
       "GMM(n:Int, x::Matrix; method=:kmeans, kind=:diag, nInit=50, nIter=10, nFinal=nIter)\n",
       "\\end{verbatim}\n",
       "Create a GMM with \\texttt{n} mixtures, given the training data \\texttt{x} and using the Expectation Maximization algorithm.  There are two ways of arriving at \\texttt{n} Gaussians: \\texttt{method=:kmeans} uses K-means clustering from the Clustering package to initialize with \\texttt{n} centers.  \\texttt{nInit} is the number of iterations for the K-means algorithm, \\texttt{nIter} the number of iterations in EM.  The method \\texttt{:split} works by initializing a single Gaussian with the data \\texttt{x} and subsequently splitting the Gaussians followed by retraining using the EM algorithm until \\texttt{n} Gaussians are obtained.  \\texttt{n} must be a power of 2 for \\texttt{method=:split}.  \\texttt{nIter} is the number of iterations in the EM algorithm, and \\texttt{nFinal} the number of iterations in the final step. \n",
       "\n",
       "\\begin{verbatim}\n",
       "GMM(n::Int, d::Int; kind=:diag)\n",
       "\\end{verbatim}\n",
       "Initialize a GMM with \\texttt{n} multivariate Gaussians of dimension \\texttt{d}. The means are all set to \\textbf{0} (the origin) and the variances to \\textbf{I}, which is silly by itself.  If \\texttt{kind=:full} is specified, the covariances are full rather than diagonal.  One should replace the values of the weights, means and covariances afterwards.\n",
       "\n",
       "\\subsection{Training functions}\n",
       "\\begin{verbatim}\n",
       "em!(gmm::GMM, x::Matrix; nIter::Int = 10, varfloor=1e-3)\n",
       "\\end{verbatim}\n",
       "Update the parameters of the GMM using the Expectation Maximization (EM) algorithm \\texttt{nIter} times, optimizing the log-likelihood given the data \\texttt{x}.   The function \\texttt{em!()} returns a vector of average log likelihoods for each of the intermediate iterations of the GMM given the training data.  \n",
       "\n",
       "\\begin{verbatim}\n",
       "llpg(gmm::GMM, x::Matrix)\n",
       "\\end{verbatim}\n",
       "Returns \\texttt{ll\\_ij = log p(x\\_i | gauss\\_j)}, the Log Likelihood Per Gaussian \\texttt{j} given data point \\texttt{x\\_i}.\n",
       "\n",
       "\\begin{verbatim}\n",
       "avll(gmm::GMM, x::Matrix)\n",
       "\\end{verbatim}\n",
       "Computes the average log likelihood of the GMM given all data points, further normalized by the feature dimension \\texttt{d = size(x,2)}. A 1-mixture GMM then has an \\texttt{avll} of \\texttt{-(log(2π) +0.5 +log(σ))} if the data \\texttt{x} is distributed as a multivariate diagonal covariance Gaussian with \\texttt{Σ = σI}.  With \\texttt{σ=1} we then have \\texttt{avll≈-1.42}. \n",
       "\n",
       "\\begin{verbatim}\n",
       "gmmposterior(gmm::GMM, x::Matrix)\n",
       "\\end{verbatim}\n",
       "Returns a tuple containing  \\texttt{p\\_ij = p(j | gmm, x\\_i)}, the posterior probability that data point \\texttt{x\\_i} 'belongs' to Gaussian \\texttt{j}, and the Log Likelihood Per Gaussian \\texttt{j} given data point \\texttt{x\\_i} as in \\texttt{llpg}. \n",
       "\n",
       "\\begin{verbatim}\n",
       "history(gmm::GMM)\n",
       "\\end{verbatim}\n",
       "Shows the history of the GMM, i.e., how it was initialized, split, how the parameters were trained, etc.  A history item contains a time of completion and an event string. \n",
       "\n",
       "You can examine a minimal example \\href{https://gist.github.com/mbeltagy/22e7fdf7e3be3fbfd97f1bde7a789b91}{using GMM for clustering}. \n",
       "\n",
       "\\subsection{Other functions}\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{split(gmm; minweight=1e-5, covfactor=0.2)}: Doubles the number of Gaussians by splitting each Gaussian into two Gaussians.  \\texttt{minweight} is used for pruning Gaussians with too little weight, these are replaced by an extra split of the Gaussian with the highest weight.  \\texttt{covfactor} controls how far apart the means of the split Gaussian are positioned. \n",
       "\n",
       "\n",
       "\\item \\texttt{kind(gmm)}: returns \\texttt{:diag} or \\texttt{:full}, depending on the type of covariance matrix\n",
       "\n",
       "\n",
       "\\item \\texttt{eltype(gmm)}: returns the datatype of \\texttt{w}, \\texttt{μ} and \\texttt{Σ} in the GMM\n",
       "\n",
       "\n",
       "\\item \\texttt{weights(gmm)}: returns the weights vector \\texttt{w}\n",
       "\n",
       "\n",
       "\\item \\texttt{means(gmm)}: returns the means \\texttt{μ} as an \\texttt{n} by \\texttt{d} matrix\n",
       "\n",
       "\n",
       "\\item \\texttt{covars(gmm)}: returns the covariances \\texttt{Σ}\n",
       "\n",
       "\n",
       "\\item \\texttt{copy(gmm)}: returns a deep copy of the GMM\n",
       "\n",
       "\n",
       "\\item \\texttt{full(gmm)}: returns a full covariance GMM based on \\texttt{gmm}\n",
       "\n",
       "\n",
       "\\item \\texttt{diag(gmm)}: returns a diagonal GMM, by ignoring off-diagonal elementsin \\texttt{gmm}\n",
       "\n",
       "\\end{itemize}\n",
       "\\subsection{Converting the GMMs}\n",
       "The element type in the GMM can be changed like you would expect.  We also have import and export functions for \\texttt{MixtureModel}, which currently only has \\texttt{Float64} element type. \n",
       "\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{convert(::Type\\{GMM\\{datatype\\}\\}, gmm)}: convert the data type of the GMM\n",
       "\n",
       "\n",
       "\\item \\texttt{float16(gmm)}, \\texttt{float32(gmm)}, \\texttt{float64(gmm)}: convenience functions for \\texttt{convert()}\n",
       "\n",
       "\n",
       "\\item \\texttt{MixtureModel(gmm)}: construct an instance of type \\texttt{MixtureModel} from the GMM.  Please note that for functions like \\texttt{pdf(m::MixtureModel, x::Matrix)} the data \\texttt{x} run \"sideways\" rather than \"down\" as in this package. \n",
       "\n",
       "\n",
       "\\item \\texttt{GMM(m::MixtureModel\\{Multivariate,Continuous,MvNormal\\})}: construct a GMM from the right kind of MixtureModel. \n",
       "\n",
       "\\end{itemize}\n",
       "\\subsection{Paralellization}\n",
       "Training a large GMM with huge quantities of data can take a significant amount of time.  We have built-in support for the parallelization infrastructure in Julia. \n",
       "\n",
       "The method \\texttt{stats()}, which is at the heart of EM algorithm, can detect multiple processors available (through \\texttt{nprocs()}).  If there is more than 1 processor available, the data is split into chunks, each chunk is mapped to a separate processor, and afterwards all the statistics from the sub-processes are aggregated.  In an SGE environment you can obtain more cores (in the example below 20) by issuing\n",
       "\n",
       "\\begin{verbatim}\n",
       "using ClusterManagers\n",
       "ClusterManagers.addprocs_sge(20)                                        \n",
       "@everywhere using GaussianMixtures\n",
       "\\end{verbatim}\n",
       "The size of the GMM and the data determine whether or not it is advantageous to do this. \n",
       "\n",
       "\\subsection{Memory}\n",
       "The \\texttt{stats()} method (see below) needs to be very efficient because for many algorithms it is at the inner loop of the calculation.  We have a highly optimized BLAS friendly and parallizable implementation, but this requires a fair bit of memory.  Therefore the input data is processed in blocks in such a way that only a limited amount of memory is used.  By default this is set at 2GB, but it can be specified though a gobal setting:\n",
       "\n",
       "\\begin{verbatim}\n",
       "setmem(gig) \n",
       "\\end{verbatim}\n",
       "Set the memory approximately used in \\texttt{stats()}, in Gigabytes. \n",
       "\n",
       "\\subsection{Baum-Welch statistics}\n",
       "At the heart of EM training, and to many other operations with GMMs, lies the computation of the Baum-Welch statistics of the data when aligning them to the GMM.  We have optimized implementations of the basic calculation of the statistics:\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{stats(gmm::GMM, x::Matrix; order=2, parallel=true)} Computes the Baum-Welch statistics up to order \\texttt{order} for the alignment of the data \\texttt{x} to the Universal Background GMM \\texttt{gmm}.  The 1st and 2nd order statistics are retuned as an \\texttt{n} x \\texttt{d} matrix, so for obtaining statistics in supervector format, flattening needs to be carried out in the right direction.  Theses statistics are \\emph{uncentered}. \n",
       "\n",
       "\\end{itemize}\n",
       "\\subsection{Random GMMs}\n",
       "Sometimes is it insteresting to generate random GMMs, and use these to genrate random points. \n",
       "\n",
       "\\begin{verbatim}\n",
       "g = rand(GMM, n, d; kind=:full, sep=2.0)\n",
       "\\end{verbatim}\n",
       "This generates a GMM with normally distributed means according to N(x|μ=0,Σ=sep*I).  The covariance matrices are also chosen random. \n",
       "\n",
       "\\begin{verbatim}\n",
       "rand(g::GMM, n)\n",
       "\\end{verbatim}\n",
       "Generate \\texttt{n} datapoints sampled from the GMM, resulting in a \\texttt{n} times \\texttt{g.d} array. \n",
       "\n",
       "\\subsection{Speaker recognition methods}\n",
       "The following methods are used in speaker- and language recognition, they may eventually move to another module. \n",
       "\n",
       "```julia\n",
       "\n",
       "[output truncated to first 200 lines]\n",
       "\n"
      ],
      "text/markdown": [
       "No docstring found for module `GaussianMixtures`.\n",
       "\n",
       "# Exported names\n",
       "\n",
       "`CSstats`, `Cstats`, `Data`, `DataOrMatrix`, `GMM`, `GMMprior`, `History`, `VGMM`, `avll`, `covars`, `dmap`, `em!`, `gmmposterior`, `gmmsplit`, `history`, `kind`, `llpg`, `maxapost`, `means`, `nparams`, `rand`, `sanitycheck!`, `setmem`, `show`, `stats`, `vec`, `weights`\n",
       "\n",
       "# Displaying contents of readme found at `D:\\TongYuan\\.julia\\packages\\GaussianMixtures\\hU9VM\\README.md`\n",
       "\n",
       "# GaussianMixtures\n",
       "\n",
       "## A Julia package for Gaussian Mixture Models (GMMs).\n",
       "\n",
       "[![Unit test](https://github.com/davidavdav/GaussianMixtures.jl/actions/workflows/test.yml/badge.svg)](https://github.com/davidavdav/GaussianMixtures.jl/actions/workflows/test.yml)\n",
       "\n",
       "This package contains support for Gaussian Mixture Models.  Basic training, likelihood calculation, model adaptation, and i/o are implemented.\n",
       "\n",
       "This Julia type is more specific than Dahua Lin's [MixtureModels](http://distributionsjl.readthedocs.org/en/latest/mixture.html), in that it deals only with normal (multivariate) distributions (a.k.a Gaussians), but it does so more efficiently, hopefully.  We have support for switching between GMM and MixtureModels types. \n",
       "\n",
       "At this moment, we have implemented both diagonal covariance and full covariance GMMs, and full covariance variational Bayes GMMs.  \n",
       "\n",
       "In training the parameters of a GMM using the Expectation Maximization (EM) algorithm, the inner loop (computing the Baum-Welch statistics) can be executed efficiently using Julia's standard parallelization infrastructure, e.g., by using SGE.  We further support very large data (larger than will fit in the combined memory of the computing cluster) though [BigData](https://github.com/davidavdav/BigData.jl), which has now been incorporated in this package. \n",
       "\n",
       "## Install\n",
       "\n",
       "```julia\n",
       "Pkg.add(\"GaussianMixtures\")\n",
       "```\n",
       "\n",
       "## Vector dimensions\n",
       "\n",
       "Some remarks on the dimension.  There are three main indexing variables:\n",
       "\n",
       "  * The Gaussian index\n",
       "  * The data point\n",
       "  * The feature dimension (for full covariance this adds to two dimensions)\n",
       "\n",
       "Often data is stored in 2D slices, and computations can be done efficiently as  matrix multiplications.  For this it is nice to have the data in standard row,column order.  However, we can't have these consistently over all three indexes. \n",
       "\n",
       "My approach is to have:\n",
       "\n",
       "  * The data index (`i`) always be a the first (row) index\n",
       "  * The feature dimension index (`k`) always to be a the second (column) index\n",
       "  * The Gaussian index (`j`) to be mixed, depending on how it is combined with either dimension above.\n",
       "\n",
       "The consequence is that \"data points run down\" in a matrix, just like records do in a DataFrame.  Hence, statistics per feature dimension occur consecutive in memory which may be advantageous for caching efficiency.  On the other hand, features belonging to the same data point are separated in memory, which probably is not according to the way they are generated, and does not extend to streamlined implementation.  The choice in which direction the data must run is an almost philosophical problem that I haven't come to a final conclusion about.  \n",
       "\n",
       "Please note that the choice for \"data points run down\" is the opposite of the convention used in [Distributions.jl](https://github.com/JuliaStats/Distributions.jl), so if you convert `GMM`s to `MixtureModel`s in order to benefit from the methods provided for these distributions, you need to transpose the data.  \n",
       "\n",
       "## Type\n",
       "\n",
       "A simplified version of the type definition for the Gaussian Mixture Model is\n",
       "\n",
       "```julia\n",
       "type GMM\n",
       "    n::Int                         # number of Gaussians\n",
       "    d::Int                         # dimension of Gaussian\n",
       "    w::Vector                      # weights: n\n",
       "    μ::Array                       # means: n x d\n",
       "    Σ::Union(Array, Vector{Array}) # diagonal covariances n x d, or Vector n of d x d full covariances\n",
       "    hist::Array{History}           # history of this GMM\n",
       "end\n",
       "```\n",
       "\n",
       "Currently, the variable `Σ` is heterogeneous both in form and interpretation, depending on whether it represents full covariance or diagonal covariance matrices.  \n",
       "\n",
       "  * full covariance: `Σ` is represented by a `Vector` of `chol(inv(Σ), :U)`\n",
       "  * diagonal covariance: `Σ` is formed by vertically stacking row-vectors of `diag(Σ)`\n",
       "\n",
       "## Constructors\n",
       "\n",
       "```julia\n",
       "GMM(weights::Vector, μ::Array,  Σ::Array, hist::Vector)\n",
       "```\n",
       "\n",
       "This is the basic outer constructor for GMM.  Here we have\n",
       "\n",
       "  * `weights` a Vector of length `n` with the weights of the mixtures\n",
       "  * `μ` a matrix of `n` by `d` means of the Gaussians\n",
       "  * `Σ` either a matrix of `n` by `d` variances of diagonal Gaussians, or a vector of `n` `Triangular` matrices of `d` by `d`, representing the Cholesky decomposition of the full covariance\n",
       "  * `hist` a vector of `History` objects describing how the GMM was obtained. (The type `History` simply contains a time `t` and a comment string `s`)\n",
       "\n",
       "```julia\n",
       "GMM(x::Matrix; kind=:diag)\n",
       "GMM(x::Vector)\n",
       "```\n",
       "\n",
       "Create a GMM with 1 mixture, i.e., a multivariate Gaussian, and initialize with mean an variance of the data in `x`.  The data in `x` must be a `nx` x `d` Matrix, where `nx` is the number of data points, or a Vector of length `nx`. \n",
       "\n",
       "```julia\n",
       "GMM(n:Int, x::Matrix; method=:kmeans, kind=:diag, nInit=50, nIter=10, nFinal=nIter)\n",
       "```\n",
       "\n",
       "Create a GMM with `n` mixtures, given the training data `x` and using the Expectation Maximization algorithm.  There are two ways of arriving at `n` Gaussians: `method=:kmeans` uses K-means clustering from the Clustering package to initialize with `n` centers.  `nInit` is the number of iterations for the K-means algorithm, `nIter` the number of iterations in EM.  The method `:split` works by initializing a single Gaussian with the data `x` and subsequently splitting the Gaussians followed by retraining using the EM algorithm until `n` Gaussians are obtained.  `n` must be a power of 2 for `method=:split`.  `nIter` is the number of iterations in the EM algorithm, and `nFinal` the number of iterations in the final step. \n",
       "\n",
       "```julia\n",
       "GMM(n::Int, d::Int; kind=:diag)\n",
       "```\n",
       "\n",
       "Initialize a GMM with `n` multivariate Gaussians of dimension `d`. The means are all set to **0** (the origin) and the variances to **I**, which is silly by itself.  If `kind=:full` is specified, the covariances are full rather than diagonal.  One should replace the values of the weights, means and covariances afterwards.\n",
       "\n",
       "## Training functions\n",
       "\n",
       "```julia\n",
       "em!(gmm::GMM, x::Matrix; nIter::Int = 10, varfloor=1e-3)\n",
       "```\n",
       "\n",
       "Update the parameters of the GMM using the Expectation Maximization (EM) algorithm `nIter` times, optimizing the log-likelihood given the data `x`.   The function `em!()` returns a vector of average log likelihoods for each of the intermediate iterations of the GMM given the training data.  \n",
       "\n",
       "```julia\n",
       "llpg(gmm::GMM, x::Matrix)\n",
       "```\n",
       "\n",
       "Returns `ll_ij = log p(x_i | gauss_j)`, the Log Likelihood Per Gaussian `j` given data point `x_i`.\n",
       "\n",
       "```julia\n",
       "avll(gmm::GMM, x::Matrix)\n",
       "```\n",
       "\n",
       "Computes the average log likelihood of the GMM given all data points, further normalized by the feature dimension `d = size(x,2)`. A 1-mixture GMM then has an `avll` of `-(log(2π) +0.5 +log(σ))` if the data `x` is distributed as a multivariate diagonal covariance Gaussian with `Σ = σI`.  With `σ=1` we then have `avll≈-1.42`. \n",
       "\n",
       "```julia\n",
       "gmmposterior(gmm::GMM, x::Matrix)\n",
       "```\n",
       "\n",
       "Returns a tuple containing  `p_ij = p(j | gmm, x_i)`, the posterior probability that data point `x_i` 'belongs' to Gaussian `j`, and the Log Likelihood Per Gaussian `j` given data point `x_i` as in `llpg`. \n",
       "\n",
       "```julia\n",
       "history(gmm::GMM)\n",
       "```\n",
       "\n",
       "Shows the history of the GMM, i.e., how it was initialized, split, how the parameters were trained, etc.  A history item contains a time of completion and an event string. \n",
       "\n",
       "You can examine a minimal example [using GMM for clustering](https://gist.github.com/mbeltagy/22e7fdf7e3be3fbfd97f1bde7a789b91). \n",
       "\n",
       "## Other functions\n",
       "\n",
       "  * `split(gmm; minweight=1e-5, covfactor=0.2)`: Doubles the number of Gaussians by splitting each Gaussian into two Gaussians.  `minweight` is used for pruning Gaussians with too little weight, these are replaced by an extra split of the Gaussian with the highest weight.  `covfactor` controls how far apart the means of the split Gaussian are positioned.\n",
       "  * `kind(gmm)`: returns `:diag` or `:full`, depending on the type of covariance matrix\n",
       "  * `eltype(gmm)`: returns the datatype of `w`, `μ` and `Σ` in the GMM\n",
       "  * `weights(gmm)`: returns the weights vector `w`\n",
       "  * `means(gmm)`: returns the means `μ` as an `n` by `d` matrix\n",
       "  * `covars(gmm)`: returns the covariances `Σ`\n",
       "  * `copy(gmm)`: returns a deep copy of the GMM\n",
       "  * `full(gmm)`: returns a full covariance GMM based on `gmm`\n",
       "  * `diag(gmm)`: returns a diagonal GMM, by ignoring off-diagonal elementsin `gmm`\n",
       "\n",
       "## Converting the GMMs\n",
       "\n",
       "The element type in the GMM can be changed like you would expect.  We also have import and export functions for `MixtureModel`, which currently only has `Float64` element type. \n",
       "\n",
       "  * `convert(::Type{GMM{datatype}}, gmm)`: convert the data type of the GMM\n",
       "  * `float16(gmm)`, `float32(gmm)`, `float64(gmm)`: convenience functions for `convert()`\n",
       "  * `MixtureModel(gmm)`: construct an instance of type `MixtureModel` from the GMM.  Please note that for functions like `pdf(m::MixtureModel, x::Matrix)` the data `x` run \"sideways\" rather than \"down\" as in this package.\n",
       "  * `GMM(m::MixtureModel{Multivariate,Continuous,MvNormal})`: construct a GMM from the right kind of MixtureModel.\n",
       "\n",
       "## Paralellization\n",
       "\n",
       "Training a large GMM with huge quantities of data can take a significant amount of time.  We have built-in support for the parallelization infrastructure in Julia. \n",
       "\n",
       "The method `stats()`, which is at the heart of EM algorithm, can detect multiple processors available (through `nprocs()`).  If there is more than 1 processor available, the data is split into chunks, each chunk is mapped to a separate processor, and afterwards all the statistics from the sub-processes are aggregated.  In an SGE environment you can obtain more cores (in the example below 20) by issuing\n",
       "\n",
       "```julia\n",
       "using ClusterManagers\n",
       "ClusterManagers.addprocs_sge(20)                                        \n",
       "@everywhere using GaussianMixtures\n",
       "```\n",
       "\n",
       "The size of the GMM and the data determine whether or not it is advantageous to do this. \n",
       "\n",
       "## Memory\n",
       "\n",
       "The `stats()` method (see below) needs to be very efficient because for many algorithms it is at the inner loop of the calculation.  We have a highly optimized BLAS friendly and parallizable implementation, but this requires a fair bit of memory.  Therefore the input data is processed in blocks in such a way that only a limited amount of memory is used.  By default this is set at 2GB, but it can be specified though a gobal setting:\n",
       "\n",
       "```julia\n",
       "setmem(gig) \n",
       "```\n",
       "\n",
       "Set the memory approximately used in `stats()`, in Gigabytes. \n",
       "\n",
       "## Baum-Welch statistics\n",
       "\n",
       "At the heart of EM training, and to many other operations with GMMs, lies the computation of the Baum-Welch statistics of the data when aligning them to the GMM.  We have optimized implementations of the basic calculation of the statistics:\n",
       "\n",
       "  * `stats(gmm::GMM, x::Matrix; order=2, parallel=true)` Computes the Baum-Welch statistics up to order `order` for the alignment of the data `x` to the Universal Background GMM `gmm`.  The 1st and 2nd order statistics are retuned as an `n` x `d` matrix, so for obtaining statistics in supervector format, flattening needs to be carried out in the right direction.  Theses statistics are *uncentered*.\n",
       "\n",
       "## Random GMMs\n",
       "\n",
       "Sometimes is it insteresting to generate random GMMs, and use these to genrate random points. \n",
       "\n",
       "```julia\n",
       "g = rand(GMM, n, d; kind=:full, sep=2.0)\n",
       "```\n",
       "\n",
       "This generates a GMM with normally distributed means according to N(x|μ=0,Σ=sep*I).  The covariance matrices are also chosen random. \n",
       "\n",
       "```julia\n",
       "rand(g::GMM, n)\n",
       "```\n",
       "\n",
       "Generate `n` datapoints sampled from the GMM, resulting in a `n` times `g.d` array. \n",
       "\n",
       "## Speaker recognition methods\n",
       "\n",
       "The following methods are used in speaker- and language recognition, they may eventually move to another module. \n",
       "\n",
       "```julia\n",
       "\n",
       "[output truncated to first 200 lines]\n"
      ],
      "text/plain": [
       "  No docstring found for module \u001b[36mGaussianMixtures\u001b[39m.\n",
       "\n",
       "\u001b[1m  Exported names\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "  \u001b[36mCSstats\u001b[39m, \u001b[36mCstats\u001b[39m, \u001b[36mData\u001b[39m, \u001b[36mDataOrMatrix\u001b[39m, \u001b[36mGMM\u001b[39m, \u001b[36mGMMprior\u001b[39m, \u001b[36mHistory\u001b[39m, \u001b[36mVGMM\u001b[39m, \u001b[36mavll\u001b[39m,\n",
       "  \u001b[36mcovars\u001b[39m, \u001b[36mdmap\u001b[39m, \u001b[36mem!\u001b[39m, \u001b[36mgmmposterior\u001b[39m, \u001b[36mgmmsplit\u001b[39m, \u001b[36mhistory\u001b[39m, \u001b[36mkind\u001b[39m, \u001b[36mllpg\u001b[39m, \u001b[36mmaxapost\u001b[39m,\n",
       "  \u001b[36mmeans\u001b[39m, \u001b[36mnparams\u001b[39m, \u001b[36mrand\u001b[39m, \u001b[36msanitycheck!\u001b[39m, \u001b[36msetmem\u001b[39m, \u001b[36mshow\u001b[39m, \u001b[36mstats\u001b[39m, \u001b[36mvec\u001b[39m, \u001b[36mweights\u001b[39m\n",
       "\n",
       "\u001b[1m  Displaying contents of readme found at\u001b[22m\n",
       "\u001b[1m \u001b[36mD:\\TongYuan\\.julia\\packages\\GaussianMixtures\\hU9VM\\README.md\u001b[39m\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "\u001b[1m  GaussianMixtures\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "\u001b[1m  A Julia package for Gaussian Mixture Models (GMMs).\u001b[22m\n",
       "\u001b[1m  =====================================================\u001b[22m\n",
       "\n",
       "  (Image: Unit test)\n",
       "  (https://github.com/davidavdav/GaussianMixtures.jl/actions/workflows/test.yml)\n",
       "\n",
       "  This package contains support for Gaussian Mixture Models. Basic training,\n",
       "  likelihood calculation, model adaptation, and i/o are implemented.\n",
       "\n",
       "  This Julia type is more specific than Dahua Lin's MixtureModels\n",
       "  (http://distributionsjl.readthedocs.org/en/latest/mixture.html), in that it\n",
       "  deals only with normal (multivariate) distributions (a.k.a Gaussians), but\n",
       "  it does so more efficiently, hopefully. We have support for switching\n",
       "  between GMM and MixtureModels types.\n",
       "\n",
       "  At this moment, we have implemented both diagonal covariance and full\n",
       "  covariance GMMs, and full covariance variational Bayes GMMs.\n",
       "\n",
       "  In training the parameters of a GMM using the Expectation Maximization (EM)\n",
       "  algorithm, the inner loop (computing the Baum-Welch statistics) can be\n",
       "  executed efficiently using Julia's standard parallelization infrastructure,\n",
       "  e.g., by using SGE. We further support very large data (larger than will fit\n",
       "  in the combined memory of the computing cluster) though BigData\n",
       "  (https://github.com/davidavdav/BigData.jl), which has now been incorporated\n",
       "  in this package.\n",
       "\n",
       "\u001b[1m  Install\u001b[22m\n",
       "\u001b[1m  =========\u001b[22m\n",
       "\n",
       "\u001b[36m  Pkg.add(\"GaussianMixtures\")\u001b[39m\n",
       "\n",
       "\u001b[1m  Vector dimensions\u001b[22m\n",
       "\u001b[1m  ===================\u001b[22m\n",
       "\n",
       "  Some remarks on the dimension. There are three main indexing variables:\n",
       "\n",
       "    •  The Gaussian index\n",
       "\n",
       "    •  The data point\n",
       "\n",
       "    •  The feature dimension (for full covariance this adds to two\n",
       "       dimensions)\n",
       "\n",
       "  Often data is stored in 2D slices, and computations can be done efficiently\n",
       "  as matrix multiplications. For this it is nice to have the data in standard\n",
       "  row,column order. However, we can't have these consistently over all three\n",
       "  indexes.\n",
       "\n",
       "  My approach is to have:\n",
       "\n",
       "    •  The data index (\u001b[36mi\u001b[39m) always be a the first (row) index\n",
       "\n",
       "    •  The feature dimension index (\u001b[36mk\u001b[39m) always to be a the second (column)\n",
       "       index\n",
       "\n",
       "    •  The Gaussian index (\u001b[36mj\u001b[39m) to be mixed, depending on how it is\n",
       "       combined with either dimension above.\n",
       "\n",
       "  The consequence is that \"data points run down\" in a matrix, just like\n",
       "  records do in a DataFrame. Hence, statistics per feature dimension occur\n",
       "  consecutive in memory which may be advantageous for caching efficiency. On\n",
       "  the other hand, features belonging to the same data point are separated in\n",
       "  memory, which probably is not according to the way they are generated, and\n",
       "  does not extend to streamlined implementation. The choice in which direction\n",
       "  the data must run is an almost philosophical problem that I haven't come to\n",
       "  a final conclusion about.\n",
       "\n",
       "  Please note that the choice for \"data points run down\" is the opposite of\n",
       "  the convention used in Distributions.jl\n",
       "  (https://github.com/JuliaStats/Distributions.jl), so if you convert \u001b[36mGMM\u001b[39ms to\n",
       "  \u001b[36mMixtureModel\u001b[39ms in order to benefit from the methods provided for these\n",
       "  distributions, you need to transpose the data.\n",
       "\n",
       "\u001b[1m  Type\u001b[22m\n",
       "\u001b[1m  ======\u001b[22m\n",
       "\n",
       "  A simplified version of the type definition for the Gaussian Mixture Model\n",
       "  is\n",
       "\n",
       "\u001b[36m  type GMM\u001b[39m\n",
       "\u001b[36m      n::Int                         # number of Gaussians\u001b[39m\n",
       "\u001b[36m      d::Int                         # dimension of Gaussian\u001b[39m\n",
       "\u001b[36m      w::Vector                      # weights: n\u001b[39m\n",
       "\u001b[36m      μ::Array                       # means: n x d\u001b[39m\n",
       "\u001b[36m      Σ::Union(Array, Vector{Array}) # diagonal covariances n x d, or Vector n of d x d full covariances\u001b[39m\n",
       "\u001b[36m      hist::Array{History}           # history of this GMM\u001b[39m\n",
       "\u001b[36m  end\u001b[39m\n",
       "\n",
       "  Currently, the variable \u001b[36mΣ\u001b[39m is heterogeneous both in form and interpretation,\n",
       "  depending on whether it represents full covariance or diagonal covariance\n",
       "  matrices.\n",
       "\n",
       "    •  full covariance: \u001b[36mΣ\u001b[39m is represented by a \u001b[36mVector\u001b[39m of \u001b[36mchol(inv(Σ), :U)\u001b[39m\n",
       "\n",
       "    •  diagonal covariance: \u001b[36mΣ\u001b[39m is formed by vertically stacking\n",
       "       row-vectors of \u001b[36mdiag(Σ)\u001b[39m\n",
       "\n",
       "\u001b[1m  Constructors\u001b[22m\n",
       "\u001b[1m  ==============\u001b[22m\n",
       "\n",
       "\u001b[36m  GMM(weights::Vector, μ::Array,  Σ::Array, hist::Vector)\u001b[39m\n",
       "\n",
       "  This is the basic outer constructor for GMM. Here we have\n",
       "\n",
       "    •  \u001b[36mweights\u001b[39m a Vector of length \u001b[36mn\u001b[39m with the weights of the mixtures\n",
       "\n",
       "    •  \u001b[36mμ\u001b[39m a matrix of \u001b[36mn\u001b[39m by \u001b[36md\u001b[39m means of the Gaussians\n",
       "\n",
       "    •  \u001b[36mΣ\u001b[39m either a matrix of \u001b[36mn\u001b[39m by \u001b[36md\u001b[39m variances of diagonal Gaussians, or a\n",
       "       vector of \u001b[36mn\u001b[39m \u001b[36mTriangular\u001b[39m matrices of \u001b[36md\u001b[39m by \u001b[36md\u001b[39m, representing the\n",
       "       Cholesky decomposition of the full covariance\n",
       "\n",
       "    •  \u001b[36mhist\u001b[39m a vector of \u001b[36mHistory\u001b[39m objects describing how the GMM was\n",
       "       obtained. (The type \u001b[36mHistory\u001b[39m simply contains a time \u001b[36mt\u001b[39m and a comment\n",
       "       string \u001b[36ms\u001b[39m)\n",
       "\n",
       "\u001b[36m  GMM(x::Matrix; kind=:diag)\u001b[39m\n",
       "\u001b[36m  GMM(x::Vector)\u001b[39m\n",
       "\n",
       "  Create a GMM with 1 mixture, i.e., a multivariate Gaussian, and initialize\n",
       "  with mean an variance of the data in \u001b[36mx\u001b[39m. The data in \u001b[36mx\u001b[39m must be a \u001b[36mnx\u001b[39m x \u001b[36md\u001b[39m\n",
       "  Matrix, where \u001b[36mnx\u001b[39m is the number of data points, or a Vector of length \u001b[36mnx\u001b[39m.\n",
       "\n",
       "\u001b[36m  GMM(n:Int, x::Matrix; method=:kmeans, kind=:diag, nInit=50, nIter=10, nFinal=nIter)\u001b[39m\n",
       "\n",
       "  Create a GMM with \u001b[36mn\u001b[39m mixtures, given the training data \u001b[36mx\u001b[39m and using the\n",
       "  Expectation Maximization algorithm. There are two ways of arriving at \u001b[36mn\u001b[39m\n",
       "  Gaussians: \u001b[36mmethod=:kmeans\u001b[39m uses K-means clustering from the Clustering\n",
       "  package to initialize with \u001b[36mn\u001b[39m centers. \u001b[36mnInit\u001b[39m is the number of iterations for\n",
       "  the K-means algorithm, \u001b[36mnIter\u001b[39m the number of iterations in EM. The method\n",
       "  \u001b[36m:split\u001b[39m works by initializing a single Gaussian with the data \u001b[36mx\u001b[39m and\n",
       "  subsequently splitting the Gaussians followed by retraining using the EM\n",
       "  algorithm until \u001b[36mn\u001b[39m Gaussians are obtained. \u001b[36mn\u001b[39m must be a power of 2 for\n",
       "  \u001b[36mmethod=:split\u001b[39m. \u001b[36mnIter\u001b[39m is the number of iterations in the EM algorithm, and\n",
       "  \u001b[36mnFinal\u001b[39m the number of iterations in the final step.\n",
       "\n",
       "\u001b[36m  GMM(n::Int, d::Int; kind=:diag)\u001b[39m\n",
       "\n",
       "  Initialize a GMM with \u001b[36mn\u001b[39m multivariate Gaussians of dimension \u001b[36md\u001b[39m. The means are\n",
       "  all set to \u001b[1m0\u001b[22m (the origin) and the variances to \u001b[1mI\u001b[22m, which is silly by itself.\n",
       "  If \u001b[36mkind=:full\u001b[39m is specified, the covariances are full rather than diagonal.\n",
       "  One should replace the values of the weights, means and covariances\n",
       "  afterwards.\n",
       "\n",
       "\u001b[1m  Training functions\u001b[22m\n",
       "\u001b[1m  ====================\u001b[22m\n",
       "\n",
       "\u001b[36m  em!(gmm::GMM, x::Matrix; nIter::Int = 10, varfloor=1e-3)\u001b[39m\n",
       "\n",
       "  Update the parameters of the GMM using the Expectation Maximization (EM)\n",
       "  algorithm \u001b[36mnIter\u001b[39m times, optimizing the log-likelihood given the data \u001b[36mx\u001b[39m. The\n",
       "  function \u001b[36mem!()\u001b[39m returns a vector of average log likelihoods for each of the\n",
       "  intermediate iterations of the GMM given the training data.\n",
       "\n",
       "\u001b[36m  llpg(gmm::GMM, x::Matrix)\u001b[39m\n",
       "\n",
       "  Returns \u001b[36mll_ij = log p(x_i | gauss_j)\u001b[39m, the Log Likelihood Per Gaussian \u001b[36mj\u001b[39m\n",
       "  given data point \u001b[36mx_i\u001b[39m.\n",
       "\n",
       "\u001b[36m  avll(gmm::GMM, x::Matrix)\u001b[39m\n",
       "\n",
       "  Computes the average log likelihood of the GMM given all data points,\n",
       "  further normalized by the feature dimension \u001b[36md = size(x,2)\u001b[39m. A 1-mixture GMM\n",
       "  then has an \u001b[36mavll\u001b[39m of \u001b[36m-(log(2π) +0.5 +log(σ))\u001b[39m if the data \u001b[36mx\u001b[39m is distributed as\n",
       "  a multivariate diagonal covariance Gaussian with \u001b[36mΣ = σI\u001b[39m. With \u001b[36mσ=1\u001b[39m we then\n",
       "  have \u001b[36mavll≈-1.42\u001b[39m.\n",
       "\n",
       "\u001b[36m  gmmposterior(gmm::GMM, x::Matrix)\u001b[39m\n",
       "\n",
       "  Returns a tuple containing \u001b[36mp_ij = p(j | gmm, x_i)\u001b[39m, the posterior probability\n",
       "  that data point \u001b[36mx_i\u001b[39m 'belongs' to Gaussian \u001b[36mj\u001b[39m, and the Log Likelihood Per\n",
       "  Gaussian \u001b[36mj\u001b[39m given data point \u001b[36mx_i\u001b[39m as in \u001b[36mllpg\u001b[39m.\n",
       "\n",
       "\u001b[36m  history(gmm::GMM)\u001b[39m\n",
       "\n",
       "  Shows the history of the GMM, i.e., how it was initialized, split, how the\n",
       "  parameters were trained, etc. A history item contains a time of completion\n",
       "  and an event string.\n",
       "\n",
       "  You can examine a minimal example using GMM for clustering\n",
       "  (https://gist.github.com/mbeltagy/22e7fdf7e3be3fbfd97f1bde7a789b91).\n",
       "\n",
       "\u001b[1m  Other functions\u001b[22m\n",
       "\u001b[1m  =================\u001b[22m\n",
       "\n",
       "    •  \u001b[36msplit(gmm; minweight=1e-5, covfactor=0.2)\u001b[39m: Doubles the number of\n",
       "       Gaussians by splitting each Gaussian into two Gaussians. \u001b[36mminweight\u001b[39m\n",
       "       is used for pruning Gaussians with too little weight, these are\n",
       "       replaced by an extra split of the Gaussian with the highest\n",
       "       weight. \u001b[36mcovfactor\u001b[39m controls how far apart the means of the split\n",
       "       Gaussian are positioned.\n",
       "\n",
       "    •  \u001b[36mkind(gmm)\u001b[39m: returns \u001b[36m:diag\u001b[39m or \u001b[36m:full\u001b[39m, depending on the type of\n",
       "       covariance matrix\n",
       "\n",
       "    •  \u001b[36meltype(gmm)\u001b[39m: returns the datatype of \u001b[36mw\u001b[39m, \u001b[36mμ\u001b[39m and \u001b[36mΣ\u001b[39m in the GMM\n",
       "\n",
       "    •  \u001b[36mweights(gmm)\u001b[39m: returns the weights vector \u001b[36mw\u001b[39m\n",
       "\n",
       "    •  \u001b[36mmeans(gmm)\u001b[39m: returns the means \u001b[36mμ\u001b[39m as an \u001b[36mn\u001b[39m by \u001b[36md\u001b[39m matrix\n",
       "\n",
       "    •  \u001b[36mcovars(gmm)\u001b[39m: returns the covariances \u001b[36mΣ\u001b[39m\n",
       "\n",
       "    •  \u001b[36mcopy(gmm)\u001b[39m: returns a deep copy of the GMM\n",
       "\n",
       "    •  \u001b[36mfull(gmm)\u001b[39m: returns a full covariance GMM based on \u001b[36mgmm\u001b[39m\n",
       "\n",
       "    •  \u001b[36mdiag(gmm)\u001b[39m: returns a diagonal GMM, by ignoring off-diagonal\n",
       "       elementsin \u001b[36mgmm\u001b[39m\n",
       "\n",
       "\u001b[1m  Converting the GMMs\u001b[22m\n",
       "\u001b[1m  =====================\u001b[22m\n",
       "\n",
       "  The element type in the GMM can be changed like you would expect. We also\n",
       "  have import and export functions for \u001b[36mMixtureModel\u001b[39m, which currently only has\n",
       "  \u001b[36mFloat64\u001b[39m element type.\n",
       "\n",
       "    •  \u001b[36mconvert(::Type{GMM{datatype}}, gmm)\u001b[39m: convert the data type of the\n",
       "       GMM\n",
       "\n",
       "    •  \u001b[36mfloat16(gmm)\u001b[39m, \u001b[36mfloat32(gmm)\u001b[39m, \u001b[36mfloat64(gmm)\u001b[39m: convenience functions\n",
       "       for \u001b[36mconvert()\u001b[39m\n",
       "\n",
       "    •  \u001b[36mMixtureModel(gmm)\u001b[39m: construct an instance of type \u001b[36mMixtureModel\u001b[39m from\n",
       "       the GMM. Please note that for functions like \u001b[36mpdf(m::MixtureModel,\n",
       "       x::Matrix)\u001b[39m the data \u001b[36mx\u001b[39m run \"sideways\" rather than \"down\" as in this\n",
       "       package.\n",
       "\n",
       "    •  \u001b[36mGMM(m::MixtureModel{Multivariate,Continuous,MvNormal})\u001b[39m: construct\n",
       "       a GMM from the right kind of MixtureModel.\n",
       "\n",
       "\u001b[1m  Paralellization\u001b[22m\n",
       "\u001b[1m  =================\u001b[22m\n",
       "\n",
       "  Training a large GMM with huge quantities of data can take a significant\n",
       "  amount of time. We have built-in support for the parallelization\n",
       "  infrastructure in Julia.\n",
       "\n",
       "  The method \u001b[36mstats()\u001b[39m, which is at the heart of EM algorithm, can detect\n",
       "  multiple processors available (through \u001b[36mnprocs()\u001b[39m). If there is more than 1\n",
       "  processor available, the data is split into chunks, each chunk is mapped to\n",
       "  a separate processor, and afterwards all the statistics from the\n",
       "  sub-processes are aggregated. In an SGE environment you can obtain more\n",
       "  cores (in the example below 20) by issuing\n",
       "\n",
       "\u001b[36m  using ClusterManagers\u001b[39m\n",
       "\u001b[36m  ClusterManagers.addprocs_sge(20)                                        \u001b[39m\n",
       "\u001b[36m  @everywhere using GaussianMixtures\u001b[39m\n",
       "\n",
       "  The size of the GMM and the data determine whether or not it is advantageous\n",
       "  to do this.\n",
       "\n",
       "\u001b[1m  Memory\u001b[22m\n",
       "\u001b[1m  ========\u001b[22m\n",
       "\n",
       "  The \u001b[36mstats()\u001b[39m method (see below) needs to be very efficient because for many\n",
       "  algorithms it is at the inner loop of the calculation. We have a highly\n",
       "  optimized BLAS friendly and parallizable implementation, but this requires a\n",
       "  fair bit of memory. Therefore the input data is processed in blocks in such\n",
       "  a way that only a limited amount of memory is used. By default this is set\n",
       "  at 2GB, but it can be specified though a gobal setting:\n",
       "\n",
       "\u001b[36m  setmem(gig) \u001b[39m\n",
       "\n",
       "  Set the memory approximately used in \u001b[36mstats()\u001b[39m, in Gigabytes.\n",
       "\n",
       "\u001b[1m  Baum-Welch statistics\u001b[22m\n",
       "\u001b[1m  =======================\u001b[22m\n",
       "\n",
       "  At the heart of EM training, and to many other operations with GMMs, lies\n",
       "  the computation of the Baum-Welch statistics of the data when aligning them\n",
       "  to the GMM. We have optimized implementations of the basic calculation of\n",
       "  the statistics:\n",
       "\n",
       "    •  \u001b[36mstats(gmm::GMM, x::Matrix; order=2, parallel=true)\u001b[39m Computes the\n",
       "       Baum-Welch statistics up to order \u001b[36morder\u001b[39m for the alignment of the\n",
       "       data \u001b[36mx\u001b[39m to the Universal Background GMM \u001b[36mgmm\u001b[39m. The 1st and 2nd order\n",
       "       statistics are retuned as an \u001b[36mn\u001b[39m x \u001b[36md\u001b[39m matrix, so for obtaining\n",
       "       statistics in supervector format, flattening needs to be carried\n",
       "       out in the right direction. Theses statistics are \u001b[4muncentered\u001b[24m.\n",
       "\n",
       "\u001b[1m  Random GMMs\u001b[22m\n",
       "\u001b[1m  =============\u001b[22m\n",
       "\n",
       "  Sometimes is it insteresting to generate random GMMs, and use these to\n",
       "  genrate random points.\n",
       "\n",
       "\u001b[36m  g = rand(GMM, n, d; kind=:full, sep=2.0)\u001b[39m\n",
       "\n",
       "  This generates a GMM with normally distributed means according to\n",
       "  N(x|μ=0,Σ=sep*I). The covariance matrices are also chosen random.\n",
       "\n",
       "\u001b[36m  rand(g::GMM, n)\u001b[39m\n",
       "\n",
       "  Generate \u001b[36mn\u001b[39m datapoints sampled from the GMM, resulting in a \u001b[36mn\u001b[39m times \u001b[36mg.d\u001b[39m\n",
       "  array.\n",
       "\n",
       "\u001b[1m  Speaker recognition methods\u001b[22m\n",
       "\u001b[1m  =============================\u001b[22m\n",
       "\n",
       "  The following methods are used in speaker- and language recognition, they\n",
       "  may eventually move to another module.\n",
       "\n",
       "  ```julia\n",
       "\n",
       "  [output truncated to first 200 lines]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?GaussianMixtures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-29T14:02:33.973000+08:00",
     "start_time": "2022-06-29T06:02:32.893Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "No documentation found.\n",
       "\n",
       "\\texttt{GaussianMixtures.gmmposterior} is a \\texttt{Function}.\n",
       "\n",
       "\\begin{verbatim}\n",
       "# 1 method for generic function \"gmmposterior\":\n",
       "[1] gmmposterior(gmm::GMM{GT, CT} where CT<:Union{AbstractMatrix{GT}, Array{LinearAlgebra.UpperTriangular{GT, Matrix{GT}}, 1}}, x::Matrix{T}) where {GT, T} in GaussianMixtures at D:\\TongYuan\\.julia\\packages\\GaussianMixtures\\hU9VM\\src\\train.jl:364\n",
       "\\end{verbatim}\n"
      ],
      "text/markdown": [
       "No documentation found.\n",
       "\n",
       "`GaussianMixtures.gmmposterior` is a `Function`.\n",
       "\n",
       "```\n",
       "# 1 method for generic function \"gmmposterior\":\n",
       "[1] gmmposterior(gmm::GMM{GT, CT} where CT<:Union{AbstractMatrix{GT}, Array{LinearAlgebra.UpperTriangular{GT, Matrix{GT}}, 1}}, x::Matrix{T}) where {GT, T} in GaussianMixtures at D:\\TongYuan\\.julia\\packages\\GaussianMixtures\\hU9VM\\src\\train.jl:364\n",
       "```\n"
      ],
      "text/plain": [
       "  No documentation found.\n",
       "\n",
       "  \u001b[36mGaussianMixtures.gmmposterior\u001b[39m is a \u001b[36mFunction\u001b[39m.\n",
       "\n",
       "\u001b[36m  # 1 method for generic function \"gmmposterior\":\u001b[39m\n",
       "\u001b[36m  [1] gmmposterior(gmm::GMM{GT, CT} where CT<:Union{AbstractMatrix{GT}, Array{LinearAlgebra.UpperTriangular{GT, Matrix{GT}}, 1}}, x::Matrix{T}) where {GT, T} in GaussianMixtures at D:\\TongYuan\\.julia\\packages\\GaussianMixtures\\hU9VM\\src\\train.jl:364\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?GaussianMixtures.gmmposterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-13T11:34:09.376000+08:00",
     "start_time": "2022-07-13T03:34:08.283Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\texttt{GMM} is the type that stores information of a Guassian Mixture Model.  Currently two main covariance types are supported: full covarariance and diagonal covariance.\n",
       "\n",
       "\\rule{\\textwidth}{1pt}\n",
       "\\texttt{GMM(n::Int, d::Int, kind::Symbol=:diag)} initializes a GMM with means 0 and Indentity covariances\n",
       "\n"
      ],
      "text/markdown": [
       "`GMM` is the type that stores information of a Guassian Mixture Model.  Currently two main covariance types are supported: full covarariance and diagonal covariance.\n",
       "\n",
       "---\n",
       "\n",
       "`GMM(n::Int, d::Int, kind::Symbol=:diag)` initializes a GMM with means 0 and Indentity covariances\n"
      ],
      "text/plain": [
       "  \u001b[36mGMM\u001b[39m is the type that stores information of a Guassian Mixture Model.\n",
       "  Currently two main covariance types are supported: full covarariance and\n",
       "  diagonal covariance.\n",
       "\n",
       "  ────────────────────────────────────────────────────────────────────────────\n",
       "\n",
       "  \u001b[36mGMM(n::Int, d::Int, kind::Symbol=:diag)\u001b[39m initializes a GMM with means 0 and\n",
       "  Indentity covariances"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?GaussianMixtures.GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-14T14:21:23.852000+08:00",
     "start_time": "2022-07-14T06:21:23.842Z"
    }
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "MethodError: no method matching GMM(::Matrix{Int64}, ::Matrix{Int64})",
     "output_type": "error",
     "traceback": [
      "MethodError: no method matching GMM(::Matrix{Int64}, ::Matrix{Int64})",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[7]:3",
      " [2] eval",
      "   @ .\\boot.jl:373 [inlined]",
      " [3] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base .\\loading.jl:1196"
     ]
    }
   ],
   "source": [
    "μ = Array([1 2 3; 4 5 6])\n",
    "Σ = Array([1 2 3; 4 5 6])\n",
    "GaussianMixtures.GMM(μ, Σ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-05T17:18:16.579000+08:00",
     "start_time": "2022-07-05T09:18:16.577Z"
    }
   },
   "outputs": [],
   "source": [
    "using Clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-29T17:00:30.697000+08:00",
     "start_time": "2022-06-29T09:00:30.693Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "No documentation found.\n",
       "\n",
       "\\texttt{Clustering.eval} is a \\texttt{Function}.\n",
       "\n",
       "\\begin{verbatim}\n",
       "# 1 method for generic function \"eval\":\n",
       "[1] eval(x) in Clustering at D:\\TongYuan\\.julia\\packages\\Clustering\\tt9vc\\src\\Clustering.jl:1\n",
       "\\end{verbatim}\n"
      ],
      "text/markdown": [
       "No documentation found.\n",
       "\n",
       "`Clustering.eval` is a `Function`.\n",
       "\n",
       "```\n",
       "# 1 method for generic function \"eval\":\n",
       "[1] eval(x) in Clustering at D:\\TongYuan\\.julia\\packages\\Clustering\\tt9vc\\src\\Clustering.jl:1\n",
       "```\n"
      ],
      "text/plain": [
       "  No documentation found.\n",
       "\n",
       "  \u001b[36mClustering.eval\u001b[39m is a \u001b[36mFunction\u001b[39m.\n",
       "\n",
       "\u001b[36m  # 1 method for generic function \"eval\":\u001b[39m\n",
       "\u001b[36m  [1] eval(x) in Clustering at D:\\TongYuan\\.julia\\packages\\Clustering\\tt9vc\\src\\Clustering.jl:1\u001b[39m"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?Clustering.eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-12T16:05:44.605000+08:00",
     "start_time": "2022-07-12T08:05:44.495Z"
    }
   },
   "outputs": [],
   "source": [
    "using Distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-11T17:34:48.418000+08:00",
     "start_time": "2022-07-11T09:34:45.836Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1mD\u001b[22m\u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1mc\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "No docstring found for module \\texttt{Distances}.\n",
       "\n",
       "\\section{Exported names}\n",
       "\\texttt{BhattacharyyaDist}, \\texttt{BrayCurtis}, \\texttt{Bregman}, \\texttt{Chebyshev}, \\texttt{ChiSqDist}, \\texttt{Cityblock}, \\texttt{CorrDist}, \\texttt{CosineDist}, \\texttt{Euclidean}, \\texttt{GenKLDivergence}, \\texttt{Hamming}, \\texttt{Haversine}, \\texttt{HellingerDist}, \\texttt{JSDivergence}, \\texttt{Jaccard}, \\texttt{KLDivergence}, \\texttt{Mahalanobis}, \\texttt{MeanAbsDeviation}, \\texttt{MeanSqDeviation}, \\texttt{Metric}, \\texttt{Minkowski}, \\texttt{NormRMSDeviation}, \\texttt{PeriodicEuclidean}, \\texttt{PreMetric}, \\texttt{RMSDeviation}, \\texttt{RenyiDivergence}, \\texttt{RogersTanimoto}, \\texttt{SemiMetric}, \\texttt{SpanNormDist}, \\texttt{SphericalAngle}, \\texttt{SqEuclidean}, \\texttt{SqMahalanobis}, \\texttt{TotalVariation}, \\texttt{WeightedCityblock}, \\texttt{WeightedEuclidean}, \\texttt{WeightedHamming}, \\texttt{WeightedMinkowski}, \\texttt{WeightedSqEuclidean}, \\texttt{bhattacharyya}, \\texttt{braycurtis}, \\texttt{bregman}, \\texttt{chebyshev}, \\texttt{chisq\\_dist}, \\texttt{cityblock}, \\texttt{colwise}, \\texttt{colwise!}, \\texttt{corr\\_dist}, \\texttt{cosine\\_dist}, \\texttt{euclidean}, \\texttt{evaluate}, \\texttt{gkl\\_divergence}, \\texttt{hamming}, \\texttt{haversine}, \\texttt{hellinger}, \\texttt{jaccard}, \\texttt{js\\_divergence}, \\texttt{kl\\_divergence}, \\texttt{mahalanobis}, \\texttt{meanad}, \\texttt{minkowski}, \\texttt{msd}, \\texttt{nrmsd}, \\texttt{pairwise}, \\texttt{pairwise!}, \\texttt{peuclidean}, \\texttt{renyi\\_divergence}, \\texttt{result\\_type}, \\texttt{rmsd}, \\texttt{rogerstanimoto}, \\texttt{spannorm\\_dist}, \\texttt{spherical\\_angle}, \\texttt{sqeuclidean}, \\texttt{sqmahalanobis}, \\texttt{totalvariation}, \\texttt{wcityblock}, \\texttt{weuclidean}, \\texttt{whamming}, \\texttt{wminkowski}, \\texttt{wsqeuclidean}\n",
       "\n",
       "\\section{Displaying contents of readme found at \\texttt{D:{\\textbackslash}TongYuan{\\textbackslash}.julia{\\textbackslash}packages{\\textbackslash}Distances{\\textbackslash}6E33b{\\textbackslash}README.md}}\n",
       "\\section{Distances.jl}\n",
       "\\href{https://github.com/JuliaStats/Distances.jl/actions?query=workflow%3ACI+branch%3Amaster}{\\begin{figure}\n",
       "\\centering\n",
       "\\includegraphics{https://github.com/JuliaStats/Distances.jl/workflows/CI/badge.svg?branch=master}\n",
       "\\caption{Build Status}\n",
       "\\end{figure}\n",
       "} \\href{http://codecov.io/github/JuliaStats/Distances.jl?branch=master}{\\begin{figure}\n",
       "\\centering\n",
       "\\includegraphics{http://codecov.io/github/JuliaStats/Distances.jl/coverage.svg?branch=master}\n",
       "\\caption{Coverage Status}\n",
       "\\end{figure}\n",
       "}\n",
       "\n",
       "A Julia package for evaluating distances(metrics) between vectors.\n",
       "\n",
       "This package also provides optimized functions to compute column-wise and pairwise distances, which are often substantially faster than a straightforward loop implementation. (See the benchmark section below for details).\n",
       "\n",
       "\\subsection{Supported distances}\n",
       "\\begin{itemize}\n",
       "\\item Euclidean distance\n",
       "\n",
       "\n",
       "\\item Squared Euclidean distance\n",
       "\n",
       "\n",
       "\\item Periodic Euclidean distance\n",
       "\n",
       "\n",
       "\\item Cityblock distance\n",
       "\n",
       "\n",
       "\\item Total variation distance\n",
       "\n",
       "\n",
       "\\item Jaccard distance\n",
       "\n",
       "\n",
       "\\item Rogers-Tanimoto distance\n",
       "\n",
       "\n",
       "\\item Chebyshev distance\n",
       "\n",
       "\n",
       "\\item Minkowski distance\n",
       "\n",
       "\n",
       "\\item Hamming distance\n",
       "\n",
       "\n",
       "\\item Cosine distance\n",
       "\n",
       "\n",
       "\\item Correlation distance\n",
       "\n",
       "\n",
       "\\item Chi-square distance\n",
       "\n",
       "\n",
       "\\item Kullback-Leibler divergence\n",
       "\n",
       "\n",
       "\\item Generalized Kullback-Leibler divergence\n",
       "\n",
       "\n",
       "\\item Rényi divergence\n",
       "\n",
       "\n",
       "\\item Jensen-Shannon divergence\n",
       "\n",
       "\n",
       "\\item Mahalanobis distance\n",
       "\n",
       "\n",
       "\\item Squared Mahalanobis distance\n",
       "\n",
       "\n",
       "\\item Bhattacharyya distance\n",
       "\n",
       "\n",
       "\\item Hellinger distance\n",
       "\n",
       "\n",
       "\\item Haversine distance\n",
       "\n",
       "\n",
       "\\item Spherical angle distance\n",
       "\n",
       "\n",
       "\\item Mean absolute deviation\n",
       "\n",
       "\n",
       "\\item Mean squared deviation\n",
       "\n",
       "\n",
       "\\item Root mean squared deviation\n",
       "\n",
       "\n",
       "\\item Normalized root mean squared deviation\n",
       "\n",
       "\n",
       "\\item Bray-Curtis dissimilarity\n",
       "\n",
       "\n",
       "\\item Bregman divergence\n",
       "\n",
       "\\end{itemize}\n",
       "For \\texttt{Euclidean distance}, \\texttt{Squared Euclidean distance}, \\texttt{Cityblock distance}, \\texttt{Minkowski distance}, and \\texttt{Hamming distance}, a weighted version is also provided.\n",
       "\n",
       "\\subsection{Basic use}\n",
       "The library supports three ways of computation: \\emph{computing the distance between} \\emph{two iterators/vectors}, \\emph{\"zip\"-wise computation}, and \\emph{pairwise computation}. Each of these computation modes works with arbitrary iterable objects of known size.\n",
       "\n",
       "\\subsubsection{Computing the distance between two iterators or vectors}\n",
       "Each distance corresponds to a \\emph{distance type}. You can always compute a certain distance between two iterators or vectors of equal length using the following syntax\n",
       "\n",
       "\\begin{verbatim}\n",
       "r = evaluate(dist, x, y)\n",
       "r = dist(x, y)\n",
       "\\end{verbatim}\n",
       "Here, \\texttt{dist} is an instance of a distance type: for example, the type for Euclidean distance is \\texttt{Euclidean} (more distance types will be introduced in the next section). You can compute the Euclidean distance between \\texttt{x} and \\texttt{y} as\n",
       "\n",
       "\\begin{verbatim}\n",
       "r = evaluate(Euclidean(), x, y)\n",
       "r = Euclidean()(x, y)\n",
       "\\end{verbatim}\n",
       "Common distances also come with convenient functions for distance evaluation. For example, you may also compute Euclidean distance between two vectors as below\n",
       "\n",
       "\\begin{verbatim}\n",
       "r = euclidean(x, y)\n",
       "\\end{verbatim}\n",
       "\\subsubsection{Computing distances between corresponding objects (\"column-wise\")}\n",
       "Suppose you have two \\texttt{m-by-n} matrix \\texttt{X} and \\texttt{Y}, then you can compute all distances between corresponding columns of \\texttt{X} and \\texttt{Y} in one batch, using the \\texttt{colwise} function, as\n",
       "\n",
       "\\begin{verbatim}\n",
       "r = colwise(dist, X, Y)\n",
       "\\end{verbatim}\n",
       "The output \\texttt{r} is a vector of length \\texttt{n}. In particular, \\texttt{r[i]} is the distance between \\texttt{X[:,i]} and \\texttt{Y[:,i]}. The batch computation typically runs considerably faster than calling \\texttt{evaluate} column-by-column.\n",
       "\n",
       "Note that either of \\texttt{X} and \\texttt{Y} can be just a single vector – then the \\texttt{colwise} function computes the distance between this vector and each column of the other argument.\n",
       "\n",
       "\\subsubsection{Computing pairwise distances}\n",
       "Let \\texttt{X} and \\texttt{Y} have \\texttt{m} and \\texttt{n} columns, respectively, and the same number of rows. Then the \\texttt{pairwise} function with the \\texttt{dims=2} argument computes distances between each pair of columns in \\texttt{X} and \\texttt{Y}:\n",
       "\n",
       "\\begin{verbatim}\n",
       "R = pairwise(dist, X, Y, dims=2)\n",
       "\\end{verbatim}\n",
       "In the output, \\texttt{R} is a matrix of size \\texttt{(m, n)}, such that \\texttt{R[i,j]} is the distance between \\texttt{X[:,i]} and \\texttt{Y[:,j]}. Computing distances for all pairs using \\texttt{pairwise} function is often remarkably faster than evaluting for each pair individually.\n",
       "\n",
       "If you just want to just compute distances between all columns of a matrix \\texttt{X}, you can write\n",
       "\n",
       "\\begin{verbatim}\n",
       "R = pairwise(dist, X, dims=2)\n",
       "\\end{verbatim}\n",
       "This statement will result in an \\texttt{m-by-m} matrix, where \\texttt{R[i,j]} is the distance between \\texttt{X[:,i]} and \\texttt{X[:,j]}. \\texttt{pairwise(dist, X)} is typically more efficient than \\texttt{pairwise(dist, X, X)}, as the former will take advantage of the symmetry when \\texttt{dist} is a semi-metric (including metric).\n",
       "\n",
       "To compute pairwise distances for matrices with observations stored in rows use the argument \\texttt{dims=1}.\n",
       "\n",
       "\\subsubsection{Computing column-wise and pairwise distances inplace}\n",
       "If the vector/matrix to store the results are pre-allocated, you may use the storage (without creating a new array) using the following syntax (\\texttt{i} being either \\texttt{1} or \\texttt{2}):\n",
       "\n",
       "\\begin{verbatim}\n",
       "colwise!(r, dist, X, Y)\n",
       "pairwise!(R, dist, X, Y, dims=i)\n",
       "pairwise!(R, dist, X, dims=i)\n",
       "\\end{verbatim}\n",
       "Please pay attention to the difference, the functions for inplace computation are \\texttt{colwise!} and \\texttt{pairwise!} (instead of \\texttt{colwise} and \\texttt{pairwise}).\n",
       "\n",
       "\\subsection{Distance type hierarchy}\n",
       "The distances are organized into a type hierarchy.\n",
       "\n",
       "At the top of this hierarchy is an abstract class \\textbf{PreMetric}, which is defined to be a function \\texttt{d} that satisfies\n",
       "\n",
       "\\begin{verbatim}\n",
       "d(x, x) == 0  for all x\n",
       "d(x, y) >= 0  for all x, y\n",
       "\\end{verbatim}\n",
       "\\textbf{SemiMetric} is a abstract type that refines \\textbf{PreMetric}. Formally, a \\emph{semi-metric} is a \\emph{pre-metric} that is also symmetric, as\n",
       "\n",
       "\\begin{verbatim}\n",
       "d(x, y) == d(y, x)  for all x, y\n",
       "\\end{verbatim}\n",
       "\\textbf{Metric} is a abstract type that further refines \\textbf{SemiMetric}. Formally, a \\emph{metric} is a \\emph{semi-metric} that also satisfies triangle inequality, as\n",
       "\n",
       "\\begin{verbatim}\n",
       "d(x, z) <= d(x, y) + d(y, z)  for all x, y, z\n",
       "\\end{verbatim}\n",
       "This type system has practical significance. For example, when computing pairwise distances between a set of vectors, you may only perform computation for half of the pairs, derive the values immediately for the remaining half by leveraging the symmetry of \\emph{semi-metrics}. Note that the types of \\texttt{SemiMetric} and \\texttt{Metric} do not completely follow the definition in mathematics as they do not require the \"distance\" to be able to distinguish between points: for these types \\texttt{x != y} does not imply that \\texttt{d(x, y) != 0} in general compared to the mathematical definition of semi-metric and metric, as this property does not change computations in practice.\n",
       "\n",
       "Each distance corresponds to a distance type. The type name and the corresponding mathematical definitions of the distances are listed in the following table.\n",
       "\n",
       "\\begin{tabular}\n",
       "{r | r | r}\n",
       "type name & convenient syntax & math definition \\\\\n",
       "\\hline\n",
       "Euclidean & \\texttt{euclidean(x, y)} & \\texttt{sqrt(sum((x - y) .\\^{} 2))} \\\\\n",
       "SqEuclidean & \\texttt{sqeuclidean(x, y)} & \\texttt{sum((x - y).\\^{}2)} \\\\\n",
       "PeriodicEuclidean & \\texttt{peuclidean(x, y, w)} & \\texttt{sqrt(sum(min(mod(abs(x - y), w), w - mod(abs(x - y), w)).\\^{}2))} \\\\\n",
       "Cityblock & \\texttt{cityblock(x, y)} & \\texttt{sum(abs(x - y))} \\\\\n",
       "TotalVariation & \\texttt{totalvariation(x, y)} & \\texttt{sum(abs(x - y)) / 2} \\\\\n",
       "Chebyshev & \\texttt{chebyshev(x, y)} & \\texttt{max(abs(x - y))} \\\\\n",
       "Minkowski & \\texttt{minkowski(x, y, p)} & \\texttt{sum(abs(x - y).\\^{}p) \\^{} (1/p)} \\\\\n",
       "Hamming & \\texttt{hamming(k, l)} & \\texttt{sum(k .!= l)} \\\\\n",
       "RogersTanimoto & \\texttt{rogerstanimoto(a, b)} & \\texttt{2(sum(a\\&!b) + sum(!a\\&b)) / (2(sum(a\\&!b) + sum(!a\\&b)) + sum(a\\&b) + sum(!a\\&!b))} \\\\\n",
       "Jaccard & \\texttt{jaccard(x, y)} & \\texttt{1 - sum(min(x, y)) / sum(max(x, y))} \\\\\n",
       "BrayCurtis & \\texttt{braycurtis(x, y)} & \\texttt{sum(abs(x - y)) / sum(abs(x + y))} \\\\\n",
       "CosineDist & \\texttt{cosine\\_dist(x, y)} & \\texttt{1 - dot(x, y) / (norm(x) * norm(y))} \\\\\n",
       "CorrDist & \\texttt{corr\\_dist(x, y)} & \\texttt{cosine\\_dist(x - mean(x), y - mean(y))} \\\\\n",
       "ChiSqDist & \\texttt{chisq\\_dist(x, y)} & \\texttt{sum((x - y).\\^{}2 / (x + y))} \\\\\n",
       "KLDivergence & \\texttt{kl\\_divergence(p, q)} & \\texttt{sum(p .* log(p ./ q))} \\\\\n",
       "GenKLDivergence & \\texttt{gkl\\_divergence(x, y)} & \\texttt{sum(p .* log(p ./ q) - p + q)} \\\\\n",
       "RenyiDivergence & \\texttt{renyi\\_divergence(p, q, k)} & \\texttt{log(sum( p .* (p ./ q) .\\^{} (k - 1))) / (k - 1)} \\\\\n",
       "JSDivergence & \\texttt{js\\_divergence(p, q)} & \\texttt{KL(p, m) / 2 + KL(q, m) / 2 with m = (p + q) / 2} \\\\\n",
       "SpanNormDist & \\texttt{spannorm\\_dist(x, y)} & \\texttt{max(x - y) - min(x - y)} \\\\\n",
       "BhattacharyyaDist & \\texttt{bhattacharyya(x, y)} & \\texttt{-log(sum(sqrt(x .* y) / sqrt(sum(x) * sum(y)))} \\\\\n",
       "HellingerDist & \\texttt{hellinger(x, y)} & \\texttt{sqrt(1 - sum(sqrt(x .* y) / sqrt(sum(x) * sum(y))))} \\\\\n",
       "Haversine & \\texttt{haversine(x, y, r = 6\\_371\\_000)} & \\href{https://en.wikipedia.org/wiki/Haversine_formula}{Haversine formula} \\\\\n",
       "SphericalAngle & \\texttt{spherical\\_angle(x, y)} & \\href{https://en.wikipedia.org/wiki/Haversine_formula}{Haversine formula} \\\\\n",
       "Mahalanobis & \\texttt{mahalanobis(x, y, Q)} & \\texttt{sqrt((x - y)' * Q * (x - y))} \\\\\n",
       "SqMahalanobis & \\texttt{sqmahalanobis(x, y, Q)} & \\texttt{(x - y)' * Q * (x - y)} \\\\\n",
       "MeanAbsDeviation & \\texttt{meanad(x, y)} & \\texttt{mean(abs.(x - y))} \\\\\n",
       "\\end{tabular}\n",
       "[output truncated to first 200 lines]\n",
       "\n"
      ],
      "text/markdown": [
       "No docstring found for module `Distances`.\n",
       "\n",
       "# Exported names\n",
       "\n",
       "`BhattacharyyaDist`, `BrayCurtis`, `Bregman`, `Chebyshev`, `ChiSqDist`, `Cityblock`, `CorrDist`, `CosineDist`, `Euclidean`, `GenKLDivergence`, `Hamming`, `Haversine`, `HellingerDist`, `JSDivergence`, `Jaccard`, `KLDivergence`, `Mahalanobis`, `MeanAbsDeviation`, `MeanSqDeviation`, `Metric`, `Minkowski`, `NormRMSDeviation`, `PeriodicEuclidean`, `PreMetric`, `RMSDeviation`, `RenyiDivergence`, `RogersTanimoto`, `SemiMetric`, `SpanNormDist`, `SphericalAngle`, `SqEuclidean`, `SqMahalanobis`, `TotalVariation`, `WeightedCityblock`, `WeightedEuclidean`, `WeightedHamming`, `WeightedMinkowski`, `WeightedSqEuclidean`, `bhattacharyya`, `braycurtis`, `bregman`, `chebyshev`, `chisq_dist`, `cityblock`, `colwise`, `colwise!`, `corr_dist`, `cosine_dist`, `euclidean`, `evaluate`, `gkl_divergence`, `hamming`, `haversine`, `hellinger`, `jaccard`, `js_divergence`, `kl_divergence`, `mahalanobis`, `meanad`, `minkowski`, `msd`, `nrmsd`, `pairwise`, `pairwise!`, `peuclidean`, `renyi_divergence`, `result_type`, `rmsd`, `rogerstanimoto`, `spannorm_dist`, `spherical_angle`, `sqeuclidean`, `sqmahalanobis`, `totalvariation`, `wcityblock`, `weuclidean`, `whamming`, `wminkowski`, `wsqeuclidean`\n",
       "\n",
       "# Displaying contents of readme found at `D:\\TongYuan\\.julia\\packages\\Distances\\6E33b\\README.md`\n",
       "\n",
       "# Distances.jl\n",
       "\n",
       "[![Build Status](https://github.com/JuliaStats/Distances.jl/workflows/CI/badge.svg?branch=master)](https://github.com/JuliaStats/Distances.jl/actions?query=workflow%3ACI+branch%3Amaster) [![Coverage Status](http://codecov.io/github/JuliaStats/Distances.jl/coverage.svg?branch=master)](http://codecov.io/github/JuliaStats/Distances.jl?branch=master)\n",
       "\n",
       "A Julia package for evaluating distances(metrics) between vectors.\n",
       "\n",
       "This package also provides optimized functions to compute column-wise and pairwise distances, which are often substantially faster than a straightforward loop implementation. (See the benchmark section below for details).\n",
       "\n",
       "## Supported distances\n",
       "\n",
       "  * Euclidean distance\n",
       "  * Squared Euclidean distance\n",
       "  * Periodic Euclidean distance\n",
       "  * Cityblock distance\n",
       "  * Total variation distance\n",
       "  * Jaccard distance\n",
       "  * Rogers-Tanimoto distance\n",
       "  * Chebyshev distance\n",
       "  * Minkowski distance\n",
       "  * Hamming distance\n",
       "  * Cosine distance\n",
       "  * Correlation distance\n",
       "  * Chi-square distance\n",
       "  * Kullback-Leibler divergence\n",
       "  * Generalized Kullback-Leibler divergence\n",
       "  * Rényi divergence\n",
       "  * Jensen-Shannon divergence\n",
       "  * Mahalanobis distance\n",
       "  * Squared Mahalanobis distance\n",
       "  * Bhattacharyya distance\n",
       "  * Hellinger distance\n",
       "  * Haversine distance\n",
       "  * Spherical angle distance\n",
       "  * Mean absolute deviation\n",
       "  * Mean squared deviation\n",
       "  * Root mean squared deviation\n",
       "  * Normalized root mean squared deviation\n",
       "  * Bray-Curtis dissimilarity\n",
       "  * Bregman divergence\n",
       "\n",
       "For `Euclidean distance`, `Squared Euclidean distance`, `Cityblock distance`, `Minkowski distance`, and `Hamming distance`, a weighted version is also provided.\n",
       "\n",
       "## Basic use\n",
       "\n",
       "The library supports three ways of computation: *computing the distance between* *two iterators/vectors*, *\"zip\"-wise computation*, and *pairwise computation*. Each of these computation modes works with arbitrary iterable objects of known size.\n",
       "\n",
       "### Computing the distance between two iterators or vectors\n",
       "\n",
       "Each distance corresponds to a *distance type*. You can always compute a certain distance between two iterators or vectors of equal length using the following syntax\n",
       "\n",
       "```julia\n",
       "r = evaluate(dist, x, y)\n",
       "r = dist(x, y)\n",
       "```\n",
       "\n",
       "Here, `dist` is an instance of a distance type: for example, the type for Euclidean distance is `Euclidean` (more distance types will be introduced in the next section). You can compute the Euclidean distance between `x` and `y` as\n",
       "\n",
       "```julia\n",
       "r = evaluate(Euclidean(), x, y)\n",
       "r = Euclidean()(x, y)\n",
       "```\n",
       "\n",
       "Common distances also come with convenient functions for distance evaluation. For example, you may also compute Euclidean distance between two vectors as below\n",
       "\n",
       "```julia\n",
       "r = euclidean(x, y)\n",
       "```\n",
       "\n",
       "### Computing distances between corresponding objects (\"column-wise\")\n",
       "\n",
       "Suppose you have two `m-by-n` matrix `X` and `Y`, then you can compute all distances between corresponding columns of `X` and `Y` in one batch, using the `colwise` function, as\n",
       "\n",
       "```julia\n",
       "r = colwise(dist, X, Y)\n",
       "```\n",
       "\n",
       "The output `r` is a vector of length `n`. In particular, `r[i]` is the distance between `X[:,i]` and `Y[:,i]`. The batch computation typically runs considerably faster than calling `evaluate` column-by-column.\n",
       "\n",
       "Note that either of `X` and `Y` can be just a single vector – then the `colwise` function computes the distance between this vector and each column of the other argument.\n",
       "\n",
       "### Computing pairwise distances\n",
       "\n",
       "Let `X` and `Y` have `m` and `n` columns, respectively, and the same number of rows. Then the `pairwise` function with the `dims=2` argument computes distances between each pair of columns in `X` and `Y`:\n",
       "\n",
       "```julia\n",
       "R = pairwise(dist, X, Y, dims=2)\n",
       "```\n",
       "\n",
       "In the output, `R` is a matrix of size `(m, n)`, such that `R[i,j]` is the distance between `X[:,i]` and `Y[:,j]`. Computing distances for all pairs using `pairwise` function is often remarkably faster than evaluting for each pair individually.\n",
       "\n",
       "If you just want to just compute distances between all columns of a matrix `X`, you can write\n",
       "\n",
       "```julia\n",
       "R = pairwise(dist, X, dims=2)\n",
       "```\n",
       "\n",
       "This statement will result in an `m-by-m` matrix, where `R[i,j]` is the distance between `X[:,i]` and `X[:,j]`. `pairwise(dist, X)` is typically more efficient than `pairwise(dist, X, X)`, as the former will take advantage of the symmetry when `dist` is a semi-metric (including metric).\n",
       "\n",
       "To compute pairwise distances for matrices with observations stored in rows use the argument `dims=1`.\n",
       "\n",
       "### Computing column-wise and pairwise distances inplace\n",
       "\n",
       "If the vector/matrix to store the results are pre-allocated, you may use the storage (without creating a new array) using the following syntax (`i` being either `1` or `2`):\n",
       "\n",
       "```julia\n",
       "colwise!(r, dist, X, Y)\n",
       "pairwise!(R, dist, X, Y, dims=i)\n",
       "pairwise!(R, dist, X, dims=i)\n",
       "```\n",
       "\n",
       "Please pay attention to the difference, the functions for inplace computation are `colwise!` and `pairwise!` (instead of `colwise` and `pairwise`).\n",
       "\n",
       "## Distance type hierarchy\n",
       "\n",
       "The distances are organized into a type hierarchy.\n",
       "\n",
       "At the top of this hierarchy is an abstract class **PreMetric**, which is defined to be a function `d` that satisfies\n",
       "\n",
       "```\n",
       "d(x, x) == 0  for all x\n",
       "d(x, y) >= 0  for all x, y\n",
       "```\n",
       "\n",
       "**SemiMetric** is a abstract type that refines **PreMetric**. Formally, a *semi-metric* is a *pre-metric* that is also symmetric, as\n",
       "\n",
       "```\n",
       "d(x, y) == d(y, x)  for all x, y\n",
       "```\n",
       "\n",
       "**Metric** is a abstract type that further refines **SemiMetric**. Formally, a *metric* is a *semi-metric* that also satisfies triangle inequality, as\n",
       "\n",
       "```\n",
       "d(x, z) <= d(x, y) + d(y, z)  for all x, y, z\n",
       "```\n",
       "\n",
       "This type system has practical significance. For example, when computing pairwise distances between a set of vectors, you may only perform computation for half of the pairs, derive the values immediately for the remaining half by leveraging the symmetry of *semi-metrics*. Note that the types of `SemiMetric` and `Metric` do not completely follow the definition in mathematics as they do not require the \"distance\" to be able to distinguish between points: for these types `x != y` does not imply that `d(x, y) != 0` in general compared to the mathematical definition of semi-metric and metric, as this property does not change computations in practice.\n",
       "\n",
       "Each distance corresponds to a distance type. The type name and the corresponding mathematical definitions of the distances are listed in the following table.\n",
       "\n",
       "|         type name |                convenient syntax |                                                                 math definition |\n",
       "| -----------------:| --------------------------------:| -------------------------------------------------------------------------------:|\n",
       "|         Euclidean |                `euclidean(x, y)` |                                                       `sqrt(sum((x - y) .^ 2))` |\n",
       "|       SqEuclidean |              `sqeuclidean(x, y)` |                                                               `sum((x - y).^2)` |\n",
       "| PeriodicEuclidean |            `peuclidean(x, y, w)` |                 `sqrt(sum(min(mod(abs(x - y), w), w - mod(abs(x - y), w)).^2))` |\n",
       "|         Cityblock |                `cityblock(x, y)` |                                                               `sum(abs(x - y))` |\n",
       "|    TotalVariation |           `totalvariation(x, y)` |                                                           `sum(abs(x - y)) / 2` |\n",
       "|         Chebyshev |                `chebyshev(x, y)` |                                                               `max(abs(x - y))` |\n",
       "|         Minkowski |             `minkowski(x, y, p)` |                                                    `sum(abs(x - y).^p) ^ (1/p)` |\n",
       "|           Hamming |                  `hamming(k, l)` |                                                                  `sum(k .!= l)` |\n",
       "|    RogersTanimoto |           `rogerstanimoto(a, b)` | `2(sum(a&!b) + sum(!a&b)) / (2(sum(a&!b) + sum(!a&b)) + sum(a&b) + sum(!a&!b))` |\n",
       "|           Jaccard |                  `jaccard(x, y)` |                                           `1 - sum(min(x, y)) / sum(max(x, y))` |\n",
       "|        BrayCurtis |               `braycurtis(x, y)` |                                             `sum(abs(x - y)) / sum(abs(x + y))` |\n",
       "|        CosineDist |              `cosine_dist(x, y)` |                                           `1 - dot(x, y) / (norm(x) * norm(y))` |\n",
       "|          CorrDist |                `corr_dist(x, y)` |                                         `cosine_dist(x - mean(x), y - mean(y))` |\n",
       "|         ChiSqDist |               `chisq_dist(x, y)` |                                                     `sum((x - y).^2 / (x + y))` |\n",
       "|      KLDivergence |            `kl_divergence(p, q)` |                                                         `sum(p .* log(p ./ q))` |\n",
       "|   GenKLDivergence |           `gkl_divergence(x, y)` |                                                 `sum(p .* log(p ./ q) - p + q)` |\n",
       "|   RenyiDivergence |      `renyi_divergence(p, q, k)` |                                 `log(sum( p .* (p ./ q) .^ (k - 1))) / (k - 1)` |\n",
       "|      JSDivergence |            `js_divergence(p, q)` |                              `KL(p, m) / 2 + KL(q, m) / 2 with m = (p + q) / 2` |\n",
       "|      SpanNormDist |            `spannorm_dist(x, y)` |                                                       `max(x - y) - min(x - y)` |\n",
       "| BhattacharyyaDist |            `bhattacharyya(x, y)` |                                `-log(sum(sqrt(x .* y) / sqrt(sum(x) * sum(y)))` |\n",
       "|     HellingerDist |                `hellinger(x, y)` |                           `sqrt(1 - sum(sqrt(x .* y) / sqrt(sum(x) * sum(y))))` |\n",
       "|         Haversine | `haversine(x, y, r = 6_371_000)` |            [Haversine formula](https://en.wikipedia.org/wiki/Haversine_formula) |\n",
       "|    SphericalAngle |          `spherical_angle(x, y)` |            [Haversine formula](https://en.wikipedia.org/wiki/Haversine_formula) |\n",
       "|       Mahalanobis |           `mahalanobis(x, y, Q)` |                                                  `sqrt((x - y)' * Q * (x - y))` |\n",
       "|     SqMahalanobis |         `sqmahalanobis(x, y, Q)` |                                                        `(x - y)' * Q * (x - y)` |\n",
       "|  MeanAbsDeviation |                   `meanad(x, y)` |                                                             `mean(abs.(x - y))` |\n",
       "\n",
       "[output truncated to first 200 lines]\n"
      ],
      "text/plain": [
       "  No docstring found for module \u001b[36mDistances\u001b[39m.\n",
       "\n",
       "\u001b[1m  Exported names\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "  \u001b[36mBhattacharyyaDist\u001b[39m, \u001b[36mBrayCurtis\u001b[39m, \u001b[36mBregman\u001b[39m, \u001b[36mChebyshev\u001b[39m, \u001b[36mChiSqDist\u001b[39m, \u001b[36mCityblock\u001b[39m,\n",
       "  \u001b[36mCorrDist\u001b[39m, \u001b[36mCosineDist\u001b[39m, \u001b[36mEuclidean\u001b[39m, \u001b[36mGenKLDivergence\u001b[39m, \u001b[36mHamming\u001b[39m, \u001b[36mHaversine\u001b[39m,\n",
       "  \u001b[36mHellingerDist\u001b[39m, \u001b[36mJSDivergence\u001b[39m, \u001b[36mJaccard\u001b[39m, \u001b[36mKLDivergence\u001b[39m, \u001b[36mMahalanobis\u001b[39m,\n",
       "  \u001b[36mMeanAbsDeviation\u001b[39m, \u001b[36mMeanSqDeviation\u001b[39m, \u001b[36mMetric\u001b[39m, \u001b[36mMinkowski\u001b[39m, \u001b[36mNormRMSDeviation\u001b[39m,\n",
       "  \u001b[36mPeriodicEuclidean\u001b[39m, \u001b[36mPreMetric\u001b[39m, \u001b[36mRMSDeviation\u001b[39m, \u001b[36mRenyiDivergence\u001b[39m, \u001b[36mRogersTanimoto\u001b[39m,\n",
       "  \u001b[36mSemiMetric\u001b[39m, \u001b[36mSpanNormDist\u001b[39m, \u001b[36mSphericalAngle\u001b[39m, \u001b[36mSqEuclidean\u001b[39m, \u001b[36mSqMahalanobis\u001b[39m,\n",
       "  \u001b[36mTotalVariation\u001b[39m, \u001b[36mWeightedCityblock\u001b[39m, \u001b[36mWeightedEuclidean\u001b[39m, \u001b[36mWeightedHamming\u001b[39m,\n",
       "  \u001b[36mWeightedMinkowski\u001b[39m, \u001b[36mWeightedSqEuclidean\u001b[39m, \u001b[36mbhattacharyya\u001b[39m, \u001b[36mbraycurtis\u001b[39m, \u001b[36mbregman\u001b[39m,\n",
       "  \u001b[36mchebyshev\u001b[39m, \u001b[36mchisq_dist\u001b[39m, \u001b[36mcityblock\u001b[39m, \u001b[36mcolwise\u001b[39m, \u001b[36mcolwise!\u001b[39m, \u001b[36mcorr_dist\u001b[39m, \u001b[36mcosine_dist\u001b[39m,\n",
       "  \u001b[36meuclidean\u001b[39m, \u001b[36mevaluate\u001b[39m, \u001b[36mgkl_divergence\u001b[39m, \u001b[36mhamming\u001b[39m, \u001b[36mhaversine\u001b[39m, \u001b[36mhellinger\u001b[39m, \u001b[36mjaccard\u001b[39m,\n",
       "  \u001b[36mjs_divergence\u001b[39m, \u001b[36mkl_divergence\u001b[39m, \u001b[36mmahalanobis\u001b[39m, \u001b[36mmeanad\u001b[39m, \u001b[36mminkowski\u001b[39m, \u001b[36mmsd\u001b[39m, \u001b[36mnrmsd\u001b[39m,\n",
       "  \u001b[36mpairwise\u001b[39m, \u001b[36mpairwise!\u001b[39m, \u001b[36mpeuclidean\u001b[39m, \u001b[36mrenyi_divergence\u001b[39m, \u001b[36mresult_type\u001b[39m, \u001b[36mrmsd\u001b[39m,\n",
       "  \u001b[36mrogerstanimoto\u001b[39m, \u001b[36mspannorm_dist\u001b[39m, \u001b[36mspherical_angle\u001b[39m, \u001b[36msqeuclidean\u001b[39m, \u001b[36msqmahalanobis\u001b[39m,\n",
       "  \u001b[36mtotalvariation\u001b[39m, \u001b[36mwcityblock\u001b[39m, \u001b[36mweuclidean\u001b[39m, \u001b[36mwhamming\u001b[39m, \u001b[36mwminkowski\u001b[39m, \u001b[36mwsqeuclidean\u001b[39m\n",
       "\n",
       "\u001b[1m  Displaying contents of readme found at\u001b[22m\n",
       "\u001b[1m \u001b[36mD:\\TongYuan\\.julia\\packages\\Distances\\6E33b\\README.md\u001b[39m\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "\u001b[1m  Distances.jl\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "  (Image: Build Status)\n",
       "  (https://github.com/JuliaStats/Distances.jl/actions?query=workflow%3ACI+branch%3Amaster)\n",
       "  (Image: Coverage Status)\n",
       "  (http://codecov.io/github/JuliaStats/Distances.jl?branch=master)\n",
       "\n",
       "  A Julia package for evaluating distances(metrics) between vectors.\n",
       "\n",
       "  This package also provides optimized functions to compute column-wise and\n",
       "  pairwise distances, which are often substantially faster than a\n",
       "  straightforward loop implementation. (See the benchmark section below for\n",
       "  details).\n",
       "\n",
       "\u001b[1m  Supported distances\u001b[22m\n",
       "\u001b[1m  =====================\u001b[22m\n",
       "\n",
       "    •  Euclidean distance\n",
       "\n",
       "    •  Squared Euclidean distance\n",
       "\n",
       "    •  Periodic Euclidean distance\n",
       "\n",
       "    •  Cityblock distance\n",
       "\n",
       "    •  Total variation distance\n",
       "\n",
       "    •  Jaccard distance\n",
       "\n",
       "    •  Rogers-Tanimoto distance\n",
       "\n",
       "    •  Chebyshev distance\n",
       "\n",
       "    •  Minkowski distance\n",
       "\n",
       "    •  Hamming distance\n",
       "\n",
       "    •  Cosine distance\n",
       "\n",
       "    •  Correlation distance\n",
       "\n",
       "    •  Chi-square distance\n",
       "\n",
       "    •  Kullback-Leibler divergence\n",
       "\n",
       "    •  Generalized Kullback-Leibler divergence\n",
       "\n",
       "    •  Rényi divergence\n",
       "\n",
       "    •  Jensen-Shannon divergence\n",
       "\n",
       "    •  Mahalanobis distance\n",
       "\n",
       "    •  Squared Mahalanobis distance\n",
       "\n",
       "    •  Bhattacharyya distance\n",
       "\n",
       "    •  Hellinger distance\n",
       "\n",
       "    •  Haversine distance\n",
       "\n",
       "    •  Spherical angle distance\n",
       "\n",
       "    •  Mean absolute deviation\n",
       "\n",
       "    •  Mean squared deviation\n",
       "\n",
       "    •  Root mean squared deviation\n",
       "\n",
       "    •  Normalized root mean squared deviation\n",
       "\n",
       "    •  Bray-Curtis dissimilarity\n",
       "\n",
       "    •  Bregman divergence\n",
       "\n",
       "  For \u001b[36mEuclidean distance\u001b[39m, \u001b[36mSquared Euclidean distance\u001b[39m, \u001b[36mCityblock distance\u001b[39m,\n",
       "  \u001b[36mMinkowski distance\u001b[39m, and \u001b[36mHamming distance\u001b[39m, a weighted version is also\n",
       "  provided.\n",
       "\n",
       "\u001b[1m  Basic use\u001b[22m\n",
       "\u001b[1m  ===========\u001b[22m\n",
       "\n",
       "  The library supports three ways of computation: \u001b[4mcomputing the distance\n",
       "  between\u001b[24m \u001b[4mtwo iterators/vectors\u001b[24m, \u001b[4m\"zip\"-wise computation\u001b[24m, and \u001b[4mpairwise\n",
       "  computation\u001b[24m. Each of these computation modes works with arbitrary iterable\n",
       "  objects of known size.\n",
       "\n",
       "\u001b[1m  Computing the distance between two iterators or vectors\u001b[22m\n",
       "\u001b[1m  –––––––––––––––––––––––––––––––––––––––––––––––––––––––––\u001b[22m\n",
       "\n",
       "  Each distance corresponds to a \u001b[4mdistance type\u001b[24m. You can always compute a\n",
       "  certain distance between two iterators or vectors of equal length using the\n",
       "  following syntax\n",
       "\n",
       "\u001b[36m  r = evaluate(dist, x, y)\u001b[39m\n",
       "\u001b[36m  r = dist(x, y)\u001b[39m\n",
       "\n",
       "  Here, \u001b[36mdist\u001b[39m is an instance of a distance type: for example, the type for\n",
       "  Euclidean distance is \u001b[36mEuclidean\u001b[39m (more distance types will be introduced in\n",
       "  the next section). You can compute the Euclidean distance between \u001b[36mx\u001b[39m and \u001b[36my\u001b[39m as\n",
       "\n",
       "\u001b[36m  r = evaluate(Euclidean(), x, y)\u001b[39m\n",
       "\u001b[36m  r = Euclidean()(x, y)\u001b[39m\n",
       "\n",
       "  Common distances also come with convenient functions for distance\n",
       "  evaluation. For example, you may also compute Euclidean distance between two\n",
       "  vectors as below\n",
       "\n",
       "\u001b[36m  r = euclidean(x, y)\u001b[39m\n",
       "\n",
       "\u001b[1m  Computing distances between corresponding objects (\"column-wise\")\u001b[22m\n",
       "\u001b[1m  –––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––\u001b[22m\n",
       "\n",
       "  Suppose you have two \u001b[36mm-by-n\u001b[39m matrix \u001b[36mX\u001b[39m and \u001b[36mY\u001b[39m, then you can compute all\n",
       "  distances between corresponding columns of \u001b[36mX\u001b[39m and \u001b[36mY\u001b[39m in one batch, using the\n",
       "  \u001b[36mcolwise\u001b[39m function, as\n",
       "\n",
       "\u001b[36m  r = colwise(dist, X, Y)\u001b[39m\n",
       "\n",
       "  The output \u001b[36mr\u001b[39m is a vector of length \u001b[36mn\u001b[39m. In particular, \u001b[36mr[i]\u001b[39m is the distance\n",
       "  between \u001b[36mX[:,i]\u001b[39m and \u001b[36mY[:,i]\u001b[39m. The batch computation typically runs considerably\n",
       "  faster than calling \u001b[36mevaluate\u001b[39m column-by-column.\n",
       "\n",
       "  Note that either of \u001b[36mX\u001b[39m and \u001b[36mY\u001b[39m can be just a single vector – then the \u001b[36mcolwise\u001b[39m\n",
       "  function computes the distance between this vector and each column of the\n",
       "  other argument.\n",
       "\n",
       "\u001b[1m  Computing pairwise distances\u001b[22m\n",
       "\u001b[1m  ––––––––––––––––––––––––––––––\u001b[22m\n",
       "\n",
       "  Let \u001b[36mX\u001b[39m and \u001b[36mY\u001b[39m have \u001b[36mm\u001b[39m and \u001b[36mn\u001b[39m columns, respectively, and the same number of rows.\n",
       "  Then the \u001b[36mpairwise\u001b[39m function with the \u001b[36mdims=2\u001b[39m argument computes distances\n",
       "  between each pair of columns in \u001b[36mX\u001b[39m and \u001b[36mY\u001b[39m:\n",
       "\n",
       "\u001b[36m  R = pairwise(dist, X, Y, dims=2)\u001b[39m\n",
       "\n",
       "  In the output, \u001b[36mR\u001b[39m is a matrix of size \u001b[36m(m, n)\u001b[39m, such that \u001b[36mR[i,j]\u001b[39m is the\n",
       "  distance between \u001b[36mX[:,i]\u001b[39m and \u001b[36mY[:,j]\u001b[39m. Computing distances for all pairs using\n",
       "  \u001b[36mpairwise\u001b[39m function is often remarkably faster than evaluting for each pair\n",
       "  individually.\n",
       "\n",
       "  If you just want to just compute distances between all columns of a matrix\n",
       "  \u001b[36mX\u001b[39m, you can write\n",
       "\n",
       "\u001b[36m  R = pairwise(dist, X, dims=2)\u001b[39m\n",
       "\n",
       "  This statement will result in an \u001b[36mm-by-m\u001b[39m matrix, where \u001b[36mR[i,j]\u001b[39m is the distance\n",
       "  between \u001b[36mX[:,i]\u001b[39m and \u001b[36mX[:,j]\u001b[39m. \u001b[36mpairwise(dist, X)\u001b[39m is typically more efficient\n",
       "  than \u001b[36mpairwise(dist, X, X)\u001b[39m, as the former will take advantage of the symmetry\n",
       "  when \u001b[36mdist\u001b[39m is a semi-metric (including metric).\n",
       "\n",
       "  To compute pairwise distances for matrices with observations stored in rows\n",
       "  use the argument \u001b[36mdims=1\u001b[39m.\n",
       "\n",
       "\u001b[1m  Computing column-wise and pairwise distances inplace\u001b[22m\n",
       "\u001b[1m  ––––––––––––––––––––––––––––––––––––––––––––––––––––––\u001b[22m\n",
       "\n",
       "  If the vector/matrix to store the results are pre-allocated, you may use the\n",
       "  storage (without creating a new array) using the following syntax (\u001b[36mi\u001b[39m being\n",
       "  either \u001b[36m1\u001b[39m or \u001b[36m2\u001b[39m):\n",
       "\n",
       "\u001b[36m  colwise!(r, dist, X, Y)\u001b[39m\n",
       "\u001b[36m  pairwise!(R, dist, X, Y, dims=i)\u001b[39m\n",
       "\u001b[36m  pairwise!(R, dist, X, dims=i)\u001b[39m\n",
       "\n",
       "  Please pay attention to the difference, the functions for inplace\n",
       "  computation are \u001b[36mcolwise!\u001b[39m and \u001b[36mpairwise!\u001b[39m (instead of \u001b[36mcolwise\u001b[39m and \u001b[36mpairwise\u001b[39m).\n",
       "\n",
       "\u001b[1m  Distance type hierarchy\u001b[22m\n",
       "\u001b[1m  =========================\u001b[22m\n",
       "\n",
       "  The distances are organized into a type hierarchy.\n",
       "\n",
       "  At the top of this hierarchy is an abstract class \u001b[1mPreMetric\u001b[22m, which is\n",
       "  defined to be a function \u001b[36md\u001b[39m that satisfies\n",
       "\n",
       "\u001b[36m  d(x, x) == 0  for all x\u001b[39m\n",
       "\u001b[36m  d(x, y) >= 0  for all x, y\u001b[39m\n",
       "\n",
       "  \u001b[1mSemiMetric\u001b[22m is a abstract type that refines \u001b[1mPreMetric\u001b[22m. Formally, a\n",
       "  \u001b[4msemi-metric\u001b[24m is a \u001b[4mpre-metric\u001b[24m that is also symmetric, as\n",
       "\n",
       "\u001b[36m  d(x, y) == d(y, x)  for all x, y\u001b[39m\n",
       "\n",
       "  \u001b[1mMetric\u001b[22m is a abstract type that further refines \u001b[1mSemiMetric\u001b[22m. Formally, a\n",
       "  \u001b[4mmetric\u001b[24m is a \u001b[4msemi-metric\u001b[24m that also satisfies triangle inequality, as\n",
       "\n",
       "\u001b[36m  d(x, z) <= d(x, y) + d(y, z)  for all x, y, z\u001b[39m\n",
       "\n",
       "  This type system has practical significance. For example, when computing\n",
       "  pairwise distances between a set of vectors, you may only perform\n",
       "  computation for half of the pairs, derive the values immediately for the\n",
       "  remaining half by leveraging the symmetry of \u001b[4msemi-metrics\u001b[24m. Note that the\n",
       "  types of \u001b[36mSemiMetric\u001b[39m and \u001b[36mMetric\u001b[39m do not completely follow the definition in\n",
       "  mathematics as they do not require the \"distance\" to be able to distinguish\n",
       "  between points: for these types \u001b[36mx != y\u001b[39m does not imply that \u001b[36md(x, y) != 0\u001b[39m in\n",
       "  general compared to the mathematical definition of semi-metric and metric,\n",
       "  as this property does not change computations in practice.\n",
       "\n",
       "  Each distance corresponds to a distance type. The type name and the\n",
       "  corresponding mathematical definitions of the distances are listed in the\n",
       "  following table.\n",
       "\n",
       "          type name              convenient syntax                                                               math definition\n",
       "  ––––––––––––––––– –––––––––––––––––––––––––––––– –––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––––\n",
       "          Euclidean                \u001b[36meuclidean(x, y)\u001b[39m                                                       \u001b[36msqrt(sum((x - y) .^ 2))\u001b[39m\n",
       "        SqEuclidean              \u001b[36msqeuclidean(x, y)\u001b[39m                                                               \u001b[36msum((x - y).^2)\u001b[39m\n",
       "  PeriodicEuclidean            \u001b[36mpeuclidean(x, y, w)\u001b[39m                 \u001b[36msqrt(sum(min(mod(abs(x - y), w), w - mod(abs(x - y), w)).^2))\u001b[39m\n",
       "          Cityblock                \u001b[36mcityblock(x, y)\u001b[39m                                                               \u001b[36msum(abs(x - y))\u001b[39m\n",
       "     TotalVariation           \u001b[36mtotalvariation(x, y)\u001b[39m                                                           \u001b[36msum(abs(x - y)) / 2\u001b[39m\n",
       "          Chebyshev                \u001b[36mchebyshev(x, y)\u001b[39m                                                               \u001b[36mmax(abs(x - y))\u001b[39m\n",
       "          Minkowski             \u001b[36mminkowski(x, y, p)\u001b[39m                                                    \u001b[36msum(abs(x - y).^p) ^ (1/p)\u001b[39m\n",
       "            Hamming                  \u001b[36mhamming(k, l)\u001b[39m                                                                  \u001b[36msum(k .!= l)\u001b[39m\n",
       "     RogersTanimoto           \u001b[36mrogerstanimoto(a, b)\u001b[39m \u001b[36m2(sum(a&!b) + sum(!a&b)) / (2(sum(a&!b) + sum(!a&b)) + sum(a&b) + sum(!a&!b))\u001b[39m\n",
       "            Jaccard                  \u001b[36mjaccard(x, y)\u001b[39m                                           \u001b[36m1 - sum(min(x, y)) / sum(max(x, y))\u001b[39m\n",
       "         BrayCurtis               \u001b[36mbraycurtis(x, y)\u001b[39m                                             \u001b[36msum(abs(x - y)) / sum(abs(x + y))\u001b[39m\n",
       "         CosineDist              \u001b[36mcosine_dist(x, y)\u001b[39m                                           \u001b[36m1 - dot(x, y) / (norm(x) * norm(y))\u001b[39m\n",
       "           CorrDist                \u001b[36mcorr_dist(x, y)\u001b[39m                                         \u001b[36mcosine_dist(x - mean(x), y - mean(y))\u001b[39m\n",
       "          ChiSqDist               \u001b[36mchisq_dist(x, y)\u001b[39m                                                     \u001b[36msum((x - y).^2 / (x + y))\u001b[39m\n",
       "       KLDivergence            \u001b[36mkl_divergence(p, q)\u001b[39m                                                         \u001b[36msum(p .* log(p ./ q))\u001b[39m\n",
       "    GenKLDivergence           \u001b[36mgkl_divergence(x, y)\u001b[39m                                                 \u001b[36msum(p .* log(p ./ q) - p + q)\u001b[39m\n",
       "    RenyiDivergence      \u001b[36mrenyi_divergence(p, q, k)\u001b[39m                                 \u001b[36mlog(sum( p .* (p ./ q) .^ (k - 1))) / (k - 1)\u001b[39m\n",
       "       JSDivergence            \u001b[36mjs_divergence(p, q)\u001b[39m                              \u001b[36mKL(p, m) / 2 + KL(q, m) / 2 with m = (p + q) / 2\u001b[39m\n",
       "       SpanNormDist            \u001b[36mspannorm_dist(x, y)\u001b[39m                                                       \u001b[36mmax(x - y) - min(x - y)\u001b[39m\n",
       "  BhattacharyyaDist            \u001b[36mbhattacharyya(x, y)\u001b[39m                                \u001b[36m-log(sum(sqrt(x .* y) / sqrt(sum(x) * sum(y)))\u001b[39m\n",
       "      HellingerDist                \u001b[36mhellinger(x, y)\u001b[39m                           \u001b[36msqrt(1 - sum(sqrt(x .* y) / sqrt(sum(x) * sum(y))))\u001b[39m\n",
       "          Haversine \u001b[36mhaversine(x, y, r = 6_371_000)\u001b[39m           Haversine formula (https://en.wikipedia.org/wiki/Haversine_formula)\n",
       "     SphericalAngle          \u001b[36mspherical_angle(x, y)\u001b[39m           Haversine formula (https://en.wikipedia.org/wiki/Haversine_formula)\n",
       "        Mahalanobis           \u001b[36mmahalanobis(x, y, Q)\u001b[39m                                                  \u001b[36msqrt((x - y)' * Q * (x - y))\u001b[39m\n",
       "      SqMahalanobis         \u001b[36msqmahalanobis(x, y, Q)\u001b[39m                                                        \u001b[36m(x - y)' * Q * (x - y)\u001b[39m\n",
       "   MeanAbsDeviation                   \u001b[36mmeanad(x, y)\u001b[39m                                                             \u001b[36mmean(abs.(x - y))\u001b[39m\n",
       "\n",
       "  [output truncated to first 200 lines]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?Distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mahalanobis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-11T17:35:29.172000+08:00",
     "start_time": "2022-07-11T09:35:29.004Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "Mahalanobis(Q; skipchecks=false) <: Metric\n",
       "\\end{verbatim}\n",
       "Create a Mahalanobis distance (i.e., a bilinear form) with covariance matrix \\texttt{Q}. Upon construction, both symmetry/self-adjointness and positive semidefiniteness are checked, where the latter check can be skipped by passing the keyword argument \\texttt{skipchecks = true}.\n",
       "\n",
       "\\section{Example:}\n",
       "```julia julia> A = collect(reshape(1:9, 3, 3)); Q = A'A;\n",
       "\n",
       "julia> dist = Mahalanobis(Q) Mahalanobis\\{Matrix\\{Int64\\}\\}([14 32 50; 32 77 122; 50 122 194])\n",
       "\n",
       "julia> dist = Mahalanobis(A, skipchecks=true) ┌ Warning: matrix is not symmetric/Hermitian └ @ Distances ... Mahalanobis\\{Matrix\\{Int64\\}\\}([1 4 7; 2 5 8; 3 6 9])\n",
       "\n"
      ],
      "text/markdown": [
       "```\n",
       "Mahalanobis(Q; skipchecks=false) <: Metric\n",
       "```\n",
       "\n",
       "Create a Mahalanobis distance (i.e., a bilinear form) with covariance matrix `Q`. Upon construction, both symmetry/self-adjointness and positive semidefiniteness are checked, where the latter check can be skipped by passing the keyword argument `skipchecks = true`.\n",
       "\n",
       "# Example:\n",
       "\n",
       "```julia julia> A = collect(reshape(1:9, 3, 3)); Q = A'A;\n",
       "\n",
       "julia> dist = Mahalanobis(Q) Mahalanobis{Matrix{Int64}}([14 32 50; 32 77 122; 50 122 194])\n",
       "\n",
       "julia> dist = Mahalanobis(A, skipchecks=true) ┌ Warning: matrix is not symmetric/Hermitian └ @ Distances ... Mahalanobis{Matrix{Int64}}([1 4 7; 2 5 8; 3 6 9])\n"
      ],
      "text/plain": [
       "\u001b[36m  Mahalanobis(Q; skipchecks=false) <: Metric\u001b[39m\n",
       "\n",
       "  Create a Mahalanobis distance (i.e., a bilinear form) with covariance matrix\n",
       "  \u001b[36mQ\u001b[39m. Upon construction, both symmetry/self-adjointness and positive\n",
       "  semidefiniteness are checked, where the latter check can be skipped by\n",
       "  passing the keyword argument \u001b[36mskipchecks = true\u001b[39m.\n",
       "\n",
       "\u001b[1m  Example:\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "  ```julia julia> A = collect(reshape(1:9, 3, 3)); Q = A'A;\n",
       "\n",
       "  julia> dist = Mahalanobis(Q) Mahalanobis{Matrix{Int64}}([14 32 50; 32 77\n",
       "  122; 50 122 194])\n",
       "\n",
       "  julia> dist = Mahalanobis(A, skipchecks=true) ┌ Warning: matrix is not\n",
       "  symmetric/Hermitian └ @ Distances ... Mahalanobis{Matrix{Int64}}([1 4 7; 2 5\n",
       "  8; 3 6 9])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?Distances.Mahalanobis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-11T17:42:13.357000+08:00",
     "start_time": "2022-07-11T09:42:13.353Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mahalanobis{Matrix{Int64}}([14 32 50; 32 77 122; 50 122 194])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist = Mahalanobis([14 32 50; 32 77 122; 50 122 194])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-11T17:36:25.870000+08:00",
     "start_time": "2022-07-11T09:36:25.867Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mahalanobis{Matrix{Int64}}([14 32 50; 32 77 122; 50 122 194])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Euclidean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-12T16:05:53.048000+08:00",
     "start_time": "2022-07-12T08:05:52.150Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "pairwise(metric::PreMetric, a::AbstractMatrix, b::AbstractMatrix=a; dims)\n",
       "\\end{verbatim}\n",
       "Compute distances between each pair of rows (if \\texttt{dims=1}) or columns (if \\texttt{dims=2}) in \\texttt{a} and \\texttt{b} according to distance \\texttt{metric}. If a single matrix \\texttt{a} is provided, compute distances between its rows or columns.\n",
       "\n",
       "\\texttt{a} and \\texttt{b} must have the same numbers of columns if \\texttt{dims=1}, or of rows if \\texttt{dims=2}.\n",
       "\n",
       "\\rule{\\textwidth}{1pt}\n",
       "\\begin{verbatim}\n",
       "pairwise(metric::PreMetric, a, b=a)\n",
       "\\end{verbatim}\n",
       "Compute distances between each element of collection \\texttt{a} and each element of collection \\texttt{b} according to distance \\texttt{metric}. If a single iterable \\texttt{a} is provided, compute distances between its elements.\n",
       "\n"
      ],
      "text/markdown": [
       "```\n",
       "pairwise(metric::PreMetric, a::AbstractMatrix, b::AbstractMatrix=a; dims)\n",
       "```\n",
       "\n",
       "Compute distances between each pair of rows (if `dims=1`) or columns (if `dims=2`) in `a` and `b` according to distance `metric`. If a single matrix `a` is provided, compute distances between its rows or columns.\n",
       "\n",
       "`a` and `b` must have the same numbers of columns if `dims=1`, or of rows if `dims=2`.\n",
       "\n",
       "---\n",
       "\n",
       "```\n",
       "pairwise(metric::PreMetric, a, b=a)\n",
       "```\n",
       "\n",
       "Compute distances between each element of collection `a` and each element of collection `b` according to distance `metric`. If a single iterable `a` is provided, compute distances between its elements.\n"
      ],
      "text/plain": [
       "\u001b[36m  pairwise(metric::PreMetric, a::AbstractMatrix, b::AbstractMatrix=a; dims)\u001b[39m\n",
       "\n",
       "  Compute distances between each pair of rows (if \u001b[36mdims=1\u001b[39m) or columns (if\n",
       "  \u001b[36mdims=2\u001b[39m) in \u001b[36ma\u001b[39m and \u001b[36mb\u001b[39m according to distance \u001b[36mmetric\u001b[39m. If a single matrix \u001b[36ma\u001b[39m is\n",
       "  provided, compute distances between its rows or columns.\n",
       "\n",
       "  \u001b[36ma\u001b[39m and \u001b[36mb\u001b[39m must have the same numbers of columns if \u001b[36mdims=1\u001b[39m, or of rows if\n",
       "  \u001b[36mdims=2\u001b[39m.\n",
       "\n",
       "  ────────────────────────────────────────────────────────────────────────────\n",
       "\n",
       "\u001b[36m  pairwise(metric::PreMetric, a, b=a)\u001b[39m\n",
       "\n",
       "  Compute distances between each element of collection \u001b[36ma\u001b[39m and each element of\n",
       "  collection \u001b[36mb\u001b[39m according to distance \u001b[36mmetric\u001b[39m. If a single iterable \u001b[36ma\u001b[39m is\n",
       "  provided, compute distances between its elements."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?Distances.pairwise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIBSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-05T17:18:51.024000+08:00",
     "start_time": "2022-07-05T09:18:50.761Z"
    }
   },
   "outputs": [],
   "source": [
    "using LIBSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-10T14:42:20.585000+08:00",
     "start_time": "2022-06-10T06:42:19.211Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1mL\u001b[22m\u001b[0m\u001b[1mI\u001b[22m\u001b[0m\u001b[1mB\u001b[22m\u001b[0m\u001b[1mS\u001b[22m\u001b[0m\u001b[1mV\u001b[22m\u001b[0m\u001b[1mM\u001b[22m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "No docstring found for module \\texttt{LIBSVM}.\n",
       "\n",
       "\\section{Exported names}\n",
       "\\texttt{EpsilonSVR}, \\texttt{Kernel}, \\texttt{LinearSVC}, \\texttt{Linearsolver}, \\texttt{NuSVC}, \\texttt{NuSVR}, \\texttt{OneClassSVM}, \\texttt{SVC}, \\texttt{fit!}, \\texttt{predict}, \\texttt{svmpredict}, \\texttt{svmtrain}, \\texttt{transform}\n",
       "\n",
       "\\section{Displaying contents of readme found at \\texttt{D:{\\textbackslash}TongYuan{\\textbackslash}.julia{\\textbackslash}packages{\\textbackslash}LIBSVM{\\textbackslash}IZl0L{\\textbackslash}README.md}}\n",
       "\\section{LIBSVM.jl}\n",
       "\\href{https://github.com/JuliaML/LIBSVM.jl/actions?query=workflow%3ACI}{\\begin{figure}\n",
       "\\centering\n",
       "\\includegraphics{https://github.com/JuliaML/LIBSVM.jl/workflows/CI/badge.svg?branch=master}\n",
       "\\caption{Build Status}\n",
       "\\end{figure}\n",
       "} \\href{https://codecov.io/gh/JuliaML/LIBSVM.jl}{\\begin{figure}\n",
       "\\centering\n",
       "\\includegraphics{https://codecov.io/gh/JuliaML/LIBSVM.jl/branch/master/graph/badge.svg?token=bGwzyTtNPn}\n",
       "\\caption{codecov}\n",
       "\\end{figure}\n",
       "}\n",
       "\n",
       "This is a Julia interface for \\href{http://www.csie.ntu.edu.tw/~cjlin/libsvm/}{LIBSVM}.\n",
       "\n",
       "\\textbf{Features:}\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item Supports all LIBSVM models: classification C-SVC, nu-SVC, regression: epsilon-SVR, nu-SVR   and distribution estimation: one-class SVM\n",
       "\n",
       "\n",
       "\\item Model objects are represented by Julia type SVM which gives you easy access to model features and can be saved e.g. as JLD file\n",
       "\n",
       "\n",
       "\\item Supports ScikitLearn.jl API\n",
       "\n",
       "\\end{itemize}\n",
       "\\subsection{Usage}\n",
       "\\subsubsection{LIBSVM API}\n",
       "This provides a lower level API similar to LIBSVM C-interface. See \\texttt{?svmtrain} for options.\n",
       "\n",
       "\\begin{verbatim}\n",
       "using LIBSVM\n",
       "using RDatasets\n",
       "using Printf\n",
       "using Statistics\n",
       "\n",
       "# Load Fisher's classic iris data\n",
       "iris = dataset(\"datasets\", \"iris\")\n",
       "\n",
       "# First four dimension of input data is features\n",
       "X = Matrix(iris[:, 1:4])'\n",
       "\n",
       "# LIBSVM handles multi-class data automatically using a one-against-one strategy\n",
       "y = iris.Species\n",
       "\n",
       "# Split the dataset into training set and testing set\n",
       "Xtrain = X[:, 1:2:end]\n",
       "Xtest  = X[:, 2:2:end]\n",
       "ytrain = y[1:2:end]\n",
       "ytest  = y[2:2:end]\n",
       "\n",
       "# Train SVM on half of the data using default parameters. See documentation\n",
       "# of svmtrain for options\n",
       "model = svmtrain(Xtrain, ytrain)\n",
       "\n",
       "# Test model on the other half of the data.\n",
       "ŷ, decision_values = svmpredict(model, Xtest);\n",
       "\n",
       "# Compute accuracy\n",
       "@printf \"Accuracy: %.2f%%\\n\" mean(ŷ .== ytest) * 100\n",
       "\\end{verbatim}\n",
       "\\subsubsection{Precomputed kernel}\n",
       "It is possible to use different kernels than those that are provided. In such a case, it is required to provide a matrix filled with precomputed kernel values.\n",
       "\n",
       "For training, a symmetric matrix is expected:\n",
       "\n",
       "\\begin{verbatim}\n",
       "K = [k(x_1, x_1)  k(x_1, x_2)  ...  k(x_1, x_l);\n",
       "     k(x_2, x_1)\n",
       "         ...                            ...\n",
       "     k(x_l, x_1)        ...         k(x_l, x_l)]\n",
       "\\end{verbatim}\n",
       "where \\texttt{x\\_i} is \\texttt{i}-th training instance and \\texttt{l} is the number of training instances.\n",
       "\n",
       "To predict \\texttt{n} instances, a matrix of shape \\texttt{(l, n)} is expected:\n",
       "\n",
       "\\begin{verbatim}\n",
       "KK = [k(x_1, t_1)  k(x_1, t_2)  ...  k(x_1, t_n);\n",
       "      k(x_2, t_1)\n",
       "          ...                            ...\n",
       "      k(x_l, t_1)        ...         k(x_l, t_n)]\n",
       "\\end{verbatim}\n",
       "where \\texttt{t\\_i} is \\texttt{i}-th instance to be predicted.\n",
       "\n",
       "\\paragraph{Example}\n",
       "\\begin{verbatim}\n",
       "# Training data\n",
       "X = [-2 -1 -1 1 1 2;\n",
       "     -1 -1 -2 1 2 1]\n",
       "y = [1, 1, 1, 2, 2, 2]\n",
       "\n",
       "# Testing data\n",
       "T = [-1 2 3;\n",
       "     -1 2 2]\n",
       "\n",
       "# Precomputed matrix for training (corresponds to linear kernel)\n",
       "K = X' * X\n",
       "\n",
       "model = svmtrain(K, y, kernel=Kernel.Precomputed)\n",
       "\n",
       "# Precomputed matrix for prediction\n",
       "KK = X' * T\n",
       "\n",
       "ỹ, _ = svmpredict(model, KK)\n",
       "\\end{verbatim}\n",
       "\\subsubsection{ScikitLearn API}\n",
       "You can alternatively use \\texttt{ScikitLearn.jl} API with same options as \\texttt{svmtrain}:\n",
       "\n",
       "\\begin{verbatim}\n",
       "using LIBSVM\n",
       "using RDatasets\n",
       "\n",
       "# Classification C-SVM\n",
       "iris = dataset(\"datasets\", \"iris\")\n",
       "X = Matrix(iris[:, 1:4])\n",
       "y = iris.Species\n",
       "\n",
       "Xtrain = X[1:2:end, :]\n",
       "Xtest  = X[2:2:end, :]\n",
       "ytrain = y[1:2:end]\n",
       "ytest  = y[2:2:end]\n",
       "\n",
       "model = fit!(SVC(), Xtrain, ytrain)\n",
       "ŷ = predict(model, Xtest)\n",
       "\\end{verbatim}\n",
       "\\begin{verbatim}\n",
       "# Epsilon-Regression\n",
       "\n",
       "whiteside = RDatasets.dataset(\"MASS\", \"whiteside\")\n",
       "X = Matrix(whiteside[:, 3:3])  # the `Gas` column\n",
       "y = whiteside.Temp\n",
       "\n",
       "model = fit!(EpsilonSVR(cost = 10., gamma = 1.), X, y)\n",
       "ŷ = predict(model, X)\n",
       "\\end{verbatim}\n",
       "\\subsection{Credits}\n",
       "The library is currently developed and maintained by Matti Pastell. It was originally developed by Simon Kornblith.\n",
       "\n",
       "\\href{http://www.csie.ntu.edu.tw/~cjlin/libsvm/}{LIBSVM} by Chih-Chung Chang and Chih-Jen Lin\n",
       "\n"
      ],
      "text/markdown": [
       "No docstring found for module `LIBSVM`.\n",
       "\n",
       "# Exported names\n",
       "\n",
       "`EpsilonSVR`, `Kernel`, `LinearSVC`, `Linearsolver`, `NuSVC`, `NuSVR`, `OneClassSVM`, `SVC`, `fit!`, `predict`, `svmpredict`, `svmtrain`, `transform`\n",
       "\n",
       "# Displaying contents of readme found at `D:\\TongYuan\\.julia\\packages\\LIBSVM\\IZl0L\\README.md`\n",
       "\n",
       "# LIBSVM.jl\n",
       "\n",
       "[![Build Status](https://github.com/JuliaML/LIBSVM.jl/workflows/CI/badge.svg?branch=master)](https://github.com/JuliaML/LIBSVM.jl/actions?query=workflow%3ACI) [![codecov](https://codecov.io/gh/JuliaML/LIBSVM.jl/branch/master/graph/badge.svg?token=bGwzyTtNPn)](https://codecov.io/gh/JuliaML/LIBSVM.jl)\n",
       "\n",
       "This is a Julia interface for [LIBSVM](http://www.csie.ntu.edu.tw/~cjlin/libsvm/).\n",
       "\n",
       "**Features:**\n",
       "\n",
       "  * Supports all LIBSVM models: classification C-SVC, nu-SVC, regression: epsilon-SVR, nu-SVR   and distribution estimation: one-class SVM\n",
       "  * Model objects are represented by Julia type SVM which gives you easy access to model features and can be saved e.g. as JLD file\n",
       "  * Supports ScikitLearn.jl API\n",
       "\n",
       "## Usage\n",
       "\n",
       "### LIBSVM API\n",
       "\n",
       "This provides a lower level API similar to LIBSVM C-interface. See `?svmtrain` for options.\n",
       "\n",
       "```julia\n",
       "using LIBSVM\n",
       "using RDatasets\n",
       "using Printf\n",
       "using Statistics\n",
       "\n",
       "# Load Fisher's classic iris data\n",
       "iris = dataset(\"datasets\", \"iris\")\n",
       "\n",
       "# First four dimension of input data is features\n",
       "X = Matrix(iris[:, 1:4])'\n",
       "\n",
       "# LIBSVM handles multi-class data automatically using a one-against-one strategy\n",
       "y = iris.Species\n",
       "\n",
       "# Split the dataset into training set and testing set\n",
       "Xtrain = X[:, 1:2:end]\n",
       "Xtest  = X[:, 2:2:end]\n",
       "ytrain = y[1:2:end]\n",
       "ytest  = y[2:2:end]\n",
       "\n",
       "# Train SVM on half of the data using default parameters. See documentation\n",
       "# of svmtrain for options\n",
       "model = svmtrain(Xtrain, ytrain)\n",
       "\n",
       "# Test model on the other half of the data.\n",
       "ŷ, decision_values = svmpredict(model, Xtest);\n",
       "\n",
       "# Compute accuracy\n",
       "@printf \"Accuracy: %.2f%%\\n\" mean(ŷ .== ytest) * 100\n",
       "```\n",
       "\n",
       "### Precomputed kernel\n",
       "\n",
       "It is possible to use different kernels than those that are provided. In such a case, it is required to provide a matrix filled with precomputed kernel values.\n",
       "\n",
       "For training, a symmetric matrix is expected:\n",
       "\n",
       "```\n",
       "K = [k(x_1, x_1)  k(x_1, x_2)  ...  k(x_1, x_l);\n",
       "     k(x_2, x_1)\n",
       "         ...                            ...\n",
       "     k(x_l, x_1)        ...         k(x_l, x_l)]\n",
       "```\n",
       "\n",
       "where `x_i` is `i`-th training instance and `l` is the number of training instances.\n",
       "\n",
       "To predict `n` instances, a matrix of shape `(l, n)` is expected:\n",
       "\n",
       "```\n",
       "KK = [k(x_1, t_1)  k(x_1, t_2)  ...  k(x_1, t_n);\n",
       "      k(x_2, t_1)\n",
       "          ...                            ...\n",
       "      k(x_l, t_1)        ...         k(x_l, t_n)]\n",
       "```\n",
       "\n",
       "where `t_i` is `i`-th instance to be predicted.\n",
       "\n",
       "#### Example\n",
       "\n",
       "```julia\n",
       "# Training data\n",
       "X = [-2 -1 -1 1 1 2;\n",
       "     -1 -1 -2 1 2 1]\n",
       "y = [1, 1, 1, 2, 2, 2]\n",
       "\n",
       "# Testing data\n",
       "T = [-1 2 3;\n",
       "     -1 2 2]\n",
       "\n",
       "# Precomputed matrix for training (corresponds to linear kernel)\n",
       "K = X' * X\n",
       "\n",
       "model = svmtrain(K, y, kernel=Kernel.Precomputed)\n",
       "\n",
       "# Precomputed matrix for prediction\n",
       "KK = X' * T\n",
       "\n",
       "ỹ, _ = svmpredict(model, KK)\n",
       "```\n",
       "\n",
       "### ScikitLearn API\n",
       "\n",
       "You can alternatively use `ScikitLearn.jl` API with same options as `svmtrain`:\n",
       "\n",
       "```julia\n",
       "using LIBSVM\n",
       "using RDatasets\n",
       "\n",
       "# Classification C-SVM\n",
       "iris = dataset(\"datasets\", \"iris\")\n",
       "X = Matrix(iris[:, 1:4])\n",
       "y = iris.Species\n",
       "\n",
       "Xtrain = X[1:2:end, :]\n",
       "Xtest  = X[2:2:end, :]\n",
       "ytrain = y[1:2:end]\n",
       "ytest  = y[2:2:end]\n",
       "\n",
       "model = fit!(SVC(), Xtrain, ytrain)\n",
       "ŷ = predict(model, Xtest)\n",
       "```\n",
       "\n",
       "```julia\n",
       "# Epsilon-Regression\n",
       "\n",
       "whiteside = RDatasets.dataset(\"MASS\", \"whiteside\")\n",
       "X = Matrix(whiteside[:, 3:3])  # the `Gas` column\n",
       "y = whiteside.Temp\n",
       "\n",
       "model = fit!(EpsilonSVR(cost = 10., gamma = 1.), X, y)\n",
       "ŷ = predict(model, X)\n",
       "```\n",
       "\n",
       "## Credits\n",
       "\n",
       "The library is currently developed and maintained by Matti Pastell. It was originally developed by Simon Kornblith.\n",
       "\n",
       "[LIBSVM](http://www.csie.ntu.edu.tw/~cjlin/libsvm/) by Chih-Chung Chang and Chih-Jen Lin\n"
      ],
      "text/plain": [
       "  No docstring found for module \u001b[36mLIBSVM\u001b[39m.\n",
       "\n",
       "\u001b[1m  Exported names\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "  \u001b[36mEpsilonSVR\u001b[39m, \u001b[36mKernel\u001b[39m, \u001b[36mLinearSVC\u001b[39m, \u001b[36mLinearsolver\u001b[39m, \u001b[36mNuSVC\u001b[39m, \u001b[36mNuSVR\u001b[39m, \u001b[36mOneClassSVM\u001b[39m, \u001b[36mSVC\u001b[39m,\n",
       "  \u001b[36mfit!\u001b[39m, \u001b[36mpredict\u001b[39m, \u001b[36msvmpredict\u001b[39m, \u001b[36msvmtrain\u001b[39m, \u001b[36mtransform\u001b[39m\n",
       "\n",
       "\u001b[1m  Displaying contents of readme found at\u001b[22m\n",
       "\u001b[1m \u001b[36mD:\\TongYuan\\.julia\\packages\\LIBSVM\\IZl0L\\README.md\u001b[39m\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "\u001b[1m  LIBSVM.jl\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "  (Image: Build Status)\n",
       "  (https://github.com/JuliaML/LIBSVM.jl/actions?query=workflow%3ACI) (Image:\n",
       "  codecov) (https://codecov.io/gh/JuliaML/LIBSVM.jl)\n",
       "\n",
       "  This is a Julia interface for LIBSVM\n",
       "  (http://www.csie.ntu.edu.tw/~cjlin/libsvm/).\n",
       "\n",
       "  \u001b[1mFeatures:\u001b[22m\n",
       "\n",
       "    •  Supports all LIBSVM models: classification C-SVC, nu-SVC,\n",
       "       regression: epsilon-SVR, nu-SVR and distribution estimation:\n",
       "       one-class SVM\n",
       "\n",
       "    •  Model objects are represented by Julia type SVM which gives you\n",
       "       easy access to model features and can be saved e.g. as JLD file\n",
       "\n",
       "    •  Supports ScikitLearn.jl API\n",
       "\n",
       "\u001b[1m  Usage\u001b[22m\n",
       "\u001b[1m  =======\u001b[22m\n",
       "\n",
       "\u001b[1m  LIBSVM API\u001b[22m\n",
       "\u001b[1m  ––––––––––––\u001b[22m\n",
       "\n",
       "  This provides a lower level API similar to LIBSVM C-interface. See \u001b[36m?svmtrain\u001b[39m\n",
       "  for options.\n",
       "\n",
       "\u001b[36m  using LIBSVM\u001b[39m\n",
       "\u001b[36m  using RDatasets\u001b[39m\n",
       "\u001b[36m  using Printf\u001b[39m\n",
       "\u001b[36m  using Statistics\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  # Load Fisher's classic iris data\u001b[39m\n",
       "\u001b[36m  iris = dataset(\"datasets\", \"iris\")\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  # First four dimension of input data is features\u001b[39m\n",
       "\u001b[36m  X = Matrix(iris[:, 1:4])'\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  # LIBSVM handles multi-class data automatically using a one-against-one strategy\u001b[39m\n",
       "\u001b[36m  y = iris.Species\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  # Split the dataset into training set and testing set\u001b[39m\n",
       "\u001b[36m  Xtrain = X[:, 1:2:end]\u001b[39m\n",
       "\u001b[36m  Xtest  = X[:, 2:2:end]\u001b[39m\n",
       "\u001b[36m  ytrain = y[1:2:end]\u001b[39m\n",
       "\u001b[36m  ytest  = y[2:2:end]\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  # Train SVM on half of the data using default parameters. See documentation\u001b[39m\n",
       "\u001b[36m  # of svmtrain for options\u001b[39m\n",
       "\u001b[36m  model = svmtrain(Xtrain, ytrain)\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  # Test model on the other half of the data.\u001b[39m\n",
       "\u001b[36m  ŷ, decision_values = svmpredict(model, Xtest);\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  # Compute accuracy\u001b[39m\n",
       "\u001b[36m  @printf \"Accuracy: %.2f%%\\n\" mean(ŷ .== ytest) * 100\u001b[39m\n",
       "\n",
       "\u001b[1m  Precomputed kernel\u001b[22m\n",
       "\u001b[1m  ––––––––––––––––––––\u001b[22m\n",
       "\n",
       "  It is possible to use different kernels than those that are provided. In\n",
       "  such a case, it is required to provide a matrix filled with precomputed\n",
       "  kernel values.\n",
       "\n",
       "  For training, a symmetric matrix is expected:\n",
       "\n",
       "\u001b[36m  K = [k(x_1, x_1)  k(x_1, x_2)  ...  k(x_1, x_l);\u001b[39m\n",
       "\u001b[36m       k(x_2, x_1)\u001b[39m\n",
       "\u001b[36m           ...                            ...\u001b[39m\n",
       "\u001b[36m       k(x_l, x_1)        ...         k(x_l, x_l)]\u001b[39m\n",
       "\n",
       "  where \u001b[36mx_i\u001b[39m is \u001b[36mi\u001b[39m-th training instance and \u001b[36ml\u001b[39m is the number of training\n",
       "  instances.\n",
       "\n",
       "  To predict \u001b[36mn\u001b[39m instances, a matrix of shape \u001b[36m(l, n)\u001b[39m is expected:\n",
       "\n",
       "\u001b[36m  KK = [k(x_1, t_1)  k(x_1, t_2)  ...  k(x_1, t_n);\u001b[39m\n",
       "\u001b[36m        k(x_2, t_1)\u001b[39m\n",
       "\u001b[36m            ...                            ...\u001b[39m\n",
       "\u001b[36m        k(x_l, t_1)        ...         k(x_l, t_n)]\u001b[39m\n",
       "\n",
       "  where \u001b[36mt_i\u001b[39m is \u001b[36mi\u001b[39m-th instance to be predicted.\n",
       "\n",
       "\u001b[1m  Example\u001b[22m\n",
       "\u001b[1m  ---------\u001b[22m\n",
       "\n",
       "\u001b[36m  # Training data\u001b[39m\n",
       "\u001b[36m  X = [-2 -1 -1 1 1 2;\u001b[39m\n",
       "\u001b[36m       -1 -1 -2 1 2 1]\u001b[39m\n",
       "\u001b[36m  y = [1, 1, 1, 2, 2, 2]\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  # Testing data\u001b[39m\n",
       "\u001b[36m  T = [-1 2 3;\u001b[39m\n",
       "\u001b[36m       -1 2 2]\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  # Precomputed matrix for training (corresponds to linear kernel)\u001b[39m\n",
       "\u001b[36m  K = X' * X\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  model = svmtrain(K, y, kernel=Kernel.Precomputed)\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  # Precomputed matrix for prediction\u001b[39m\n",
       "\u001b[36m  KK = X' * T\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  ỹ, _ = svmpredict(model, KK)\u001b[39m\n",
       "\n",
       "\u001b[1m  ScikitLearn API\u001b[22m\n",
       "\u001b[1m  –––––––––––––––––\u001b[22m\n",
       "\n",
       "  You can alternatively use \u001b[36mScikitLearn.jl\u001b[39m API with same options as \u001b[36msvmtrain\u001b[39m:\n",
       "\n",
       "\u001b[36m  using LIBSVM\u001b[39m\n",
       "\u001b[36m  using RDatasets\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  # Classification C-SVM\u001b[39m\n",
       "\u001b[36m  iris = dataset(\"datasets\", \"iris\")\u001b[39m\n",
       "\u001b[36m  X = Matrix(iris[:, 1:4])\u001b[39m\n",
       "\u001b[36m  y = iris.Species\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  Xtrain = X[1:2:end, :]\u001b[39m\n",
       "\u001b[36m  Xtest  = X[2:2:end, :]\u001b[39m\n",
       "\u001b[36m  ytrain = y[1:2:end]\u001b[39m\n",
       "\u001b[36m  ytest  = y[2:2:end]\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  model = fit!(SVC(), Xtrain, ytrain)\u001b[39m\n",
       "\u001b[36m  ŷ = predict(model, Xtest)\u001b[39m\n",
       "\n",
       "\u001b[36m  # Epsilon-Regression\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  whiteside = RDatasets.dataset(\"MASS\", \"whiteside\")\u001b[39m\n",
       "\u001b[36m  X = Matrix(whiteside[:, 3:3])  # the `Gas` column\u001b[39m\n",
       "\u001b[36m  y = whiteside.Temp\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  model = fit!(EpsilonSVR(cost = 10., gamma = 1.), X, y)\u001b[39m\n",
       "\u001b[36m  ŷ = predict(model, X)\u001b[39m\n",
       "\n",
       "\u001b[1m  Credits\u001b[22m\n",
       "\u001b[1m  =========\u001b[22m\n",
       "\n",
       "  The library is currently developed and maintained by Matti Pastell. It was\n",
       "  originally developed by Simon Kornblith.\n",
       "\n",
       "  LIBSVM (http://www.csie.ntu.edu.tw/~cjlin/libsvm/) by Chih-Chung Chang and\n",
       "  Chih-Jen Lin"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?LIBSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-10T14:42:50.230000+08:00",
     "start_time": "2022-06-10T06:42:50.174Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1mL\u001b[22m\u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1mr\u001b[22m\u001b[0m\u001b[1mS\u001b[22m\u001b[0m\u001b[1mV\u001b[22m\u001b[0m\u001b[1mC\u001b[22m \u001b[0m\u001b[1mL\u001b[22m\u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1mr\u001b[22m\u001b[0m\u001b[1ms\u001b[22mol\u001b[0m\u001b[1mv\u001b[22mer \u001b[0m\u001b[1mL\u001b[22m\u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1mr\u001b[22mIndice\u001b[0m\u001b[1ms\u001b[22m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "Linear SVM using LIBLINEAR\n",
       "\n"
      ],
      "text/markdown": [
       "Linear SVM using LIBLINEAR\n"
      ],
      "text/plain": [
       "  Linear SVM using LIBLINEAR"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-10T15:02:00.714000+08:00",
     "start_time": "2022-06-10T07:02:00.640Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1mp\u001b[22m\u001b[0m\u001b[1mr\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1md\u001b[22m\u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1mc\u001b[22m\u001b[0m\u001b[1mt\u001b[22m \u001b[0m\u001b[1mp\u001b[22m\u001b[0m\u001b[1mr\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1md\u001b[22m\u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1mc\u001b[22m\u001b[0m\u001b[1mt\u001b[22m_dist \u001b[0m\u001b[1mp\u001b[22m\u001b[0m\u001b[1mr\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1md\u001b[22m\u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1mc\u001b[22m\u001b[0m\u001b[1mt\u001b[22m_proba \u001b[0m\u001b[1mp\u001b[22m\u001b[0m\u001b[1mr\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1md\u001b[22m\u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1mc\u001b[22m\u001b[0m\u001b[1mt\u001b[22m_log_proba svm\u001b[0m\u001b[1mp\u001b[22m\u001b[0m\u001b[1mr\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1md\u001b[22m\u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1mc\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "No documentation found.\n",
       "\n",
       "\\texttt{ScikitLearnBase.predict} is a \\texttt{Function}.\n",
       "\n",
       "\\begin{verbatim}\n",
       "# 2 methods for generic function \"predict\":\n",
       "[1] predict(model::LinearSVC, X::AbstractArray) in LIBSVM at D:\\TongYuan\\.julia\\packages\\LIBSVM\\IZl0L\\src\\ScikitLearnAPI.jl:11\n",
       "[2] predict(model::Union{LIBSVM.AbstractSVC, LIBSVM.AbstractSVR}, X::AbstractArray) in LIBSVM at D:\\TongYuan\\.julia\\packages\\LIBSVM\\IZl0L\\src\\ScikitLearnAPI.jl:6\n",
       "\\end{verbatim}\n"
      ],
      "text/markdown": [
       "No documentation found.\n",
       "\n",
       "`ScikitLearnBase.predict` is a `Function`.\n",
       "\n",
       "```\n",
       "# 2 methods for generic function \"predict\":\n",
       "[1] predict(model::LinearSVC, X::AbstractArray) in LIBSVM at D:\\TongYuan\\.julia\\packages\\LIBSVM\\IZl0L\\src\\ScikitLearnAPI.jl:11\n",
       "[2] predict(model::Union{LIBSVM.AbstractSVC, LIBSVM.AbstractSVR}, X::AbstractArray) in LIBSVM at D:\\TongYuan\\.julia\\packages\\LIBSVM\\IZl0L\\src\\ScikitLearnAPI.jl:6\n",
       "```\n"
      ],
      "text/plain": [
       "  No documentation found.\n",
       "\n",
       "  \u001b[36mScikitLearnBase.predict\u001b[39m is a \u001b[36mFunction\u001b[39m.\n",
       "\n",
       "\u001b[36m  # 2 methods for generic function \"predict\":\u001b[39m\n",
       "\u001b[36m  [1] predict(model::LinearSVC, X::AbstractArray) in LIBSVM at D:\\TongYuan\\.julia\\packages\\LIBSVM\\IZl0L\\src\\ScikitLearnAPI.jl:11\u001b[39m\n",
       "\u001b[36m  [2] predict(model::Union{LIBSVM.AbstractSVC, LIBSVM.AbstractSVR}, X::AbstractArray) in LIBSVM at D:\\TongYuan\\.julia\\packages\\LIBSVM\\IZl0L\\src\\ScikitLearnAPI.jl:6\u001b[39m"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ScikitLearnBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-05T17:19:07.643000+08:00",
     "start_time": "2022-07-05T09:19:07.641Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "using ScikitLearnBase #这个包开放了 scikit-learn 接口"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-10T15:31:23.627000+08:00",
     "start_time": "2022-06-10T07:31:23.523Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1mS\u001b[22m\u001b[0m\u001b[1mc\u001b[22m\u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1mk\u001b[22m\u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1mL\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1mr\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1mB\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1me\u001b[22m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "No docstring found for module \\texttt{ScikitLearnBase}.\n",
       "\n",
       "\\section{Exported names}\n",
       "\\texttt{@declare\\_hyperparameters}, \\texttt{BaseClassifier}, \\texttt{BaseEstimator}, \\texttt{BaseRegressor}, \\texttt{clone}, \\texttt{decision\\_function}, \\texttt{declare\\_hyperparameters}, \\texttt{fit!}, \\texttt{fit\\_predict!}, \\texttt{fit\\_transform!}, \\texttt{get\\_classes}, \\texttt{get\\_components}, \\texttt{get\\_feature\\_names}, \\texttt{get\\_params}, \\texttt{inverse\\_transform}, \\texttt{is\\_classifier}, \\texttt{is\\_pairwise}, \\texttt{partial\\_fit!}, \\texttt{predict}, \\texttt{predict\\_dist}, \\texttt{predict\\_log\\_proba}, \\texttt{predict\\_proba}, \\texttt{sample}, \\texttt{score}, \\texttt{score\\_samples}, \\texttt{set\\_params!}, \\texttt{transform}\n",
       "\n",
       "\\section{Displaying contents of readme found at \\texttt{D:{\\textbackslash}TongYuan{\\textbackslash}.julia{\\textbackslash}packages{\\textbackslash}ScikitLearnBase{\\textbackslash}QnTv8{\\textbackslash}README.md}}\n",
       "\\subsection{ScikitLearnBase.jl}\n",
       "This package exposes the scikit-learn interface. Packages that implement this interface can be used in conjunction with \\href{https://github.com/cstjean/ScikitLearn.jl}{ScikitLearn.jl} (pipelines, cross-validation, hyperparameter tuning, ...)\n",
       "\n",
       "This is an intentionally slim package ({\\textasciitilde}100 LOC, no dependencies). That way, ML libraries can \\texttt{import ScikitLearnBase} without dragging along all of \\texttt{ScikitLearn}'s dependencies.\n",
       "\n",
       "\\subsection{Overview}\n",
       "The docs contain \\href{http://scikitlearnjl.readthedocs.org/en/latest/api/}{an overview of the API} and a \\href{docs/API.md}{more thorough specification}.\n",
       "\n",
       "There are two implementation strategies for an existing machine learning package:\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item \\emph{Create a new type that wraps the existing type}. The new type can usually be written entirely on top of the existing codebase (i.e. without modifying it). This gives more implementation freedom, and a more consistent interface amongst the various ScikitLearn.jl models. Here's an \\href{https://github.com/cstjean/DecisionTree.jl/blob/2722950c8f0c5e5c62204364308e28d4123383cb/src/scikitlearnAPI.jl}{example} from DecisionTree.jl\n",
       "\n",
       "\n",
       "\\item \\emph{Use the existing type}. This requires less code, and is usually better when the model type already contains the hyperparameters / fitting arguments.\n",
       "\n",
       "\\end{itemize}\n",
       "\\subsection{Example}\n",
       "For models with simple hyperparameters, it boils down to this:\n",
       "\n",
       "\\begin{verbatim}\n",
       "import ScikitLearnBase\n",
       "\n",
       "type NaiveBayes\n",
       "    # The model hyperparameters (not learned from data)\n",
       "    bias::Float64\n",
       "\n",
       "    # The parameters learned from data\n",
       "    counts::Matrix{Int}\n",
       "    \n",
       "    # A constructor that accepts the hyperparameters as keyword arguments\n",
       "    # with sensible defaults\n",
       "    NaiveBayes(; bias=0.0f0) = new(bias)\n",
       "end\n",
       "\n",
       "# This will define `clone`, `set_params!` and `get_params` for the model\n",
       "ScikitLearnBase.@declare_hyperparameters(NaiveBayes, [:bias])\n",
       "\n",
       "# NaiveBayes is a classifier\n",
       "ScikitLearnBase.is_classifier(::NaiveBayes) = true   # not required for transformers\n",
       "\n",
       "function ScikitLearnBase.fit!(model::NaiveBayes, X, y)\n",
       "    # X should be of size (n_sample, n_feature)\n",
       "    .... # modify model.counts here\n",
       "    return model\n",
       "end\n",
       "\n",
       "function ScikitLearnBase.predict(model::NaiveBayes, X)\n",
       "    .... # returns a vector of predicted classes here\n",
       "end\n",
       "\\end{verbatim}\n",
       "Models with more complex hyperparameter specifications should implement \\texttt{clone}, \\texttt{get\\_params} and \\texttt{set\\_params!} explicitly instead of using \\texttt{@declare\\_hyperparameters}. \n",
       "\n",
       "More examples of PRs that implement the interface: \\href{https://github.com/davidavdav/GaussianMixtures.jl/pull/18/files}{GaussianMixtures.jl}, \\href{https://github.com/STOR-i/GaussianProcesses.jl/pull/17/files}{GaussianProcesses.jl}, \\href{https://github.com/bensadeghi/DecisionTree.jl/pull/29/files}{DecisionTree.jl}, \\href{https://github.com/madeleineudell/LowRankModels.jl/pull/56/files}{LowRankModels.jl}\n",
       "\n",
       "Note: if the model performs unsupervised learning, implement \\texttt{transform} instead of \\texttt{predict}.\n",
       "\n",
       "Once your library implements the API, \\href{https://github.com/cstjean/ScikitLearn.jl/issues}{file an issue/PR} to add it to the \\href{http://scikitlearnjl.readthedocs.io/en/latest/models/#julia-models}{list of models}.\n",
       "\n"
      ],
      "text/markdown": [
       "No docstring found for module `ScikitLearnBase`.\n",
       "\n",
       "# Exported names\n",
       "\n",
       "`@declare_hyperparameters`, `BaseClassifier`, `BaseEstimator`, `BaseRegressor`, `clone`, `decision_function`, `declare_hyperparameters`, `fit!`, `fit_predict!`, `fit_transform!`, `get_classes`, `get_components`, `get_feature_names`, `get_params`, `inverse_transform`, `is_classifier`, `is_pairwise`, `partial_fit!`, `predict`, `predict_dist`, `predict_log_proba`, `predict_proba`, `sample`, `score`, `score_samples`, `set_params!`, `transform`\n",
       "\n",
       "# Displaying contents of readme found at `D:\\TongYuan\\.julia\\packages\\ScikitLearnBase\\QnTv8\\README.md`\n",
       "\n",
       "## ScikitLearnBase.jl\n",
       "\n",
       "This package exposes the scikit-learn interface. Packages that implement this interface can be used in conjunction with [ScikitLearn.jl](https://github.com/cstjean/ScikitLearn.jl) (pipelines, cross-validation, hyperparameter tuning, ...)\n",
       "\n",
       "This is an intentionally slim package (~100 LOC, no dependencies). That way, ML libraries can `import ScikitLearnBase` without dragging along all of `ScikitLearn`'s dependencies.\n",
       "\n",
       "## Overview\n",
       "\n",
       "The docs contain [an overview of the API](http://scikitlearnjl.readthedocs.org/en/latest/api/) and a [more thorough specification](docs/API.md).\n",
       "\n",
       "There are two implementation strategies for an existing machine learning package:\n",
       "\n",
       "  * *Create a new type that wraps the existing type*. The new type can usually be written entirely on top of the existing codebase (i.e. without modifying it). This gives more implementation freedom, and a more consistent interface amongst the various ScikitLearn.jl models. Here's an [example](https://github.com/cstjean/DecisionTree.jl/blob/2722950c8f0c5e5c62204364308e28d4123383cb/src/scikitlearnAPI.jl) from DecisionTree.jl\n",
       "  * *Use the existing type*. This requires less code, and is usually better when the model type already contains the hyperparameters / fitting arguments.\n",
       "\n",
       "## Example\n",
       "\n",
       "For models with simple hyperparameters, it boils down to this:\n",
       "\n",
       "```julia\n",
       "import ScikitLearnBase\n",
       "\n",
       "type NaiveBayes\n",
       "    # The model hyperparameters (not learned from data)\n",
       "    bias::Float64\n",
       "\n",
       "    # The parameters learned from data\n",
       "    counts::Matrix{Int}\n",
       "    \n",
       "    # A constructor that accepts the hyperparameters as keyword arguments\n",
       "    # with sensible defaults\n",
       "    NaiveBayes(; bias=0.0f0) = new(bias)\n",
       "end\n",
       "\n",
       "# This will define `clone`, `set_params!` and `get_params` for the model\n",
       "ScikitLearnBase.@declare_hyperparameters(NaiveBayes, [:bias])\n",
       "\n",
       "# NaiveBayes is a classifier\n",
       "ScikitLearnBase.is_classifier(::NaiveBayes) = true   # not required for transformers\n",
       "\n",
       "function ScikitLearnBase.fit!(model::NaiveBayes, X, y)\n",
       "    # X should be of size (n_sample, n_feature)\n",
       "    .... # modify model.counts here\n",
       "    return model\n",
       "end\n",
       "\n",
       "function ScikitLearnBase.predict(model::NaiveBayes, X)\n",
       "    .... # returns a vector of predicted classes here\n",
       "end\n",
       "```\n",
       "\n",
       "Models with more complex hyperparameter specifications should implement `clone`, `get_params` and `set_params!` explicitly instead of using `@declare_hyperparameters`. \n",
       "\n",
       "More examples of PRs that implement the interface: [GaussianMixtures.jl](https://github.com/davidavdav/GaussianMixtures.jl/pull/18/files), [GaussianProcesses.jl](https://github.com/STOR-i/GaussianProcesses.jl/pull/17/files), [DecisionTree.jl](https://github.com/bensadeghi/DecisionTree.jl/pull/29/files), [LowRankModels.jl](https://github.com/madeleineudell/LowRankModels.jl/pull/56/files)\n",
       "\n",
       "Note: if the model performs unsupervised learning, implement `transform` instead of `predict`.\n",
       "\n",
       "Once your library implements the API, [file an issue/PR](https://github.com/cstjean/ScikitLearn.jl/issues) to add it to the [list of models](http://scikitlearnjl.readthedocs.io/en/latest/models/#julia-models).\n"
      ],
      "text/plain": [
       "  No docstring found for module \u001b[36mScikitLearnBase\u001b[39m.\n",
       "\n",
       "\u001b[1m  Exported names\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "  \u001b[36m@declare_hyperparameters\u001b[39m, \u001b[36mBaseClassifier\u001b[39m, \u001b[36mBaseEstimator\u001b[39m, \u001b[36mBaseRegressor\u001b[39m,\n",
       "  \u001b[36mclone\u001b[39m, \u001b[36mdecision_function\u001b[39m, \u001b[36mdeclare_hyperparameters\u001b[39m, \u001b[36mfit!\u001b[39m, \u001b[36mfit_predict!\u001b[39m,\n",
       "  \u001b[36mfit_transform!\u001b[39m, \u001b[36mget_classes\u001b[39m, \u001b[36mget_components\u001b[39m, \u001b[36mget_feature_names\u001b[39m, \u001b[36mget_params\u001b[39m,\n",
       "  \u001b[36minverse_transform\u001b[39m, \u001b[36mis_classifier\u001b[39m, \u001b[36mis_pairwise\u001b[39m, \u001b[36mpartial_fit!\u001b[39m, \u001b[36mpredict\u001b[39m,\n",
       "  \u001b[36mpredict_dist\u001b[39m, \u001b[36mpredict_log_proba\u001b[39m, \u001b[36mpredict_proba\u001b[39m, \u001b[36msample\u001b[39m, \u001b[36mscore\u001b[39m,\n",
       "  \u001b[36mscore_samples\u001b[39m, \u001b[36mset_params!\u001b[39m, \u001b[36mtransform\u001b[39m\n",
       "\n",
       "\u001b[1m  Displaying contents of readme found at\u001b[22m\n",
       "\u001b[1m \u001b[36mD:\\TongYuan\\.julia\\packages\\ScikitLearnBase\\QnTv8\\README.md\u001b[39m\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "\u001b[1m  ScikitLearnBase.jl\u001b[22m\n",
       "\u001b[1m  ====================\u001b[22m\n",
       "\n",
       "  This package exposes the scikit-learn interface. Packages that implement\n",
       "  this interface can be used in conjunction with ScikitLearn.jl\n",
       "  (https://github.com/cstjean/ScikitLearn.jl) (pipelines, cross-validation,\n",
       "  hyperparameter tuning, ...)\n",
       "\n",
       "  This is an intentionally slim package (~100 LOC, no dependencies). That way,\n",
       "  ML libraries can \u001b[36mimport ScikitLearnBase\u001b[39m without dragging along all of\n",
       "  \u001b[36mScikitLearn\u001b[39m's dependencies.\n",
       "\n",
       "\u001b[1m  Overview\u001b[22m\n",
       "\u001b[1m  ==========\u001b[22m\n",
       "\n",
       "  The docs contain an overview of the API\n",
       "  (http://scikitlearnjl.readthedocs.org/en/latest/api/) and a more thorough\n",
       "  specification (docs/API.md).\n",
       "\n",
       "  There are two implementation strategies for an existing machine learning\n",
       "  package:\n",
       "\n",
       "    •  \u001b[4mCreate a new type that wraps the existing type\u001b[24m. The new type can\n",
       "       usually be written entirely on top of the existing codebase (i.e.\n",
       "       without modifying it). This gives more implementation freedom, and\n",
       "       a more consistent interface amongst the various ScikitLearn.jl\n",
       "       models. Here's an example\n",
       "       (https://github.com/cstjean/DecisionTree.jl/blob/2722950c8f0c5e5c62204364308e28d4123383cb/src/scikitlearnAPI.jl)\n",
       "       from DecisionTree.jl\n",
       "\n",
       "    •  \u001b[4mUse the existing type\u001b[24m. This requires less code, and is usually\n",
       "       better when the model type already contains the hyperparameters /\n",
       "       fitting arguments.\n",
       "\n",
       "\u001b[1m  Example\u001b[22m\n",
       "\u001b[1m  =========\u001b[22m\n",
       "\n",
       "  For models with simple hyperparameters, it boils down to this:\n",
       "\n",
       "\u001b[36m  import ScikitLearnBase\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  type NaiveBayes\u001b[39m\n",
       "\u001b[36m      # The model hyperparameters (not learned from data)\u001b[39m\n",
       "\u001b[36m      bias::Float64\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m      # The parameters learned from data\u001b[39m\n",
       "\u001b[36m      counts::Matrix{Int}\u001b[39m\n",
       "\u001b[36m      \u001b[39m\n",
       "\u001b[36m      # A constructor that accepts the hyperparameters as keyword arguments\u001b[39m\n",
       "\u001b[36m      # with sensible defaults\u001b[39m\n",
       "\u001b[36m      NaiveBayes(; bias=0.0f0) = new(bias)\u001b[39m\n",
       "\u001b[36m  end\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  # This will define `clone`, `set_params!` and `get_params` for the model\u001b[39m\n",
       "\u001b[36m  ScikitLearnBase.@declare_hyperparameters(NaiveBayes, [:bias])\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  # NaiveBayes is a classifier\u001b[39m\n",
       "\u001b[36m  ScikitLearnBase.is_classifier(::NaiveBayes) = true   # not required for transformers\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  function ScikitLearnBase.fit!(model::NaiveBayes, X, y)\u001b[39m\n",
       "\u001b[36m      # X should be of size (n_sample, n_feature)\u001b[39m\n",
       "\u001b[36m      .... # modify model.counts here\u001b[39m\n",
       "\u001b[36m      return model\u001b[39m\n",
       "\u001b[36m  end\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  function ScikitLearnBase.predict(model::NaiveBayes, X)\u001b[39m\n",
       "\u001b[36m      .... # returns a vector of predicted classes here\u001b[39m\n",
       "\u001b[36m  end\u001b[39m\n",
       "\n",
       "  Models with more complex hyperparameter specifications should implement\n",
       "  \u001b[36mclone\u001b[39m, \u001b[36mget_params\u001b[39m and \u001b[36mset_params!\u001b[39m explicitly instead of using\n",
       "  \u001b[36m@declare_hyperparameters\u001b[39m.\n",
       "\n",
       "  More examples of PRs that implement the interface: GaussianMixtures.jl\n",
       "  (https://github.com/davidavdav/GaussianMixtures.jl/pull/18/files),\n",
       "  GaussianProcesses.jl\n",
       "  (https://github.com/STOR-i/GaussianProcesses.jl/pull/17/files),\n",
       "  DecisionTree.jl\n",
       "  (https://github.com/bensadeghi/DecisionTree.jl/pull/29/files),\n",
       "  LowRankModels.jl\n",
       "  (https://github.com/madeleineudell/LowRankModels.jl/pull/56/files)\n",
       "\n",
       "  Note: if the model performs unsupervised learning, implement \u001b[36mtransform\u001b[39m\n",
       "  instead of \u001b[36mpredict\u001b[39m.\n",
       "\n",
       "  Once your library implements the API, file an issue/PR\n",
       "  (https://github.com/cstjean/ScikitLearn.jl/issues) to add it to the list of\n",
       "  models (http://scikitlearnjl.readthedocs.io/en/latest/models/#julia-models)."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?ScikitLearnBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-10T15:30:30.171000+08:00",
     "start_time": "2022-06-10T07:30:30.163Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "No documentation found.\n",
       "\n",
       "\\texttt{ScikitLearnBase.fit!} is a \\texttt{Function}.\n",
       "\n",
       "\\begin{verbatim}\n",
       "# 8 methods for generic function \"fit!\":\n",
       "[1] fit!(model::LinearSVC, X::AbstractMatrix, y::AbstractVector) in LIBSVM at D:\\TongYuan\\.julia\\packages\\LIBSVM\\IZl0L\\src\\ScikitLearnAPI.jl:100\n",
       "[2] fit!(model::Union{LIBSVM.AbstractSVC, LIBSVM.AbstractSVR}, X::AbstractMatrix) in LIBSVM at D:\\TongYuan\\.julia\\packages\\LIBSVM\\IZl0L\\src\\ScikitLearnAPI.jl:68\n",
       "[3] fit!(model::Union{LIBSVM.AbstractSVC, LIBSVM.AbstractSVR}, X::AbstractMatrix, y::AbstractVector) in LIBSVM at D:\\TongYuan\\.julia\\packages\\LIBSVM\\IZl0L\\src\\ScikitLearnAPI.jl:68\n",
       "[4] fit!(rf::RandomForestRegressor, X::AbstractMatrix, y::AbstractVector) in DecisionTree at D:\\TongYuan\\.julia\\packages\\DecisionTree\\y3ehj\\src\\scikitlearnAPI.jl:300\n",
       "[5] fit!(rf::RandomForestClassifier, X::AbstractMatrix, y::AbstractVector) in DecisionTree at D:\\TongYuan\\.julia\\packages\\DecisionTree\\y3ehj\\src\\scikitlearnAPI.jl:216\n",
       "[6] fit!(dt::DecisionTreeRegressor, X::AbstractMatrix, y::AbstractVector) in DecisionTree at D:\\TongYuan\\.julia\\packages\\DecisionTree\\y3ehj\\src\\scikitlearnAPI.jl:139\n",
       "[7] fit!(ada::AdaBoostStumpClassifier, X, y) in DecisionTree at D:\\TongYuan\\.julia\\packages\\DecisionTree\\y3ehj\\src\\scikitlearnAPI.jl:359\n",
       "[8] fit!(dt::DecisionTreeClassifier, X, y) in DecisionTree at D:\\TongYuan\\.julia\\packages\\DecisionTree\\y3ehj\\src\\scikitlearnAPI.jl:52\n",
       "\\end{verbatim}\n"
      ],
      "text/markdown": [
       "No documentation found.\n",
       "\n",
       "`ScikitLearnBase.fit!` is a `Function`.\n",
       "\n",
       "```\n",
       "# 8 methods for generic function \"fit!\":\n",
       "[1] fit!(model::LinearSVC, X::AbstractMatrix, y::AbstractVector) in LIBSVM at D:\\TongYuan\\.julia\\packages\\LIBSVM\\IZl0L\\src\\ScikitLearnAPI.jl:100\n",
       "[2] fit!(model::Union{LIBSVM.AbstractSVC, LIBSVM.AbstractSVR}, X::AbstractMatrix) in LIBSVM at D:\\TongYuan\\.julia\\packages\\LIBSVM\\IZl0L\\src\\ScikitLearnAPI.jl:68\n",
       "[3] fit!(model::Union{LIBSVM.AbstractSVC, LIBSVM.AbstractSVR}, X::AbstractMatrix, y::AbstractVector) in LIBSVM at D:\\TongYuan\\.julia\\packages\\LIBSVM\\IZl0L\\src\\ScikitLearnAPI.jl:68\n",
       "[4] fit!(rf::RandomForestRegressor, X::AbstractMatrix, y::AbstractVector) in DecisionTree at D:\\TongYuan\\.julia\\packages\\DecisionTree\\y3ehj\\src\\scikitlearnAPI.jl:300\n",
       "[5] fit!(rf::RandomForestClassifier, X::AbstractMatrix, y::AbstractVector) in DecisionTree at D:\\TongYuan\\.julia\\packages\\DecisionTree\\y3ehj\\src\\scikitlearnAPI.jl:216\n",
       "[6] fit!(dt::DecisionTreeRegressor, X::AbstractMatrix, y::AbstractVector) in DecisionTree at D:\\TongYuan\\.julia\\packages\\DecisionTree\\y3ehj\\src\\scikitlearnAPI.jl:139\n",
       "[7] fit!(ada::AdaBoostStumpClassifier, X, y) in DecisionTree at D:\\TongYuan\\.julia\\packages\\DecisionTree\\y3ehj\\src\\scikitlearnAPI.jl:359\n",
       "[8] fit!(dt::DecisionTreeClassifier, X, y) in DecisionTree at D:\\TongYuan\\.julia\\packages\\DecisionTree\\y3ehj\\src\\scikitlearnAPI.jl:52\n",
       "```\n"
      ],
      "text/plain": [
       "  No documentation found.\n",
       "\n",
       "  \u001b[36mScikitLearnBase.fit!\u001b[39m is a \u001b[36mFunction\u001b[39m.\n",
       "\n",
       "\u001b[36m  # 8 methods for generic function \"fit!\":\u001b[39m\n",
       "\u001b[36m  [1] fit!(model::LinearSVC, X::AbstractMatrix, y::AbstractVector) in LIBSVM at D:\\TongYuan\\.julia\\packages\\LIBSVM\\IZl0L\\src\\ScikitLearnAPI.jl:100\u001b[39m\n",
       "\u001b[36m  [2] fit!(model::Union{LIBSVM.AbstractSVC, LIBSVM.AbstractSVR}, X::AbstractMatrix) in LIBSVM at D:\\TongYuan\\.julia\\packages\\LIBSVM\\IZl0L\\src\\ScikitLearnAPI.jl:68\u001b[39m\n",
       "\u001b[36m  [3] fit!(model::Union{LIBSVM.AbstractSVC, LIBSVM.AbstractSVR}, X::AbstractMatrix, y::AbstractVector) in LIBSVM at D:\\TongYuan\\.julia\\packages\\LIBSVM\\IZl0L\\src\\ScikitLearnAPI.jl:68\u001b[39m\n",
       "\u001b[36m  [4] fit!(rf::RandomForestRegressor, X::AbstractMatrix, y::AbstractVector) in DecisionTree at D:\\TongYuan\\.julia\\packages\\DecisionTree\\y3ehj\\src\\scikitlearnAPI.jl:300\u001b[39m\n",
       "\u001b[36m  [5] fit!(rf::RandomForestClassifier, X::AbstractMatrix, y::AbstractVector) in DecisionTree at D:\\TongYuan\\.julia\\packages\\DecisionTree\\y3ehj\\src\\scikitlearnAPI.jl:216\u001b[39m\n",
       "\u001b[36m  [6] fit!(dt::DecisionTreeRegressor, X::AbstractMatrix, y::AbstractVector) in DecisionTree at D:\\TongYuan\\.julia\\packages\\DecisionTree\\y3ehj\\src\\scikitlearnAPI.jl:139\u001b[39m\n",
       "\u001b[36m  [7] fit!(ada::AdaBoostStumpClassifier, X, y) in DecisionTree at D:\\TongYuan\\.julia\\packages\\DecisionTree\\y3ehj\\src\\scikitlearnAPI.jl:359\u001b[39m\n",
       "\u001b[36m  [8] fit!(dt::DecisionTreeClassifier, X, y) in DecisionTree at D:\\TongYuan\\.julia\\packages\\DecisionTree\\y3ehj\\src\\scikitlearnAPI.jl:52\u001b[39m"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?ScikitLearnBase.fit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-10T15:02:36.073000+08:00",
     "start_time": "2022-06-10T07:02:36.070Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "No documentation found.\n",
       "\n",
       "\\texttt{ScikitLearnBase.predict} is a \\texttt{Function}.\n",
       "\n",
       "\\begin{verbatim}\n",
       "# 2 methods for generic function \"predict\":\n",
       "[1] predict(model::LinearSVC, X::AbstractArray) in LIBSVM at D:\\TongYuan\\.julia\\packages\\LIBSVM\\IZl0L\\src\\ScikitLearnAPI.jl:11\n",
       "[2] predict(model::Union{LIBSVM.AbstractSVC, LIBSVM.AbstractSVR}, X::AbstractArray) in LIBSVM at D:\\TongYuan\\.julia\\packages\\LIBSVM\\IZl0L\\src\\ScikitLearnAPI.jl:6\n",
       "\\end{verbatim}\n"
      ],
      "text/markdown": [
       "No documentation found.\n",
       "\n",
       "`ScikitLearnBase.predict` is a `Function`.\n",
       "\n",
       "```\n",
       "# 2 methods for generic function \"predict\":\n",
       "[1] predict(model::LinearSVC, X::AbstractArray) in LIBSVM at D:\\TongYuan\\.julia\\packages\\LIBSVM\\IZl0L\\src\\ScikitLearnAPI.jl:11\n",
       "[2] predict(model::Union{LIBSVM.AbstractSVC, LIBSVM.AbstractSVR}, X::AbstractArray) in LIBSVM at D:\\TongYuan\\.julia\\packages\\LIBSVM\\IZl0L\\src\\ScikitLearnAPI.jl:6\n",
       "```\n"
      ],
      "text/plain": [
       "  No documentation found.\n",
       "\n",
       "  \u001b[36mScikitLearnBase.predict\u001b[39m is a \u001b[36mFunction\u001b[39m.\n",
       "\n",
       "\u001b[36m  # 2 methods for generic function \"predict\":\u001b[39m\n",
       "\u001b[36m  [1] predict(model::LinearSVC, X::AbstractArray) in LIBSVM at D:\\TongYuan\\.julia\\packages\\LIBSVM\\IZl0L\\src\\ScikitLearnAPI.jl:11\u001b[39m\n",
       "\u001b[36m  [2] predict(model::Union{LIBSVM.AbstractSVC, LIBSVM.AbstractSVR}, X::AbstractArray) in LIBSVM at D:\\TongYuan\\.julia\\packages\\LIBSVM\\IZl0L\\src\\ScikitLearnAPI.jl:6\u001b[39m"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?ScikitLearnBase.predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DecisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-05T17:19:47.828000+08:00",
     "start_time": "2022-07-05T09:19:47.791Z"
    }
   },
   "outputs": [],
   "source": [
    "using DecisionTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-10T15:33:21.120000+08:00",
     "start_time": "2022-06-10T07:33:20.998Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1mD\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1mc\u001b[22m\u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1mT\u001b[22m\u001b[0m\u001b[1mr\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1me\u001b[22m \u001b[0m\u001b[1mD\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1mc\u001b[22m\u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1mT\u001b[22m\u001b[0m\u001b[1mr\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1me\u001b[22mRegressor \u001b[0m\u001b[1mD\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1mc\u001b[22m\u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1mT\u001b[22m\u001b[0m\u001b[1mr\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1me\u001b[22mClassifier\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "No docstring found for module \\texttt{DecisionTree}.\n",
       "\n",
       "\\section{Exported names}\n",
       "\\texttt{AdaBoostStumpClassifier}, \\texttt{ConfusionMatrix}, \\texttt{DecisionTreeClassifier}, \\texttt{DecisionTreeRegressor}, \\texttt{Ensemble}, \\texttt{InfoLeaf}, \\texttt{InfoNode}, \\texttt{Leaf}, \\texttt{Node}, \\texttt{R2}, \\texttt{RandomForestClassifier}, \\texttt{RandomForestRegressor}, \\texttt{apply\\_adaboost\\_stumps}, \\texttt{apply\\_adaboost\\_stumps\\_proba}, \\texttt{apply\\_forest}, \\texttt{apply\\_forest\\_proba}, \\texttt{apply\\_tree}, \\texttt{apply\\_tree\\_proba}, \\texttt{build\\_adaboost\\_stumps}, \\texttt{build\\_forest}, \\texttt{build\\_stump}, \\texttt{build\\_tree}, \\texttt{confusion\\_matrix}, \\texttt{depth}, \\texttt{fit!}, \\texttt{get\\_classes}, \\texttt{load\\_data}, \\texttt{majority\\_vote}, \\texttt{mean\\_squared\\_error}, \\texttt{nfoldCV\\_forest}, \\texttt{nfoldCV\\_stumps}, \\texttt{nfoldCV\\_tree}, \\texttt{predict}, \\texttt{predict\\_proba}, \\texttt{print\\_tree}, \\texttt{prune\\_tree}, \\texttt{wrap}\n",
       "\n",
       "\\section{Displaying contents of readme found at \\texttt{D:{\\textbackslash}TongYuan{\\textbackslash}.julia{\\textbackslash}packages{\\textbackslash}DecisionTree{\\textbackslash}y3ehj{\\textbackslash}README.md}}\n",
       "\\section{DecisionTree.jl}\n",
       "\\href{https://github.com/JuliaAI/DecisionTree.jl/actions?query=workflow%3ACI}{\\begin{figure}\n",
       "\\centering\n",
       "\\includegraphics{https://github.com/JuliaAI/DecisionTree.jl/workflows/CI/badge.svg}\n",
       "\\caption{CI}\n",
       "\\end{figure}\n",
       "} \\href{https://codecov.io/gh/JuliaAI/DecisionTree.jl}{\\begin{figure}\n",
       "\\centering\n",
       "\\includegraphics{https://codecov.io/gh/JuliaAI/DecisionTree.jl/branch/master/graph/badge.svg}\n",
       "\\caption{Codecov}\n",
       "\\end{figure}\n",
       "} \\href{https://juliahub.com/docs/DecisionTree/pEDeB/0.10.11/}{\\begin{figure}\n",
       "\\centering\n",
       "\\includegraphics{https://img.shields.io/badge/docs-stable-blue.svg}\n",
       "\\caption{Docs Stable}\n",
       "\\end{figure}\n",
       "}\n",
       "\n",
       "Julia implementation of Decision Tree (CART) and Random Forest algorithms\n",
       "\n",
       "Created and developed by Ben Sadeghi (@bensadeghi). Now maintained by the \\href{https://github.com/JuliaAI}{JuliaAI} organization.\n",
       "\n",
       "Available via:\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item \\href{https://github.com/IBM/AutoMLPipeline.jl}{AutoMLPipeline.jl} - create complex ML pipeline structures using simple expressions\n",
       "\n",
       "\n",
       "\\item \\href{https://github.com/ppalmes/CombineML.jl}{CombineML.jl} - a heterogeneous ensemble learning package\n",
       "\n",
       "\n",
       "\\item \\href{https://alan-turing-institute.github.io/MLJ.jl/dev/}{MLJ.jl} - a machine learning framework for Julia\n",
       "\n",
       "\n",
       "\\item \\href{https://github.com/cstjean/ScikitLearn.jl}{ScikitLearn.jl} - Julia implementation of the scikit-learn API\n",
       "\n",
       "\\end{itemize}\n",
       "\\subsection{Classification}\n",
       "\\begin{itemize}\n",
       "\\item pre-pruning (max depth, min leaf size)\n",
       "\n",
       "\n",
       "\\item post-pruning (pessimistic pruning)\n",
       "\n",
       "\n",
       "\\item multi-threaded bagging (random forests)\n",
       "\n",
       "\n",
       "\\item adaptive boosting (decision stumps)\n",
       "\n",
       "\n",
       "\\item cross validation (n-fold)\n",
       "\n",
       "\n",
       "\\item support for ordered features (encoded as \\texttt{Real}s or \\texttt{String}s)\n",
       "\n",
       "\\end{itemize}\n",
       "\\subsection{Regression}\n",
       "\\begin{itemize}\n",
       "\\item pre-pruning (max depth, min leaf size)\n",
       "\n",
       "\n",
       "\\item multi-threaded bagging (random forests)\n",
       "\n",
       "\n",
       "\\item cross validation (n-fold)\n",
       "\n",
       "\n",
       "\\item support for numerical features\n",
       "\n",
       "\\end{itemize}\n",
       "\\textbf{Note that regression is implied if labels/targets are of type Array\\{Float\\}}\n",
       "\n",
       "\\subsection{Installation}\n",
       "You can install DecisionTree.jl using Julia's package manager\n",
       "\n",
       "\\begin{verbatim}\n",
       "Pkg.add(\"DecisionTree\")\n",
       "\\end{verbatim}\n",
       "\\subsection{ScikitLearn.jl API}\n",
       "DecisionTree.jl supports the \\href{https://github.com/cstjean/ScikitLearn.jl}{ScikitLearn.jl} interface and algorithms (cross-validation, hyperparameter tuning, pipelines, etc.)\n",
       "\n",
       "Available models: \\texttt{DecisionTreeClassifier, DecisionTreeRegressor, RandomForestClassifier, RandomForestRegressor, AdaBoostStumpClassifier}. See each model's help (eg. \\texttt{?DecisionTreeRegressor} at the REPL) for more information\n",
       "\n",
       "\\subsubsection{Classification Example}\n",
       "Load DecisionTree package\n",
       "\n",
       "\\begin{verbatim}\n",
       "using DecisionTree\n",
       "\\end{verbatim}\n",
       "Separate Fisher's Iris dataset features and labels\n",
       "\n",
       "\\begin{verbatim}\n",
       "features, labels = load_data(\"iris\")    # also see \"adult\" and \"digits\" datasets\n",
       "\n",
       "# the data loaded are of type Array{Any}\n",
       "# cast them to concrete types for better performance\n",
       "features = float.(features)\n",
       "labels   = string.(labels)\n",
       "\\end{verbatim}\n",
       "Pruned Tree Classifier\n",
       "\n",
       "\\begin{verbatim}\n",
       "# train depth-truncated classifier\n",
       "model = DecisionTreeClassifier(max_depth=2)\n",
       "fit!(model, features, labels)\n",
       "# pretty print of the tree, to a depth of 5 nodes (optional)\n",
       "print_tree(model, 5)\n",
       "# apply learned model\n",
       "predict(model, [5.9,3.0,5.1,1.9])\n",
       "# get the probability of each label\n",
       "predict_proba(model, [5.9,3.0,5.1,1.9])\n",
       "println(get_classes(model)) # returns the ordering of the columns in predict_proba's output\n",
       "# run n-fold cross validation over 3 CV folds\n",
       "# See ScikitLearn.jl for installation instructions\n",
       "using ScikitLearn.CrossValidation: cross_val_score\n",
       "accuracy = cross_val_score(model, features, labels, cv=3)\n",
       "\\end{verbatim}\n",
       "Also, have a look at these \\href{https://github.com/cstjean/ScikitLearn.jl/blob/master/examples/Classifier_Comparison_Julia.ipynb}{classification} and \\href{https://github.com/cstjean/ScikitLearn.jl/blob/master/examples/Decision_Tree_Regression_Julia.ipynb}{regression} notebooks.\n",
       "\n",
       "\\subsection{Native API}\n",
       "\\subsubsection{Classification Example}\n",
       "Decision Tree Classifier\n",
       "\n",
       "\\begin{verbatim}\n",
       "# train full-tree classifier\n",
       "model = build_tree(labels, features)\n",
       "# prune tree: merge leaves having >= 90% combined purity (default: 100%)\n",
       "model = prune_tree(model, 0.9)\n",
       "# pretty print of the tree, to a depth of 5 nodes (optional)\n",
       "print_tree(model, 5)\n",
       "# apply learned model\n",
       "apply_tree(model, [5.9,3.0,5.1,1.9])\n",
       "# apply model to all the sames\n",
       "preds = apply_tree(model, features)\n",
       "# generate confusion matrix, along with accuracy and kappa scores\n",
       "confusion_matrix(labels, preds)\n",
       "# get the probability of each label\n",
       "apply_tree_proba(model, [5.9,3.0,5.1,1.9], [\"Iris-setosa\", \"Iris-versicolor\", \"Iris-virginica\"])\n",
       "# run 3-fold cross validation of pruned tree,\n",
       "n_folds=3\n",
       "accuracy = nfoldCV_tree(labels, features, n_folds)\n",
       "\n",
       "# set of classification parameters and respective default values\n",
       "# pruning_purity: purity threshold used for post-pruning (default: 1.0, no pruning)\n",
       "# max_depth: maximum depth of the decision tree (default: -1, no maximum)\n",
       "# min_samples_leaf: the minimum number of samples each leaf needs to have (default: 1)\n",
       "# min_samples_split: the minimum number of samples in needed for a split (default: 2)\n",
       "# min_purity_increase: minimum purity needed for a split (default: 0.0)\n",
       "# n_subfeatures: number of features to select at random (default: 0, keep all)\n",
       "# keyword rng: the random number generator or seed to use (default Random.GLOBAL_RNG)\n",
       "n_subfeatures=0; max_depth=-1; min_samples_leaf=1; min_samples_split=2\n",
       "min_purity_increase=0.0; pruning_purity = 1.0; seed=3\n",
       "\n",
       "model    =   build_tree(labels, features,\n",
       "                        n_subfeatures,\n",
       "                        max_depth,\n",
       "                        min_samples_leaf,\n",
       "                        min_samples_split,\n",
       "                        min_purity_increase;\n",
       "                        rng = seed)\n",
       "\n",
       "accuracy = nfoldCV_tree(labels, features,\n",
       "                        n_folds,\n",
       "                        pruning_purity,\n",
       "                        max_depth,\n",
       "                        min_samples_leaf,\n",
       "                        min_samples_split,\n",
       "                        min_purity_increase;\n",
       "                        verbose = true,\n",
       "                        rng = seed)\n",
       "\\end{verbatim}\n",
       "Random Forest Classifier\n",
       "\n",
       "\\begin{verbatim}\n",
       "# train random forest classifier\n",
       "# using 2 random features, 10 trees, 0.5 portion of samples per tree, and a maximum tree depth of 6\n",
       "model = build_forest(labels, features, 2, 10, 0.5, 6)\n",
       "# apply learned model\n",
       "apply_forest(model, [5.9,3.0,5.1,1.9])\n",
       "# get the probability of each label\n",
       "apply_forest_proba(model, [5.9,3.0,5.1,1.9], [\"Iris-setosa\", \"Iris-versicolor\", \"Iris-virginica\"])\n",
       "# run 3-fold cross validation for forests, using 2 random features per split\n",
       "n_folds=3; n_subfeatures=2\n",
       "accuracy = nfoldCV_forest(labels, features, n_folds, n_subfeatures)\n",
       "\n",
       "# set of classification parameters and respective default values\n",
       "# n_subfeatures: number of features to consider at random per split (default: -1, sqrt(# features))\n",
       "# n_trees: number of trees to train (default: 10)\n",
       "# partial_sampling: fraction of samples to train each tree on (default: 0.7)\n",
       "# max_depth: maximum depth of the decision trees (default: no maximum)\n",
       "# min_samples_leaf: the minimum number of samples each leaf needs to have (default: 5)\n",
       "# min_samples_split: the minimum number of samples in needed for a split (default: 2)\n",
       "# min_purity_increase: minimum purity needed for a split (default: 0.0)\n",
       "# keyword rng: the random number generator or seed to use (default Random.GLOBAL_RNG)\n",
       "#              multi-threaded forests must be seeded with an `Int`\n",
       "n_subfeatures=-1; n_trees=10; partial_sampling=0.7; max_depth=-1\n",
       "min_samples_leaf=5; min_samples_split=2; min_purity_increase=0.0; seed=3\n",
       "\n",
       "model    =   build_forest(labels, features,\n",
       "                          n_subfeatures,\n",
       "                          n_trees,\n",
       "                          partial_sampling,\n",
       "                          max_depth,\n",
       "                          min_samples_leaf,\n",
       "                          min_samples_split,\n",
       "                          min_purity_increase;\n",
       "                          rng = seed)\n",
       "\n",
       "accuracy = nfoldCV_forest(labels, features,\n",
       "                          n_folds,\n",
       "                          n_subfeatures,\n",
       "                          n_trees,\n",
       "                          partial_sampling,\n",
       "                          max_depth,\n",
       "                          min_samples_leaf,\n",
       "                          min_samples_split,\n",
       "                          min_purity_increase;\n",
       "                          verbose = true,\n",
       "                          rng = seed)\n",
       "\\end{verbatim}\n",
       "Adaptive-Boosted Decision Stumps Classifier\n",
       "\n",
       "\\begin{verbatim}\n",
       "# train adaptive-boosted stumps, using 7 iterations\n",
       "model, coeffs = build_adaboost_stumps(labels, features, 7);\n",
       "# apply learned model\n",
       "apply_adaboost_stumps(model, coeffs, [5.9,3.0,5.1,1.9])\n",
       "# get the probability of each label\n",
       "apply_adaboost_stumps_proba(model, coeffs, [5.9,3.0,5.1,1.9], [\"Iris-setosa\", \"Iris-versicolor\", \"Iris-virginica\"])\n",
       "# run 3-fold cross validation for boosted stumps, using 7 iterations\n",
       "n_iterations=7; n_folds=3\n",
       "accuracy = nfoldCV_stumps(labels, features,\n",
       "                          n_folds,\n",
       "                          n_iterations;\n",
       "                          verbose = true)\n",
       "\\end{verbatim}\n",
       "\\subsubsection{Regression Example}\n",
       "```julia n, m = 10\\^{}3, 5 features = randn(n, m) weights = rand(-2:2, m) labels = features * weights\n",
       "\n",
       "[output truncated to first 200 lines]\n",
       "\n"
      ],
      "text/markdown": [
       "No docstring found for module `DecisionTree`.\n",
       "\n",
       "# Exported names\n",
       "\n",
       "`AdaBoostStumpClassifier`, `ConfusionMatrix`, `DecisionTreeClassifier`, `DecisionTreeRegressor`, `Ensemble`, `InfoLeaf`, `InfoNode`, `Leaf`, `Node`, `R2`, `RandomForestClassifier`, `RandomForestRegressor`, `apply_adaboost_stumps`, `apply_adaboost_stumps_proba`, `apply_forest`, `apply_forest_proba`, `apply_tree`, `apply_tree_proba`, `build_adaboost_stumps`, `build_forest`, `build_stump`, `build_tree`, `confusion_matrix`, `depth`, `fit!`, `get_classes`, `load_data`, `majority_vote`, `mean_squared_error`, `nfoldCV_forest`, `nfoldCV_stumps`, `nfoldCV_tree`, `predict`, `predict_proba`, `print_tree`, `prune_tree`, `wrap`\n",
       "\n",
       "# Displaying contents of readme found at `D:\\TongYuan\\.julia\\packages\\DecisionTree\\y3ehj\\README.md`\n",
       "\n",
       "# DecisionTree.jl\n",
       "\n",
       "[![CI](https://github.com/JuliaAI/DecisionTree.jl/workflows/CI/badge.svg)](https://github.com/JuliaAI/DecisionTree.jl/actions?query=workflow%3ACI) [![Codecov](https://codecov.io/gh/JuliaAI/DecisionTree.jl/branch/master/graph/badge.svg)](https://codecov.io/gh/JuliaAI/DecisionTree.jl) [![Docs Stable](https://img.shields.io/badge/docs-stable-blue.svg)](https://juliahub.com/docs/DecisionTree/pEDeB/0.10.11/)\n",
       "\n",
       "Julia implementation of Decision Tree (CART) and Random Forest algorithms\n",
       "\n",
       "Created and developed by Ben Sadeghi (@bensadeghi). Now maintained by the [JuliaAI](https://github.com/JuliaAI) organization.\n",
       "\n",
       "Available via:\n",
       "\n",
       "  * [AutoMLPipeline.jl](https://github.com/IBM/AutoMLPipeline.jl) - create complex ML pipeline structures using simple expressions\n",
       "  * [CombineML.jl](https://github.com/ppalmes/CombineML.jl) - a heterogeneous ensemble learning package\n",
       "  * [MLJ.jl](https://alan-turing-institute.github.io/MLJ.jl/dev/) - a machine learning framework for Julia\n",
       "  * [ScikitLearn.jl](https://github.com/cstjean/ScikitLearn.jl) - Julia implementation of the scikit-learn API\n",
       "\n",
       "## Classification\n",
       "\n",
       "  * pre-pruning (max depth, min leaf size)\n",
       "  * post-pruning (pessimistic pruning)\n",
       "  * multi-threaded bagging (random forests)\n",
       "  * adaptive boosting (decision stumps)\n",
       "  * cross validation (n-fold)\n",
       "  * support for ordered features (encoded as `Real`s or `String`s)\n",
       "\n",
       "## Regression\n",
       "\n",
       "  * pre-pruning (max depth, min leaf size)\n",
       "  * multi-threaded bagging (random forests)\n",
       "  * cross validation (n-fold)\n",
       "  * support for numerical features\n",
       "\n",
       "**Note that regression is implied if labels/targets are of type Array{Float}**\n",
       "\n",
       "## Installation\n",
       "\n",
       "You can install DecisionTree.jl using Julia's package manager\n",
       "\n",
       "```julia\n",
       "Pkg.add(\"DecisionTree\")\n",
       "```\n",
       "\n",
       "## ScikitLearn.jl API\n",
       "\n",
       "DecisionTree.jl supports the [ScikitLearn.jl](https://github.com/cstjean/ScikitLearn.jl) interface and algorithms (cross-validation, hyperparameter tuning, pipelines, etc.)\n",
       "\n",
       "Available models: `DecisionTreeClassifier, DecisionTreeRegressor, RandomForestClassifier, RandomForestRegressor, AdaBoostStumpClassifier`. See each model's help (eg. `?DecisionTreeRegressor` at the REPL) for more information\n",
       "\n",
       "### Classification Example\n",
       "\n",
       "Load DecisionTree package\n",
       "\n",
       "```julia\n",
       "using DecisionTree\n",
       "```\n",
       "\n",
       "Separate Fisher's Iris dataset features and labels\n",
       "\n",
       "```julia\n",
       "features, labels = load_data(\"iris\")    # also see \"adult\" and \"digits\" datasets\n",
       "\n",
       "# the data loaded are of type Array{Any}\n",
       "# cast them to concrete types for better performance\n",
       "features = float.(features)\n",
       "labels   = string.(labels)\n",
       "```\n",
       "\n",
       "Pruned Tree Classifier\n",
       "\n",
       "```julia\n",
       "# train depth-truncated classifier\n",
       "model = DecisionTreeClassifier(max_depth=2)\n",
       "fit!(model, features, labels)\n",
       "# pretty print of the tree, to a depth of 5 nodes (optional)\n",
       "print_tree(model, 5)\n",
       "# apply learned model\n",
       "predict(model, [5.9,3.0,5.1,1.9])\n",
       "# get the probability of each label\n",
       "predict_proba(model, [5.9,3.0,5.1,1.9])\n",
       "println(get_classes(model)) # returns the ordering of the columns in predict_proba's output\n",
       "# run n-fold cross validation over 3 CV folds\n",
       "# See ScikitLearn.jl for installation instructions\n",
       "using ScikitLearn.CrossValidation: cross_val_score\n",
       "accuracy = cross_val_score(model, features, labels, cv=3)\n",
       "```\n",
       "\n",
       "Also, have a look at these [classification](https://github.com/cstjean/ScikitLearn.jl/blob/master/examples/Classifier_Comparison_Julia.ipynb) and [regression](https://github.com/cstjean/ScikitLearn.jl/blob/master/examples/Decision_Tree_Regression_Julia.ipynb) notebooks.\n",
       "\n",
       "## Native API\n",
       "\n",
       "### Classification Example\n",
       "\n",
       "Decision Tree Classifier\n",
       "\n",
       "```julia\n",
       "# train full-tree classifier\n",
       "model = build_tree(labels, features)\n",
       "# prune tree: merge leaves having >= 90% combined purity (default: 100%)\n",
       "model = prune_tree(model, 0.9)\n",
       "# pretty print of the tree, to a depth of 5 nodes (optional)\n",
       "print_tree(model, 5)\n",
       "# apply learned model\n",
       "apply_tree(model, [5.9,3.0,5.1,1.9])\n",
       "# apply model to all the sames\n",
       "preds = apply_tree(model, features)\n",
       "# generate confusion matrix, along with accuracy and kappa scores\n",
       "confusion_matrix(labels, preds)\n",
       "# get the probability of each label\n",
       "apply_tree_proba(model, [5.9,3.0,5.1,1.9], [\"Iris-setosa\", \"Iris-versicolor\", \"Iris-virginica\"])\n",
       "# run 3-fold cross validation of pruned tree,\n",
       "n_folds=3\n",
       "accuracy = nfoldCV_tree(labels, features, n_folds)\n",
       "\n",
       "# set of classification parameters and respective default values\n",
       "# pruning_purity: purity threshold used for post-pruning (default: 1.0, no pruning)\n",
       "# max_depth: maximum depth of the decision tree (default: -1, no maximum)\n",
       "# min_samples_leaf: the minimum number of samples each leaf needs to have (default: 1)\n",
       "# min_samples_split: the minimum number of samples in needed for a split (default: 2)\n",
       "# min_purity_increase: minimum purity needed for a split (default: 0.0)\n",
       "# n_subfeatures: number of features to select at random (default: 0, keep all)\n",
       "# keyword rng: the random number generator or seed to use (default Random.GLOBAL_RNG)\n",
       "n_subfeatures=0; max_depth=-1; min_samples_leaf=1; min_samples_split=2\n",
       "min_purity_increase=0.0; pruning_purity = 1.0; seed=3\n",
       "\n",
       "model    =   build_tree(labels, features,\n",
       "                        n_subfeatures,\n",
       "                        max_depth,\n",
       "                        min_samples_leaf,\n",
       "                        min_samples_split,\n",
       "                        min_purity_increase;\n",
       "                        rng = seed)\n",
       "\n",
       "accuracy = nfoldCV_tree(labels, features,\n",
       "                        n_folds,\n",
       "                        pruning_purity,\n",
       "                        max_depth,\n",
       "                        min_samples_leaf,\n",
       "                        min_samples_split,\n",
       "                        min_purity_increase;\n",
       "                        verbose = true,\n",
       "                        rng = seed)\n",
       "```\n",
       "\n",
       "Random Forest Classifier\n",
       "\n",
       "```julia\n",
       "# train random forest classifier\n",
       "# using 2 random features, 10 trees, 0.5 portion of samples per tree, and a maximum tree depth of 6\n",
       "model = build_forest(labels, features, 2, 10, 0.5, 6)\n",
       "# apply learned model\n",
       "apply_forest(model, [5.9,3.0,5.1,1.9])\n",
       "# get the probability of each label\n",
       "apply_forest_proba(model, [5.9,3.0,5.1,1.9], [\"Iris-setosa\", \"Iris-versicolor\", \"Iris-virginica\"])\n",
       "# run 3-fold cross validation for forests, using 2 random features per split\n",
       "n_folds=3; n_subfeatures=2\n",
       "accuracy = nfoldCV_forest(labels, features, n_folds, n_subfeatures)\n",
       "\n",
       "# set of classification parameters and respective default values\n",
       "# n_subfeatures: number of features to consider at random per split (default: -1, sqrt(# features))\n",
       "# n_trees: number of trees to train (default: 10)\n",
       "# partial_sampling: fraction of samples to train each tree on (default: 0.7)\n",
       "# max_depth: maximum depth of the decision trees (default: no maximum)\n",
       "# min_samples_leaf: the minimum number of samples each leaf needs to have (default: 5)\n",
       "# min_samples_split: the minimum number of samples in needed for a split (default: 2)\n",
       "# min_purity_increase: minimum purity needed for a split (default: 0.0)\n",
       "# keyword rng: the random number generator or seed to use (default Random.GLOBAL_RNG)\n",
       "#              multi-threaded forests must be seeded with an `Int`\n",
       "n_subfeatures=-1; n_trees=10; partial_sampling=0.7; max_depth=-1\n",
       "min_samples_leaf=5; min_samples_split=2; min_purity_increase=0.0; seed=3\n",
       "\n",
       "model    =   build_forest(labels, features,\n",
       "                          n_subfeatures,\n",
       "                          n_trees,\n",
       "                          partial_sampling,\n",
       "                          max_depth,\n",
       "                          min_samples_leaf,\n",
       "                          min_samples_split,\n",
       "                          min_purity_increase;\n",
       "                          rng = seed)\n",
       "\n",
       "accuracy = nfoldCV_forest(labels, features,\n",
       "                          n_folds,\n",
       "                          n_subfeatures,\n",
       "                          n_trees,\n",
       "                          partial_sampling,\n",
       "                          max_depth,\n",
       "                          min_samples_leaf,\n",
       "                          min_samples_split,\n",
       "                          min_purity_increase;\n",
       "                          verbose = true,\n",
       "                          rng = seed)\n",
       "```\n",
       "\n",
       "Adaptive-Boosted Decision Stumps Classifier\n",
       "\n",
       "```julia\n",
       "# train adaptive-boosted stumps, using 7 iterations\n",
       "model, coeffs = build_adaboost_stumps(labels, features, 7);\n",
       "# apply learned model\n",
       "apply_adaboost_stumps(model, coeffs, [5.9,3.0,5.1,1.9])\n",
       "# get the probability of each label\n",
       "apply_adaboost_stumps_proba(model, coeffs, [5.9,3.0,5.1,1.9], [\"Iris-setosa\", \"Iris-versicolor\", \"Iris-virginica\"])\n",
       "# run 3-fold cross validation for boosted stumps, using 7 iterations\n",
       "n_iterations=7; n_folds=3\n",
       "accuracy = nfoldCV_stumps(labels, features,\n",
       "                          n_folds,\n",
       "                          n_iterations;\n",
       "                          verbose = true)\n",
       "```\n",
       "\n",
       "### Regression Example\n",
       "\n",
       "```julia n, m = 10^3, 5 features = randn(n, m) weights = rand(-2:2, m) labels = features * weights\n",
       "\n",
       "[output truncated to first 200 lines]\n"
      ],
      "text/plain": [
       "  No docstring found for module \u001b[36mDecisionTree\u001b[39m.\n",
       "\n",
       "\u001b[1m  Exported names\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "  \u001b[36mAdaBoostStumpClassifier\u001b[39m, \u001b[36mConfusionMatrix\u001b[39m, \u001b[36mDecisionTreeClassifier\u001b[39m,\n",
       "  \u001b[36mDecisionTreeRegressor\u001b[39m, \u001b[36mEnsemble\u001b[39m, \u001b[36mInfoLeaf\u001b[39m, \u001b[36mInfoNode\u001b[39m, \u001b[36mLeaf\u001b[39m, \u001b[36mNode\u001b[39m, \u001b[36mR2\u001b[39m,\n",
       "  \u001b[36mRandomForestClassifier\u001b[39m, \u001b[36mRandomForestRegressor\u001b[39m, \u001b[36mapply_adaboost_stumps\u001b[39m,\n",
       "  \u001b[36mapply_adaboost_stumps_proba\u001b[39m, \u001b[36mapply_forest\u001b[39m, \u001b[36mapply_forest_proba\u001b[39m, \u001b[36mapply_tree\u001b[39m,\n",
       "  \u001b[36mapply_tree_proba\u001b[39m, \u001b[36mbuild_adaboost_stumps\u001b[39m, \u001b[36mbuild_forest\u001b[39m, \u001b[36mbuild_stump\u001b[39m,\n",
       "  \u001b[36mbuild_tree\u001b[39m, \u001b[36mconfusion_matrix\u001b[39m, \u001b[36mdepth\u001b[39m, \u001b[36mfit!\u001b[39m, \u001b[36mget_classes\u001b[39m, \u001b[36mload_data\u001b[39m,\n",
       "  \u001b[36mmajority_vote\u001b[39m, \u001b[36mmean_squared_error\u001b[39m, \u001b[36mnfoldCV_forest\u001b[39m, \u001b[36mnfoldCV_stumps\u001b[39m,\n",
       "  \u001b[36mnfoldCV_tree\u001b[39m, \u001b[36mpredict\u001b[39m, \u001b[36mpredict_proba\u001b[39m, \u001b[36mprint_tree\u001b[39m, \u001b[36mprune_tree\u001b[39m, \u001b[36mwrap\u001b[39m\n",
       "\n",
       "\u001b[1m  Displaying contents of readme found at\u001b[22m\n",
       "\u001b[1m \u001b[36mD:\\TongYuan\\.julia\\packages\\DecisionTree\\y3ehj\\README.md\u001b[39m\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "\u001b[1m  DecisionTree.jl\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "  (Image: CI)\n",
       "  (https://github.com/JuliaAI/DecisionTree.jl/actions?query=workflow%3ACI)\n",
       "  (Image: Codecov) (https://codecov.io/gh/JuliaAI/DecisionTree.jl) (Image:\n",
       "  Docs Stable) (https://juliahub.com/docs/DecisionTree/pEDeB/0.10.11/)\n",
       "\n",
       "  Julia implementation of Decision Tree (CART) and Random Forest algorithms\n",
       "\n",
       "  Created and developed by Ben Sadeghi (@bensadeghi). Now maintained by the\n",
       "  JuliaAI (https://github.com/JuliaAI) organization.\n",
       "\n",
       "  Available via:\n",
       "\n",
       "    •  AutoMLPipeline.jl (https://github.com/IBM/AutoMLPipeline.jl) -\n",
       "       create complex ML pipeline structures using simple expressions\n",
       "\n",
       "    •  CombineML.jl (https://github.com/ppalmes/CombineML.jl) - a\n",
       "       heterogeneous ensemble learning package\n",
       "\n",
       "    •  MLJ.jl (https://alan-turing-institute.github.io/MLJ.jl/dev/) - a\n",
       "       machine learning framework for Julia\n",
       "\n",
       "    •  ScikitLearn.jl (https://github.com/cstjean/ScikitLearn.jl) - Julia\n",
       "       implementation of the scikit-learn API\n",
       "\n",
       "\u001b[1m  Classification\u001b[22m\n",
       "\u001b[1m  ================\u001b[22m\n",
       "\n",
       "    •  pre-pruning (max depth, min leaf size)\n",
       "\n",
       "    •  post-pruning (pessimistic pruning)\n",
       "\n",
       "    •  multi-threaded bagging (random forests)\n",
       "\n",
       "    •  adaptive boosting (decision stumps)\n",
       "\n",
       "    •  cross validation (n-fold)\n",
       "\n",
       "    •  support for ordered features (encoded as \u001b[36mReal\u001b[39ms or \u001b[36mString\u001b[39ms)\n",
       "\n",
       "\u001b[1m  Regression\u001b[22m\n",
       "\u001b[1m  ============\u001b[22m\n",
       "\n",
       "    •  pre-pruning (max depth, min leaf size)\n",
       "\n",
       "    •  multi-threaded bagging (random forests)\n",
       "\n",
       "    •  cross validation (n-fold)\n",
       "\n",
       "    •  support for numerical features\n",
       "\n",
       "  \u001b[1mNote that regression is implied if labels/targets are of type Array{Float}\u001b[22m\n",
       "\n",
       "\u001b[1m  Installation\u001b[22m\n",
       "\u001b[1m  ==============\u001b[22m\n",
       "\n",
       "  You can install DecisionTree.jl using Julia's package manager\n",
       "\n",
       "\u001b[36m  Pkg.add(\"DecisionTree\")\u001b[39m\n",
       "\n",
       "\u001b[1m  ScikitLearn.jl API\u001b[22m\n",
       "\u001b[1m  ====================\u001b[22m\n",
       "\n",
       "  DecisionTree.jl supports the ScikitLearn.jl\n",
       "  (https://github.com/cstjean/ScikitLearn.jl) interface and algorithms\n",
       "  (cross-validation, hyperparameter tuning, pipelines, etc.)\n",
       "\n",
       "  Available models: \u001b[36mDecisionTreeClassifier, DecisionTreeRegressor,\n",
       "  RandomForestClassifier, RandomForestRegressor, AdaBoostStumpClassifier\u001b[39m. See\n",
       "  each model's help (eg. \u001b[36m?DecisionTreeRegressor\u001b[39m at the REPL) for more\n",
       "  information\n",
       "\n",
       "\u001b[1m  Classification Example\u001b[22m\n",
       "\u001b[1m  ––––––––––––––––––––––––\u001b[22m\n",
       "\n",
       "  Load DecisionTree package\n",
       "\n",
       "\u001b[36m  using DecisionTree\u001b[39m\n",
       "\n",
       "  Separate Fisher's Iris dataset features and labels\n",
       "\n",
       "\u001b[36m  features, labels = load_data(\"iris\")    # also see \"adult\" and \"digits\" datasets\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  # the data loaded are of type Array{Any}\u001b[39m\n",
       "\u001b[36m  # cast them to concrete types for better performance\u001b[39m\n",
       "\u001b[36m  features = float.(features)\u001b[39m\n",
       "\u001b[36m  labels   = string.(labels)\u001b[39m\n",
       "\n",
       "  Pruned Tree Classifier\n",
       "\n",
       "\u001b[36m  # train depth-truncated classifier\u001b[39m\n",
       "\u001b[36m  model = DecisionTreeClassifier(max_depth=2)\u001b[39m\n",
       "\u001b[36m  fit!(model, features, labels)\u001b[39m\n",
       "\u001b[36m  # pretty print of the tree, to a depth of 5 nodes (optional)\u001b[39m\n",
       "\u001b[36m  print_tree(model, 5)\u001b[39m\n",
       "\u001b[36m  # apply learned model\u001b[39m\n",
       "\u001b[36m  predict(model, [5.9,3.0,5.1,1.9])\u001b[39m\n",
       "\u001b[36m  # get the probability of each label\u001b[39m\n",
       "\u001b[36m  predict_proba(model, [5.9,3.0,5.1,1.9])\u001b[39m\n",
       "\u001b[36m  println(get_classes(model)) # returns the ordering of the columns in predict_proba's output\u001b[39m\n",
       "\u001b[36m  # run n-fold cross validation over 3 CV folds\u001b[39m\n",
       "\u001b[36m  # See ScikitLearn.jl for installation instructions\u001b[39m\n",
       "\u001b[36m  using ScikitLearn.CrossValidation: cross_val_score\u001b[39m\n",
       "\u001b[36m  accuracy = cross_val_score(model, features, labels, cv=3)\u001b[39m\n",
       "\n",
       "  Also, have a look at these classification\n",
       "  (https://github.com/cstjean/ScikitLearn.jl/blob/master/examples/Classifier_Comparison_Julia.ipynb)\n",
       "  and regression\n",
       "  (https://github.com/cstjean/ScikitLearn.jl/blob/master/examples/Decision_Tree_Regression_Julia.ipynb)\n",
       "  notebooks.\n",
       "\n",
       "\u001b[1m  Native API\u001b[22m\n",
       "\u001b[1m  ============\u001b[22m\n",
       "\n",
       "\u001b[1m  Classification Example\u001b[22m\n",
       "\u001b[1m  ––––––––––––––––––––––––\u001b[22m\n",
       "\n",
       "  Decision Tree Classifier\n",
       "\n",
       "\u001b[36m  # train full-tree classifier\u001b[39m\n",
       "\u001b[36m  model = build_tree(labels, features)\u001b[39m\n",
       "\u001b[36m  # prune tree: merge leaves having >= 90% combined purity (default: 100%)\u001b[39m\n",
       "\u001b[36m  model = prune_tree(model, 0.9)\u001b[39m\n",
       "\u001b[36m  # pretty print of the tree, to a depth of 5 nodes (optional)\u001b[39m\n",
       "\u001b[36m  print_tree(model, 5)\u001b[39m\n",
       "\u001b[36m  # apply learned model\u001b[39m\n",
       "\u001b[36m  apply_tree(model, [5.9,3.0,5.1,1.9])\u001b[39m\n",
       "\u001b[36m  # apply model to all the sames\u001b[39m\n",
       "\u001b[36m  preds = apply_tree(model, features)\u001b[39m\n",
       "\u001b[36m  # generate confusion matrix, along with accuracy and kappa scores\u001b[39m\n",
       "\u001b[36m  confusion_matrix(labels, preds)\u001b[39m\n",
       "\u001b[36m  # get the probability of each label\u001b[39m\n",
       "\u001b[36m  apply_tree_proba(model, [5.9,3.0,5.1,1.9], [\"Iris-setosa\", \"Iris-versicolor\", \"Iris-virginica\"])\u001b[39m\n",
       "\u001b[36m  # run 3-fold cross validation of pruned tree,\u001b[39m\n",
       "\u001b[36m  n_folds=3\u001b[39m\n",
       "\u001b[36m  accuracy = nfoldCV_tree(labels, features, n_folds)\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  # set of classification parameters and respective default values\u001b[39m\n",
       "\u001b[36m  # pruning_purity: purity threshold used for post-pruning (default: 1.0, no pruning)\u001b[39m\n",
       "\u001b[36m  # max_depth: maximum depth of the decision tree (default: -1, no maximum)\u001b[39m\n",
       "\u001b[36m  # min_samples_leaf: the minimum number of samples each leaf needs to have (default: 1)\u001b[39m\n",
       "\u001b[36m  # min_samples_split: the minimum number of samples in needed for a split (default: 2)\u001b[39m\n",
       "\u001b[36m  # min_purity_increase: minimum purity needed for a split (default: 0.0)\u001b[39m\n",
       "\u001b[36m  # n_subfeatures: number of features to select at random (default: 0, keep all)\u001b[39m\n",
       "\u001b[36m  # keyword rng: the random number generator or seed to use (default Random.GLOBAL_RNG)\u001b[39m\n",
       "\u001b[36m  n_subfeatures=0; max_depth=-1; min_samples_leaf=1; min_samples_split=2\u001b[39m\n",
       "\u001b[36m  min_purity_increase=0.0; pruning_purity = 1.0; seed=3\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  model    =   build_tree(labels, features,\u001b[39m\n",
       "\u001b[36m                          n_subfeatures,\u001b[39m\n",
       "\u001b[36m                          max_depth,\u001b[39m\n",
       "\u001b[36m                          min_samples_leaf,\u001b[39m\n",
       "\u001b[36m                          min_samples_split,\u001b[39m\n",
       "\u001b[36m                          min_purity_increase;\u001b[39m\n",
       "\u001b[36m                          rng = seed)\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  accuracy = nfoldCV_tree(labels, features,\u001b[39m\n",
       "\u001b[36m                          n_folds,\u001b[39m\n",
       "\u001b[36m                          pruning_purity,\u001b[39m\n",
       "\u001b[36m                          max_depth,\u001b[39m\n",
       "\u001b[36m                          min_samples_leaf,\u001b[39m\n",
       "\u001b[36m                          min_samples_split,\u001b[39m\n",
       "\u001b[36m                          min_purity_increase;\u001b[39m\n",
       "\u001b[36m                          verbose = true,\u001b[39m\n",
       "\u001b[36m                          rng = seed)\u001b[39m\n",
       "\n",
       "  Random Forest Classifier\n",
       "\n",
       "\u001b[36m  # train random forest classifier\u001b[39m\n",
       "\u001b[36m  # using 2 random features, 10 trees, 0.5 portion of samples per tree, and a maximum tree depth of 6\u001b[39m\n",
       "\u001b[36m  model = build_forest(labels, features, 2, 10, 0.5, 6)\u001b[39m\n",
       "\u001b[36m  # apply learned model\u001b[39m\n",
       "\u001b[36m  apply_forest(model, [5.9,3.0,5.1,1.9])\u001b[39m\n",
       "\u001b[36m  # get the probability of each label\u001b[39m\n",
       "\u001b[36m  apply_forest_proba(model, [5.9,3.0,5.1,1.9], [\"Iris-setosa\", \"Iris-versicolor\", \"Iris-virginica\"])\u001b[39m\n",
       "\u001b[36m  # run 3-fold cross validation for forests, using 2 random features per split\u001b[39m\n",
       "\u001b[36m  n_folds=3; n_subfeatures=2\u001b[39m\n",
       "\u001b[36m  accuracy = nfoldCV_forest(labels, features, n_folds, n_subfeatures)\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  # set of classification parameters and respective default values\u001b[39m\n",
       "\u001b[36m  # n_subfeatures: number of features to consider at random per split (default: -1, sqrt(# features))\u001b[39m\n",
       "\u001b[36m  # n_trees: number of trees to train (default: 10)\u001b[39m\n",
       "\u001b[36m  # partial_sampling: fraction of samples to train each tree on (default: 0.7)\u001b[39m\n",
       "\u001b[36m  # max_depth: maximum depth of the decision trees (default: no maximum)\u001b[39m\n",
       "\u001b[36m  # min_samples_leaf: the minimum number of samples each leaf needs to have (default: 5)\u001b[39m\n",
       "\u001b[36m  # min_samples_split: the minimum number of samples in needed for a split (default: 2)\u001b[39m\n",
       "\u001b[36m  # min_purity_increase: minimum purity needed for a split (default: 0.0)\u001b[39m\n",
       "\u001b[36m  # keyword rng: the random number generator or seed to use (default Random.GLOBAL_RNG)\u001b[39m\n",
       "\u001b[36m  #              multi-threaded forests must be seeded with an `Int`\u001b[39m\n",
       "\u001b[36m  n_subfeatures=-1; n_trees=10; partial_sampling=0.7; max_depth=-1\u001b[39m\n",
       "\u001b[36m  min_samples_leaf=5; min_samples_split=2; min_purity_increase=0.0; seed=3\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  model    =   build_forest(labels, features,\u001b[39m\n",
       "\u001b[36m                            n_subfeatures,\u001b[39m\n",
       "\u001b[36m                            n_trees,\u001b[39m\n",
       "\u001b[36m                            partial_sampling,\u001b[39m\n",
       "\u001b[36m                            max_depth,\u001b[39m\n",
       "\u001b[36m                            min_samples_leaf,\u001b[39m\n",
       "\u001b[36m                            min_samples_split,\u001b[39m\n",
       "\u001b[36m                            min_purity_increase;\u001b[39m\n",
       "\u001b[36m                            rng = seed)\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  accuracy = nfoldCV_forest(labels, features,\u001b[39m\n",
       "\u001b[36m                            n_folds,\u001b[39m\n",
       "\u001b[36m                            n_subfeatures,\u001b[39m\n",
       "\u001b[36m                            n_trees,\u001b[39m\n",
       "\u001b[36m                            partial_sampling,\u001b[39m\n",
       "\u001b[36m                            max_depth,\u001b[39m\n",
       "\u001b[36m                            min_samples_leaf,\u001b[39m\n",
       "\u001b[36m                            min_samples_split,\u001b[39m\n",
       "\u001b[36m                            min_purity_increase;\u001b[39m\n",
       "\u001b[36m                            verbose = true,\u001b[39m\n",
       "\u001b[36m                            rng = seed)\u001b[39m\n",
       "\n",
       "  Adaptive-Boosted Decision Stumps Classifier\n",
       "\n",
       "\u001b[36m  # train adaptive-boosted stumps, using 7 iterations\u001b[39m\n",
       "\u001b[36m  model, coeffs = build_adaboost_stumps(labels, features, 7);\u001b[39m\n",
       "\u001b[36m  # apply learned model\u001b[39m\n",
       "\u001b[36m  apply_adaboost_stumps(model, coeffs, [5.9,3.0,5.1,1.9])\u001b[39m\n",
       "\u001b[36m  # get the probability of each label\u001b[39m\n",
       "\u001b[36m  apply_adaboost_stumps_proba(model, coeffs, [5.9,3.0,5.1,1.9], [\"Iris-setosa\", \"Iris-versicolor\", \"Iris-virginica\"])\u001b[39m\n",
       "\u001b[36m  # run 3-fold cross validation for boosted stumps, using 7 iterations\u001b[39m\n",
       "\u001b[36m  n_iterations=7; n_folds=3\u001b[39m\n",
       "\u001b[36m  accuracy = nfoldCV_stumps(labels, features,\u001b[39m\n",
       "\u001b[36m                            n_folds,\u001b[39m\n",
       "\u001b[36m                            n_iterations;\u001b[39m\n",
       "\u001b[36m                            verbose = true)\u001b[39m\n",
       "\n",
       "\u001b[1m  Regression Example\u001b[22m\n",
       "\u001b[1m  ––––––––––––––––––––\u001b[22m\n",
       "\n",
       "  ```julia n, m = 10^3, 5 features = randn(n, m) weights = rand(-2:2, m)\n",
       "  labels = features * weights\n",
       "\n",
       "  [output truncated to first 200 lines]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?DecisionTree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultivariateStats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-05T17:20:09.306000+08:00",
     "start_time": "2022-07-05T09:20:09.304Z"
    }
   },
   "outputs": [],
   "source": [
    "using MultivariateStats #多元统计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-10T16:29:43.747000+08:00",
     "start_time": "2022-06-10T08:29:42.058Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1mM\u001b[22m\u001b[0m\u001b[1mu\u001b[22m\u001b[0m\u001b[1ml\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1mv\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1mr\u001b[22m\u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1mS\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "No docstring found for module \\texttt{MultivariateStats}.\n",
       "\n",
       "\\section{Exported names}\n",
       "\\texttt{CCA}, \\texttt{FactorAnalysis}, \\texttt{ICA}, \\texttt{KernelPCA}, \\texttt{LinearDiscriminant}, \\texttt{MDS}, \\texttt{MulticlassLDA}, \\texttt{MulticlassLDAStats}, \\texttt{PCA}, \\texttt{PPCA}, \\texttt{SubspaceLDA}, \\texttt{Whitening}, \\texttt{bayespca}, \\texttt{betweenclass\\_scatter}, \\texttt{ccacov}, \\texttt{ccasvd}, \\texttt{centralize}, \\texttt{classical\\_mds}, \\texttt{classmeans}, \\texttt{classweights}, \\texttt{cor}, \\texttt{correlations}, \\texttt{cov\\_whitening}, \\texttt{cov\\_whitening!}, \\texttt{decentralize}, \\texttt{dmat2gram}, \\texttt{dmat2gram!}, \\texttt{eigvals}, \\texttt{eigvecs}, \\texttt{evaluate}, \\texttt{facm}, \\texttt{faem}, \\texttt{fastica!}, \\texttt{fit}, \\texttt{gram2dmat}, \\texttt{gram2dmat!}, \\texttt{indim}, \\texttt{invsqrtm}, \\texttt{isotonic}, \\texttt{ldacov}, \\texttt{llsq}, \\texttt{loadings}, \\texttt{mclda\\_solve}, \\texttt{multiclass\\_lda}, \\texttt{multiclass\\_lda\\_stats}, \\texttt{outdim}, \\texttt{pcacov}, \\texttt{pcasvd}, \\texttt{ppcaem}, \\texttt{ppcaml}, \\texttt{predict}, \\texttt{principalratio}, \\texttt{principalvar}, \\texttt{principalvars}, \\texttt{projection}, \\texttt{reconstruct}, \\texttt{ridge}, \\texttt{stress}, \\texttt{tprincipalvar}, \\texttt{transform}, \\texttt{tresidualvar}, \\texttt{tvar}, \\texttt{var}, \\texttt{withclass\\_scatter}, \\texttt{xindim}, \\texttt{xmean}, \\texttt{xprojection}, \\texttt{xtransform}, \\texttt{yindim}, \\texttt{ymean}, \\texttt{yprojection}, \\texttt{ytransform}\n",
       "\n",
       "\\section{Displaying contents of readme found at \\texttt{D:{\\textbackslash}TongYuan{\\textbackslash}.julia{\\textbackslash}packages{\\textbackslash}MultivariateStats{\\textbackslash}zLpz8{\\textbackslash}README.md}}\n",
       "\\section{Multivariate Statistics}\n",
       "A Julia package for multivariate statistics and data analysis (e.g. dimensionality reduction).\n",
       "\n",
       "\\href{https://coveralls.io/r/JuliaStats/MultivariateStats.jl?branch=master}{\\begin{figure}\n",
       "\\centering\n",
       "\\includegraphics{https://coveralls.io/repos/JuliaStats/MultivariateStats.jl/badge.svg?branch=master}\n",
       "\\caption{Coverage Status}\n",
       "\\end{figure}\n",
       "} \\href{https://travis-ci.org/JuliaStats/MultivariateStats.jl}{\\begin{figure}\n",
       "\\centering\n",
       "\\includegraphics{https://travis-ci.org/JuliaStats/MultivariateStats.jl.svg?branch=master}\n",
       "\\caption{Build Status}\n",
       "\\end{figure}\n",
       "} \\href{https://github.com/JuliaStats/MultivariateStats.jl/actions/workflows/ci.yml}{\\begin{figure}\n",
       "\\centering\n",
       "\\includegraphics{https://github.com/JuliaStats/MultivariateStats.jl/actions/workflows/ci.yml/badge.svg}\n",
       "\\caption{CI}\n",
       "\\end{figure}\n",
       "} \\href{https://juliastats.org/MultivariateStats.jl/stable}{\\begin{figure}\n",
       "\\centering\n",
       "\\includegraphics{https://img.shields.io/badge/docs-stable-blue.svg}\n",
       "\\caption{}\n",
       "\\end{figure}\n",
       "} \\href{https://juliastats.org/MultivariateStats.jl/dev}{\\begin{figure}\n",
       "\\centering\n",
       "\\includegraphics{https://img.shields.io/badge/docs-dev-blue.svg}\n",
       "\\caption{}\n",
       "\\end{figure}\n",
       "}\n",
       "\n",
       "\\subsection{Functionalities}\n",
       "\\paragraph{Available}\n",
       "\\begin{itemize}\n",
       "\\item Linear Least Square Regression\n",
       "\n",
       "\n",
       "\\item Ridge Regression\n",
       "\n",
       "\n",
       "\\item Isotonic Regression\n",
       "\n",
       "\n",
       "\\item Data Whitening\n",
       "\n",
       "\n",
       "\\item Principal Components Analysis (PCA)\n",
       "\n",
       "\n",
       "\\item Canonical Correlation Analysis (CCA)\n",
       "\n",
       "\n",
       "\\item Classical Multidimensional Scaling (MDS)\n",
       "\n",
       "\n",
       "\\item Linear Discriminant Analysis (LDA)\n",
       "\n",
       "\n",
       "\\item Multi-class LDA\n",
       "\n",
       "\n",
       "\\item Independent Component Analysis (ICA), FastICA\n",
       "\n",
       "\n",
       "\\item Probabilistic PCA\n",
       "\n",
       "\n",
       "\\item Factor Analysis\n",
       "\n",
       "\n",
       "\\item Kernel PCA\n",
       "\n",
       "\\end{itemize}\n",
       "\\paragraph{Future Plan}\n",
       "\\begin{itemize}\n",
       "\\item Partial Least Square (PLS)\n",
       "\n",
       "\n",
       "\\item Other algorithms for ICA (\\emph{e.g.} JADE)\n",
       "\n",
       "\\end{itemize}\n"
      ],
      "text/markdown": [
       "No docstring found for module `MultivariateStats`.\n",
       "\n",
       "# Exported names\n",
       "\n",
       "`CCA`, `FactorAnalysis`, `ICA`, `KernelPCA`, `LinearDiscriminant`, `MDS`, `MulticlassLDA`, `MulticlassLDAStats`, `PCA`, `PPCA`, `SubspaceLDA`, `Whitening`, `bayespca`, `betweenclass_scatter`, `ccacov`, `ccasvd`, `centralize`, `classical_mds`, `classmeans`, `classweights`, `cor`, `correlations`, `cov_whitening`, `cov_whitening!`, `decentralize`, `dmat2gram`, `dmat2gram!`, `eigvals`, `eigvecs`, `evaluate`, `facm`, `faem`, `fastica!`, `fit`, `gram2dmat`, `gram2dmat!`, `indim`, `invsqrtm`, `isotonic`, `ldacov`, `llsq`, `loadings`, `mclda_solve`, `multiclass_lda`, `multiclass_lda_stats`, `outdim`, `pcacov`, `pcasvd`, `ppcaem`, `ppcaml`, `predict`, `principalratio`, `principalvar`, `principalvars`, `projection`, `reconstruct`, `ridge`, `stress`, `tprincipalvar`, `transform`, `tresidualvar`, `tvar`, `var`, `withclass_scatter`, `xindim`, `xmean`, `xprojection`, `xtransform`, `yindim`, `ymean`, `yprojection`, `ytransform`\n",
       "\n",
       "# Displaying contents of readme found at `D:\\TongYuan\\.julia\\packages\\MultivariateStats\\zLpz8\\README.md`\n",
       "\n",
       "# Multivariate Statistics\n",
       "\n",
       "A Julia package for multivariate statistics and data analysis (e.g. dimensionality reduction).\n",
       "\n",
       "[![Coverage Status](https://coveralls.io/repos/JuliaStats/MultivariateStats.jl/badge.svg?branch=master)](https://coveralls.io/r/JuliaStats/MultivariateStats.jl?branch=master) [![Build Status](https://travis-ci.org/JuliaStats/MultivariateStats.jl.svg?branch=master)](https://travis-ci.org/JuliaStats/MultivariateStats.jl) [![CI](https://github.com/JuliaStats/MultivariateStats.jl/actions/workflows/ci.yml/badge.svg)](https://github.com/JuliaStats/MultivariateStats.jl/actions/workflows/ci.yml) [![](https://img.shields.io/badge/docs-stable-blue.svg)](https://juliastats.org/MultivariateStats.jl/stable) [![](https://img.shields.io/badge/docs-dev-blue.svg)](https://juliastats.org/MultivariateStats.jl/dev)\n",
       "\n",
       "## Functionalities\n",
       "\n",
       "#### Available\n",
       "\n",
       "  * Linear Least Square Regression\n",
       "  * Ridge Regression\n",
       "  * Isotonic Regression\n",
       "  * Data Whitening\n",
       "  * Principal Components Analysis (PCA)\n",
       "  * Canonical Correlation Analysis (CCA)\n",
       "  * Classical Multidimensional Scaling (MDS)\n",
       "  * Linear Discriminant Analysis (LDA)\n",
       "  * Multi-class LDA\n",
       "  * Independent Component Analysis (ICA), FastICA\n",
       "  * Probabilistic PCA\n",
       "  * Factor Analysis\n",
       "  * Kernel PCA\n",
       "\n",
       "#### Future Plan\n",
       "\n",
       "  * Partial Least Square (PLS)\n",
       "  * Other algorithms for ICA (*e.g.* JADE)\n"
      ],
      "text/plain": [
       "  No docstring found for module \u001b[36mMultivariateStats\u001b[39m.\n",
       "\n",
       "\u001b[1m  Exported names\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "  \u001b[36mCCA\u001b[39m, \u001b[36mFactorAnalysis\u001b[39m, \u001b[36mICA\u001b[39m, \u001b[36mKernelPCA\u001b[39m, \u001b[36mLinearDiscriminant\u001b[39m, \u001b[36mMDS\u001b[39m, \u001b[36mMulticlassLDA\u001b[39m,\n",
       "  \u001b[36mMulticlassLDAStats\u001b[39m, \u001b[36mPCA\u001b[39m, \u001b[36mPPCA\u001b[39m, \u001b[36mSubspaceLDA\u001b[39m, \u001b[36mWhitening\u001b[39m, \u001b[36mbayespca\u001b[39m,\n",
       "  \u001b[36mbetweenclass_scatter\u001b[39m, \u001b[36mccacov\u001b[39m, \u001b[36mccasvd\u001b[39m, \u001b[36mcentralize\u001b[39m, \u001b[36mclassical_mds\u001b[39m, \u001b[36mclassmeans\u001b[39m,\n",
       "  \u001b[36mclassweights\u001b[39m, \u001b[36mcor\u001b[39m, \u001b[36mcorrelations\u001b[39m, \u001b[36mcov_whitening\u001b[39m, \u001b[36mcov_whitening!\u001b[39m,\n",
       "  \u001b[36mdecentralize\u001b[39m, \u001b[36mdmat2gram\u001b[39m, \u001b[36mdmat2gram!\u001b[39m, \u001b[36meigvals\u001b[39m, \u001b[36meigvecs\u001b[39m, \u001b[36mevaluate\u001b[39m, \u001b[36mfacm\u001b[39m, \u001b[36mfaem\u001b[39m,\n",
       "  \u001b[36mfastica!\u001b[39m, \u001b[36mfit\u001b[39m, \u001b[36mgram2dmat\u001b[39m, \u001b[36mgram2dmat!\u001b[39m, \u001b[36mindim\u001b[39m, \u001b[36minvsqrtm\u001b[39m, \u001b[36misotonic\u001b[39m, \u001b[36mldacov\u001b[39m,\n",
       "  \u001b[36mllsq\u001b[39m, \u001b[36mloadings\u001b[39m, \u001b[36mmclda_solve\u001b[39m, \u001b[36mmulticlass_lda\u001b[39m, \u001b[36mmulticlass_lda_stats\u001b[39m, \u001b[36moutdim\u001b[39m,\n",
       "  \u001b[36mpcacov\u001b[39m, \u001b[36mpcasvd\u001b[39m, \u001b[36mppcaem\u001b[39m, \u001b[36mppcaml\u001b[39m, \u001b[36mpredict\u001b[39m, \u001b[36mprincipalratio\u001b[39m, \u001b[36mprincipalvar\u001b[39m,\n",
       "  \u001b[36mprincipalvars\u001b[39m, \u001b[36mprojection\u001b[39m, \u001b[36mreconstruct\u001b[39m, \u001b[36mridge\u001b[39m, \u001b[36mstress\u001b[39m, \u001b[36mtprincipalvar\u001b[39m,\n",
       "  \u001b[36mtransform\u001b[39m, \u001b[36mtresidualvar\u001b[39m, \u001b[36mtvar\u001b[39m, \u001b[36mvar\u001b[39m, \u001b[36mwithclass_scatter\u001b[39m, \u001b[36mxindim\u001b[39m, \u001b[36mxmean\u001b[39m,\n",
       "  \u001b[36mxprojection\u001b[39m, \u001b[36mxtransform\u001b[39m, \u001b[36myindim\u001b[39m, \u001b[36mymean\u001b[39m, \u001b[36myprojection\u001b[39m, \u001b[36mytransform\u001b[39m\n",
       "\n",
       "\u001b[1m  Displaying contents of readme found at\u001b[22m\n",
       "\u001b[1m \u001b[36mD:\\TongYuan\\.julia\\packages\\MultivariateStats\\zLpz8\\README.md\u001b[39m\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "\u001b[1m  Multivariate Statistics\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "  A Julia package for multivariate statistics and data analysis (e.g.\n",
       "  dimensionality reduction).\n",
       "\n",
       "  (Image: Coverage Status)\n",
       "  (https://coveralls.io/r/JuliaStats/MultivariateStats.jl?branch=master)\n",
       "  (Image: Build Status)\n",
       "  (https://travis-ci.org/JuliaStats/MultivariateStats.jl) (Image: CI)\n",
       "  (https://github.com/JuliaStats/MultivariateStats.jl/actions/workflows/ci.yml)\n",
       "  (Image: ) (https://juliastats.org/MultivariateStats.jl/stable) (Image: )\n",
       "  (https://juliastats.org/MultivariateStats.jl/dev)\n",
       "\n",
       "\u001b[1m  Functionalities\u001b[22m\n",
       "\u001b[1m  =================\u001b[22m\n",
       "\n",
       "\u001b[1m  Available\u001b[22m\n",
       "\u001b[1m  -----------\u001b[22m\n",
       "\n",
       "    •  Linear Least Square Regression\n",
       "\n",
       "    •  Ridge Regression\n",
       "\n",
       "    •  Isotonic Regression\n",
       "\n",
       "    •  Data Whitening\n",
       "\n",
       "    •  Principal Components Analysis (PCA)\n",
       "\n",
       "    •  Canonical Correlation Analysis (CCA)\n",
       "\n",
       "    •  Classical Multidimensional Scaling (MDS)\n",
       "\n",
       "    •  Linear Discriminant Analysis (LDA)\n",
       "\n",
       "    •  Multi-class LDA\n",
       "\n",
       "    •  Independent Component Analysis (ICA), FastICA\n",
       "\n",
       "    •  Probabilistic PCA\n",
       "\n",
       "    •  Factor Analysis\n",
       "\n",
       "    •  Kernel PCA\n",
       "\n",
       "\u001b[1m  Future Plan\u001b[22m\n",
       "\u001b[1m  -------------\u001b[22m\n",
       "\n",
       "    •  Partial Least Square (PLS)\n",
       "\n",
       "    •  Other algorithms for ICA (\u001b[4me.g.\u001b[24m JADE)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?MultivariateStats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-06-10T16:41:02.403000+08:00",
     "start_time": "2022-06-10T08:41:02.203Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "Fit a statistical model.\n",
       "\n",
       "\\rule{\\textwidth}{1pt}\n",
       "\\begin{verbatim}\n",
       "fit(Histogram, data[, weight][, edges]; closed=:left, nbins)\n",
       "\\end{verbatim}\n",
       "Fit a histogram to \\texttt{data}.\n",
       "\n",
       "\\section{Arguments}\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{data}: either a vector (for a 1-dimensional histogram), or a tuple of vectors of equal length (for an \\emph{n}-dimensional histogram).\n",
       "\n",
       "\n",
       "\\item \\texttt{weight}: an optional \\texttt{AbstractWeights} (of the same length as the data vectors), denoting the weight each observation contributes to the bin. If no weight vector is supplied, each observation has weight 1.\n",
       "\n",
       "\n",
       "\\item \\texttt{edges}: a vector (typically an \\texttt{AbstractRange} object), or tuple of vectors, that gives the edges of the bins along each dimension. If no edges are provided, these are determined from the data.\n",
       "\n",
       "\\end{itemize}\n",
       "\\section{Keyword arguments}\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{closed}: if \\texttt{:left} (the default), the bin intervals are left-closed [a,b); if \\texttt{:right}, intervals are right-closed (a,b].\n",
       "\n",
       "\n",
       "\\item \\texttt{nbins}: if no \\texttt{edges} argument is supplied, the approximate number of bins to use along each dimension (can be either a single integer, or a tuple of integers).\n",
       "\n",
       "\\end{itemize}\n",
       "\\section{Examples}\n",
       "\\begin{verbatim}\n",
       "# Univariate\n",
       "h = fit(Histogram, rand(100))\n",
       "h = fit(Histogram, rand(100), 0:0.1:1.0)\n",
       "h = fit(Histogram, rand(100), nbins=10)\n",
       "h = fit(Histogram, rand(100), weights(rand(100)), 0:0.1:1.0)\n",
       "h = fit(Histogram, [20], 0:20:100)\n",
       "h = fit(Histogram, [20], 0:20:100, closed=:right)\n",
       "\n",
       "# Multivariate\n",
       "h = fit(Histogram, (rand(100),rand(100)))\n",
       "h = fit(Histogram, (rand(100),rand(100)),nbins=10)\n",
       "\\end{verbatim}\n",
       "\\rule{\\textwidth}{1pt}\n",
       "\\begin{verbatim}\n",
       "fit(ZScoreTransform, X; dims=nothing, center=true, scale=true)\n",
       "\\end{verbatim}\n",
       "Fit standardization parameters to vector or matrix \\texttt{X} and return a \\texttt{ZScoreTransform} transformation object.\n",
       "\n",
       "\\section{Keyword arguments}\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{dims}: if \\texttt{1} fit standardization parameters in column-wise fashion; if \\texttt{2} fit in row-wise fashion. The default is \\texttt{nothing}, which is equivalent to \\texttt{dims=2} with a deprecation warning.\n",
       "\n",
       "\n",
       "\\item \\texttt{center}: if \\texttt{true} (the default) center data so that its mean is zero.\n",
       "\n",
       "\n",
       "\\item \\texttt{scale}: if \\texttt{true} (the default) scale the data so that its variance is equal to one.\n",
       "\n",
       "\\end{itemize}\n",
       "\\section{Examples}\n",
       "\\begin{verbatim}\n",
       "julia> using StatsBase\n",
       "\n",
       "julia> X = [0.0 -0.5 0.5; 0.0 1.0 2.0]\n",
       "2×3 Matrix{Float64}:\n",
       " 0.0  -0.5  0.5\n",
       " 0.0   1.0  2.0\n",
       "\n",
       "julia> dt = fit(ZScoreTransform, X, dims=2)\n",
       "ZScoreTransform{Float64, Vector{Float64}}(2, 2, [0.0, 1.0], [0.5, 1.0])\n",
       "\n",
       "julia> StatsBase.transform(dt, X)\n",
       "2×3 Matrix{Float64}:\n",
       "  0.0  -1.0  1.0\n",
       " -1.0   0.0  1.0\n",
       "\\end{verbatim}\n",
       "\\rule{\\textwidth}{1pt}\n",
       "\\begin{verbatim}\n",
       "fit(UnitRangeTransform, X; dims=nothing, unit=true)\n",
       "\\end{verbatim}\n",
       "Fit a scaling parameters to vector or matrix \\texttt{X} and return a \\texttt{UnitRangeTransform} transformation object.\n",
       "\n",
       "\\section{Keyword arguments}\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{dims}: if \\texttt{1} fit standardization parameters in column-wise fashion;\n",
       "\n",
       "\\end{itemize}\n",
       "if \\texttt{2} fit in row-wise fashion. The default is \\texttt{nothing}.\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{unit}: if \\texttt{true} (the default) shift the minimum data to zero.\n",
       "\n",
       "\\end{itemize}\n",
       "\\section{Examples}\n",
       "\\begin{verbatim}\n",
       "julia> using StatsBase\n",
       "\n",
       "julia> X = [0.0 -0.5 0.5; 0.0 1.0 2.0]\n",
       "2×3 Matrix{Float64}:\n",
       " 0.0  -0.5  0.5\n",
       " 0.0   1.0  2.0\n",
       "\n",
       "julia> dt = fit(UnitRangeTransform, X, dims=2)\n",
       "UnitRangeTransform{Float64, Vector{Float64}}(2, 2, true, [-0.5, 0.0], [1.0, 0.5])\n",
       "\n",
       "julia> StatsBase.transform(dt, X)\n",
       "2×3 Matrix{Float64}:\n",
       " 0.5  0.0  1.0\n",
       " 0.0  0.5  1.0\n",
       "\\end{verbatim}\n",
       "\\rule{\\textwidth}{1pt}\n",
       "\\begin{verbatim}\n",
       "fit(Whitening, X::AbstractMatrix{T}; kwargs...)\n",
       "\\end{verbatim}\n",
       "Estimate a whitening transform from the data given in \\texttt{X}.\n",
       "\n",
       "This function returns an instance of \\href{@ref}{\\texttt{Whitening}}\n",
       "\n",
       "\\textbf{Keyword Arguments:}\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{regcoef}: The regularization coefficient. The covariance will be regularized as follows when \\texttt{regcoef} is positive \\texttt{C + (eigmax(C) * regcoef) * eye(d)}. Default values is \\texttt{zero(T)}.\n",
       "\n",
       "\n",
       "\\item \\texttt{dims}: if \\texttt{1} the transformation calculated from the row samples. fit standardization parameters in column-wise fashion; if \\texttt{2} the transformation calculated from the column samples. The default is \\texttt{nothing}, which is equivalent to \\texttt{dims=2} with a deprecation warning.\n",
       "\n",
       "\n",
       "\\item \\texttt{mean}: The mean vector, which can be either of:\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{0}: the input data has already been centralized\n",
       "\n",
       "\n",
       "\\item \\texttt{nothing}: this function will compute the mean (\\textbf{default})\n",
       "\n",
       "\n",
       "\\item a pre-computed mean vector\n",
       "\n",
       "\\end{itemize}\n",
       "\\end{itemize}\n",
       "\\textbf{Note:} This function internally relies on \\href{@ref}{\\texttt{cov\\_whitening}} to derive the transformation \\texttt{W}.\n",
       "\n",
       "\\rule{\\textwidth}{1pt}\n",
       "\\begin{verbatim}\n",
       "fit(PCA, X; ...)\n",
       "\\end{verbatim}\n",
       "Perform PCA over the data given in a matrix \\texttt{X}. Each column of \\texttt{X} is an \\textbf{observation}.\n",
       "\n",
       "\\textbf{Keyword arguments}\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{method}: The choice of methods:\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{:auto}: use \\texttt{:cov} when \\texttt{d < n} or \\texttt{:svd} otherwise (\\emph{default}).\n",
       "\n",
       "\n",
       "\\item \\texttt{:cov}: based on covariance matrix decomposition.\n",
       "\n",
       "\n",
       "\\item \\texttt{:svd}: based on SVD of the input data.\n",
       "\n",
       "\\end{itemize}\n",
       "\n",
       "\\item \\texttt{maxoutdim}: The output dimension, i.e. dimension of the transformed space (\\emph{min(d, nc-1)})\n",
       "\n",
       "\n",
       "\\item \\texttt{pratio}: The ratio of variances preserved in the principal subspace (\\emph{0.99})\n",
       "\n",
       "\n",
       "\\item \\texttt{mean}: The mean vector, which can be either of\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{0}: the input data has already been centralized\n",
       "\n",
       "\n",
       "\\item \\texttt{nothing}: this function will compute the mean (\\emph{default})\n",
       "\n",
       "\n",
       "\\item a pre-computed mean vector\n",
       "\n",
       "\\end{itemize}\n",
       "\\end{itemize}\n",
       "\\textbf{Notes:}\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item The output dimension \\texttt{p} depends on both \\texttt{maxoutdim} and \\texttt{pratio}, as follows. Suppose the first \\texttt{k} principal components preserve at least \\texttt{pratio} of the total variance, while the first \\texttt{k-1} preserves less than \\texttt{pratio}, then the actual output dimension will be $\\min(k, maxoutdim)$.\n",
       "\n",
       "\n",
       "\\item This function calls \\href{@ref}{\\texttt{pcacov}} or \\href{@ref}{\\texttt{pcasvd}} internally, depending on the choice of method.\n",
       "\n",
       "\\end{itemize}\n",
       "\\rule{\\textwidth}{1pt}\n",
       "\\begin{verbatim}\n",
       "fit(PPCA, X; ...)\n",
       "\\end{verbatim}\n",
       "Perform probabilistic PCA over the data given in a matrix \\texttt{X}. Each column of \\texttt{X} is an observation. This method returns an instance of \\href{@ref}{\\texttt{PPCA}}.\n",
       "\n",
       "\\textbf{Keyword arguments:}\n",
       "\n",
       "Let \\texttt{(d, n) = size(X)} be respectively the input dimension and the number of observations:\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{method}: The choice of methods:\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{:ml}: use maximum likelihood version of probabilistic PCA (\\emph{default})\n",
       "\n",
       "\n",
       "\\item \\texttt{:em}: use EM version of probabilistic PCA\n",
       "\n",
       "\n",
       "\\item \\texttt{:bayes}: use Bayesian PCA\n",
       "\n",
       "\\end{itemize}\n",
       "\n",
       "\\item \\texttt{maxoutdim}: Maximum output dimension (\\emph{default} \\texttt{d-1})\n",
       "\n",
       "\n",
       "\\item \\texttt{mean}: The mean vector, which can be either of:\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{0}: the input data has already been centralized\n",
       "\n",
       "\n",
       "\\item \\texttt{nothing}: this function will compute the mean (\\emph{default})\n",
       "\n",
       "\n",
       "\\item a pre-computed mean vector\n",
       "\n",
       "\\end{itemize}\n",
       "\n",
       "\\item \\texttt{tol}: Convergence tolerance (\\emph{default} \\texttt{1.0e-6})\n",
       "\n",
       "\n",
       "\\item \\texttt{maxiter}: Maximum number of iterations (\\emph{default} \\texttt{1000})\n",
       "\n",
       "\\end{itemize}\n",
       "\\textbf{Notes:} This function calls \\href{@ref}{\\texttt{ppcaml}}, \\href{@ref}{\\texttt{ppcaem}} or \\href{@ref}{\\texttt{bayespca}} internally, depending on the choice of method.\n",
       "\n",
       "\\rule{\\textwidth}{1pt}\n",
       "Fit \\texttt{KernelCenter} object\n",
       "\n",
       "\\rule{\\textwidth}{1pt}\n",
       "\\begin{verbatim}\n",
       "fit(KernelPCA, X; ...)\n",
       "\\end{verbatim}\n",
       "Perform kernel PCA over the data given in a matrix \\texttt{X}. Each column of \\texttt{X} is an observation.\n",
       "\n",
       "This method returns an instance of \\href{@ref}{\\texttt{KernelPCA}}.\n",
       "\n",
       "\\textbf{Keyword arguments:}\n",
       "\n",
       "Let \\texttt{(d, n) = size(X)} be respectively the input dimension and the number of observations:\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{kernel}: The kernel function. This functions accepts two vector arguments \\texttt{x} and \\texttt{y},\n",
       "\n",
       "\\end{itemize}\n",
       "and returns a scalar value (\\emph{default:} \\texttt{(x,y)->x'y})\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{solver}: The choice of solver:\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{:eig}: uses \\texttt{LinearAlgebra.eigen} (\\emph{default})\n",
       "\n",
       "\n",
       "\\item \\texttt{:eigs}: uses \\texttt{Arpack.eigs} (always used for sparse data)\n",
       "\n",
       "\\end{itemize}\n",
       "\n",
       "\\item \\texttt{maxoutdim}:  Maximum output dimension (\\emph{default} \\texttt{min(d, n)})\n",
       "\n",
       "\n",
       "\\item \\texttt{inverse}: Whether to perform calculation for inverse transform for non-precomputed kernels (\\emph{default} \\texttt{false})\n",
       "\n",
       "\n",
       "\\item \\texttt{β}: Hyperparameter of the ridge regression that learns the inverse transform (\\emph{default} \\texttt{1} when \\texttt{inverse} is \\texttt{true}).\n",
       "\n",
       "\n",
       "\\item \\texttt{tol}: Convergence tolerance for \\texttt{eigs} solver (\\emph{default} \\texttt{0.0})\n",
       "\n",
       "\n",
       "\\item \\texttt{maxiter}: Maximum number of iterations for \\texttt{eigs} solver (\\emph{default} \\texttt{300})\n",
       "\n",
       "\\end{itemize}\n",
       "\\rule{\\textwidth}{1pt}\n",
       "\\begin{verbatim}\n",
       "fit(CCA, X, Y; ...)\n",
       "\\end{verbatim}\n",
       "Perform CCA over the data given in matrices \\texttt{X} and \\texttt{Y}. Each column of \\texttt{X} and \\texttt{Y} is an observation.\n",
       "\n",
       "\\texttt{X} and \\texttt{Y} should have the same number of columns (denoted by \\texttt{n} below).\n",
       "\n",
       "This method returns an instance of \\href{@ref}{\\texttt{CCA}}.\n",
       "\n",
       "\\textbf{Keyword arguments:}\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{method}: The choice of methods:\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{:cov}: based on covariance matrices\n",
       "\n",
       "\n",
       "\\item \\texttt{:svd}: based on SVD of the input data (\\emph{default})\n",
       "\n",
       "\\end{itemize}\n",
       "\n",
       "\\item \\texttt{outdim}: The output dimension, \\emph{i.e} dimension of the common space (\\emph{default}: \\texttt{min(dx, dy, n)})\n",
       "\n",
       "\n",
       "\\item \\texttt{mean}: The mean vector, which can be either of:\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{0}: the input data has already been centralized\n",
       "\n",
       "\n",
       "\\item \\texttt{nothing}: this function will compute the mean (\\emph{default})\n",
       "\n",
       "\n",
       "\\item a pre-computed mean vector\n",
       "\n",
       "\\end{itemize}\n",
       "\\end{itemize}\n",
       "\\textbf{Notes:} This function calls \\href{@ref}{\\texttt{ccacov}} or \\href{@ref}{\\texttt{ccasvd}} internally, depending on the choice of method.\n",
       "\n",
       "\\rule{\\textwidth}{1pt}\n",
       "\\begin{verbatim}\n",
       "fit(MDS, X; kwargs...)\n",
       "\\end{verbatim}\n",
       "Compute an embedding of \\texttt{X} points by classical multidimensional scaling (MDS). There are two calling options, specified via the required keyword argument \\texttt{distances}:\n",
       "\n",
       "\\begin{verbatim}\n",
       "mds = fit(MDS, X; distances=false, maxoutdim=size(X,1)-1)\n",
       "\\end{verbatim}\n",
       "where \\texttt{X} is the data matrix. Distances between pairs of columns of \\texttt{X} are computed using the Euclidean norm. This is equivalent to performing PCA on \\texttt{X}.\n",
       "\n",
       "\\begin{verbatim}\n",
       "mds = fit(MDS, D; distances=true, maxoutdim=size(D,1)-1)\n",
       "\\end{verbatim}\n",
       "where \\texttt{D} is a symmetric matrix \\texttt{D} of distances between points.\n",
       "\n",
       "\\rule{\\textwidth}{1pt}\n",
       "\\begin{verbatim}\n",
       "fit(LinearDiscriminant, Xp, Xn; covestimator = SimpleCovariance())\n",
       "\\end{verbatim}\n",
       "Performs LDA given both positive and negative samples. The function accepts follwing parameters:\n",
       "\n",
       "\\textbf{Parameters}\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{Xp}: The sample matrix of the positive class.\n",
       "\n",
       "\n",
       "\\item \\texttt{Xn}: The sample matrix of the negative class.\n",
       "\n",
       "\\end{itemize}\n",
       "\\textbf{Keyword arguments:}\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{covestimator}: Custom covariance estimator for between-class covariance. The covariance matrix will be calculated as \\texttt{cov(covestimator\\_between, \\#=data=\\#; dims=2, mean=zeros(\\#=...=\\#)}. Custom covariance estimators, available in other packages, may result in more robust discriminants for data with more features than observations.\n",
       "\n",
       "\\end{itemize}\n",
       "\\rule{\\textwidth}{1pt}\n",
       "\\begin{verbatim}\n",
       "fit(MulticlassLDA, nc, X, y; ...)\n",
       "\\end{verbatim}\n",
       "Perform multi-class LDA over a given data set \\texttt{X} and collecttion of labels \\texttt{y}.\n",
       "\n",
       "This function returns the resultant multi-class LDA model as an instance of \\href{@ref}{\\texttt{MulticlassLDA}}.\n",
       "\n",
       "\\emph{Parameters}\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{nc}:  the number of classes\n",
       "\n",
       "\n",
       "\\item \\texttt{X}:   the matrix of input samples, of size \\texttt{(d, n)}. Each column in \\texttt{X} is an observation.\n",
       "\n",
       "\n",
       "\\item \\texttt{y}:   the vector of class labels, of length \\texttt{n}. Each element of \\texttt{y} must be an integer between \\texttt{1} and \\texttt{nc}.\n",
       "\n",
       "\\end{itemize}\n",
       "\\textbf{Keyword arguments}\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{method}: The choice of methods:\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{:gevd}: based on generalized eigenvalue decomposition (\\emph{default}).\n",
       "\n",
       "\n",
       "\\item \\texttt{:whiten}: first derive a whitening transform from \\texttt{Sw} and then solve the problem based on eigenvalue\n",
       "\n",
       "\\end{itemize}\n",
       "decomposition of the whiten \\texttt{Sb}.\n",
       "\n",
       "\n",
       "\\item \\texttt{outdim}: The output dimension, i.e. dimension of the transformed space \\texttt{min(d, nc-1)}\n",
       "\n",
       "\n",
       "\\item \\texttt{regcoef}: The regularization coefficient (\\emph{default:} \\texttt{1.0e-6}). A positive value \\texttt{regcoef * eigmax(Sw)}   is added to the diagonal of \\texttt{Sw} to improve numerical stability.\n",
       "\n",
       "\n",
       "\\item \\texttt{covestimator\\_between}: Custom covariance estimator for between-class covariance (\\emph{default:} \\texttt{SimpleCovariance()}).   The covariance matrix will be calculated as \\texttt{cov(covestimator\\_between, \\#=data=\\#; dims=2, mean=zeros(\\#=...=\\#))}.   Custom covariance estimators, available in other packages, may result in more robust discriminants for data   with more features than observations.\n",
       "\n",
       "\n",
       "\\item \\texttt{covestimator\\_within}:  Custom covariance estimator for within-class covariance (\\emph{default:} \\texttt{SimpleCovariance()}).   The covariance matrix will be calculated as \\texttt{cov(covestimator\\_within, \\#=data=\\#; dims=2, mean=zeros(nc))}.   Custom covariance estimators, available in other packages, may result in more robust discriminants for data   with more features than observations.\n",
       "\n",
       "\\end{itemize}\n",
       "\\textbf{Notes:}\n",
       "\n",
       "The resultant projection matrix $P$ satisfies:\n",
       "\n",
       "$$\\mathbf{P}^T (\\mathbf{S}_w + \\kappa \\mathbf{I}) \\mathbf{P} = \\mathbf{I}$$\n",
       "Here, $\\kappa$ equals \\texttt{regcoef * eigmax(Sw)}. The columns of $P$ are arranged in descending order of the corresponding generalized eigenvalues.\n",
       "\n",
       "Note that \\href{@ref}{\\texttt{MulticlassLDA}} does not currently support the normalized version using $\\mathbf{S}_w^*$ and $\\mathbf{S}_b^*$ (see \\href{@ref}{\\texttt{SubspaceLDA}}).\n",
       "\n",
       "\\rule{\\textwidth}{1pt}\n",
       "\\begin{verbatim}\n",
       "fit(SubspaceLDA, X, labels; normalize=true)\n",
       "\\end{verbatim}\n",
       "Fit an subspace projection of LDA model using the equivalent of $\\mathbf{S}_w^*$ and $\\mathbf{S}_b^*$`.\n",
       "\n",
       "Note: Subspace LDA also supports the normalized version of LDA via the \\texttt{normalize} keyword.\n",
       "\n",
       "\\rule{\\textwidth}{1pt}\n",
       "\\begin{verbatim}\n",
       "fit(ICA, X, k; ...)\n",
       "\\end{verbatim}\n",
       "Perform ICA over the data set given in \\texttt{X}.\n",
       "\n",
       "\\textbf{Parameters:} -\\texttt{X}: The data matrix, of size $(m, n)$. Each row corresponds to a mixed signal, while each column corresponds to an observation (\\emph{e.g} all signal value at a particular time step). -\\texttt{k}: The number of independent components to recover.\n",
       "\n",
       "\\textbf{Keyword Arguments:}\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{alg}: The choice of algorithm (\\emph{default} \\texttt{:fastica})\n",
       "\n",
       "\n",
       "\\item \\texttt{fun}: The approx neg-entropy functor (\\emph{default} \\href{@ref}{\\texttt{Tanh}})\n",
       "\n",
       "\n",
       "\\item \\texttt{do\\_whiten}: Whether to perform pre-whitening (\\emph{default} \\texttt{true})\n",
       "\n",
       "\n",
       "\\item \\texttt{maxiter}: Maximum number of iterations (\\emph{default} \\texttt{100})\n",
       "\n",
       "\n",
       "\\item \\texttt{tol}: Tolerable change of $W$ at convergence (\\emph{default} \\texttt{1.0e-6})\n",
       "\n",
       "\n",
       "\\item \\texttt{mean}: The mean vector, which can be either of:\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{0}: the input data has already been centralized\n",
       "\n",
       "\n",
       "\\item \\texttt{nothing}: this function will compute the mean (\\emph{default})\n",
       "\n",
       "\n",
       "\\item a pre-computed mean vector\n",
       "\n",
       "\\end{itemize}\n",
       "\n",
       "\\item \\texttt{winit}: Initial guess of $W$, which should be either of:\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item empty matrix: the function will perform random initialization (\\emph{default})\n",
       "\n",
       "\n",
       "\\item a matrix of size $(k, k)$ (when \\texttt{do\\_whiten})\n",
       "\n",
       "\n",
       "\\item a matrix of size $(m, k)$ (when \\texttt{!do\\_whiten})\n",
       "\n",
       "\\end{itemize}\n",
       "\\end{itemize}\n",
       "Returns the resultant ICA model, an instance of type \\href{@ref}{\\texttt{ICA}}.\n",
       "\n",
       "\\textbf{Note:} If \\texttt{do\\_whiten} is \\texttt{true}, the return \\texttt{W} satisfies $\\mathbf{W}^T \\mathbf{C} \\mathbf{W} = \\mathbf{I}$, otherwise $W$ is orthonormal, \\emph{i.e} $\\mathbf{W}^T \\mathbf{W} = \\mathbf{I}$.\n",
       "\n",
       "\\rule{\\textwidth}{1pt}\n",
       "\\begin{verbatim}\n",
       "fit(FactorAnalysis, X; ...)\n",
       "\\end{verbatim}\n",
       "Perform factor analysis over the data given in a matrix \\texttt{X}. Each column of \\texttt{X} is an observation. This method returns an instance of \\href{@ref}{\\texttt{FactorAnalysis}}.\n",
       "\n",
       "\\textbf{Keyword arguments:}\n",
       "\n",
       "Let \\texttt{(d, n) = size(X)} be respectively the input dimension and the number of observations:\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{method}: The choice of methods:\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{:em}: use EM version of factor analysis\n",
       "\n",
       "\n",
       "\\item \\texttt{:cm}: use CM version of factor analysis (\\emph{default})\n",
       "\n",
       "\\end{itemize}\n",
       "\n",
       "\\item \\texttt{maxoutdim}: Maximum output dimension (\\emph{default} \\texttt{d-1})\n",
       "\n",
       "\n",
       "\\item \\texttt{mean}: The mean vector, which can be either of:\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{0}: the input data has already been centralized\n",
       "\n",
       "\n",
       "\\item \\texttt{nothing}: this function will compute the mean (\\emph{default})\n",
       "\n",
       "\n",
       "\\item a pre-computed mean vector\n",
       "\n",
       "\\end{itemize}\n",
       "\n",
       "\\item \\texttt{tol}: Convergence tolerance (\\emph{default} \\texttt{1.0e-6})\n",
       "\n",
       "\n",
       "\\item \\texttt{maxiter}: Maximum number of iterations (\\emph{default} \\texttt{1000})\n",
       "\n",
       "\n",
       "\\item \\texttt{η}: Variance low bound (\\emph{default} \\texttt{1.0e-6})\n",
       "\n",
       "\\end{itemize}\n",
       "\\textbf{Notes:} This function calls \\href{@ref}{\\texttt{facm}} or \\href{@ref}{\\texttt{faem}} internally, depending on the choice of method.\n",
       "\n"
      ],
      "text/markdown": [
       "Fit a statistical model.\n",
       "\n",
       "---\n",
       "\n",
       "```\n",
       "fit(Histogram, data[, weight][, edges]; closed=:left, nbins)\n",
       "```\n",
       "\n",
       "Fit a histogram to `data`.\n",
       "\n",
       "# Arguments\n",
       "\n",
       "  * `data`: either a vector (for a 1-dimensional histogram), or a tuple of vectors of equal length (for an *n*-dimensional histogram).\n",
       "  * `weight`: an optional `AbstractWeights` (of the same length as the data vectors), denoting the weight each observation contributes to the bin. If no weight vector is supplied, each observation has weight 1.\n",
       "  * `edges`: a vector (typically an `AbstractRange` object), or tuple of vectors, that gives the edges of the bins along each dimension. If no edges are provided, these are determined from the data.\n",
       "\n",
       "# Keyword arguments\n",
       "\n",
       "  * `closed`: if `:left` (the default), the bin intervals are left-closed [a,b); if `:right`, intervals are right-closed (a,b].\n",
       "  * `nbins`: if no `edges` argument is supplied, the approximate number of bins to use along each dimension (can be either a single integer, or a tuple of integers).\n",
       "\n",
       "# Examples\n",
       "\n",
       "```julia\n",
       "# Univariate\n",
       "h = fit(Histogram, rand(100))\n",
       "h = fit(Histogram, rand(100), 0:0.1:1.0)\n",
       "h = fit(Histogram, rand(100), nbins=10)\n",
       "h = fit(Histogram, rand(100), weights(rand(100)), 0:0.1:1.0)\n",
       "h = fit(Histogram, [20], 0:20:100)\n",
       "h = fit(Histogram, [20], 0:20:100, closed=:right)\n",
       "\n",
       "# Multivariate\n",
       "h = fit(Histogram, (rand(100),rand(100)))\n",
       "h = fit(Histogram, (rand(100),rand(100)),nbins=10)\n",
       "```\n",
       "\n",
       "---\n",
       "\n",
       "```\n",
       "fit(ZScoreTransform, X; dims=nothing, center=true, scale=true)\n",
       "```\n",
       "\n",
       "Fit standardization parameters to vector or matrix `X` and return a `ZScoreTransform` transformation object.\n",
       "\n",
       "# Keyword arguments\n",
       "\n",
       "  * `dims`: if `1` fit standardization parameters in column-wise fashion; if `2` fit in row-wise fashion. The default is `nothing`, which is equivalent to `dims=2` with a deprecation warning.\n",
       "  * `center`: if `true` (the default) center data so that its mean is zero.\n",
       "  * `scale`: if `true` (the default) scale the data so that its variance is equal to one.\n",
       "\n",
       "# Examples\n",
       "\n",
       "```jldoctest\n",
       "julia> using StatsBase\n",
       "\n",
       "julia> X = [0.0 -0.5 0.5; 0.0 1.0 2.0]\n",
       "2×3 Matrix{Float64}:\n",
       " 0.0  -0.5  0.5\n",
       " 0.0   1.0  2.0\n",
       "\n",
       "julia> dt = fit(ZScoreTransform, X, dims=2)\n",
       "ZScoreTransform{Float64, Vector{Float64}}(2, 2, [0.0, 1.0], [0.5, 1.0])\n",
       "\n",
       "julia> StatsBase.transform(dt, X)\n",
       "2×3 Matrix{Float64}:\n",
       "  0.0  -1.0  1.0\n",
       " -1.0   0.0  1.0\n",
       "```\n",
       "\n",
       "---\n",
       "\n",
       "```\n",
       "fit(UnitRangeTransform, X; dims=nothing, unit=true)\n",
       "```\n",
       "\n",
       "Fit a scaling parameters to vector or matrix `X` and return a `UnitRangeTransform` transformation object.\n",
       "\n",
       "# Keyword arguments\n",
       "\n",
       "  * `dims`: if `1` fit standardization parameters in column-wise fashion;\n",
       "\n",
       "if `2` fit in row-wise fashion. The default is `nothing`.\n",
       "\n",
       "  * `unit`: if `true` (the default) shift the minimum data to zero.\n",
       "\n",
       "# Examples\n",
       "\n",
       "```jldoctest\n",
       "julia> using StatsBase\n",
       "\n",
       "julia> X = [0.0 -0.5 0.5; 0.0 1.0 2.0]\n",
       "2×3 Matrix{Float64}:\n",
       " 0.0  -0.5  0.5\n",
       " 0.0   1.0  2.0\n",
       "\n",
       "julia> dt = fit(UnitRangeTransform, X, dims=2)\n",
       "UnitRangeTransform{Float64, Vector{Float64}}(2, 2, true, [-0.5, 0.0], [1.0, 0.5])\n",
       "\n",
       "julia> StatsBase.transform(dt, X)\n",
       "2×3 Matrix{Float64}:\n",
       " 0.5  0.0  1.0\n",
       " 0.0  0.5  1.0\n",
       "```\n",
       "\n",
       "---\n",
       "\n",
       "```\n",
       "fit(Whitening, X::AbstractMatrix{T}; kwargs...)\n",
       "```\n",
       "\n",
       "Estimate a whitening transform from the data given in `X`.\n",
       "\n",
       "This function returns an instance of [`Whitening`](@ref)\n",
       "\n",
       "**Keyword Arguments:**\n",
       "\n",
       "  * `regcoef`: The regularization coefficient. The covariance will be regularized as follows when `regcoef` is positive `C + (eigmax(C) * regcoef) * eye(d)`. Default values is `zero(T)`.\n",
       "  * `dims`: if `1` the transformation calculated from the row samples. fit standardization parameters in column-wise fashion; if `2` the transformation calculated from the column samples. The default is `nothing`, which is equivalent to `dims=2` with a deprecation warning.\n",
       "  * `mean`: The mean vector, which can be either of:\n",
       "\n",
       "      * `0`: the input data has already been centralized\n",
       "      * `nothing`: this function will compute the mean (**default**)\n",
       "      * a pre-computed mean vector\n",
       "\n",
       "**Note:** This function internally relies on [`cov_whitening`](@ref) to derive the transformation `W`.\n",
       "\n",
       "---\n",
       "\n",
       "```\n",
       "fit(PCA, X; ...)\n",
       "```\n",
       "\n",
       "Perform PCA over the data given in a matrix `X`. Each column of `X` is an **observation**.\n",
       "\n",
       "**Keyword arguments**\n",
       "\n",
       "  * `method`: The choice of methods:\n",
       "\n",
       "      * `:auto`: use `:cov` when `d < n` or `:svd` otherwise (*default*).\n",
       "      * `:cov`: based on covariance matrix decomposition.\n",
       "      * `:svd`: based on SVD of the input data.\n",
       "  * `maxoutdim`: The output dimension, i.e. dimension of the transformed space (*min(d, nc-1)*)\n",
       "  * `pratio`: The ratio of variances preserved in the principal subspace (*0.99*)\n",
       "  * `mean`: The mean vector, which can be either of\n",
       "\n",
       "      * `0`: the input data has already been centralized\n",
       "      * `nothing`: this function will compute the mean (*default*)\n",
       "      * a pre-computed mean vector\n",
       "\n",
       "**Notes:**\n",
       "\n",
       "  * The output dimension `p` depends on both `maxoutdim` and `pratio`, as follows. Suppose the first `k` principal components preserve at least `pratio` of the total variance, while the first `k-1` preserves less than `pratio`, then the actual output dimension will be $\\min(k, maxoutdim)$.\n",
       "  * This function calls [`pcacov`](@ref) or [`pcasvd`](@ref) internally, depending on the choice of method.\n",
       "\n",
       "---\n",
       "\n",
       "```\n",
       "fit(PPCA, X; ...)\n",
       "```\n",
       "\n",
       "Perform probabilistic PCA over the data given in a matrix `X`. Each column of `X` is an observation. This method returns an instance of [`PPCA`](@ref).\n",
       "\n",
       "**Keyword arguments:**\n",
       "\n",
       "Let `(d, n) = size(X)` be respectively the input dimension and the number of observations:\n",
       "\n",
       "  * `method`: The choice of methods:\n",
       "\n",
       "      * `:ml`: use maximum likelihood version of probabilistic PCA (*default*)\n",
       "      * `:em`: use EM version of probabilistic PCA\n",
       "      * `:bayes`: use Bayesian PCA\n",
       "  * `maxoutdim`: Maximum output dimension (*default* `d-1`)\n",
       "  * `mean`: The mean vector, which can be either of:\n",
       "\n",
       "      * `0`: the input data has already been centralized\n",
       "      * `nothing`: this function will compute the mean (*default*)\n",
       "      * a pre-computed mean vector\n",
       "  * `tol`: Convergence tolerance (*default* `1.0e-6`)\n",
       "  * `maxiter`: Maximum number of iterations (*default* `1000`)\n",
       "\n",
       "**Notes:** This function calls [`ppcaml`](@ref), [`ppcaem`](@ref) or [`bayespca`](@ref) internally, depending on the choice of method.\n",
       "\n",
       "---\n",
       "\n",
       "Fit `KernelCenter` object\n",
       "\n",
       "---\n",
       "\n",
       "```\n",
       "fit(KernelPCA, X; ...)\n",
       "```\n",
       "\n",
       "Perform kernel PCA over the data given in a matrix `X`. Each column of `X` is an observation.\n",
       "\n",
       "This method returns an instance of [`KernelPCA`](@ref).\n",
       "\n",
       "**Keyword arguments:**\n",
       "\n",
       "Let `(d, n) = size(X)` be respectively the input dimension and the number of observations:\n",
       "\n",
       "  * `kernel`: The kernel function. This functions accepts two vector arguments `x` and `y`,\n",
       "\n",
       "and returns a scalar value (*default:* `(x,y)->x'y`)\n",
       "\n",
       "  * `solver`: The choice of solver:\n",
       "\n",
       "      * `:eig`: uses `LinearAlgebra.eigen` (*default*)\n",
       "      * `:eigs`: uses `Arpack.eigs` (always used for sparse data)\n",
       "  * `maxoutdim`:  Maximum output dimension (*default* `min(d, n)`)\n",
       "  * `inverse`: Whether to perform calculation for inverse transform for non-precomputed kernels (*default* `false`)\n",
       "  * `β`: Hyperparameter of the ridge regression that learns the inverse transform (*default* `1` when `inverse` is `true`).\n",
       "  * `tol`: Convergence tolerance for `eigs` solver (*default* `0.0`)\n",
       "  * `maxiter`: Maximum number of iterations for `eigs` solver (*default* `300`)\n",
       "\n",
       "---\n",
       "\n",
       "```\n",
       "fit(CCA, X, Y; ...)\n",
       "```\n",
       "\n",
       "Perform CCA over the data given in matrices `X` and `Y`. Each column of `X` and `Y` is an observation.\n",
       "\n",
       "`X` and `Y` should have the same number of columns (denoted by `n` below).\n",
       "\n",
       "This method returns an instance of [`CCA`](@ref).\n",
       "\n",
       "**Keyword arguments:**\n",
       "\n",
       "  * `method`: The choice of methods:\n",
       "\n",
       "      * `:cov`: based on covariance matrices\n",
       "      * `:svd`: based on SVD of the input data (*default*)\n",
       "  * `outdim`: The output dimension, *i.e* dimension of the common space (*default*: `min(dx, dy, n)`)\n",
       "  * `mean`: The mean vector, which can be either of:\n",
       "\n",
       "      * `0`: the input data has already been centralized\n",
       "      * `nothing`: this function will compute the mean (*default*)\n",
       "      * a pre-computed mean vector\n",
       "\n",
       "**Notes:** This function calls [`ccacov`](@ref) or [`ccasvd`](@ref) internally, depending on the choice of method.\n",
       "\n",
       "---\n",
       "\n",
       "```\n",
       "fit(MDS, X; kwargs...)\n",
       "```\n",
       "\n",
       "Compute an embedding of `X` points by classical multidimensional scaling (MDS). There are two calling options, specified via the required keyword argument `distances`:\n",
       "\n",
       "```\n",
       "mds = fit(MDS, X; distances=false, maxoutdim=size(X,1)-1)\n",
       "```\n",
       "\n",
       "where `X` is the data matrix. Distances between pairs of columns of `X` are computed using the Euclidean norm. This is equivalent to performing PCA on `X`.\n",
       "\n",
       "```\n",
       "mds = fit(MDS, D; distances=true, maxoutdim=size(D,1)-1)\n",
       "```\n",
       "\n",
       "where `D` is a symmetric matrix `D` of distances between points.\n",
       "\n",
       "---\n",
       "\n",
       "```\n",
       "fit(LinearDiscriminant, Xp, Xn; covestimator = SimpleCovariance())\n",
       "```\n",
       "\n",
       "Performs LDA given both positive and negative samples. The function accepts follwing parameters:\n",
       "\n",
       "**Parameters**\n",
       "\n",
       "  * `Xp`: The sample matrix of the positive class.\n",
       "  * `Xn`: The sample matrix of the negative class.\n",
       "\n",
       "**Keyword arguments:**\n",
       "\n",
       "  * `covestimator`: Custom covariance estimator for between-class covariance. The covariance matrix will be calculated as `cov(covestimator_between, #=data=#; dims=2, mean=zeros(#=...=#)`. Custom covariance estimators, available in other packages, may result in more robust discriminants for data with more features than observations.\n",
       "\n",
       "---\n",
       "\n",
       "```\n",
       "fit(MulticlassLDA, nc, X, y; ...)\n",
       "```\n",
       "\n",
       "Perform multi-class LDA over a given data set `X` and collecttion of labels `y`.\n",
       "\n",
       "This function returns the resultant multi-class LDA model as an instance of [`MulticlassLDA`](@ref).\n",
       "\n",
       "*Parameters*\n",
       "\n",
       "  * `nc`:  the number of classes\n",
       "  * `X`:   the matrix of input samples, of size `(d, n)`. Each column in `X` is an observation.\n",
       "  * `y`:   the vector of class labels, of length `n`. Each element of `y` must be an integer between `1` and `nc`.\n",
       "\n",
       "**Keyword arguments**\n",
       "\n",
       "  * `method`: The choice of methods:\n",
       "\n",
       "      * `:gevd`: based on generalized eigenvalue decomposition (*default*).\n",
       "      * `:whiten`: first derive a whitening transform from `Sw` and then solve the problem based on eigenvalue\n",
       "\n",
       "    decomposition of the whiten `Sb`.\n",
       "  * `outdim`: The output dimension, i.e. dimension of the transformed space `min(d, nc-1)`\n",
       "  * `regcoef`: The regularization coefficient (*default:* `1.0e-6`). A positive value `regcoef * eigmax(Sw)`   is added to the diagonal of `Sw` to improve numerical stability.\n",
       "  * `covestimator_between`: Custom covariance estimator for between-class covariance (*default:* `SimpleCovariance()`).   The covariance matrix will be calculated as `cov(covestimator_between, #=data=#; dims=2, mean=zeros(#=...=#))`.   Custom covariance estimators, available in other packages, may result in more robust discriminants for data   with more features than observations.\n",
       "  * `covestimator_within`:  Custom covariance estimator for within-class covariance (*default:* `SimpleCovariance()`).   The covariance matrix will be calculated as `cov(covestimator_within, #=data=#; dims=2, mean=zeros(nc))`.   Custom covariance estimators, available in other packages, may result in more robust discriminants for data   with more features than observations.\n",
       "\n",
       "**Notes:**\n",
       "\n",
       "The resultant projection matrix $P$ satisfies:\n",
       "\n",
       "$$\n",
       "\\mathbf{P}^T (\\mathbf{S}_w + \\kappa \\mathbf{I}) \\mathbf{P} = \\mathbf{I}\n",
       "$$\n",
       "\n",
       "Here, $\\kappa$ equals `regcoef * eigmax(Sw)`. The columns of $P$ are arranged in descending order of the corresponding generalized eigenvalues.\n",
       "\n",
       "Note that [`MulticlassLDA`](@ref) does not currently support the normalized version using $\\mathbf{S}_w^*$ and $\\mathbf{S}_b^*$ (see [`SubspaceLDA`](@ref)).\n",
       "\n",
       "---\n",
       "\n",
       "```\n",
       "fit(SubspaceLDA, X, labels; normalize=true)\n",
       "```\n",
       "\n",
       "Fit an subspace projection of LDA model using the equivalent of $\\mathbf{S}_w^*$ and $\\mathbf{S}_b^*$`.\n",
       "\n",
       "Note: Subspace LDA also supports the normalized version of LDA via the `normalize` keyword.\n",
       "\n",
       "---\n",
       "\n",
       "```\n",
       "fit(ICA, X, k; ...)\n",
       "```\n",
       "\n",
       "Perform ICA over the data set given in `X`.\n",
       "\n",
       "**Parameters:** -`X`: The data matrix, of size $(m, n)$. Each row corresponds to a mixed signal, while each column corresponds to an observation (*e.g* all signal value at a particular time step). -`k`: The number of independent components to recover.\n",
       "\n",
       "**Keyword Arguments:**\n",
       "\n",
       "  * `alg`: The choice of algorithm (*default* `:fastica`)\n",
       "  * `fun`: The approx neg-entropy functor (*default* [`Tanh`](@ref))\n",
       "  * `do_whiten`: Whether to perform pre-whitening (*default* `true`)\n",
       "  * `maxiter`: Maximum number of iterations (*default* `100`)\n",
       "  * `tol`: Tolerable change of $W$ at convergence (*default* `1.0e-6`)\n",
       "  * `mean`: The mean vector, which can be either of:\n",
       "\n",
       "      * `0`: the input data has already been centralized\n",
       "      * `nothing`: this function will compute the mean (*default*)\n",
       "      * a pre-computed mean vector\n",
       "  * `winit`: Initial guess of $W$, which should be either of:\n",
       "\n",
       "      * empty matrix: the function will perform random initialization (*default*)\n",
       "      * a matrix of size $(k, k)$ (when `do_whiten`)\n",
       "      * a matrix of size $(m, k)$ (when `!do_whiten`)\n",
       "\n",
       "Returns the resultant ICA model, an instance of type [`ICA`](@ref).\n",
       "\n",
       "**Note:** If `do_whiten` is `true`, the return `W` satisfies $\\mathbf{W}^T \\mathbf{C} \\mathbf{W} = \\mathbf{I}$, otherwise $W$ is orthonormal, *i.e* $\\mathbf{W}^T \\mathbf{W} = \\mathbf{I}$.\n",
       "\n",
       "---\n",
       "\n",
       "```\n",
       "fit(FactorAnalysis, X; ...)\n",
       "```\n",
       "\n",
       "Perform factor analysis over the data given in a matrix `X`. Each column of `X` is an observation. This method returns an instance of [`FactorAnalysis`](@ref).\n",
       "\n",
       "**Keyword arguments:**\n",
       "\n",
       "Let `(d, n) = size(X)` be respectively the input dimension and the number of observations:\n",
       "\n",
       "  * `method`: The choice of methods:\n",
       "\n",
       "      * `:em`: use EM version of factor analysis\n",
       "      * `:cm`: use CM version of factor analysis (*default*)\n",
       "  * `maxoutdim`: Maximum output dimension (*default* `d-1`)\n",
       "  * `mean`: The mean vector, which can be either of:\n",
       "\n",
       "      * `0`: the input data has already been centralized\n",
       "      * `nothing`: this function will compute the mean (*default*)\n",
       "      * a pre-computed mean vector\n",
       "  * `tol`: Convergence tolerance (*default* `1.0e-6`)\n",
       "  * `maxiter`: Maximum number of iterations (*default* `1000`)\n",
       "  * `η`: Variance low bound (*default* `1.0e-6`)\n",
       "\n",
       "**Notes:** This function calls [`facm`](@ref) or [`faem`](@ref) internally, depending on the choice of method.\n"
      ],
      "text/plain": [
       "  Fit a statistical model.\n",
       "\n",
       "  ────────────────────────────────────────────────────────────────────────────\n",
       "\n",
       "\u001b[36m  fit(Histogram, data[, weight][, edges]; closed=:left, nbins)\u001b[39m\n",
       "\n",
       "  Fit a histogram to \u001b[36mdata\u001b[39m.\n",
       "\n",
       "\u001b[1m  Arguments\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "    •  \u001b[36mdata\u001b[39m: either a vector (for a 1-dimensional histogram), or a tuple\n",
       "       of vectors of equal length (for an \u001b[4mn\u001b[24m-dimensional histogram).\n",
       "\n",
       "    •  \u001b[36mweight\u001b[39m: an optional \u001b[36mAbstractWeights\u001b[39m (of the same length as the\n",
       "       data vectors), denoting the weight each observation contributes to\n",
       "       the bin. If no weight vector is supplied, each observation has\n",
       "       weight 1.\n",
       "\n",
       "    •  \u001b[36medges\u001b[39m: a vector (typically an \u001b[36mAbstractRange\u001b[39m object), or tuple of\n",
       "       vectors, that gives the edges of the bins along each dimension. If\n",
       "       no edges are provided, these are determined from the data.\n",
       "\n",
       "\u001b[1m  Keyword arguments\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "    •  \u001b[36mclosed\u001b[39m: if \u001b[36m:left\u001b[39m (the default), the bin intervals are left-closed\n",
       "       [a,b); if \u001b[36m:right\u001b[39m, intervals are right-closed (a,b].\n",
       "\n",
       "    •  \u001b[36mnbins\u001b[39m: if no \u001b[36medges\u001b[39m argument is supplied, the approximate number of\n",
       "       bins to use along each dimension (can be either a single integer,\n",
       "       or a tuple of integers).\n",
       "\n",
       "\u001b[1m  Examples\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "\u001b[36m  # Univariate\u001b[39m\n",
       "\u001b[36m  h = fit(Histogram, rand(100))\u001b[39m\n",
       "\u001b[36m  h = fit(Histogram, rand(100), 0:0.1:1.0)\u001b[39m\n",
       "\u001b[36m  h = fit(Histogram, rand(100), nbins=10)\u001b[39m\n",
       "\u001b[36m  h = fit(Histogram, rand(100), weights(rand(100)), 0:0.1:1.0)\u001b[39m\n",
       "\u001b[36m  h = fit(Histogram, [20], 0:20:100)\u001b[39m\n",
       "\u001b[36m  h = fit(Histogram, [20], 0:20:100, closed=:right)\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  # Multivariate\u001b[39m\n",
       "\u001b[36m  h = fit(Histogram, (rand(100),rand(100)))\u001b[39m\n",
       "\u001b[36m  h = fit(Histogram, (rand(100),rand(100)),nbins=10)\u001b[39m\n",
       "\n",
       "  ────────────────────────────────────────────────────────────────────────────\n",
       "\n",
       "\u001b[36m  fit(ZScoreTransform, X; dims=nothing, center=true, scale=true)\u001b[39m\n",
       "\n",
       "  Fit standardization parameters to vector or matrix \u001b[36mX\u001b[39m and return a\n",
       "  \u001b[36mZScoreTransform\u001b[39m transformation object.\n",
       "\n",
       "\u001b[1m  Keyword arguments\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "    •  \u001b[36mdims\u001b[39m: if \u001b[36m1\u001b[39m fit standardization parameters in column-wise fashion;\n",
       "       if \u001b[36m2\u001b[39m fit in row-wise fashion. The default is \u001b[36mnothing\u001b[39m, which is\n",
       "       equivalent to \u001b[36mdims=2\u001b[39m with a deprecation warning.\n",
       "\n",
       "    •  \u001b[36mcenter\u001b[39m: if \u001b[36mtrue\u001b[39m (the default) center data so that its mean is\n",
       "       zero.\n",
       "\n",
       "    •  \u001b[36mscale\u001b[39m: if \u001b[36mtrue\u001b[39m (the default) scale the data so that its variance\n",
       "       is equal to one.\n",
       "\n",
       "\u001b[1m  Examples\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "\u001b[36m  julia> using StatsBase\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> X = [0.0 -0.5 0.5; 0.0 1.0 2.0]\u001b[39m\n",
       "\u001b[36m  2×3 Matrix{Float64}:\u001b[39m\n",
       "\u001b[36m   0.0  -0.5  0.5\u001b[39m\n",
       "\u001b[36m   0.0   1.0  2.0\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> dt = fit(ZScoreTransform, X, dims=2)\u001b[39m\n",
       "\u001b[36m  ZScoreTransform{Float64, Vector{Float64}}(2, 2, [0.0, 1.0], [0.5, 1.0])\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> StatsBase.transform(dt, X)\u001b[39m\n",
       "\u001b[36m  2×3 Matrix{Float64}:\u001b[39m\n",
       "\u001b[36m    0.0  -1.0  1.0\u001b[39m\n",
       "\u001b[36m   -1.0   0.0  1.0\u001b[39m\n",
       "\n",
       "  ────────────────────────────────────────────────────────────────────────────\n",
       "\n",
       "\u001b[36m  fit(UnitRangeTransform, X; dims=nothing, unit=true)\u001b[39m\n",
       "\n",
       "  Fit a scaling parameters to vector or matrix \u001b[36mX\u001b[39m and return a\n",
       "  \u001b[36mUnitRangeTransform\u001b[39m transformation object.\n",
       "\n",
       "\u001b[1m  Keyword arguments\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "    •  \u001b[36mdims\u001b[39m: if \u001b[36m1\u001b[39m fit standardization parameters in column-wise fashion;\n",
       "\n",
       "  if \u001b[36m2\u001b[39m fit in row-wise fashion. The default is \u001b[36mnothing\u001b[39m.\n",
       "\n",
       "    •  \u001b[36munit\u001b[39m: if \u001b[36mtrue\u001b[39m (the default) shift the minimum data to zero.\n",
       "\n",
       "\u001b[1m  Examples\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "\u001b[36m  julia> using StatsBase\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> X = [0.0 -0.5 0.5; 0.0 1.0 2.0]\u001b[39m\n",
       "\u001b[36m  2×3 Matrix{Float64}:\u001b[39m\n",
       "\u001b[36m   0.0  -0.5  0.5\u001b[39m\n",
       "\u001b[36m   0.0   1.0  2.0\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> dt = fit(UnitRangeTransform, X, dims=2)\u001b[39m\n",
       "\u001b[36m  UnitRangeTransform{Float64, Vector{Float64}}(2, 2, true, [-0.5, 0.0], [1.0, 0.5])\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> StatsBase.transform(dt, X)\u001b[39m\n",
       "\u001b[36m  2×3 Matrix{Float64}:\u001b[39m\n",
       "\u001b[36m   0.5  0.0  1.0\u001b[39m\n",
       "\u001b[36m   0.0  0.5  1.0\u001b[39m\n",
       "\n",
       "  ────────────────────────────────────────────────────────────────────────────\n",
       "\n",
       "\u001b[36m  fit(Whitening, X::AbstractMatrix{T}; kwargs...)\u001b[39m\n",
       "\n",
       "  Estimate a whitening transform from the data given in \u001b[36mX\u001b[39m.\n",
       "\n",
       "  This function returns an instance of \u001b[36mWhitening\u001b[39m\n",
       "\n",
       "  \u001b[1mKeyword Arguments:\u001b[22m\n",
       "\n",
       "    •  \u001b[36mregcoef\u001b[39m: The regularization coefficient. The covariance will be\n",
       "       regularized as follows when \u001b[36mregcoef\u001b[39m is positive \u001b[36mC + (eigmax(C) *\n",
       "       regcoef) * eye(d)\u001b[39m. Default values is \u001b[36mzero(T)\u001b[39m.\n",
       "\n",
       "    •  \u001b[36mdims\u001b[39m: if \u001b[36m1\u001b[39m the transformation calculated from the row samples. fit\n",
       "       standardization parameters in column-wise fashion; if \u001b[36m2\u001b[39m the\n",
       "       transformation calculated from the column samples. The default is\n",
       "       \u001b[36mnothing\u001b[39m, which is equivalent to \u001b[36mdims=2\u001b[39m with a deprecation warning.\n",
       "\n",
       "    •  \u001b[36mmean\u001b[39m: The mean vector, which can be either of:\n",
       "       • \u001b[36m0\u001b[39m: the input data has already been centralized\n",
       "       • \u001b[36mnothing\u001b[39m: this function will compute the mean (\u001b[1mdefault\u001b[22m)\n",
       "       • a pre-computed mean vector\n",
       "\n",
       "  \u001b[1mNote:\u001b[22m This function internally relies on \u001b[36mcov_whitening\u001b[39m to derive the\n",
       "  transformation \u001b[36mW\u001b[39m.\n",
       "\n",
       "  ────────────────────────────────────────────────────────────────────────────\n",
       "\n",
       "\u001b[36m  fit(PCA, X; ...)\u001b[39m\n",
       "\n",
       "  Perform PCA over the data given in a matrix \u001b[36mX\u001b[39m. Each column of \u001b[36mX\u001b[39m is an\n",
       "  \u001b[1mobservation\u001b[22m.\n",
       "\n",
       "  \u001b[1mKeyword arguments\u001b[22m\n",
       "\n",
       "    •  \u001b[36mmethod\u001b[39m: The choice of methods:\n",
       "       • \u001b[36m:auto\u001b[39m: use \u001b[36m:cov\u001b[39m when \u001b[36md < n\u001b[39m or \u001b[36m:svd\u001b[39m otherwise (\u001b[4mdefault\u001b[24m).\n",
       "       • \u001b[36m:cov\u001b[39m: based on covariance matrix decomposition.\n",
       "       • \u001b[36m:svd\u001b[39m: based on SVD of the input data.\n",
       "\n",
       "    •  \u001b[36mmaxoutdim\u001b[39m: The output dimension, i.e. dimension of the transformed\n",
       "       space (\u001b[4mmin(d, nc-1)\u001b[24m)\n",
       "\n",
       "    •  \u001b[36mpratio\u001b[39m: The ratio of variances preserved in the principal subspace\n",
       "       (\u001b[4m0.99\u001b[24m)\n",
       "\n",
       "    •  \u001b[36mmean\u001b[39m: The mean vector, which can be either of\n",
       "       • \u001b[36m0\u001b[39m: the input data has already been centralized\n",
       "       • \u001b[36mnothing\u001b[39m: this function will compute the mean (\u001b[4mdefault\u001b[24m)\n",
       "       • a pre-computed mean vector\n",
       "\n",
       "  \u001b[1mNotes:\u001b[22m\n",
       "\n",
       "    •  The output dimension \u001b[36mp\u001b[39m depends on both \u001b[36mmaxoutdim\u001b[39m and \u001b[36mpratio\u001b[39m, as\n",
       "       follows. Suppose the first \u001b[36mk\u001b[39m principal components preserve at\n",
       "       least \u001b[36mpratio\u001b[39m of the total variance, while the first \u001b[36mk-1\u001b[39m preserves\n",
       "       less than \u001b[36mpratio\u001b[39m, then the actual output dimension will be \u001b[35m\\min(k,\n",
       "       maxoutdim)\u001b[39m.\n",
       "\n",
       "    •  This function calls \u001b[36mpcacov\u001b[39m or \u001b[36mpcasvd\u001b[39m internally, depending on the\n",
       "       choice of method.\n",
       "\n",
       "  ────────────────────────────────────────────────────────────────────────────\n",
       "\n",
       "\u001b[36m  fit(PPCA, X; ...)\u001b[39m\n",
       "\n",
       "  Perform probabilistic PCA over the data given in a matrix \u001b[36mX\u001b[39m. Each column of\n",
       "  \u001b[36mX\u001b[39m is an observation. This method returns an instance of \u001b[36mPPCA\u001b[39m.\n",
       "\n",
       "  \u001b[1mKeyword arguments:\u001b[22m\n",
       "\n",
       "  Let \u001b[36m(d, n) = size(X)\u001b[39m be respectively the input dimension and the number of\n",
       "  observations:\n",
       "\n",
       "    •  \u001b[36mmethod\u001b[39m: The choice of methods:\n",
       "       • \u001b[36m:ml\u001b[39m: use maximum likelihood version of probabilistic PCA\n",
       "       (\u001b[4mdefault\u001b[24m)\n",
       "       • \u001b[36m:em\u001b[39m: use EM version of probabilistic PCA\n",
       "       • \u001b[36m:bayes\u001b[39m: use Bayesian PCA\n",
       "\n",
       "    •  \u001b[36mmaxoutdim\u001b[39m: Maximum output dimension (\u001b[4mdefault\u001b[24m \u001b[36md-1\u001b[39m)\n",
       "\n",
       "    •  \u001b[36mmean\u001b[39m: The mean vector, which can be either of:\n",
       "       • \u001b[36m0\u001b[39m: the input data has already been centralized\n",
       "       • \u001b[36mnothing\u001b[39m: this function will compute the mean (\u001b[4mdefault\u001b[24m)\n",
       "       • a pre-computed mean vector\n",
       "\n",
       "    •  \u001b[36mtol\u001b[39m: Convergence tolerance (\u001b[4mdefault\u001b[24m \u001b[36m1.0e-6\u001b[39m)\n",
       "\n",
       "    •  \u001b[36mmaxiter\u001b[39m: Maximum number of iterations (\u001b[4mdefault\u001b[24m \u001b[36m1000\u001b[39m)\n",
       "\n",
       "  \u001b[1mNotes:\u001b[22m This function calls \u001b[36mppcaml\u001b[39m, \u001b[36mppcaem\u001b[39m or \u001b[36mbayespca\u001b[39m internally, depending\n",
       "  on the choice of method.\n",
       "\n",
       "  ────────────────────────────────────────────────────────────────────────────\n",
       "\n",
       "  Fit \u001b[36mKernelCenter\u001b[39m object\n",
       "\n",
       "  ────────────────────────────────────────────────────────────────────────────\n",
       "\n",
       "\u001b[36m  fit(KernelPCA, X; ...)\u001b[39m\n",
       "\n",
       "  Perform kernel PCA over the data given in a matrix \u001b[36mX\u001b[39m. Each column of \u001b[36mX\u001b[39m is an\n",
       "  observation.\n",
       "\n",
       "  This method returns an instance of \u001b[36mKernelPCA\u001b[39m.\n",
       "\n",
       "  \u001b[1mKeyword arguments:\u001b[22m\n",
       "\n",
       "  Let \u001b[36m(d, n) = size(X)\u001b[39m be respectively the input dimension and the number of\n",
       "  observations:\n",
       "\n",
       "    •  \u001b[36mkernel\u001b[39m: The kernel function. This functions accepts two vector\n",
       "       arguments \u001b[36mx\u001b[39m and \u001b[36my\u001b[39m,\n",
       "\n",
       "  and returns a scalar value (\u001b[4mdefault:\u001b[24m \u001b[36m(x,y)->x'y\u001b[39m)\n",
       "\n",
       "    •  \u001b[36msolver\u001b[39m: The choice of solver:\n",
       "       • \u001b[36m:eig\u001b[39m: uses \u001b[36mLinearAlgebra.eigen\u001b[39m (\u001b[4mdefault\u001b[24m)\n",
       "       • \u001b[36m:eigs\u001b[39m: uses \u001b[36mArpack.eigs\u001b[39m (always used for sparse data)\n",
       "\n",
       "    •  \u001b[36mmaxoutdim\u001b[39m: Maximum output dimension (\u001b[4mdefault\u001b[24m \u001b[36mmin(d, n)\u001b[39m)\n",
       "\n",
       "    •  \u001b[36minverse\u001b[39m: Whether to perform calculation for inverse transform for\n",
       "       non-precomputed kernels (\u001b[4mdefault\u001b[24m \u001b[36mfalse\u001b[39m)\n",
       "\n",
       "    •  \u001b[36mβ\u001b[39m: Hyperparameter of the ridge regression that learns the inverse\n",
       "       transform (\u001b[4mdefault\u001b[24m \u001b[36m1\u001b[39m when \u001b[36minverse\u001b[39m is \u001b[36mtrue\u001b[39m).\n",
       "\n",
       "    •  \u001b[36mtol\u001b[39m: Convergence tolerance for \u001b[36meigs\u001b[39m solver (\u001b[4mdefault\u001b[24m \u001b[36m0.0\u001b[39m)\n",
       "\n",
       "    •  \u001b[36mmaxiter\u001b[39m: Maximum number of iterations for \u001b[36meigs\u001b[39m solver (\u001b[4mdefault\u001b[24m\n",
       "       \u001b[36m300\u001b[39m)\n",
       "\n",
       "  ────────────────────────────────────────────────────────────────────────────\n",
       "\n",
       "\u001b[36m  fit(CCA, X, Y; ...)\u001b[39m\n",
       "\n",
       "  Perform CCA over the data given in matrices \u001b[36mX\u001b[39m and \u001b[36mY\u001b[39m. Each column of \u001b[36mX\u001b[39m and \u001b[36mY\u001b[39m\n",
       "  is an observation.\n",
       "\n",
       "  \u001b[36mX\u001b[39m and \u001b[36mY\u001b[39m should have the same number of columns (denoted by \u001b[36mn\u001b[39m below).\n",
       "\n",
       "  This method returns an instance of \u001b[36mCCA\u001b[39m.\n",
       "\n",
       "  \u001b[1mKeyword arguments:\u001b[22m\n",
       "\n",
       "    •  \u001b[36mmethod\u001b[39m: The choice of methods:\n",
       "       • \u001b[36m:cov\u001b[39m: based on covariance matrices\n",
       "       • \u001b[36m:svd\u001b[39m: based on SVD of the input data (\u001b[4mdefault\u001b[24m)\n",
       "\n",
       "    •  \u001b[36moutdim\u001b[39m: The output dimension, \u001b[4mi.e\u001b[24m dimension of the common space\n",
       "       (\u001b[4mdefault\u001b[24m: \u001b[36mmin(dx, dy, n)\u001b[39m)\n",
       "\n",
       "    •  \u001b[36mmean\u001b[39m: The mean vector, which can be either of:\n",
       "       • \u001b[36m0\u001b[39m: the input data has already been centralized\n",
       "       • \u001b[36mnothing\u001b[39m: this function will compute the mean (\u001b[4mdefault\u001b[24m)\n",
       "       • a pre-computed mean vector\n",
       "\n",
       "  \u001b[1mNotes:\u001b[22m This function calls \u001b[36mccacov\u001b[39m or \u001b[36mccasvd\u001b[39m internally, depending on the\n",
       "  choice of method.\n",
       "\n",
       "  ────────────────────────────────────────────────────────────────────────────\n",
       "\n",
       "\u001b[36m  fit(MDS, X; kwargs...)\u001b[39m\n",
       "\n",
       "  Compute an embedding of \u001b[36mX\u001b[39m points by classical multidimensional scaling\n",
       "  (MDS). There are two calling options, specified via the required keyword\n",
       "  argument \u001b[36mdistances\u001b[39m:\n",
       "\n",
       "\u001b[36m  mds = fit(MDS, X; distances=false, maxoutdim=size(X,1)-1)\u001b[39m\n",
       "\n",
       "  where \u001b[36mX\u001b[39m is the data matrix. Distances between pairs of columns of \u001b[36mX\u001b[39m are\n",
       "  computed using the Euclidean norm. This is equivalent to performing PCA on\n",
       "  \u001b[36mX\u001b[39m.\n",
       "\n",
       "\u001b[36m  mds = fit(MDS, D; distances=true, maxoutdim=size(D,1)-1)\u001b[39m\n",
       "\n",
       "  where \u001b[36mD\u001b[39m is a symmetric matrix \u001b[36mD\u001b[39m of distances between points.\n",
       "\n",
       "  ────────────────────────────────────────────────────────────────────────────\n",
       "\n",
       "\u001b[36m  fit(LinearDiscriminant, Xp, Xn; covestimator = SimpleCovariance())\u001b[39m\n",
       "\n",
       "  Performs LDA given both positive and negative samples. The function accepts\n",
       "  follwing parameters:\n",
       "\n",
       "  \u001b[1mParameters\u001b[22m\n",
       "\n",
       "    •  \u001b[36mXp\u001b[39m: The sample matrix of the positive class.\n",
       "\n",
       "    •  \u001b[36mXn\u001b[39m: The sample matrix of the negative class.\n",
       "\n",
       "  \u001b[1mKeyword arguments:\u001b[22m\n",
       "\n",
       "    •  \u001b[36mcovestimator\u001b[39m: Custom covariance estimator for between-class\n",
       "       covariance. The covariance matrix will be calculated as\n",
       "       \u001b[36mcov(covestimator_between, #=data=#; dims=2, mean=zeros(#=...=#)\u001b[39m.\n",
       "       Custom covariance estimators, available in other packages, may\n",
       "       result in more robust discriminants for data with more features\n",
       "       than observations.\n",
       "\n",
       "  ────────────────────────────────────────────────────────────────────────────\n",
       "\n",
       "\u001b[36m  fit(MulticlassLDA, nc, X, y; ...)\u001b[39m\n",
       "\n",
       "  Perform multi-class LDA over a given data set \u001b[36mX\u001b[39m and collecttion of labels \u001b[36my\u001b[39m.\n",
       "\n",
       "  This function returns the resultant multi-class LDA model as an instance of\n",
       "  \u001b[36mMulticlassLDA\u001b[39m.\n",
       "\n",
       "  \u001b[4mParameters\u001b[24m\n",
       "\n",
       "    •  \u001b[36mnc\u001b[39m: the number of classes\n",
       "\n",
       "    •  \u001b[36mX\u001b[39m: the matrix of input samples, of size \u001b[36m(d, n)\u001b[39m. Each column in \u001b[36mX\u001b[39m\n",
       "       is an observation.\n",
       "\n",
       "    •  \u001b[36my\u001b[39m: the vector of class labels, of length \u001b[36mn\u001b[39m. Each element of \u001b[36my\u001b[39m must\n",
       "       be an integer between \u001b[36m1\u001b[39m and \u001b[36mnc\u001b[39m.\n",
       "\n",
       "  \u001b[1mKeyword arguments\u001b[22m\n",
       "\n",
       "    •  \u001b[36mmethod\u001b[39m: The choice of methods:\n",
       "       • \u001b[36m:gevd\u001b[39m: based on generalized eigenvalue decomposition\n",
       "       (\u001b[4mdefault\u001b[24m).\n",
       "       • \u001b[36m:whiten\u001b[39m: first derive a whitening transform from \u001b[36mSw\u001b[39m and\n",
       "       then solve the problem based on eigenvalue\n",
       "       decomposition of the whiten \u001b[36mSb\u001b[39m.\n",
       "\n",
       "    •  \u001b[36moutdim\u001b[39m: The output dimension, i.e. dimension of the transformed\n",
       "       space \u001b[36mmin(d, nc-1)\u001b[39m\n",
       "\n",
       "    •  \u001b[36mregcoef\u001b[39m: The regularization coefficient (\u001b[4mdefault:\u001b[24m \u001b[36m1.0e-6\u001b[39m). A\n",
       "       positive value \u001b[36mregcoef * eigmax(Sw)\u001b[39m is added to the diagonal of \u001b[36mSw\u001b[39m\n",
       "       to improve numerical stability.\n",
       "\n",
       "    •  \u001b[36mcovestimator_between\u001b[39m: Custom covariance estimator for\n",
       "       between-class covariance (\u001b[4mdefault:\u001b[24m \u001b[36mSimpleCovariance()\u001b[39m). The\n",
       "       covariance matrix will be calculated as \u001b[36mcov(covestimator_between,\n",
       "       #=data=#; dims=2, mean=zeros(#=...=#))\u001b[39m. Custom covariance\n",
       "       estimators, available in other packages, may result in more robust\n",
       "       discriminants for data with more features than observations.\n",
       "\n",
       "    •  \u001b[36mcovestimator_within\u001b[39m: Custom covariance estimator for within-class\n",
       "       covariance (\u001b[4mdefault:\u001b[24m \u001b[36mSimpleCovariance()\u001b[39m). The covariance matrix\n",
       "       will be calculated as \u001b[36mcov(covestimator_within, #=data=#; dims=2,\n",
       "       mean=zeros(nc))\u001b[39m. Custom covariance estimators, available in other\n",
       "       packages, may result in more robust discriminants for data with\n",
       "       more features than observations.\n",
       "\n",
       "  \u001b[1mNotes:\u001b[22m\n",
       "\n",
       "  The resultant projection matrix \u001b[35mP\u001b[39m satisfies:\n",
       "\n",
       "\u001b[35m  \\mathbf{P}^T (\\mathbf{S}_w + \\kappa \\mathbf{I}) \\mathbf{P} = \\mathbf{I}\u001b[39m\n",
       "\n",
       "  Here, \u001b[35m\\kappa\u001b[39m equals \u001b[36mregcoef * eigmax(Sw)\u001b[39m. The columns of \u001b[35mP\u001b[39m are arranged in\n",
       "  descending order of the corresponding generalized eigenvalues.\n",
       "\n",
       "  Note that \u001b[36mMulticlassLDA\u001b[39m does not currently support the normalized version\n",
       "  using \u001b[35m\\mathbf{S}_w^*\u001b[39m and \u001b[35m\\mathbf{S}_b^*\u001b[39m (see \u001b[36mSubspaceLDA\u001b[39m).\n",
       "\n",
       "  ────────────────────────────────────────────────────────────────────────────\n",
       "\n",
       "\u001b[36m  fit(SubspaceLDA, X, labels; normalize=true)\u001b[39m\n",
       "\n",
       "  Fit an subspace projection of LDA model using the equivalent of\n",
       "  \u001b[35m\\mathbf{S}_w^*\u001b[39m and \u001b[35m\\mathbf{S}_b^*\u001b[39m`.\n",
       "\n",
       "  Note: Subspace LDA also supports the normalized version of LDA via the\n",
       "  \u001b[36mnormalize\u001b[39m keyword.\n",
       "\n",
       "  ────────────────────────────────────────────────────────────────────────────\n",
       "\n",
       "\u001b[36m  fit(ICA, X, k; ...)\u001b[39m\n",
       "\n",
       "  Perform ICA over the data set given in \u001b[36mX\u001b[39m.\n",
       "\n",
       "  \u001b[1mParameters:\u001b[22m -\u001b[36mX\u001b[39m: The data matrix, of size \u001b[35m(m, n)\u001b[39m. Each row corresponds to a\n",
       "  mixed signal, while each column corresponds to an observation (\u001b[4me.g\u001b[24m all\n",
       "  signal value at a particular time step). -\u001b[36mk\u001b[39m: The number of independent\n",
       "  components to recover.\n",
       "\n",
       "  \u001b[1mKeyword Arguments:\u001b[22m\n",
       "\n",
       "    •  \u001b[36malg\u001b[39m: The choice of algorithm (\u001b[4mdefault\u001b[24m \u001b[36m:fastica\u001b[39m)\n",
       "\n",
       "    •  \u001b[36mfun\u001b[39m: The approx neg-entropy functor (\u001b[4mdefault\u001b[24m \u001b[36mTanh\u001b[39m)\n",
       "\n",
       "    •  \u001b[36mdo_whiten\u001b[39m: Whether to perform pre-whitening (\u001b[4mdefault\u001b[24m \u001b[36mtrue\u001b[39m)\n",
       "\n",
       "    •  \u001b[36mmaxiter\u001b[39m: Maximum number of iterations (\u001b[4mdefault\u001b[24m \u001b[36m100\u001b[39m)\n",
       "\n",
       "    •  \u001b[36mtol\u001b[39m: Tolerable change of \u001b[35mW\u001b[39m at convergence (\u001b[4mdefault\u001b[24m \u001b[36m1.0e-6\u001b[39m)\n",
       "\n",
       "    •  \u001b[36mmean\u001b[39m: The mean vector, which can be either of:\n",
       "       • \u001b[36m0\u001b[39m: the input data has already been centralized\n",
       "       • \u001b[36mnothing\u001b[39m: this function will compute the mean (\u001b[4mdefault\u001b[24m)\n",
       "       • a pre-computed mean vector\n",
       "\n",
       "    •  \u001b[36mwinit\u001b[39m: Initial guess of \u001b[35mW\u001b[39m, which should be either of:\n",
       "       • empty matrix: the function will perform random\n",
       "       initialization (\u001b[4mdefault\u001b[24m)\n",
       "       • a matrix of size \u001b[35m(k, k)\u001b[39m (when \u001b[36mdo_whiten\u001b[39m)\n",
       "       • a matrix of size \u001b[35m(m, k)\u001b[39m (when \u001b[36m!do_whiten\u001b[39m)\n",
       "\n",
       "  Returns the resultant ICA model, an instance of type \u001b[36mICA\u001b[39m.\n",
       "\n",
       "  \u001b[1mNote:\u001b[22m If \u001b[36mdo_whiten\u001b[39m is \u001b[36mtrue\u001b[39m, the return \u001b[36mW\u001b[39m satisfies \u001b[35m\\mathbf{W}^T \\mathbf{C}\n",
       "  \\mathbf{W} = \\mathbf{I}\u001b[39m, otherwise \u001b[35mW\u001b[39m is orthonormal, \u001b[4mi.e\u001b[24m \u001b[35m\\mathbf{W}^T\n",
       "  \\mathbf{W} = \\mathbf{I}\u001b[39m.\n",
       "\n",
       "  ────────────────────────────────────────────────────────────────────────────\n",
       "\n",
       "\u001b[36m  fit(FactorAnalysis, X; ...)\u001b[39m\n",
       "\n",
       "  Perform factor analysis over the data given in a matrix \u001b[36mX\u001b[39m. Each column of \u001b[36mX\u001b[39m\n",
       "  is an observation. This method returns an instance of \u001b[36mFactorAnalysis\u001b[39m.\n",
       "\n",
       "  \u001b[1mKeyword arguments:\u001b[22m\n",
       "\n",
       "  Let \u001b[36m(d, n) = size(X)\u001b[39m be respectively the input dimension and the number of\n",
       "  observations:\n",
       "\n",
       "    •  \u001b[36mmethod\u001b[39m: The choice of methods:\n",
       "       • \u001b[36m:em\u001b[39m: use EM version of factor analysis\n",
       "       • \u001b[36m:cm\u001b[39m: use CM version of factor analysis (\u001b[4mdefault\u001b[24m)\n",
       "\n",
       "    •  \u001b[36mmaxoutdim\u001b[39m: Maximum output dimension (\u001b[4mdefault\u001b[24m \u001b[36md-1\u001b[39m)\n",
       "\n",
       "    •  \u001b[36mmean\u001b[39m: The mean vector, which can be either of:\n",
       "       • \u001b[36m0\u001b[39m: the input data has already been centralized\n",
       "       • \u001b[36mnothing\u001b[39m: this function will compute the mean (\u001b[4mdefault\u001b[24m)\n",
       "       • a pre-computed mean vector\n",
       "\n",
       "    •  \u001b[36mtol\u001b[39m: Convergence tolerance (\u001b[4mdefault\u001b[24m \u001b[36m1.0e-6\u001b[39m)\n",
       "\n",
       "    •  \u001b[36mmaxiter\u001b[39m: Maximum number of iterations (\u001b[4mdefault\u001b[24m \u001b[36m1000\u001b[39m)\n",
       "\n",
       "    •  \u001b[36mη\u001b[39m: Variance low bound (\u001b[4mdefault\u001b[24m \u001b[36m1.0e-6\u001b[39m)\n",
       "\n",
       "  \u001b[1mNotes:\u001b[22m This function calls \u001b[36mfacm\u001b[39m or \u001b[36mfaem\u001b[39m internally, depending on the choice\n",
       "  of method."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?MultivariateStats.fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-27T16:53:26.003000+08:00",
     "start_time": "2022-07-27T08:53:25.610Z"
    }
   },
   "outputs": [],
   "source": [
    "using Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-27T16:53:48.664000+08:00",
     "start_time": "2022-07-27T08:53:48.662Z"
    }
   },
   "outputs": [],
   "source": [
    "using Test:Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-15T16:29:20.711000+08:00",
     "start_time": "2022-07-15T08:29:19.971Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1mT\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mt\u001b[22m \u001b[0m\u001b[1mT\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mt\u001b[22mSetException @\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mt\u001b[22m @\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mt\u001b[22mset @\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mt\u001b[22m_warn @\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mt\u001b[22m_skip @\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mt\u001b[22m_logs\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "Simple unit testing functionality:\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{@test}\n",
       "\n",
       "\n",
       "\\item \\texttt{@test\\_throws}\n",
       "\n",
       "\\end{itemize}\n",
       "All tests belong to a \\emph{test set}. There is a default, task-level test set that throws on the first failure. Users can choose to wrap their tests in (possibly nested) test sets that will store results and summarize them at the end of the test set with \\texttt{@testset}.\n",
       "\n"
      ],
      "text/markdown": [
       "Simple unit testing functionality:\n",
       "\n",
       "  * `@test`\n",
       "  * `@test_throws`\n",
       "\n",
       "All tests belong to a *test set*. There is a default, task-level test set that throws on the first failure. Users can choose to wrap their tests in (possibly nested) test sets that will store results and summarize them at the end of the test set with `@testset`.\n"
      ],
      "text/plain": [
       "  Simple unit testing functionality:\n",
       "\n",
       "    •  \u001b[36m@test\u001b[39m\n",
       "\n",
       "    •  \u001b[36m@test_throws\u001b[39m\n",
       "\n",
       "  All tests belong to a \u001b[4mtest set\u001b[24m. There is a default, task-level test set that\n",
       "  throws on the first failure. Users can choose to wrap their tests in\n",
       "  (possibly nested) test sets that will store results and summarize them at\n",
       "  the end of the test set with \u001b[36m@testset\u001b[39m."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-15T16:29:50.406000+08:00",
     "start_time": "2022-07-15T08:29:49.907Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19-element Vector{Symbol}:\n",
       " Symbol(\"@inferred\")\n",
       " Symbol(\"@test\")\n",
       " Symbol(\"@test_broken\")\n",
       " Symbol(\"@test_deprecated\")\n",
       " Symbol(\"@test_logs\")\n",
       " Symbol(\"@test_nowarn\")\n",
       " Symbol(\"@test_skip\")\n",
       " Symbol(\"@test_throws\")\n",
       " Symbol(\"@test_warn\")\n",
       " Symbol(\"@testset\")\n",
       " :GenericArray\n",
       " :GenericDict\n",
       " :GenericOrder\n",
       " :GenericSet\n",
       " :GenericString\n",
       " :Test\n",
       " :TestSetException\n",
       " :detect_ambiguities\n",
       " :detect_unbound_args"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names(Test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StableRNGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-22T10:03:10.130000+08:00",
     "start_time": "2022-07-22T02:03:09.876Z"
    }
   },
   "outputs": [],
   "source": [
    "using StableRNGs #提供稳定的可重现的随机数种子"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-22T10:03:18.446000+08:00",
     "start_time": "2022-07-22T02:03:18.372Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1mS\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1mb\u001b[22m\u001b[0m\u001b[1ml\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1mR\u001b[22m\u001b[0m\u001b[1mN\u001b[22m\u001b[0m\u001b[1mG\u001b[22m\u001b[0m\u001b[1ms\u001b[22m \u001b[0m\u001b[1mS\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1mb\u001b[22m\u001b[0m\u001b[1ml\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1mR\u001b[22m\u001b[0m\u001b[1mN\u001b[22m\u001b[0m\u001b[1mG\u001b[22m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "No docstring found for module \\texttt{StableRNGs}.\n",
       "\n",
       "\\section{Exported names}\n",
       "\\texttt{StableRNG}\n",
       "\n",
       "\\section{Displaying contents of readme found at \\texttt{D:{\\textbackslash}TongYuan{\\textbackslash}.julia{\\textbackslash}packages{\\textbackslash}StableRNGs{\\textbackslash}uwZ1I{\\textbackslash}README.md}}\n",
       "\\section{StableRNGs}\n",
       "\\href{https://travis-ci.org/JuliaRandom/StableRNGs.jl}{\\begin{figure}\n",
       "\\centering\n",
       "\\includegraphics{https://travis-ci.org/JuliaRandom/StableRNGs.jl.svg?branch=master}\n",
       "\\caption{Build Status}\n",
       "\\end{figure}\n",
       "}\n",
       "\n",
       "This package intends to provide a simple RNG with \\emph{stable} streams, suitable for tests in packages which need reproducible streams of random numbers across Julia versions. Indeed, the Julia RNGs provided by default are \\href{https://docs.julialang.org/en/v1.4/stdlib/Random/#Reproducibility-1}{documented} to have non-stable streams (which for example enables some performance improvements).\n",
       "\n",
       "The \\texttt{StableRNG} type provided by this package strives for stability, but if bugs which require breaking this promise are found, a new major version will be released with the fix. Note that this package did \\emph{not} reach version 1.0, which means it is not stable \\emph{yet}, although no changes are expected.\n",
       "\n",
       "\\texttt{StableRNG} is currently an alias for \\texttt{LehmerRNG}, and implements a well understood linear congruential generator (LCG); an LCG is not state of the art, but is fast and is believed to have reasonably good statistical properties [1], suitable at least for tests of a wide range of packages. The choice of this particular RNG is based on its simplicity, which limits the chances for bugs. Note that only \\texttt{StableRNG} is exported from the package, and should be the only type used in client code; \\texttt{LehmerRNG} might be renamed, or might be made a distinct type from \\texttt{StableRNG} in any upcoming \\emph{minor} (i.e. non-breaking) release.\n",
       "\n",
       "Currently, this RNG requires explicit seeding (in the constructor or via \\texttt{Random.seed!}), i.e. no random seed will be chosen for the user as is the case in e.g. \\texttt{MersenneTwister()}.\n",
       "\n",
       "The currently stable (guaranteed) API is\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item construction: \\texttt{rng = StableRNG(seed::Integer)} (in particular the alias \\texttt{LehmerRNG} is currently \\emph{not} part of the API)\n",
       "\n",
       "\n",
       "\\item seeding: \\texttt{Random.seed!(rng::StableRNG, seed::Integer)} (with \\texttt{0 <= seed <= typemax(UInt64)})\n",
       "\n",
       "\n",
       "\\item \\texttt{rand(rng, X)} where \\texttt{X} is any of the standard bit \\texttt{Integer} types (\\texttt{Bool}, \\texttt{Int8}, \\texttt{Int16}, \\texttt{Int32}, \\texttt{Int64}, \\texttt{Int128}, \\texttt{UInt8}, \\texttt{UInt16}, \\texttt{UInt32}, \\texttt{UInt64}, \\texttt{UInt128})\n",
       "\n",
       "\n",
       "\\item \\texttt{rand(rng, X)}, \\texttt{randn(rng, X)}, \\texttt{randexp(rng, X)} where \\texttt{X} is a standard bit \\texttt{AbstractFloat} types (\\texttt{Float16}, \\texttt{Float32}, \\texttt{Float64})\n",
       "\n",
       "\n",
       "\\item array versions for these types, including the mutating methods \\texttt{rand!}, \\texttt{randn!} and \\texttt{randexp!}\n",
       "\n",
       "\n",
       "\\item \\texttt{rand(rng, ::AbstractArray)} (e.g. \\texttt{rand(rng, 1:9)}); the streams are the same on 32-bits and 64-bits architectures\n",
       "\n",
       "\\end{itemize}\n",
       "Note that the generated streams of numbers for scalars and arrays are the same, i.e. \\texttt{rand(rng, X, n)} is equal to \\texttt{[rand(rng, X) for \\_=1:n]} for a given \\texttt{rng} state.\n",
       "\n",
       "Please open an issue for missing needed APIs.\n",
       "\n",
       "[1] \\texttt{LehmerRNG} is implemented after the specific constants published by Melissa E. O'Neill in this \\href{https://gist.github.com/imneme/aeae7628565f15fb3fef54be8533e39c}{C++ implementation}, and passes the Big Crush test (thanks to Kristoffer Carlsson for running it). See also for example this \\href{https://lemire.me/blog/2019/03/19/the-fastest-conventional-random-number-generator-that-can-pass-big-crush/}{blog post}.\n",
       "\n",
       "\\subsection{Usage}\n",
       "In your tests, simply initialize an RNG with a given seed, and use it instead of the default provided one, e.g.\n",
       "\n",
       "\\begin{verbatim}\n",
       "rng = StableRNG(123)\n",
       "A = randn(rng, 10, 10) # instead of randn(10, 10)\n",
       "@test inv(inv(A)) ≈ A\n",
       "\\end{verbatim}\n"
      ],
      "text/markdown": [
       "No docstring found for module `StableRNGs`.\n",
       "\n",
       "# Exported names\n",
       "\n",
       "`StableRNG`\n",
       "\n",
       "# Displaying contents of readme found at `D:\\TongYuan\\.julia\\packages\\StableRNGs\\uwZ1I\\README.md`\n",
       "\n",
       "# StableRNGs\n",
       "\n",
       "[![Build Status](https://travis-ci.org/JuliaRandom/StableRNGs.jl.svg?branch=master)](https://travis-ci.org/JuliaRandom/StableRNGs.jl)\n",
       "\n",
       "This package intends to provide a simple RNG with *stable* streams, suitable for tests in packages which need reproducible streams of random numbers across Julia versions. Indeed, the Julia RNGs provided by default are [documented](https://docs.julialang.org/en/v1.4/stdlib/Random/#Reproducibility-1) to have non-stable streams (which for example enables some performance improvements).\n",
       "\n",
       "The `StableRNG` type provided by this package strives for stability, but if bugs which require breaking this promise are found, a new major version will be released with the fix. Note that this package did *not* reach version 1.0, which means it is not stable *yet*, although no changes are expected.\n",
       "\n",
       "`StableRNG` is currently an alias for `LehmerRNG`, and implements a well understood linear congruential generator (LCG); an LCG is not state of the art, but is fast and is believed to have reasonably good statistical properties [1], suitable at least for tests of a wide range of packages. The choice of this particular RNG is based on its simplicity, which limits the chances for bugs. Note that only `StableRNG` is exported from the package, and should be the only type used in client code; `LehmerRNG` might be renamed, or might be made a distinct type from `StableRNG` in any upcoming *minor* (i.e. non-breaking) release.\n",
       "\n",
       "Currently, this RNG requires explicit seeding (in the constructor or via `Random.seed!`), i.e. no random seed will be chosen for the user as is the case in e.g. `MersenneTwister()`.\n",
       "\n",
       "The currently stable (guaranteed) API is\n",
       "\n",
       "  * construction: `rng = StableRNG(seed::Integer)` (in particular the alias `LehmerRNG` is currently *not* part of the API)\n",
       "  * seeding: `Random.seed!(rng::StableRNG, seed::Integer)` (with `0 <= seed <= typemax(UInt64)`)\n",
       "  * `rand(rng, X)` where `X` is any of the standard bit `Integer` types (`Bool`, `Int8`, `Int16`, `Int32`, `Int64`, `Int128`, `UInt8`, `UInt16`, `UInt32`, `UInt64`, `UInt128`)\n",
       "  * `rand(rng, X)`, `randn(rng, X)`, `randexp(rng, X)` where `X` is a standard bit `AbstractFloat` types (`Float16`, `Float32`, `Float64`)\n",
       "  * array versions for these types, including the mutating methods `rand!`, `randn!` and `randexp!`\n",
       "  * `rand(rng, ::AbstractArray)` (e.g. `rand(rng, 1:9)`); the streams are the same on 32-bits and 64-bits architectures\n",
       "\n",
       "Note that the generated streams of numbers for scalars and arrays are the same, i.e. `rand(rng, X, n)` is equal to `[rand(rng, X) for _=1:n]` for a given `rng` state.\n",
       "\n",
       "Please open an issue for missing needed APIs.\n",
       "\n",
       "[1] `LehmerRNG` is implemented after the specific constants published by Melissa E. O'Neill in this [C++ implementation](https://gist.github.com/imneme/aeae7628565f15fb3fef54be8533e39c), and passes the Big Crush test (thanks to Kristoffer Carlsson for running it). See also for example this [blog post](https://lemire.me/blog/2019/03/19/the-fastest-conventional-random-number-generator-that-can-pass-big-crush/).\n",
       "\n",
       "## Usage\n",
       "\n",
       "In your tests, simply initialize an RNG with a given seed, and use it instead of the default provided one, e.g.\n",
       "\n",
       "```julia\n",
       "rng = StableRNG(123)\n",
       "A = randn(rng, 10, 10) # instead of randn(10, 10)\n",
       "@test inv(inv(A)) ≈ A\n",
       "```\n"
      ],
      "text/plain": [
       "  No docstring found for module \u001b[36mStableRNGs\u001b[39m.\n",
       "\n",
       "\u001b[1m  Exported names\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "  \u001b[36mStableRNG\u001b[39m\n",
       "\n",
       "\u001b[1m  Displaying contents of readme found at\u001b[22m\n",
       "\u001b[1m \u001b[36mD:\\TongYuan\\.julia\\packages\\StableRNGs\\uwZ1I\\README.md\u001b[39m\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "\u001b[1m  StableRNGs\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "  (Image: Build Status) (https://travis-ci.org/JuliaRandom/StableRNGs.jl)\n",
       "\n",
       "  This package intends to provide a simple RNG with \u001b[4mstable\u001b[24m streams, suitable\n",
       "  for tests in packages which need reproducible streams of random numbers\n",
       "  across Julia versions. Indeed, the Julia RNGs provided by default are\n",
       "  documented\n",
       "  (https://docs.julialang.org/en/v1.4/stdlib/Random/#Reproducibility-1) to\n",
       "  have non-stable streams (which for example enables some performance\n",
       "  improvements).\n",
       "\n",
       "  The \u001b[36mStableRNG\u001b[39m type provided by this package strives for stability, but if\n",
       "  bugs which require breaking this promise are found, a new major version will\n",
       "  be released with the fix. Note that this package did \u001b[4mnot\u001b[24m reach version 1.0,\n",
       "  which means it is not stable \u001b[4myet\u001b[24m, although no changes are expected.\n",
       "\n",
       "  \u001b[36mStableRNG\u001b[39m is currently an alias for \u001b[36mLehmerRNG\u001b[39m, and implements a well\n",
       "  understood linear congruential generator (LCG); an LCG is not state of the\n",
       "  art, but is fast and is believed to have reasonably good statistical\n",
       "  properties [1], suitable at least for tests of a wide range of packages. The\n",
       "  choice of this particular RNG is based on its simplicity, which limits the\n",
       "  chances for bugs. Note that only \u001b[36mStableRNG\u001b[39m is exported from the package, and\n",
       "  should be the only type used in client code; \u001b[36mLehmerRNG\u001b[39m might be renamed, or\n",
       "  might be made a distinct type from \u001b[36mStableRNG\u001b[39m in any upcoming \u001b[4mminor\u001b[24m (i.e.\n",
       "  non-breaking) release.\n",
       "\n",
       "  Currently, this RNG requires explicit seeding (in the constructor or via\n",
       "  \u001b[36mRandom.seed!\u001b[39m), i.e. no random seed will be chosen for the user as is the\n",
       "  case in e.g. \u001b[36mMersenneTwister()\u001b[39m.\n",
       "\n",
       "  The currently stable (guaranteed) API is\n",
       "\n",
       "    •  construction: \u001b[36mrng = StableRNG(seed::Integer)\u001b[39m (in particular the\n",
       "       alias \u001b[36mLehmerRNG\u001b[39m is currently \u001b[4mnot\u001b[24m part of the API)\n",
       "\n",
       "    •  seeding: \u001b[36mRandom.seed!(rng::StableRNG, seed::Integer)\u001b[39m (with \u001b[36m0 <=\n",
       "       seed <= typemax(UInt64)\u001b[39m)\n",
       "\n",
       "    •  \u001b[36mrand(rng, X)\u001b[39m where \u001b[36mX\u001b[39m is any of the standard bit \u001b[36mInteger\u001b[39m types\n",
       "       (\u001b[36mBool\u001b[39m, \u001b[36mInt8\u001b[39m, \u001b[36mInt16\u001b[39m, \u001b[36mInt32\u001b[39m, \u001b[36mInt64\u001b[39m, \u001b[36mInt128\u001b[39m, \u001b[36mUInt8\u001b[39m, \u001b[36mUInt16\u001b[39m, \u001b[36mUInt32\u001b[39m,\n",
       "       \u001b[36mUInt64\u001b[39m, \u001b[36mUInt128\u001b[39m)\n",
       "\n",
       "    •  \u001b[36mrand(rng, X)\u001b[39m, \u001b[36mrandn(rng, X)\u001b[39m, \u001b[36mrandexp(rng, X)\u001b[39m where \u001b[36mX\u001b[39m is a standard\n",
       "       bit \u001b[36mAbstractFloat\u001b[39m types (\u001b[36mFloat16\u001b[39m, \u001b[36mFloat32\u001b[39m, \u001b[36mFloat64\u001b[39m)\n",
       "\n",
       "    •  array versions for these types, including the mutating methods\n",
       "       \u001b[36mrand!\u001b[39m, \u001b[36mrandn!\u001b[39m and \u001b[36mrandexp!\u001b[39m\n",
       "\n",
       "    •  \u001b[36mrand(rng, ::AbstractArray)\u001b[39m (e.g. \u001b[36mrand(rng, 1:9)\u001b[39m); the streams are\n",
       "       the same on 32-bits and 64-bits architectures\n",
       "\n",
       "  Note that the generated streams of numbers for scalars and arrays are the\n",
       "  same, i.e. \u001b[36mrand(rng, X, n)\u001b[39m is equal to \u001b[36m[rand(rng, X) for _=1:n]\u001b[39m for a given\n",
       "  \u001b[36mrng\u001b[39m state.\n",
       "\n",
       "  Please open an issue for missing needed APIs.\n",
       "\n",
       "  [1] \u001b[36mLehmerRNG\u001b[39m is implemented after the specific constants published by\n",
       "  Melissa E. O'Neill in this C++ implementation\n",
       "  (https://gist.github.com/imneme/aeae7628565f15fb3fef54be8533e39c), and\n",
       "  passes the Big Crush test (thanks to Kristoffer Carlsson for running it).\n",
       "  See also for example this blog post\n",
       "  (https://lemire.me/blog/2019/03/19/the-fastest-conventional-random-number-generator-that-can-pass-big-crush/).\n",
       "\n",
       "\u001b[1m  Usage\u001b[22m\n",
       "\u001b[1m  =======\u001b[22m\n",
       "\n",
       "  In your tests, simply initialize an RNG with a given seed, and use it\n",
       "  instead of the default provided one, e.g.\n",
       "\n",
       "\u001b[36m  rng = StableRNG(123)\u001b[39m\n",
       "\u001b[36m  A = randn(rng, 10, 10) # instead of randn(10, 10)\u001b[39m\n",
       "\u001b[36m  @test inv(inv(A)) ≈ A\u001b[39m"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?StableRNGs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-19T08:42:21.704000+08:00",
     "start_time": "2022-08-19T00:42:21.332Z"
    }
   },
   "outputs": [],
   "source": [
    "using Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-19T08:44:15.449000+08:00",
     "start_time": "2022-08-19T00:44:15.008Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "view(A, inds...)\n",
       "\\end{verbatim}\n",
       "Like \\href{@ref}{\\texttt{getindex}}, but returns a lightweight array that lazily references (or is effectively a \\emph{view} into) the parent array \\texttt{A} at the given index or indices \\texttt{inds} instead of eagerly extracting elements or constructing a copied subset. Calling \\href{@ref}{\\texttt{getindex}} or \\href{@ref}{\\texttt{setindex!}} on the returned value (often a \\href{@ref}{\\texttt{SubArray}}) computes the indices to access or modify the parent array on the fly.  The behavior is undefined if the shape of the parent array is changed after \\texttt{view} is called because there is no bound check for the parent array; e.g., it may cause a segmentation fault.\n",
       "\n",
       "Some immutable parent arrays (like ranges) may choose to simply recompute a new array in some circumstances instead of returning a \\texttt{SubArray} if doing so is efficient and provides compatible semantics.\n",
       "\n",
       "\\begin{quote}\n",
       "\\textbf{compat}\n",
       "\n",
       "Julia 1.6\n",
       "\n",
       "In Julia 1.6 or later, \\texttt{view} can be called on an \\texttt{AbstractString}, returning a \\texttt{SubString}.\n",
       "\n",
       "\\end{quote}\n",
       "\\section{Examples}\n",
       "\\begin{verbatim}\n",
       "julia> A = [1 2; 3 4]\n",
       "2×2 Matrix{Int64}:\n",
       " 1  2\n",
       " 3  4\n",
       "\n",
       "julia> b = view(A, :, 1)\n",
       "2-element view(::Matrix{Int64}, :, 1) with eltype Int64:\n",
       " 1\n",
       " 3\n",
       "\n",
       "julia> fill!(b, 0)\n",
       "2-element view(::Matrix{Int64}, :, 1) with eltype Int64:\n",
       " 0\n",
       " 0\n",
       "\n",
       "julia> A # Note A has changed even though we modified b\n",
       "2×2 Matrix{Int64}:\n",
       " 0  2\n",
       " 0  4\n",
       "\n",
       "julia> view(2:5, 2:3) # returns a range as type is immutable\n",
       "3:4\n",
       "\\end{verbatim}\n"
      ],
      "text/markdown": [
       "```\n",
       "view(A, inds...)\n",
       "```\n",
       "\n",
       "Like [`getindex`](@ref), but returns a lightweight array that lazily references (or is effectively a *view* into) the parent array `A` at the given index or indices `inds` instead of eagerly extracting elements or constructing a copied subset. Calling [`getindex`](@ref) or [`setindex!`](@ref) on the returned value (often a [`SubArray`](@ref)) computes the indices to access or modify the parent array on the fly.  The behavior is undefined if the shape of the parent array is changed after `view` is called because there is no bound check for the parent array; e.g., it may cause a segmentation fault.\n",
       "\n",
       "Some immutable parent arrays (like ranges) may choose to simply recompute a new array in some circumstances instead of returning a `SubArray` if doing so is efficient and provides compatible semantics.\n",
       "\n",
       "!!! compat \"Julia 1.6\"\n",
       "    In Julia 1.6 or later, `view` can be called on an `AbstractString`, returning a `SubString`.\n",
       "\n",
       "\n",
       "# Examples\n",
       "\n",
       "```jldoctest\n",
       "julia> A = [1 2; 3 4]\n",
       "2×2 Matrix{Int64}:\n",
       " 1  2\n",
       " 3  4\n",
       "\n",
       "julia> b = view(A, :, 1)\n",
       "2-element view(::Matrix{Int64}, :, 1) with eltype Int64:\n",
       " 1\n",
       " 3\n",
       "\n",
       "julia> fill!(b, 0)\n",
       "2-element view(::Matrix{Int64}, :, 1) with eltype Int64:\n",
       " 0\n",
       " 0\n",
       "\n",
       "julia> A # Note A has changed even though we modified b\n",
       "2×2 Matrix{Int64}:\n",
       " 0  2\n",
       " 0  4\n",
       "\n",
       "julia> view(2:5, 2:3) # returns a range as type is immutable\n",
       "3:4\n",
       "```\n"
      ],
      "text/plain": [
       "\u001b[36m  view(A, inds...)\u001b[39m\n",
       "\n",
       "  Like \u001b[36mgetindex\u001b[39m, but returns a lightweight array that lazily references (or is\n",
       "  effectively a \u001b[4mview\u001b[24m into) the parent array \u001b[36mA\u001b[39m at the given index or indices\n",
       "  \u001b[36minds\u001b[39m instead of eagerly extracting elements or constructing a copied subset.\n",
       "  Calling \u001b[36mgetindex\u001b[39m or \u001b[36msetindex!\u001b[39m on the returned value (often a \u001b[36mSubArray\u001b[39m)\n",
       "  computes the indices to access or modify the parent array on the fly. The\n",
       "  behavior is undefined if the shape of the parent array is changed after \u001b[36mview\u001b[39m\n",
       "  is called because there is no bound check for the parent array; e.g., it may\n",
       "  cause a segmentation fault.\n",
       "\n",
       "  Some immutable parent arrays (like ranges) may choose to simply recompute a\n",
       "  new array in some circumstances instead of returning a \u001b[36mSubArray\u001b[39m if doing so\n",
       "  is efficient and provides compatible semantics.\n",
       "\n",
       "\u001b[39m\u001b[1m  │ \u001b[22m\u001b[39m\u001b[1mJulia 1.6\u001b[22m\n",
       "\u001b[39m\u001b[1m  │\u001b[22m\n",
       "\u001b[39m\u001b[1m  │\u001b[22m  In Julia 1.6 or later, \u001b[36mview\u001b[39m can be called on an \u001b[36mAbstractString\u001b[39m,\n",
       "\u001b[39m\u001b[1m  │\u001b[22m  returning a \u001b[36mSubString\u001b[39m.\n",
       "\n",
       "\u001b[1m  Examples\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "\u001b[36m  julia> A = [1 2; 3 4]\u001b[39m\n",
       "\u001b[36m  2×2 Matrix{Int64}:\u001b[39m\n",
       "\u001b[36m   1  2\u001b[39m\n",
       "\u001b[36m   3  4\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> b = view(A, :, 1)\u001b[39m\n",
       "\u001b[36m  2-element view(::Matrix{Int64}, :, 1) with eltype Int64:\u001b[39m\n",
       "\u001b[36m   1\u001b[39m\n",
       "\u001b[36m   3\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> fill!(b, 0)\u001b[39m\n",
       "\u001b[36m  2-element view(::Matrix{Int64}, :, 1) with eltype Int64:\u001b[39m\n",
       "\u001b[36m   0\u001b[39m\n",
       "\u001b[36m   0\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> A # Note A has changed even though we modified b\u001b[39m\n",
       "\u001b[36m  2×2 Matrix{Int64}:\u001b[39m\n",
       "\u001b[36m   0  2\u001b[39m\n",
       "\u001b[36m   0  4\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> view(2:5, 2:3) # returns a range as type is immutable\u001b[39m\n",
       "\u001b[36m  3:4\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?Base.view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ScikitLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-29T10:23:18.261000+08:00",
     "start_time": "2022-08-29T02:23:17.799Z"
    }
   },
   "outputs": [],
   "source": [
    "using ScikitLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-29T10:23:19.643000+08:00",
     "start_time": "2022-08-29T02:23:19.625Z"
    }
   },
   "outputs": [],
   "source": [
    "using ScikitLearn.CrossValidation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-08-29T10:23:21.139000+08:00",
     "start_time": "2022-08-29T02:23:20.577Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "No docstring or readme file found for module \\texttt{ScikitLearn.CrossValidation}.\n",
       "\n",
       "\\section{Exported names}\n",
       "\\texttt{KFold}, \\texttt{LabelKFold}, \\texttt{LabelShuffleSplit}, \\texttt{LeaveOneLabelOut}, \\texttt{LeaveOneOut}, \\texttt{LeavePLabelOut}, \\texttt{LeavePOut}, \\texttt{ShuffleSplit}, \\texttt{StratifiedKFold}, \\texttt{StratifiedShuffleSplit}, \\texttt{cross\\_val\\_predict}, \\texttt{cross\\_val\\_score}, \\texttt{train\\_test\\_split}\n",
       "\n"
      ],
      "text/markdown": [
       "No docstring or readme file found for module `ScikitLearn.CrossValidation`.\n",
       "\n",
       "# Exported names\n",
       "\n",
       "`KFold`, `LabelKFold`, `LabelShuffleSplit`, `LeaveOneLabelOut`, `LeaveOneOut`, `LeavePLabelOut`, `LeavePOut`, `ShuffleSplit`, `StratifiedKFold`, `StratifiedShuffleSplit`, `cross_val_predict`, `cross_val_score`, `train_test_split`\n"
      ],
      "text/plain": [
       "  No docstring or readme file found for module \u001b[36mScikitLearn.CrossValidation\u001b[39m.\n",
       "\n",
       "\u001b[1m  Exported names\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "  \u001b[36mKFold\u001b[39m, \u001b[36mLabelKFold\u001b[39m, \u001b[36mLabelShuffleSplit\u001b[39m, \u001b[36mLeaveOneLabelOut\u001b[39m, \u001b[36mLeaveOneOut\u001b[39m,\n",
       "  \u001b[36mLeavePLabelOut\u001b[39m, \u001b[36mLeavePOut\u001b[39m, \u001b[36mShuffleSplit\u001b[39m, \u001b[36mStratifiedKFold\u001b[39m,\n",
       "  \u001b[36mStratifiedShuffleSplit\u001b[39m, \u001b[36mcross_val_predict\u001b[39m, \u001b[36mcross_val_score\u001b[39m, \u001b[36mtrain_test_split\u001b[39m"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?ScikitLearn.CrossValidation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JLD2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T10:14:04.569000+08:00",
     "start_time": "2022-09-06T02:14:01.930Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JLDFile D:\\Jupyter notebook\\Tutorial\\Julia\\Julia基础\\x.jld2 (read-only)\n",
       " └─🔢 x"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using JLD2\n",
    "file = joinpath(pwd(), \"x.jld2\")\n",
    "x = (1, \"a\")\n",
    "jldsave(file;x)\n",
    "p = jldopen(file,\"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-09-06T10:14:25.617000+08:00",
     "start_time": "2022-09-06T02:14:24.537Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, \"a\")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p[\"x\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Julia 1.7.3",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.3"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "250.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
