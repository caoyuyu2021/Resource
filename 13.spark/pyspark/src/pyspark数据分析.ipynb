{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a484336",
   "metadata": {},
   "source": [
    "<font face=\"微软雅黑\" color=green size=5>SparkDataFrame基础</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a79f86b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-29T06:56:58.580612Z",
     "start_time": "2023-11-29T06:56:49.547589Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\Lib\\site-packages\\pyspark\\pandas\\__init__.py:50: UserWarning: 'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.pandas as ps\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "import findspark\n",
    "spark_home = \"D:\\\\Anaconda\\\\Lib\\\\site-packages\\\\pyspark\"\n",
    "python_path = \"D:\\\\Anaconda\\\\python\"\n",
    "findspark.init(spark_home,python_path)\n",
    "spark = SparkSession.builder.appName('SparkDataFrame').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0376257",
   "metadata": {},
   "source": [
    "# 读取SparkDataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9119d76",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-13T03:36:17.939751Z",
     "start_time": "2023-11-13T03:36:13.030557Z"
    }
   },
   "outputs": [],
   "source": [
    "df_iris = spark.read.csv(\"../data/iris.csv\", header=True, inferSchema=True)\n",
    "#header参数设置为True，表示第一行包含列名。inferSchema参数设置为True，表示Spark将尝试推断每列的数据类型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ca46cf5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T08:29:00.745527Z",
     "start_time": "2023-11-06T08:29:00.739501Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sepallength: double (nullable = true)\n",
      " |-- sepalwidth: double (nullable = true)\n",
      " |-- petallength: double (nullable = true)\n",
      " |-- petalwidth: double (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_iris.printSchema() #查看数据的结构和列的数据类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30c701f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T08:29:04.023024Z",
     "start_time": "2023-11-06T08:29:03.999354Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sepallength: double (nullable = true)\n",
      " |-- sepalwidth: double (nullable = true)\n",
      " |-- petallength: double (nullable = true)\n",
      " |-- petalwidth: double (nullable = true)\n",
      " |-- label: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_iris = df_iris.withColumn(\"label\", df_iris[\"label\"].cast(\"float\"))#转换数据类型\n",
    "df_iris.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c440768e",
   "metadata": {},
   "source": [
    "# 创建SparkDataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e37843",
   "metadata": {},
   "source": [
    "## 使用RDD来创建"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244f3257",
   "metadata": {},
   "source": [
    "主要使用RDD的toDF方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3718971c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T09:26:03.339327Z",
     "start_time": "2023-10-27T09:25:32.910448Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+\n",
      "| name|age|score|\n",
      "+-----+---+-----+\n",
      "|  Sam| 28|   88|\n",
      "|Flora| 28|   90|\n",
      "|  Run|  1|   60|\n",
      "+-----+---+-----+\n",
      "\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- score: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"Sam\", 28, 88), (\"Flora\", 28, 90), (\"Run\", 1, 60)] #list中的每一个元素都是元组\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "dfFromRDD1 = rdd.toDF([\"name\", \"age\", \"score\"])\n",
    "dfFromRDD1.show()\n",
    "dfFromRDD1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7a7d0e",
   "metadata": {},
   "source": [
    "## 使用python的DataFrame来创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "89d64df2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T07:39:02.496697Z",
     "start_time": "2023-11-08T07:38:48.177778Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    name  age  score\n",
      "0    Sam   28     88\n",
      "1  Flora   28     90\n",
      "2    Run    1     60\n",
      "+-----+---+-----+\n",
      "| name|age|score|\n",
      "+-----+---+-----+\n",
      "|  Sam| 28|   88|\n",
      "|Flora| 28|   90|\n",
      "|  Run|  1|   60|\n",
      "+-----+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#第一种\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "df = pd.DataFrame([['Sam', 28, 88], ['Flora', 28, 90], ['Run', 1, 60]],\n",
    "                  columns=['name', 'age', 'score'])\n",
    "print(df)\n",
    "Spark_df = spark.createDataFrame(df)\n",
    "Spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "87c4d589",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T07:40:43.269189Z",
     "start_time": "2023-11-08T07:40:28.185315Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+\n",
      "| name|age|score|\n",
      "+-----+---+-----+\n",
      "|  Sam| 28|   88|\n",
      "|Flora| 28|   90|\n",
      "|  Run|  1|   60|\n",
      "+-----+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#第二种\n",
    "import pyspark.pandas as ps\n",
    "Spark_df = ps.from_pandas(df).to_spark()\n",
    "Spark_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86204a08",
   "metadata": {},
   "source": [
    "## 使用pyspark.pandas来创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a583336",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T07:13:57.567934Z",
     "start_time": "2023-11-08T07:13:57.498622Z"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.pandas as ps\n",
    "import findspark\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "list_values = [['Sam', 28, 88], ['Flora', 28, 90], ['Run', 1, 60]]\n",
    "Spark_df = ps.DataFrame(data=list_values, columns=['name', 'age', 'score']).to_spark()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af4f55f",
   "metadata": {},
   "source": [
    "### 创建空DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "042b3f5c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T07:15:13.369745Z",
     "start_time": "2023-11-08T07:15:13.307742Z"
    }
   },
   "outputs": [],
   "source": [
    "a = ps.DataFrame(data=[], columns=['name', 'age', 'score']).to_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5399d975",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T07:15:29.363233Z",
     "start_time": "2023-11-08T07:15:15.003637Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ea13c7",
   "metadata": {},
   "source": [
    "## 使用List来创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8b5332f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T09:36:31.850887Z",
     "start_time": "2023-10-27T09:36:17.368289Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+\n",
      "| name|age|score|\n",
      "+-----+---+-----+\n",
      "|  Sam| 28|   88|\n",
      "|Flora| 28|   90|\n",
      "|  Run|  1|   60|\n",
      "+-----+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "list_values = [['Sam', 28, 88], ['Flora', 28, 90], ['Run', 1, 60]]\n",
    "Spark_df = spark.createDataFrame(list_values, ['name', 'age', 'score'])\n",
    "Spark_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd78715",
   "metadata": {},
   "source": [
    "## 读取数据文件来创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f5cda01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T09:37:27.838873Z",
     "start_time": "2023-10-27T09:37:27.485718Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-----------+----------+-----+\n",
      "|sepallength|sepalwidth|petallength|petalwidth|label|\n",
      "+-----------+----------+-----------+----------+-----+\n",
      "|        5.1|       3.5|        1.4|       0.2|    0|\n",
      "|        4.9|       3.0|        1.4|       0.2|    0|\n",
      "|        4.7|       3.2|        1.3|       0.2|    0|\n",
      "|        4.6|       3.1|        1.5|       0.2|    0|\n",
      "|        5.0|       3.6|        1.4|       0.2|    0|\n",
      "+-----------+----------+-----------+----------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- sepallength: double (nullable = true)\n",
      " |-- sepalwidth: double (nullable = true)\n",
      " |-- petallength: double (nullable = true)\n",
      " |-- petalwidth: double (nullable = true)\n",
      " |-- label: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4.1 CSV文件\n",
    "df = spark.read.option(\"header\", \"true\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .option(\"delimiter\", \",\")\\\n",
    "    .csv(\"../data/iris.csv\")\n",
    "df.show(5)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013a2491",
   "metadata": {},
   "source": [
    "## 通过读取数据库来创建"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a5b08ee7",
   "metadata": {},
   "source": [
    "# 5.1 读取hive数据\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS src (key INT, value STRING) USING hive\")\n",
    "spark.sql(\"LOAD DATA LOCAL INPATH 'data/kv1.txt' INTO TABLE src\")\n",
    "df = spark.sql(\"SELECT key, value FROM src WHERE key < 10 ORDER BY key\")\n",
    "df.show(5)\n",
    "\n",
    "# 5.2 读取mysql数据\n",
    "url = \"jdbc:mysql://localhost:3306/test\"\n",
    "df = spark.read.format(\"jdbc\") \\\n",
    " .option(\"url\", url) \\\n",
    " .option(\"dbtable\", \"runoob_tbl\") \\\n",
    " .option(\"user\", \"root\") \\\n",
    " .option(\"password\", \"8888\") \\\n",
    " .load()\\\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99764ecb",
   "metadata": {},
   "source": [
    "# 保存SparkDataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb281c53",
   "metadata": {},
   "source": [
    "在将DataFrame保存为CSV文件时，需要注意以下事项：\n",
    "\n",
    "- 输出文件夹或文件路径必须是一个不存在的路径，否则将会抛出错误。\n",
    "- 如果输出文件夹已经存在，Spark会自动为保存的CSV文件添加后缀以避免文件名冲突。\n",
    "- 默认情况下，每个CSV文件的文件名将为part-00000、part-00001等，可以在write.csv方法中通过option(\"pathGlobFilter\", \"*.csv\")设置文件名。\n",
    "- 如果DataFrame包含NULL值，则在保存为CSV文件时，NULL值将被保存为空字符串。\n",
    "- 如果DataFrame的列中包含特殊字符或引号，记得在保存CSV文件时使用适当的转义字符。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a307630",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-13T03:37:20.573477Z",
     "start_time": "2023-11-13T03:37:20.089538Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-----------+----------+-----+\n",
      "|sepallength|sepalwidth|petallength|petalwidth|label|\n",
      "+-----------+----------+-----------+----------+-----+\n",
      "|        5.1|       3.5|        1.4|       0.2|    0|\n",
      "|        4.9|       3.0|        1.4|       0.2|    0|\n",
      "|        4.7|       3.2|        1.3|       0.2|    0|\n",
      "|        4.6|       3.1|        1.5|       0.2|    0|\n",
      "|        5.0|       3.6|        1.4|       0.2|    0|\n",
      "+-----------+----------+-----------+----------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_iris = spark.read.csv(\"../data/iris.csv\", header=True, inferSchema=True)\n",
    "df_iris.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d705ddd",
   "metadata": {},
   "source": [
    "## 保存DataFrame为多个CSV文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60a305df",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-13T03:38:43.015612Z",
     "start_time": "2023-11-13T03:38:42.714939Z"
    }
   },
   "outputs": [],
   "source": [
    "save_path = \"../data/iris\"\n",
    "df_iris.write.csv(save_path, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2eb2f4",
   "metadata": {},
   "source": [
    "## 保存DataFrame为单个CSV文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77d4a41c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-13T03:39:54.721718Z",
     "start_time": "2023-11-13T03:39:54.549938Z"
    }
   },
   "outputs": [],
   "source": [
    "save_path = \"../data/iris_single\"\n",
    "df_iris.coalesce(1).write.csv(save_path, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab750968",
   "metadata": {},
   "source": [
    "# 常用的SparkDataFrame API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a62f215",
   "metadata": {},
   "source": [
    "## 数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d410e1b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-29T06:57:48.938950Z",
     "start_time": "2023-11-29T06:57:19.630348Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+----+\n",
      "| name|age|score| sex|\n",
      "+-----+---+-----+----+\n",
      "|  Sam| 28|   88|   M|\n",
      "|Flora| 28|   90|   F|\n",
      "|  Run|  1|   60|null|\n",
      "|Peter| 55|  100|   M|\n",
      "|  Mei| 54|   95|   F|\n",
      "|  Mei| 54|   95|   F|\n",
      "+-----+---+-----+----+\n",
      "\n",
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- score: long (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 创建一个SparkDataFrame\n",
    "rdd = spark.sparkContext.parallelize([(\"Sam\", 28, 88, \"M\"),\n",
    "                                      (\"Flora\", 28, 90, \"F\"),\n",
    "                                      (\"Run\", 1, 60, None),\n",
    "                                      (\"Peter\", 55, 100, \"M\"),\n",
    "                                      (\"Mei\", 54, 95, \"F\")])\n",
    "df = rdd.toDF([\"name\", \"age\", \"score\", \"sex\"])\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabba226",
   "metadata": {},
   "source": [
    "## DataFrame的APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ab558c",
   "metadata": {},
   "source": [
    "### 查看类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2cda9208",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T09:37:06.678149Z",
     "start_time": "2023-11-06T09:37:06.673049Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('name', 'string'), ('age', 'bigint'), ('score', 'bigint'), ('sex', 'string')]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451d192d",
   "metadata": {},
   "source": [
    "### 查询行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35fdf463",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T08:30:23.790845Z",
     "start_time": "2023-11-06T08:29:55.687823Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+----+\n",
      "| name|age|score| sex|\n",
      "+-----+---+-----+----+\n",
      "|  Sam| 28|   88|   M|\n",
      "|Flora| 28|   90|   F|\n",
      "|  Run|  1|   60|null|\n",
      "|Peter| 55|  100|   M|\n",
      "|  Mei| 54|   95|   F|\n",
      "+-----+---+-----+----+\n",
      "\n",
      "+-----+---+-----+----+\n",
      "| name|age|score| sex|\n",
      "+-----+---+-----+----+\n",
      "|  Sam| 28|   88|   M|\n",
      "|Flora| 28|   90|   F|\n",
      "|  Run|  1|   60|null|\n",
      "+-----+---+-----+----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#指定要打印的行数\n",
    "df.show()\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a25bcf7c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T09:59:23.466884Z",
     "start_time": "2023-10-27T09:59:08.530049Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='Sam', age=28, score=88, sex='M'),\n",
       " Row(name='Flora', age=28, score=90, sex='F'),\n",
       " Row(name='Run', age=1, score=60, sex=None),\n",
       " Row(name='Peter', age=55, score=100, sex='M'),\n",
       " Row(name='Mei', age=54, score=95, sex='F')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 以列表形式返回行\n",
    "df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0cf91f6e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T09:45:27.744329Z",
     "start_time": "2023-10-27T09:45:13.025971Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#查询总行数\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4243118f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T08:30:33.158872Z",
     "start_time": "2023-11-06T08:30:28.603469Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(name='Sam', age=28, score=88, sex='M')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看第1条数据\n",
    "df.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1dfb7106",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T08:30:56.274928Z",
     "start_time": "2023-11-06T08:30:33.158872Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='Sam', age=28, score=88, sex='M'),\n",
       " Row(name='Flora', age=28, score=90, sex='F')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#获取头几行到本地\n",
    "df.head(2)\n",
    "df.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "558cf62b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T08:31:10.532299Z",
     "start_time": "2023-11-06T08:30:56.276921Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+---+\n",
      "| name|age|score|sex|\n",
      "+-----+---+-----+---+\n",
      "|Flora| 28|   90|  F|\n",
      "|Peter| 55|  100|  M|\n",
      "|  Mei| 54|   95|  F|\n",
      "+-----+---+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 按照一定规则从df随机抽样数据\n",
    "df.sample(0.5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5943df",
   "metadata": {},
   "source": [
    "### 查询列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "40e3e927",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T10:01:49.699894Z",
     "start_time": "2023-10-27T10:01:49.694605Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name', 'age', 'score', 'sex']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#查询列名\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bcd1bd09",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T10:02:00.416229Z",
     "start_time": "2023-10-27T10:02:00.410725Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('name', 'string'), ('age', 'bigint'), ('score', 'bigint'), ('sex', 'string')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#查询列类型\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "94f2b636",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T10:00:18.477003Z",
     "start_time": "2023-10-27T10:00:03.521868Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|               age|\n",
      "+-------+------------------+\n",
      "|  count|                 5|\n",
      "|   mean|              33.2|\n",
      "| stddev|22.353970564532826|\n",
      "|    min|                 1|\n",
      "|    max|                55|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 返回列的基础统计信息\n",
    "df.describe(['age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bf4a1336",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T09:59:56.858664Z",
     "start_time": "2023-10-27T09:59:41.501619Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------------------+------------------+----+\n",
      "|summary| name|               age|             score| sex|\n",
      "+-------+-----+------------------+------------------+----+\n",
      "|  count|    5|                 5|                 5|   4|\n",
      "|   mean| null|              33.2|              86.6|null|\n",
      "| stddev| null|22.353970564532826|15.582040944625966|null|\n",
      "|    min|Flora|                 1|                60|   F|\n",
      "|    max|  Sam|                55|               100|   M|\n",
      "+-------+-----+------------------+------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#查询概况\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d1c7d69c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T10:00:36.905121Z",
     "start_time": "2023-10-27T10:00:22.062117Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "| sex|score|\n",
      "+----+-----+\n",
      "|   M|   88|\n",
      "|   F|   90|\n",
      "|null|   60|\n",
      "|   M|  100|\n",
      "|   F|   95|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 选定指定列并按照一定顺序呈现\n",
    "df.select(\"sex\", \"score\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ef89aa56",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T10:02:23.877539Z",
     "start_time": "2023-10-27T10:02:08.862474Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------+\n",
      "|  age_freqItems|sex_freqItems|\n",
      "+---------------+-------------+\n",
      "|[55, 1, 28, 54]| [M, null, F]|\n",
      "+---------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 查看指定列的枚举值\n",
    "df.freqItems([\"age\",\"sex\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "932da64b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T10:03:03.443367Z",
     "start_time": "2023-10-27T10:02:47.906392Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+------------------+------------------+----+\n",
      "|summary| name|               age|             score| sex|\n",
      "+-------+-----+------------------+------------------+----+\n",
      "|  count|    5|                 5|                 5|   4|\n",
      "|   mean| null|              33.2|              86.6|null|\n",
      "| stddev| null|22.353970564532826|15.582040944625966|null|\n",
      "|    min|Flora|                 1|                60|   F|\n",
      "|    25%| null|                28|                88|null|\n",
      "|    50%| null|                28|                90|null|\n",
      "|    75%| null|                54|                95|null|\n",
      "|    max|  Sam|                55|               100|   M|\n",
      "+-------+-----+------------------+------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc77971c",
   "metadata": {},
   "source": [
    "### 查询null列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "22ff9552",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T09:57:36.461415Z",
     "start_time": "2023-10-27T09:57:36.452053Z"
    }
   },
   "outputs": [],
   "source": [
    "#查询某列为null的行\n",
    "from pyspark.sql.functions import isnull\n",
    "df1 = df.filter(isnull(\"sex\"))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60291f7",
   "metadata": {},
   "source": [
    "### 选择特定行数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3b8aff25",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T10:16:30.753382Z",
     "start_time": "2023-10-27T10:16:15.866583Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------------------+\n",
      "| name|((id >= 2) AND (id <= 4))|\n",
      "+-----+-------------------------+\n",
      "|  Sam|                    false|\n",
      "|Flora|                    false|\n",
      "|  Run|                    false|\n",
      "|Peter|                    false|\n",
      "|  Mei|                    false|\n",
      "+-----+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#选择dataframe中间的特定行数，该函数是不确定的，因为它的结果取决于分区 ID。\n",
    "from pyspark.sql.functions import monotonically_increasing_id #设定自增长列\n",
    "dfWithIndex = df.withColumn('id',monotonically_increasing_id()) #添加自增长列\n",
    "dfWithIndex.select(dfWithIndex.name, dfWithIndex.id.between(2, 4)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c67a11eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T10:21:55.121371Z",
     "start_time": "2023-10-27T10:20:17.595283Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+----+---+\n",
      "| name|age|score| sex| id|\n",
      "+-----+---+-----+----+---+\n",
      "|  Sam| 28|   88|   M|  0|\n",
      "|Flora| 28|   90|   F|  1|\n",
      "|  Run|  1|   60|null|  2|\n",
      "|Peter| 55|  100|   M|  3|\n",
      "|  Mei| 54|   95|   F|  4|\n",
      "+-----+---+-----+----+---+\n",
      "\n",
      "+-----+-------------------------+\n",
      "| name|((id >= 2) AND (id <= 4))|\n",
      "+-----+-------------------------+\n",
      "|  Sam|                    false|\n",
      "|Flora|                    false|\n",
      "|  Run|                     true|\n",
      "|Peter|                     true|\n",
      "|  Mei|                     true|\n",
      "+-----+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 使用`zipWithIndex`方法和`toDF`函数添加递增列\n",
    "df_with_incremental_values = df.rdd.zipWithIndex().map(lambda x: x[0] + (x[1],)).toDF(df.columns + ['id'])\n",
    "\n",
    "# 显示结果\n",
    "df_with_incremental_values.show()\n",
    "df_with_incremental_values.select(df_with_incremental_values.name, df_with_incremental_values.id.between(2, 4)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4d488e",
   "metadata": {},
   "source": [
    "### 选择特定列数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98896fbb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T07:25:11.063960Z",
     "start_time": "2023-11-08T07:24:57.115895Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='Sam', age=28),\n",
       " Row(name='Flora', age=28),\n",
       " Row(name='Run', age=1),\n",
       " Row(name='Peter', age=55),\n",
       " Row(name='Mei', age=54)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs = df.select('name','age')\n",
    "dfs.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f534b2",
   "metadata": {},
   "source": [
    "### 排序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c2d4271f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T10:06:17.750357Z",
     "start_time": "2023-10-27T10:06:03.250457Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+----+\n",
      "| name|age|score| sex|\n",
      "+-----+---+-----+----+\n",
      "|Peter| 55|  100|   M|\n",
      "|  Mei| 54|   95|   F|\n",
      "|  Sam| 28|   88|   M|\n",
      "|Flora| 28|   90|   F|\n",
      "|  Run|  1|   60|null|\n",
      "+-----+---+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#降序\n",
    "df.orderBy(df['age'].desc()).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7c76aba0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T10:07:50.753435Z",
     "start_time": "2023-10-27T10:07:35.450035Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+----+\n",
      "| name|age|score| sex|\n",
      "+-----+---+-----+----+\n",
      "|  Run|  1|   60|null|\n",
      "|  Sam| 28|   88|   M|\n",
      "|Flora| 28|   90|   F|\n",
      "|  Mei| 54|   95|   F|\n",
      "|Peter| 55|  100|   M|\n",
      "+-----+---+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#升序\n",
    "df.orderBy(df['age']).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8db093",
   "metadata": {},
   "source": [
    "### 去重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "53cb3ef9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-27T10:24:39.932692Z",
     "start_time": "2023-10-27T10:24:25.456089Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+----+\n",
      "| name|age|score| sex|\n",
      "+-----+---+-----+----+\n",
      "|  Sam| 28|   88|   M|\n",
      "|Flora| 28|   90|   F|\n",
      "|  Run|  1|   60|null|\n",
      "|Peter| 55|  100|   M|\n",
      "|  Mei| 54|   95|   F|\n",
      "+-----+---+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 对数据集进行去重\n",
    "df.distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "de18df74",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T01:48:18.103261Z",
     "start_time": "2023-10-30T01:48:02.264357Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+----+\n",
      "| name|age|score| sex|\n",
      "+-----+---+-----+----+\n",
      "|  Run|  1|   60|null|\n",
      "|Flora| 28|   90|   F|\n",
      "|  Sam| 28|   88|   M|\n",
      "+-----+---+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 对指定列去重\n",
    "df.dropDuplicates([\"sex\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4b46150e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T01:49:53.588729Z",
     "start_time": "2023-10-30T01:48:22.779681Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| C1| C2|\n",
      "+---+---+\n",
      "|  a|  1|\n",
      "|  a|  1|\n",
      "|  b|  3|\n",
      "|  c|  4|\n",
      "+---+---+\n",
      "\n",
      "+---+---+\n",
      "| C1| C2|\n",
      "+---+---+\n",
      "|  a|  1|\n",
      "|  b|  3|\n",
      "+---+---+\n",
      "\n",
      "+---+---+\n",
      "| C1| C2|\n",
      "+---+---+\n",
      "|  a|  1|\n",
      "|  c|  4|\n",
      "+---+---+\n",
      "\n",
      "+---+---+\n",
      "| C1| C2|\n",
      "+---+---+\n",
      "|  c|  4|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 根据指定的df对df进行去重\n",
    "df1 = spark.createDataFrame(\n",
    "        [(\"a\", 1), (\"a\", 1), (\"b\",  3), (\"c\", 4)], [\"C1\", \"C2\"])\n",
    "df2 = spark.createDataFrame([(\"a\", 1), (\"b\", 3)], [\"C1\", \"C2\"])\n",
    "df3 = df1.exceptAll(df2)  # 没有去重的功效\n",
    "df4 = df1.subtract(df2)  # 有去重的奇效\n",
    "df1.show()\n",
    "df2.show()\n",
    "df3.show()\n",
    "df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "dd93d5e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T01:50:23.687837Z",
     "start_time": "2023-10-30T01:49:53.592722Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| C1| C2|\n",
      "+---+---+\n",
      "|  a|  1|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 返回两个DataFrame的交集\n",
    "df1 = spark.createDataFrame(\n",
    "        [(\"a\", 1), (\"a\", 1), (\"b\",  3), (\"c\", 4)], [\"C1\", \"C2\"])\n",
    "df2 = spark.createDataFrame([(\"a\", 1), (\"b\", 4)], [\"C1\", \"C2\"])\n",
    "df1.intersectAll(df2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a184f0d",
   "metadata": {},
   "source": [
    "### 丢弃列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d5cac8ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T01:50:50.099453Z",
     "start_time": "2023-10-30T01:50:35.018185Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+----+\n",
      "| name|score| sex|\n",
      "+-----+-----+----+\n",
      "|  Sam|   88|   M|\n",
      "|Flora|   90|   F|\n",
      "|  Run|   60|null|\n",
      "|Peter|  100|   M|\n",
      "|  Mei|   95|   F|\n",
      "+-----+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 丢弃指定列\n",
    "df.drop('age').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "385345f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T01:52:25.712927Z",
     "start_time": "2023-10-30T01:52:10.491577Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+---+\n",
      "| name|age|score|sex|\n",
      "+-----+---+-----+---+\n",
      "|  Sam| 28|   88|  M|\n",
      "|Flora| 28|   90|  F|\n",
      "|Peter| 55|  100|  M|\n",
      "|  Mei| 54|   95|  F|\n",
      "+-----+---+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 丢弃空值\n",
    "df.dropna(how='all', subset=['sex']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc50d2f4",
   "metadata": {},
   "source": [
    "### 新增列"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd407ea",
   "metadata": {},
   "source": [
    "#### lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "108e1125",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T02:40:00.864766Z",
     "start_time": "2023-11-17T02:39:45.033693Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+----+-------+\n",
      "| name|age|score| sex|country|\n",
      "+-----+---+-----+----+-------+\n",
      "|  Sam| 28|   88|   M|  China|\n",
      "|Flora| 28|   90|   F|  China|\n",
      "|  Run|  1|   60|null|  China|\n",
      "|Peter| 55|  100|   M|  China|\n",
      "|  Mei| 54|   95|   F|  China|\n",
      "+-----+---+-----+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "# 添加常量列，值都是相同的\n",
    "df.withColumn(\"country\", lit(\"China\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b75672d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T02:42:49.717724Z",
     "start_time": "2023-11-17T02:42:33.688709Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+----+-------+\n",
      "| name|age|score| sex|new_age|\n",
      "+-----+---+-----+----+-------+\n",
      "|  Sam| 28|   88|   M|     33|\n",
      "|Flora| 28|   90|   F|     33|\n",
      "|  Run|  1|   60|null|      6|\n",
      "|Peter| 55|  100|   M|     60|\n",
      "|  Mei| 54|   95|   F|     59|\n",
      "+-----+---+-----+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 增加年龄列的值\n",
    "df.withColumn(\"new_age\", F.col(\"age\") + lit(5)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc095ba",
   "metadata": {},
   "source": [
    "#### withColumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b8f30660",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T01:52:47.942012Z",
     "start_time": "2023-10-30T01:52:32.952337Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+----+----------+\n",
      "| name|age|score| sex|birth_year|\n",
      "+-----+---+-----+----+----------+\n",
      "|  Sam| 28|   88|   M|      1993|\n",
      "|Flora| 28|   90|   F|      1993|\n",
      "|  Run|  1|   60|null|      2020|\n",
      "|Peter| 55|  100|   M|      1966|\n",
      "|  Mei| 54|   95|   F|      1967|\n",
      "+-----+---+-----+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 新增相关列，新增的列和之前列有关\n",
    "df1 = df.withColumn(\"birth_year\", 2021 - df.age)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7944fc2",
   "metadata": {},
   "source": [
    "#### monotonically_increasing_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f072c285",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T09:18:32.828138Z",
     "start_time": "2023-11-10T09:18:18.566801Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|country|\n",
      "+-------+\n",
      "|  China|\n",
      "|    USA|\n",
      "|  China|\n",
      "|     UK|\n",
      "|  India|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 创建另一个SparkDataFrame\n",
    "list_values = [[\"China\"],[\"USA\"],[\"China\"],[\"UK\"],[\"India\"]]\n",
    "df1 = spark.createDataFrame(list_values, [\"country\"])\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b4563c37",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T10:16:52.160808Z",
     "start_time": "2023-11-09T10:16:23.496011Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+----+-------+\n",
      "| name|age|score| sex|country|\n",
      "+-----+---+-----+----+-------+\n",
      "|  Sam| 28|   88|   M|  China|\n",
      "|Flora| 28|   90|   F|    USA|\n",
      "|  Run|  1|   60|null|  China|\n",
      "|Peter| 55|  100|   M|     UK|\n",
      "|  Mei| 54|   95|   F|  India|\n",
      "+-----+---+-----+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#新增无关列，新增的列和之前列无关\n",
    "#先设置自增长列，根据自增长列的序号join，最后匹配，自增长列和分块有关，不同块值不同，并不能完全自增长\n",
    "temp = df.withColumn('Row_ID', monotonically_increasing_id())\n",
    "temp1 = df1.withColumn('Row_ID', monotonically_increasing_id())\n",
    "temp2 = temp.join(temp1, temp['Row_ID'] == temp1['Row_ID'], 'left').drop('Row_ID')\n",
    "temp2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0c4a11",
   "metadata": {},
   "source": [
    "#### zipWithIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09af3ec3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T09:25:02.483746Z",
     "start_time": "2023-11-10T09:22:43.529746Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+----+-------+\n",
      "| name|age|score| sex|country|\n",
      "+-----+---+-----+----+-------+\n",
      "|  Sam| 28|   88|   M|  China|\n",
      "|Flora| 28|   90|   F|    USA|\n",
      "|  Run|  1|   60|null|  China|\n",
      "|Peter| 55|  100|   M|     UK|\n",
      "|  Mei| 54|   95|   F|  India|\n",
      "+-----+---+-----+----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 使用`zipWithIndex`方法和`toDF`函数添加递增列\n",
    "temp = df.rdd.zipWithIndex().map(lambda x: x[0] + (x[1],)).toDF(df.columns + ['Row_ID'])\n",
    "temp1 = df1.rdd.zipWithIndex().map(lambda x: x[0] + (x[1],)).toDF(df1.columns + ['Row_ID'])\n",
    "temp2 = temp.join(temp1, on=['Row_ID'], how='left').drop('Row_ID')\n",
    "temp2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32859a5",
   "metadata": {},
   "source": [
    "### 合并列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b272ff9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T01:21:47.063403Z",
     "start_time": "2023-11-10T01:21:32.905813Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+----+---------+\n",
      "| name|age|score| sex|age_score|\n",
      "+-----+---+-----+----+---------+\n",
      "|  Sam| 28|   88|   M|    28_88|\n",
      "|Flora| 28|   90|   F|    28_90|\n",
      "|  Run|  1|   60|null|     1_60|\n",
      "|Peter| 55|  100|   M|   55_100|\n",
      "|  Mei| 54|   95|   F|    54_95|\n",
      "+-----+---+-----+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = df.withColumn(\"age_score\", F.concat_ws('_', F.col(\"age\"), F.col(\"score\")))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc6a1612",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T01:15:11.209580Z",
     "start_time": "2023-11-10T01:14:56.814382Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+----+---------+\n",
      "| name|age|score| sex|age_score|\n",
      "+-----+---+-----+----+---------+\n",
      "|  Sam| 28|   88|   M|     2888|\n",
      "|Flora| 28|   90|   F|     2890|\n",
      "|  Run|  1|   60|null|      160|\n",
      "|Peter| 55|  100|   M|    55100|\n",
      "|  Mei| 54|   95|   F|     5495|\n",
      "+-----+---+-----+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df.withColumn(\"age_score\",F.concat(\"age\",\"score\"))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37321f6",
   "metadata": {},
   "source": [
    "### 重命名列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6afbad57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T01:53:31.445520Z",
     "start_time": "2023-10-30T01:53:16.775144Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+------+\n",
      "| name|age|score|gender|\n",
      "+-----+---+-----+------+\n",
      "|  Sam| 28|   88|     M|\n",
      "|Flora| 28|   90|     F|\n",
      "|  Run|  1|   60|  null|\n",
      "|Peter| 55|  100|     M|\n",
      "|  Mei| 54|   95|     F|\n",
      "+-----+---+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 重命名列名\n",
    "df1 = df.withColumnRenamed(\"sex\", \"gender\")\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613a44ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#取别名\n",
    "df['age'].alias('age_value')\n",
    "df.select(df['age'].alias('age_value'),'name').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a43a2f9",
   "metadata": {},
   "source": [
    "### 修改列类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "916e3ef6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T08:48:32.854051Z",
     "start_time": "2023-11-06T08:48:18.838117Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+----+\n",
      "| name|age|score| sex|\n",
      "+-----+---+-----+----+\n",
      "|  Sam| 28|   88|   M|\n",
      "|Flora| 28|   90|   F|\n",
      "|  Run|  1|   60|null|\n",
      "|Peter| 55|  100|   M|\n",
      "|  Mei| 54|   95|   F|\n",
      "+-----+---+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"age\", df[\"age\"].cast('string'))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "099059db",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T09:15:48.598169Z",
     "start_time": "2023-11-06T09:15:07.299673Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+----+\n",
      "| name|age|score| sex|\n",
      "+-----+---+-----+----+\n",
      "|  Sam| 28|   88|   M|\n",
      "|Flora| 28|   90|   F|\n",
      "|  Run|  1|   60|null|\n",
      "|Peter| 55|  100|   M|\n",
      "|  Mei| 54|   95|   F|\n",
      "+-----+---+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"age\", df[\"age\"].astype('string'))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5749ebfe",
   "metadata": {},
   "source": [
    "### 填充列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "35cf18cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T01:59:04.982317Z",
     "start_time": "2023-10-30T01:58:35.463584Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|  C1|  C2|\n",
      "+----+----+\n",
      "|   a|null|\n",
      "|   a|   1|\n",
      "|null|   3|\n",
      "|   c|   4|\n",
      "+----+----+\n",
      "\n",
      "+---+---+\n",
      "| C1| C2|\n",
      "+---+---+\n",
      "|  a| 99|\n",
      "|  a|  1|\n",
      "|  d|  3|\n",
      "|  c|  4|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 空值填充操作\n",
    "df1 = spark.createDataFrame(\n",
    "        [(\"a\", None), (\"a\", 1), (None,  3), (\"c\", 4)], [\"C1\", \"C2\"])\n",
    "# df2 = df1.na.fill({\"C1\": \"d\", \"C2\": 99})\n",
    "df2 = df1.fillna({\"C1\": \"d\", \"C2\": 99})\n",
    "df1.show()\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796e653b",
   "metadata": {},
   "source": [
    "### 条件过滤"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e16b2fc",
   "metadata": {},
   "source": [
    "#### filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a8e9864d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T09:20:32.784980Z",
     "start_time": "2023-11-06T09:19:51.601585Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+---+\n",
      "| name|age|score|sex|\n",
      "+-----+---+-----+---+\n",
      "|Peter| 55|  100|  M|\n",
      "|  Mei| 54|   95|  F|\n",
      "+-----+---+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 根据条件过滤\n",
    "df.filter(df.age.between(50, 90)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5db710ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T09:34:48.682750Z",
     "start_time": "2023-11-06T09:34:06.445305Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+-----+---+\n",
      "|name|age|score|sex|\n",
      "+----+---+-----+---+\n",
      "+----+---+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 可以使用正则的匹配\n",
    "df.filter(df.name.rlike('Me$')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f52507fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T09:31:52.401152Z",
     "start_time": "2023-11-06T09:31:11.117303Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+-----+---+\n",
      "|name|age|score|sex|\n",
      "+----+---+-----+---+\n",
      "| Mei| 54|   95|  F|\n",
      "+----+---+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 返回含有关键词的行\n",
    "df.filter(df.name.like('Mei')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "68d09974",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T09:25:48.821779Z",
     "start_time": "2023-11-06T09:25:07.244196Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+-----+----+\n",
      "|name|age|score| sex|\n",
      "+----+---+-----+----+\n",
      "| Run|  1|   60|null|\n",
      "+----+---+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 筛选非空的行\n",
    "df.filter(df.sex.isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e3134270",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T09:24:43.433972Z",
     "start_time": "2023-11-06T09:24:02.127640Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+---+\n",
      "| name|age|score|sex|\n",
      "+-----+---+-----+---+\n",
      "|  Sam| 28|   88|  M|\n",
      "|Flora| 28|   90|  F|\n",
      "|Peter| 55|  100|  M|\n",
      "|  Mei| 54|   95|  F|\n",
      "+-----+---+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 筛选非空的行\n",
    "df.filter(df.sex.isNotNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e0d38a70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T09:21:58.451409Z",
     "start_time": "2023-11-06T09:21:17.091493Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+---+\n",
      "| name|age|score|sex|\n",
      "+-----+---+-----+---+\n",
      "|Peter| 55|  100|  M|\n",
      "+-----+---+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 是否包含某个关键词\n",
    "df.filter(df.age.contains(55)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9b1654c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T09:19:02.095606Z",
     "start_time": "2023-11-06T09:18:21.041188Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+---+\n",
      "| name|age|score|sex|\n",
      "+-----+---+-----+---+\n",
      "|Peter| 55|  100|  M|\n",
      "|  Mei| 54|   95|  F|\n",
      "+-----+---+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#单条件过滤\n",
    "df.filter(df.age>50).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c30522",
   "metadata": {},
   "outputs": [],
   "source": [
    "#多条件过滤\n",
    "df.filter((df.age>50) | (df.age<10)).show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c3c21f23",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T02:00:00.949470Z",
     "start_time": "2023-10-30T01:59:45.097997Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+-----+----+\n",
      "|name|age|score| sex|\n",
      "+----+---+-----+----+\n",
      "| Run|  1|   60|null|\n",
      "+----+---+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(\"age<18\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04805cbe",
   "metadata": {},
   "source": [
    "#### where"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11ad3b85",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T01:43:13.045612Z",
     "start_time": "2023-11-10T01:42:58.722582Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+---+\n",
      "| name|age|score|sex|\n",
      "+-----+---+-----+---+\n",
      "|  Sam| 28|   88|  M|\n",
      "|Flora| 28|   90|  F|\n",
      "+-----+---+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#条件选择\n",
    "df.where(df.age==28).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65622e09",
   "metadata": {},
   "source": [
    "#### when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "177a7026",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T10:34:01.786458Z",
     "start_time": "2023-11-17T10:33:45.588682Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----+\n",
      "| name|age|判断|\n",
      "+-----+---+----+\n",
      "|  Sam| 28|   1|\n",
      "|Flora| 28|   1|\n",
      "|  Run|  1|  -1|\n",
      "|Peter| 55|   1|\n",
      "|  Mei| 54|   1|\n",
      "+-----+---+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#when(condition, value1).otherwise(value2)联合使用：\n",
    "#当满足条件condition的指赋值为values1,不满足条件的则赋值为values2.\n",
    "#otherwise表示，不满足条件的情况下，应该赋值为啥\n",
    "df.select(df.name, df.age, F.when(df.age > 4, 1).when(df.age < 3, -1).otherwise(0).alias(\"判断\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ec73fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#多条件需要用括号括起来，并且只能用&，|符号\n",
    "df.select(df.name, df.age, F.when((df.age > 4)&(df.age < 2), 1).when(df.age < 3, -1).otherwise(0).alias(\"判断\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f653c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.withColumn(\"value_th\", F.when(df[\"age\"] >= 10, 1).when(df[\"value\"] <= 20, -1).otherwise(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d4454d",
   "metadata": {},
   "source": [
    "#### isin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2820dd66",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T01:46:31.242756Z",
     "start_time": "2023-11-10T01:46:17.041901Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+---+\n",
      "| name|age|score|sex|\n",
      "+-----+---+-----+---+\n",
      "|  Sam| 28|   88|  M|\n",
      "|Flora| 28|   90|  F|\n",
      "+-----+---+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 列表参数\n",
    "filter_list = [\"Sam\", \"Flora\"]\n",
    "\n",
    "# 使用isin函数进行筛选\n",
    "filtered_df = df.filter(F.col(\"name\").isin(filter_list))\n",
    "filtered_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "43ca6cd5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T09:28:22.815863Z",
     "start_time": "2023-11-06T09:27:41.729097Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+---+\n",
      "| name|age|score|sex|\n",
      "+-----+---+-----+---+\n",
      "|Peter| 55|  100|  M|\n",
      "|  Mei| 54|   95|  F|\n",
      "+-----+---+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 返回包含某些值的行\n",
    "df.filter(df.name.isin('Peter', 'Mei')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed051ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T01:50:20.468006Z",
     "start_time": "2023-11-10T01:50:20.463271Z"
    }
   },
   "source": [
    "#### coalesce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b41a04ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T01:49:42.854688Z",
     "start_time": "2023-11-10T01:49:28.574119Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+----+--------+\n",
      "| name|age|score| sex|age_name|\n",
      "+-----+---+-----+----+--------+\n",
      "|  Sam| 28|   88|   M|      28|\n",
      "|Flora| 28|   90|   F|      28|\n",
      "|  Run|  1|   60|null|       1|\n",
      "|Peter| 55|  100|   M|      55|\n",
      "|  Mei| 54|   95|   F|      54|\n",
      "+-----+---+-----+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#返回第一个不为null的列\n",
    "df.withColumn('age_name',F.coalesce(df.age,df.name)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348a49ea",
   "metadata": {},
   "source": [
    "#### expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "80768de6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T01:53:46.106983Z",
     "start_time": "2023-11-10T01:53:31.718253Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|upper(name)|\n",
      "+-----------+\n",
      "|        SAM|\n",
      "|      FLORA|\n",
      "|        RUN|\n",
      "|      PETER|\n",
      "|        MEI|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#expr()方法解析给定的SQL表达式\n",
    "df.select(F.expr('upper(name)')).show() #字母大写"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9415a268",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T01:54:29.101986Z",
     "start_time": "2023-11-10T01:54:14.750603Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|upper(name)|\n",
      "+-----------+\n",
      "|        SAM|\n",
      "|      FLORA|\n",
      "|        RUN|\n",
      "|      PETER|\n",
      "|        MEI|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#expr(~) 方法通常可以使用 PySpark DataFrame 的 selectExpr(~) 方法编写得更简洁\n",
    "df.selectExpr('upper(name)').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c661a689",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T01:55:25.620995Z",
     "start_time": "2023-11-10T01:55:11.228947Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|result|\n",
      "+------+\n",
      "|  true|\n",
      "| false|\n",
      "| false|\n",
      "| false|\n",
      "| false|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#使用expr方法解析复杂的SQL表达式\n",
    "df.select(F.expr('age > 20 AND name LIKE \"S%\"').alias('result')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "380fd3c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T01:57:39.523083Z",
     "start_time": "2023-11-10T01:57:24.811853Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+----+------+\n",
      "| name|age|score| sex|status|\n",
      "+-----+---+-----+----+------+\n",
      "|  Sam| 28|   88|   M|     1|\n",
      "|Flora| 28|   90|   F|     1|\n",
      "|  Run|  1|   60|null|     1|\n",
      "|Peter| 55|  100|   M|     0|\n",
      "|  Mei| 54|   95|   F|     0|\n",
      "+-----+---+-----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "col = F.expr('CASE WHEN age < 40 THEN \"1\" ELSE \"0\" END').alias('result')\n",
    "df.withColumn('status', col).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011a4f26",
   "metadata": {},
   "source": [
    "#### substring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4f154cae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-10T01:58:59.585436Z",
     "start_time": "2023-11-10T01:58:45.491698Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+----+-----+\n",
      "| name|age|score| sex|age_1|\n",
      "+-----+---+-----+----+-----+\n",
      "|  Sam| 28|   88|   M|   am|\n",
      "|Flora| 28|   90|   F|  lor|\n",
      "|  Run|  1|   60|null|   un|\n",
      "|Peter| 55|  100|   M|  ete|\n",
      "|  Mei| 54|   95|   F|   ei|\n",
      "+-----+---+-----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"age_1\", F.substring(df.name, 2, 3)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c498866",
   "metadata": {},
   "source": [
    "### join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83158639",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-08T07:34:51.528968Z",
     "start_time": "2023-11-08T07:34:22.449290Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+-------+\n",
      "|df1_id|df1_num|df2_num|\n",
      "+------+-------+-------+\n",
      "|     d|      1|   null|\n",
      "|     c|      4|   null|\n",
      "|     b|      3|      3|\n",
      "|     a|      1|      1|\n",
      "+------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame([(\"a\", 1), (\"d\", 1), (\"b\", 3), (\"c\", 4)],\n",
    "                            [\"id\", \"num1\"])\n",
    "df2 = spark.createDataFrame([(\"a\", 1), (\"b\", 3)], [\"id\", \"num2\"])\n",
    "df1.join(df2, df1.id == df2.id, 'left').select(df1.id.alias(\"df1_id\"),\n",
    "                                               df1.num1.alias(\"df1_num\"),\n",
    "                                               df2.num2.alias(\"df2_num\")\n",
    "                                               ).sort([\"df1_id\"], ascending=False)\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e6fef0",
   "metadata": {},
   "source": [
    "### 分组聚合"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ac313e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T08:31:30.903052Z",
     "start_time": "2023-11-06T08:31:30.900168Z"
    }
   },
   "source": [
    "整合后GroupedData类型可用的方法（均返回DataFrame类型）：\n",
    "- avg(\\*cols)     ——   计算每组中一列或多列的平均值\n",
    "- count()          ——   计算每组中一共有多少行，返回DataFrame有2列，一列为分组的组名，另一列为行总数\n",
    "- max(\\*cols)    ——   计算每组中一列或多列的最大值\n",
    "- mean(\\*cols)  ——  计算每组中一列或多列的平均值\n",
    "- min(\\*cols)     ——  计算每组中一列或多列的最小值\n",
    "- sum(\\*cols)    ——   计算每组中一列或多列的总和"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da050ec4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T08:31:30.895767Z",
     "start_time": "2023-11-06T08:31:16.448289Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+--------+------------+\n",
      "| sex|最小年龄|平均年龄|    姓名集合|\n",
      "+----+--------+--------+------------+\n",
      "|   M|      28|    41.5|[Sam, Peter]|\n",
      "|   F|      28|    41.0|[Flora, Mei]|\n",
      "|null|       1|     1.0|       [Run]|\n",
      "+----+--------+--------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 根据某几列进行聚合，如有多列用列表写在一起，如 df.groupBy([\"sex\", \"age\"])\n",
    "df.groupBy(\"sex\").agg(F.min(df.age).alias(\"最小年龄\"),\n",
    "                      F.expr(\"avg(age)\").alias(\"平均年龄\"),\n",
    "                      F.expr(\"collect_list(name)\").alias(\"姓名集合\")\n",
    "                      ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "350fcc80",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T08:32:29.443238Z",
     "start_time": "2023-11-06T08:32:01.671632Z"
    }
   },
   "outputs": [],
   "source": [
    "# 对每一行进行函数方法的应用\n",
    "def f(person):\n",
    "    print(person.name)\n",
    "df.foreach(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ebef2d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T08:32:45.988727Z",
     "start_time": "2023-11-06T08:32:32.131560Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+------+\n",
      "| name|age|score|   sex|\n",
      "+-----+---+-----+------+\n",
      "|  Sam| 28|   88|  Male|\n",
      "|Flora| 28|   90|Female|\n",
      "|  Run|  1|   60|  null|\n",
      "|Peter| 55|  100|  Male|\n",
      "|  Mei| 54|   95|Female|\n",
      "+-----+---+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 修改df里的某些值\n",
    "df1 = df.na.replace({\"M\": \"Male\", \"F\": \"Female\"})\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201e05e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#计算中位数\n",
    "df.approxQuantile(\"age\", [0.5], 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2b8b47",
   "metadata": {},
   "source": [
    "### union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d16f2fb9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T08:35:14.523624Z",
     "start_time": "2023-11-06T08:34:20.200235Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| id|num|\n",
      "+---+---+\n",
      "|  a|  1|\n",
      "|  d|  1|\n",
      "|  b|  3|\n",
      "|  c|  4|\n",
      "|  a|  1|\n",
      "|  b|  3|\n",
      "+---+---+\n",
      "\n",
      "+---+---+\n",
      "| id|num|\n",
      "+---+---+\n",
      "|  a|  1|\n",
      "|  d|  1|\n",
      "|  b|  3|\n",
      "|  c|  4|\n",
      "|  a|  1|\n",
      "|  b|  3|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 相当于SQL里的union all操作\n",
    "df1 = spark.createDataFrame(\n",
    "        [(\"a\", 1), (\"d\", 1), (\"b\",  3), (\"c\", 4)], [\"id\", \"num\"])\n",
    "df2 = spark.createDataFrame([(\"a\", 1), (\"b\", 3)], [\"id\", \"num\"])\n",
    "df1.union(df2).show()\n",
    "df1.unionAll(df2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "755d3e4d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T08:36:11.216577Z",
     "start_time": "2023-11-06T08:35:43.451461Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+\n",
      "|col0|col1|col2|\n",
      "+----+----+----+\n",
      "|   1|   2|   3|\n",
      "|   6|   4|   5|\n",
      "+----+----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 根据列名来进行合并数据集\n",
    "df1 = spark.createDataFrame([[1, 2, 3]], [\"col0\", \"col1\", \"col2\"])\n",
    "df2 = spark.createDataFrame([[4, 5, 6]], [\"col1\", \"col2\", \"col0\"])\n",
    "df1.unionByName(df2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d04e932",
   "metadata": {},
   "source": [
    "### 集合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "089437a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T08:39:14.891471Z",
     "start_time": "2023-11-06T08:38:47.492669Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+\n",
      "|label|sentence|\n",
      "+-----+--------+\n",
      "|    1|     asf|\n",
      "|    2|    2143|\n",
      "|    3|    rfds|\n",
      "+-----+--------+\n",
      "\n",
      "+-----+--------+\n",
      "|label|sentence|\n",
      "+-----+--------+\n",
      "|    1|     asf|\n",
      "|    2|    2143|\n",
      "|    4|  f8934y|\n",
      "+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentenceDataFrame = spark.createDataFrame(\n",
    "    ((1, \"asf\"), (2, \"2143\"), (3, \"rfds\"))).toDF(\"label\", \"sentence\")\n",
    "sentenceDataFrame.show()\n",
    "\n",
    "sentenceDataFrame1 = spark.createDataFrame(\n",
    "    ((1, \"asf\"), (2, \"2143\"), (4, \"f8934y\"))).toDF(\"label\", \"sentence\")\n",
    "sentenceDataFrame1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "486a6416",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T08:38:31.695533Z",
     "start_time": "2023-11-06T08:38:03.895383Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|sentence|\n",
      "+--------+\n",
      "|  f8934y|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 差集\n",
    "newDF = sentenceDataFrame1.select(\"sentence\").subtract(sentenceDataFrame.select(\"sentence\"))\n",
    "newDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e26990b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T08:39:51.585082Z",
     "start_time": "2023-11-06T08:39:23.475982Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|sentence|\n",
      "+--------+\n",
      "|     asf|\n",
      "|    2143|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 交集\n",
    "newDF = sentenceDataFrame1.select(\"sentence\").intersect(sentenceDataFrame.select(\"sentence\"))\n",
    "newDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "50302746",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T08:40:22.622395Z",
     "start_time": "2023-11-06T08:39:55.167921Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|sentence|\n",
      "+--------+\n",
      "|     asf|\n",
      "|    2143|\n",
      "|  f8934y|\n",
      "|     asf|\n",
      "|    2143|\n",
      "|    rfds|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 并集\n",
    "newDF = sentenceDataFrame1.select(\"sentence\").union(sentenceDataFrame.select(\"sentence\"))\n",
    "newDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d6c0b4e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T08:40:56.574051Z",
     "start_time": "2023-11-06T08:40:28.795023Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|sentence|\n",
      "+--------+\n",
      "|     asf|\n",
      "|    2143|\n",
      "|  f8934y|\n",
      "|    rfds|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 并集 + 去重\n",
    "newDF = sentenceDataFrame1.select(\"sentence\").union(sentenceDataFrame.select(\"sentence\")).distinct()\n",
    "newDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b994db",
   "metadata": {},
   "source": [
    "## 自定义函数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d892d414",
   "metadata": {},
   "source": [
    "- 在spark 1.x 版本的时候，利用pyspark包中的 udf() 来开发用户自定义函数，自定义的函数只能接收单一数值，因此当你向自定义函数中传递 dataframe的一个列时，自定义函数内部的处理方式就是执行了for循环，将传入列中的每个值逐一处理，这样效率比较低，在加上scala进程与python进程间数据传递、序列化与反序列化的时间消耗，用户自定义函数的执行效率比较低。**单点类型**\n",
    "- 在spark 2.x 版本中，以上的问题有了很大改善，新增了用户自定义函数的定义接口pandas_udf，并且对scala进程与python进程间的数据传递过程做了优化，数据从scala进程到python进程时，需要进行序列化，而当python进程执行完毕，将数据传递回scala进行时，则无需进行反序列化，这是因为使用了Apache Arrow 这种基于内存的列式数据存储格式，这样就节省了时间，当然需要将spark参数spark.sql.execution.arrow.enabled设置为true。总的来说，目前在实际开发中，用到的基本都是pandas_udf了。**pandas.Series类型**\n",
    "\n",
    "**用户自定义的python函数是会提交到python进程中执行的，因此函数中使用的应该是python和各个包(pyspark之外)的函数与数对象，而不是pyspark中的函数与数据对象。**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550097e6",
   "metadata": {},
   "source": [
    "### udf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4b53c2",
   "metadata": {},
   "source": [
    "#### 匿名函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bec12f4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T08:32:24.791960Z",
     "start_time": "2023-11-17T08:31:53.326584Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+----+-----+\n",
      "| name|age|score| sex|age+1|\n",
      "+-----+---+-----+----+-----+\n",
      "|  Sam| 28|   88|   M|   29|\n",
      "|Flora| 28|   90|   F|   29|\n",
      "|  Run|  1|   60|null|    2|\n",
      "|Peter| 55|  100|   M|   56|\n",
      "|  Mei| 54|   95|   F|   55|\n",
      "+-----+---+-----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "func = F.udf(lambda x: x+1)\n",
    "df1 = df.withColumn('age+1',func(df[\"age\"]))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47ce8c9d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T08:54:28.516359Z",
     "start_time": "2023-11-17T08:53:57.230095Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+----+-----+\n",
      "| name|age|score| sex|age+1|\n",
      "+-----+---+-----+----+-----+\n",
      "|  Sam| 28|   88|   M|   29|\n",
      "|Flora| 28|   90|   F|   29|\n",
      "|  Run|  1|   60|null|    2|\n",
      "|Peter| 55|  100|   M|   56|\n",
      "|  Mei| 54|   95|   F|   55|\n",
      "+-----+---+-----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "#Use udf to define a row-at-a-time udf\n",
    "@udf()\n",
    "# Input/output are both a single double value\n",
    "def plus_one(x):\n",
    "    return x + 1\n",
    "df1 = df.withColumn('age+1',plus_one(df.age))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65ce655",
   "metadata": {},
   "source": [
    "#### 普通函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1f5ba0d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T08:36:46.917980Z",
     "start_time": "2023-11-17T08:36:15.222872Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+----+------+\n",
      "| name|age|score| sex|degree|\n",
      "+-----+---+-----+----+------+\n",
      "|  Sam| 28|   88|   M|     B|\n",
      "|Flora| 28|   90|   F|     A|\n",
      "|  Run|  1|   60|null|     C|\n",
      "|Peter| 55|  100|   M|     A|\n",
      "|  Mei| 54|   95|   F|     A|\n",
      "+-----+---+-----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 定义一个函数来计算平方\n",
    "def score(x):\n",
    "    if x > 80 and x < 90:\n",
    "        return 'B'\n",
    "    elif x >= 90:\n",
    "        return 'A'\n",
    "    else:\n",
    "        return 'C'\n",
    "\n",
    "# 注册UDF\n",
    "score_udf = F.udf(score)\n",
    "\n",
    "# 使用UDF对DataFrame进行操作\n",
    "df1 = df.withColumn('degree', score_udf(df['score']))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39003517",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T08:55:36.882427Z",
     "start_time": "2023-11-17T08:55:05.381213Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+----+------+\n",
      "| name|age|score| sex|degree|\n",
      "+-----+---+-----+----+------+\n",
      "|  Sam| 28|   88|   M|     B|\n",
      "|Flora| 28|   90|   F|     A|\n",
      "|  Run|  1|   60|null|     C|\n",
      "|Peter| 55|  100|   M|     A|\n",
      "|  Mei| 54|   95|   F|     A|\n",
      "+-----+---+-----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@udf()\n",
    "# 定义一个函数来计算平方\n",
    "def score(x):\n",
    "    if x > 80 and x < 90:\n",
    "        return 'B'\n",
    "    elif x >= 90:\n",
    "        return 'A'\n",
    "    else:\n",
    "        return 'C'\n",
    "\n",
    "# 使用UDF对DataFrame进行操作\n",
    "df1 = df.withColumn('degree', score(df['score']))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e4b181",
   "metadata": {},
   "source": [
    "### pandas_udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03d57e50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T10:11:26.736298Z",
     "start_time": "2023-11-17T10:11:26.732130Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf, PandasUDFType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e783f4",
   "metadata": {},
   "source": [
    "pandas_udf函数支持定义三种类型的函数，分别为SCALAR、GROUP_MAP、GROUP_AGG，在pyspark.sql.function.PandasUDFType 中做了定义，默认SCALAR类型。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ef9d0b",
   "metadata": {},
   "source": [
    "#### SCALAR 类型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0f3851",
   "metadata": {},
   "source": [
    "SCALAR 类型自定义函数可以**接收多个列的输入，也只能接受列输入，但永远只会输出一个列**，并且返回的数据类型是pyspark.sql.types 中定义的数据类型。应用SCALAR类型自定义函数时，在自定义函数内部应当执行的一些逻辑不那么复杂的操作，如果不是这样，你可以使用其它的方式来满足需求。例如当你需要对每一条数据用不同的、比较复杂的逻辑来处理，那可以用 GROUP_MAP 类型来自定义函数，或者将dataframe转换为rdd，然后用mapValue函数来处理。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5161f6af",
   "metadata": {},
   "source": [
    "该类型支持用户自定义向量型操作的函数，也就是pandas、numpy中的向量操作，传入的格式为pd.Series，传出的格式为pd.Series。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a8ad63",
   "metadata": {},
   "source": [
    "**简单应用**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9c1501",
   "metadata": {},
   "source": [
    "使用scipy去计算一个值的累计正太分布概率值，stats.norm.cdf 可以使用在一个标量或者是一个pandas.Series上面。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "18d10c19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T09:36:22.712002Z",
     "start_time": "2023-11-17T09:35:41.776993Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+----+----------------------+\n",
      "| name|age|score| sex|cumulative_probability|\n",
      "+-----+---+-----+----+----------------------+\n",
      "|  Sam| 28|   88|   M|                   1.0|\n",
      "|Flora| 28|   90|   F|                   1.0|\n",
      "|  Run|  1|   60|null|    0.8413447460685429|\n",
      "|Peter| 55|  100|   M|                   1.0|\n",
      "|  Mei| 54|   95|   F|                   1.0|\n",
      "+-----+---+-----+----+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "@pandas_udf(returnType='double')\n",
    "def cdf(v):\n",
    "    return pd.Series(stats.norm.cdf(v))\n",
    "\n",
    "df1 = df.withColumn('cumulative_probability',cdf(df.age))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3a3425",
   "metadata": {},
   "source": [
    "**复杂应用**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de207836",
   "metadata": {},
   "source": [
    "线性插值，不使用pyspark的版本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7262b795",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T09:42:01.224925Z",
     "start_time": "2023-11-17T09:41:07.498797Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+-----+\n",
      "| id|name| age|age_1|\n",
      "+---+----+----+-----+\n",
      "|  1|  55|  23| 23.0|\n",
      "|  2|  66|null| 23.0|\n",
      "|  3|  60|  34| 34.0|\n",
      "|  4|3000|  35| 35.0|\n",
      "+---+----+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import np\n",
    "from pyspark.ml.linalg import scipy\n",
    "df = spark.createDataFrame([(1, 55, 23), (2, 66, None), (3, 60, 34),\n",
    "                            (4, 3000, 35)], ['id', 'name', 'age'])\n",
    "col = 'age'\n",
    "#转为Pandas\n",
    "dfs = df.toPandas()\n",
    "\n",
    "@pandas_udf('double')\n",
    "#特定列的线性插值\n",
    "def ir(x_in):\n",
    "    filter_data = dfs[col].values\n",
    "    filter_data1 = dfs.drop(col, axis=1)\n",
    "    #定义纵坐标，删除缺失值\n",
    "    y = filter_data[np.where(np.isnan(filter_data) != 1)]\n",
    "    #定义横坐标，没有缺失值的索引\n",
    "    x0 = np.array(range(len(filter_data)))\n",
    "    x = x0[np.where(np.isnan(filter_data) != 1)]\n",
    "    #构造拟合函数\n",
    "    irf = scipy.interpolate.interp1d(x,\n",
    "                                     y,\n",
    "                                     kind='linear',\n",
    "                                     fill_value=\"extrapolate\")\n",
    "    #获取缺失值的索引，方便进行插值\n",
    "    x_new = np.where(np.isnan(x_in) == 1)\n",
    "    #拟合缺失数据\n",
    "    y_new = irf(x_new)\n",
    "    x1 = x_in.copy()\n",
    "    x1[x_new] = y_new\n",
    "    return x1\n",
    "\n",
    "df1 = df.withColumn(\"age_1\", ir(df[col]))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe39e71b",
   "metadata": {},
   "source": [
    "#### GROUP_MAP 类型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5756b72",
   "metadata": {},
   "source": [
    "该类型支持用户将dataframe按照一组字段分组，然后对分组后的多个dataframe分别用户自定义函数处理，在用户自定义函数中，处理的对象是pandas 的DataFrame。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d7cc252",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T10:31:41.178775Z",
     "start_time": "2023-11-17T10:31:23.056304Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+----+\n",
      "| name|age|score| sex|\n",
      "+-----+---+-----+----+\n",
      "|  Run|  1|   63|null|\n",
      "|  Sam| 28|   91|   M|\n",
      "|Flora| 28|   93|   F|\n",
      "|  Mei| 54|   98|   F|\n",
      "|Peter| 55|  103|   M|\n",
      "+-----+---+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#先按年龄分组，最后按成绩+3排序输出\n",
    "def func(df):\n",
    "    df['score'] = df['score'] + 3\n",
    "    df.sort_values(by=['score'],inplace=True, ignore_index=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "func_udf = pandas_udf(func, df.schema, PandasUDFType.GROUPED_MAP)\n",
    "df1 = df.groupby('age').apply(func_udf)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a36cbb2",
   "metadata": {},
   "source": [
    "#### GROUP_AGG 类型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e629f611",
   "metadata": {},
   "source": [
    "这种类型用得不多，因为可以直接被`dataframe.groupby().agg(sf.min())` 这样的函数取代。它的功能就是分组计算统计值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43aad40f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-17T10:18:38.907397Z",
     "start_time": "2023-11-17T10:18:20.344977Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\Lib\\site-packages\\pyspark\\sql\\pandas\\functions.py:399: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+\n",
      "|age|func(score)|\n",
      "+---+-----------+\n",
      "|  1|         60|\n",
      "| 28|         89|\n",
      "| 54|         95|\n",
      "| 55|        100|\n",
      "+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType,IntegerType,StructType,StructField\n",
    "def func(x):\n",
    "    return x.mean()\n",
    "\n",
    "func_udf = pandas_udf(func, IntegerType(), PandasUDFType.GROUPED_AGG)\n",
    "df1 = df.groupby('age').agg(func_udf(df['score']))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1dd3c20",
   "metadata": {},
   "source": [
    "## 格式转换"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74900031",
   "metadata": {},
   "source": [
    "### 转pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "33866c6f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T09:06:41.850013Z",
     "start_time": "2023-11-06T09:06:27.733240Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>age</th>\n",
       "      <th>score</th>\n",
       "      <th>sex</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sam</td>\n",
       "      <td>28</td>\n",
       "      <td>88</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Flora</td>\n",
       "      <td>28</td>\n",
       "      <td>90</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Run</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Peter</td>\n",
       "      <td>55</td>\n",
       "      <td>100</td>\n",
       "      <td>M</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mei</td>\n",
       "      <td>54</td>\n",
       "      <td>95</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    name  age  score   sex\n",
       "0    Sam   28     88     M\n",
       "1  Flora   28     90     F\n",
       "2    Run    1     60  None\n",
       "3  Peter   55    100     M\n",
       "4    Mei   54     95     F"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a825486b",
   "metadata": {},
   "source": [
    "转化为pandas，但是该数据要读入内存，如果数据量大的话，很难跑得动\n",
    "\n",
    "两者的异同：\n",
    "\n",
    "- Pyspark DataFrame是在分布式节点上运行一些数据操作，而pandas是不可能的；\n",
    "- Pyspark DataFrame的数据反映比较缓慢，没有Pandas那么及时反映；\n",
    "- Pyspark DataFrame的数据框是不可变的，不能任意添加列，只能通过合并进行；\n",
    "- pandas比Pyspark DataFrame有更多方便的操作以及很强大"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f4c9cd",
   "metadata": {},
   "source": [
    "### 转pyspark.pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fc14f74a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T09:11:39.794837Z",
     "start_time": "2023-11-06T09:11:11.949328Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\Lib\\site-packages\\pyspark\\pandas\\utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `to_spark`, the existing index is lost when converting to Spark DataFrame.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+----+\n",
      "| name|age|score| sex|\n",
      "+-----+---+-----+----+\n",
      "|  Sam| 28|   88|   M|\n",
      "|Flora| 28|   90|   F|\n",
      "|  Run|  1|   60|null|\n",
      "|Peter| 55|  100|   M|\n",
      "|  Mei| 54|   95|   F|\n",
      "+-----+---+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.pandas as ps\n",
    "df1 = df.toPandas()\n",
    "df = ps.DataFrame(df1).to_spark()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8234a8",
   "metadata": {},
   "source": [
    "### 转rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3e867df6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-06T09:14:56.784578Z",
     "start_time": "2023-11-06T09:14:02.230429Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-----+----+\n",
      "| name|age|score| sex|\n",
      "+-----+---+-----+----+\n",
      "|  Sam| 28|   88|   M|\n",
      "|Flora| 28|   90|   F|\n",
      "|  Run|  1|   60|null|\n",
      "|Peter| 55|  100|   M|\n",
      "|  Mei| 54|   95|   F|\n",
      "+-----+---+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rdd_df = df.rdd\n",
    "df = rdd_df.toDF()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d5deb2",
   "metadata": {},
   "source": [
    "# SparkDataFrame时间处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "791c7e4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T09:56:45.423891Z",
     "start_time": "2023-11-09T09:56:31.430964Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+\n",
      "| id|      time|\n",
      "+---+----------+\n",
      "|  1|2020-02-03|\n",
      "|  2|2019-03-05|\n",
      "|  3|2021-03-09|\n",
      "+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data=[[\"1\",\"2020-02-03\"],[\"2\",\"2019-03-05\"],[\"3\",\"2021-03-09\"]]\n",
    "df=spark.createDataFrame(data, [\"id\",\"time\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4098b340",
   "metadata": {},
   "source": [
    "## 日期"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9448f9d9",
   "metadata": {},
   "source": [
    "### 当前日期"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fc89db",
   "metadata": {},
   "source": [
    "获取当前系统日期。默认情况下，数据将以yyyy-dd-mm格式返回。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e4ac728",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T09:48:48.046902Z",
     "start_time": "2023-11-09T09:48:33.683758Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|current_date|\n",
      "+------------+\n",
      "|  2023-11-09|\n",
      "+------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(F.current_date().alias(\"current_date\")).show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d527b9d9",
   "metadata": {},
   "source": [
    "### 日期格式"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3b487e",
   "metadata": {},
   "source": [
    "解析日期并转换yyyy-dd-mm为MM-dd-yyyy格式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8adfde6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T09:49:52.738819Z",
     "start_time": "2023-11-09T09:49:38.569633Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|      time|date_format|\n",
      "+----------+-----------+\n",
      "|2020-02-01| 02-01-2020|\n",
      "|2019-03-01| 03-01-2019|\n",
      "|2021-03-01| 03-01-2021|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(F.col(\"time\"), F.date_format(F.col(\"time\"), \"MM-dd-yyyy\").alias(\"date_format\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e46f05",
   "metadata": {},
   "source": [
    "### 字符串转日期格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bee8a817",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T09:52:34.121823Z",
     "start_time": "2023-11-09T09:52:20.067577Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|      time|   to_date|\n",
      "+----------+----------+\n",
      "|2020-02-01|2020-02-01|\n",
      "|2019-03-01|2019-03-01|\n",
      "|2021-03-01|2021-03-01|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(F.col(\"time\"), F.to_date(F.col(\"time\"), \"yyyy-MM-dd\").alias(\"to_date\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91738968",
   "metadata": {},
   "source": [
    "### 天数之差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d100dc52",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T09:53:40.906330Z",
     "start_time": "2023-11-09T09:53:26.495630Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+\n",
      "|      time|datediff|\n",
      "+----------+--------+\n",
      "|2020-02-01|    1377|\n",
      "|2019-03-01|    1714|\n",
      "|2021-03-01|     983|\n",
      "+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(F.col(\"time\"), F.datediff(F.current_date(), F.col(\"time\")).alias(\"datediff\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b544a9e5",
   "metadata": {},
   "source": [
    "### 月数只差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5f88f5f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T09:54:37.484476Z",
     "start_time": "2023-11-09T09:54:23.306809Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+\n",
      "|      time|months_between|\n",
      "+----------+--------------+\n",
      "|2020-02-01|   45.25806452|\n",
      "|2019-03-01|   56.25806452|\n",
      "|2021-03-01|   32.25806452|\n",
      "+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(F.col(\"time\"), F.months_between(F.current_date(),F.col(\"time\")).alias(\"months_between\")  ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2501365",
   "metadata": {},
   "source": [
    "### 截断指定单位的日期"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8bb50d9b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T09:57:17.745801Z",
     "start_time": "2023-11-09T09:57:03.538262Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+----------+-----------+\n",
      "|      time|Month_Trunc|Month_Year|Month_Trunc|\n",
      "+----------+-----------+----------+-----------+\n",
      "|2020-02-03| 2020-02-01|2020-01-01| 2020-02-01|\n",
      "|2019-03-05| 2019-03-01|2019-01-01| 2019-03-01|\n",
      "|2021-03-09| 2021-03-01|2021-01-01| 2021-03-01|\n",
      "+----------+-----------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(F.col(\"time\"),\n",
    "          F.trunc(F.col(\"time\"), \"Month\").alias(\"Month_Trunc\"),\n",
    "          F.trunc(F.col(\"time\"), \"Year\").alias(\"Month_Year\"),\n",
    "          F.trunc(F.col(\"time\"), \"Month\").alias(\"Month_Trunc\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0af3b4",
   "metadata": {},
   "source": [
    "### 月、日加减法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc6af4f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T09:57:38.787985Z",
     "start_time": "2023-11-09T09:57:24.394510Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+----------+----------+\n",
      "|      time|add_months|sub_months|  date_add|  date_sub|\n",
      "+----------+----------+----------+----------+----------+\n",
      "|2020-02-03|2020-05-03|2019-11-03|2020-02-07|2020-01-30|\n",
      "|2019-03-05|2019-06-05|2018-12-05|2019-03-09|2019-03-01|\n",
      "|2021-03-09|2021-06-09|2020-12-09|2021-03-13|2021-03-05|\n",
      "+----------+----------+----------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(F.col(\"time\"),\n",
    "          F.add_months(F.col(\"time\"), 3).alias(\"add_months\"),\n",
    "          F.add_months(F.col(\"time\"), -3).alias(\"sub_months\"),\n",
    "          F.date_add(F.col(\"time\"), 4).alias(\"date_add\"),\n",
    "          F.date_sub(F.col(\"time\"), 4).alias(\"date_sub\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d7850c",
   "metadata": {},
   "source": [
    "### 年、月、下一天、一年中第几个星期"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "24a94d77",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T09:58:11.518763Z",
     "start_time": "2023-11-09T09:57:57.387804Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+-----+----------+----------+\n",
      "|      time|year|month|  next_day|weekofyear|\n",
      "+----------+----+-----+----------+----------+\n",
      "|2020-02-03|2020|    2|2020-02-09|         6|\n",
      "|2019-03-05|2019|    3|2019-03-10|        10|\n",
      "|2021-03-09|2021|    3|2021-03-14|        10|\n",
      "+----------+----+-----+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(F.col(\"time\"),\n",
    "          F.year(F.col(\"time\")).alias(\"year\"),\n",
    "          F.month(F.col(\"time\")).alias(\"month\"),\n",
    "          F.next_day(F.col(\"time\"), \"Sunday\").alias(\"next_day\"),\n",
    "          F.weekofyear(F.col(\"time\")).alias(\"weekofyear\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f65e892",
   "metadata": {},
   "source": [
    "### 星期几、月日、年日"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97a7055",
   "metadata": {},
   "source": [
    "- 查询星期几\n",
    "- 一个月中的第几天\n",
    "- 一年中的第几天"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f583b68b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T09:59:21.729499Z",
     "start_time": "2023-11-09T09:59:07.611194Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+----------+---------+\n",
      "|      time|dayofweek|dayofmonth|dayofyear|\n",
      "+----------+---------+----------+---------+\n",
      "|2020-02-03|        2|         3|       34|\n",
      "|2019-03-05|        3|         5|       64|\n",
      "|2021-03-09|        3|         9|       68|\n",
      "+----------+---------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\n",
    "    F.col(\"time\"),\n",
    "    F.dayofweek(F.col(\"time\")).alias(\"dayofweek\"),\n",
    "    F.dayofmonth(F.col(\"time\")).alias(\"dayofmonth\"),\n",
    "    F.dayofyear(F.col(\"time\")).alias(\"dayofyear\"),\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fd5c07",
   "metadata": {},
   "source": [
    "## 时间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0f7c160f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T10:00:12.894567Z",
     "start_time": "2023-11-09T09:59:58.618813Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------------+\n",
      "|id |time                   |\n",
      "+---+-----------------------+\n",
      "|1  |02-01-2020 11 01 19 06 |\n",
      "|2  |03-01-2019 12 01 19 406|\n",
      "|3  |03-01-2021 12 01 19 406|\n",
      "+---+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [[\"1\", \"02-01-2020 11 01 19 06\"], [\"2\", \"03-01-2019 12 01 19 406\"],\n",
    "        [\"3\", \"03-01-2021 12 01 19 406\"]]\n",
    "df2 = spark.createDataFrame(data, [\"id\", \"time\"])\n",
    "df2.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b18f3f",
   "metadata": {},
   "source": [
    "### 返回当前时间戳"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "228de2f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T10:00:48.961751Z",
     "start_time": "2023-11-09T10:00:34.599830Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|   current_timestamp|\n",
      "+--------------------+\n",
      "|2023-11-09 18:00:...|\n",
      "|2023-11-09 18:00:...|\n",
      "|2023-11-09 18:00:...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#默认格式yyyy-MM-dd HH:mm:ss\n",
    "df2.select(F.current_timestamp().alias(\"current_timestamp\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fb974d",
   "metadata": {},
   "source": [
    "### 字符串转为时间戳"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "09b99d06",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T10:02:05.539309Z",
     "start_time": "2023-11-09T10:01:51.244951Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-----------------------+\n",
      "|time                   |to_timestamp           |\n",
      "+-----------------------+-----------------------+\n",
      "|02-01-2020 11 01 19 06 |2020-02-01 11:01:19.06 |\n",
      "|03-01-2019 12 01 19 406|2019-03-01 12:01:19.406|\n",
      "|03-01-2021 12 01 19 406|2021-03-01 12:01:19.406|\n",
      "+-----------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select(\n",
    "    F.col(\"time\"),\n",
    "    F.to_timestamp(F.col(\"time\"),\"MM-dd-yyyy HH mm ss SSS\").alias(\"to_timestamp\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bf2167",
   "metadata": {},
   "source": [
    "### 获取小时、分钟、秒"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c5ac1c35",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-09T10:03:19.900934Z",
     "start_time": "2023-11-09T10:03:05.467779Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+----+------+------+\n",
      "|time                   |hour|minute|second|\n",
      "+-----------------------+----+------+------+\n",
      "|2020-02-01 11:01:19.06 |11  |1     |19    |\n",
      "|2019-03-01 12:01:19.406|12  |1     |19    |\n",
      "|2021-03-01 12:01:19.406|12  |1     |19    |\n",
      "+-----------------------+----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 数据\n",
    "data = [[\"1\", \"2020-02-01 11:01:19.06\"], [\"2\", \"2019-03-01 12:01:19.406\"],\n",
    "        [\"3\", \"2021-03-01 12:01:19.406\"]]\n",
    "df3 = spark.createDataFrame(data, [\"id\", \"time\"])\n",
    "\n",
    "# 提取小时、分钟、秒\n",
    "df3.select(F.col(\"time\"),\n",
    "           F.hour(F.col(\"time\")).alias(\"hour\"),\n",
    "           F.minute(F.col(\"time\")).alias(\"minute\"),\n",
    "           F.second(F.col(\"time\")).alias(\"second\")).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f149e4",
   "metadata": {},
   "source": [
    "## 滑动窗口"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0722cc8d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T10:02:19.670129Z",
     "start_time": "2023-11-24T10:02:19.665914Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3846591d",
   "metadata": {},
   "source": [
    "Window函数分类为三种:\n",
    "- 排名函数 ranking functions包括:\n",
    "  - row_number()\n",
    "  - rank()\n",
    "  - dense_rank()\n",
    "  - percent_rank()\n",
    "  - ntile()\n",
    "  - orderBy()\n",
    "- 解析函数 analytic functions包括:\n",
    "  - cume_dist()\n",
    "  - lag()\n",
    "  - lead()\n",
    "- 聚合函数 aggregate functions包括:\n",
    "  - sum()\n",
    "  - first()\n",
    "  - last()\n",
    "  - max()\n",
    "  - min()\n",
    "  - mean()\n",
    "  - stddev()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9840e9",
   "metadata": {},
   "source": [
    "### 创建一个 PySpark DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84bd638a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T10:03:45.381732Z",
     "start_time": "2023-11-24T10:03:24.215715Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+\n",
      "|name   |department|salary|\n",
      "+-------+----------+------+\n",
      "|Ali    |Sales     |8000  |\n",
      "|Bob    |Sales     |7000  |\n",
      "|Cindy  |Sales     |7500  |\n",
      "|Davd   |Finance   |10000 |\n",
      "|Elena  |Sales     |8000  |\n",
      "|Fancy  |Finance   |12000 |\n",
      "|George |Finance   |11000 |\n",
      "|Haffman|Marketing |7000  |\n",
      "|Ilaja  |Marketing |8000  |\n",
      "|Joey   |Sales     |9000  |\n",
      "+-------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_salary = [\n",
    "    (\"Ali\", \"Sales\", 8000),\n",
    "    (\"Bob\", \"Sales\", 7000),\n",
    "    (\"Cindy\", \"Sales\", 7500),\n",
    "    (\"Davd\", \"Finance\", 10000),\n",
    "    (\"Elena\", \"Sales\", 8000),\n",
    "    (\"Fancy\", \"Finance\", 12000),\n",
    "    (\"George\", \"Finance\", 11000),\n",
    "    (\"Haffman\", \"Marketing\", 7000),\n",
    "    (\"Ilaja\", \"Marketing\", 8000),\n",
    "    (\"Joey\", \"Sales\", 9000)]\n",
    " \n",
    "columns= [\"name\", \"department\", \"salary\"]\n",
    "df = spark.createDataFrame(data = employee_salary, schema = columns)\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee83e98",
   "metadata": {},
   "source": [
    "### 窗口函数 ranking functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db9a6ac",
   "metadata": {},
   "source": [
    "#### row_number()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3370f50",
   "metadata": {},
   "source": [
    "row_number() 窗口函数用于给出从1开始到每个窗口分区的结果的连续行号。 与 groupBy 不同 Window 以 partitionBy 作为分组条件，orderBy 对 Window 分组内的数据进行排序。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1db9f56",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T10:07:02.969539Z",
     "start_time": "2023-11-24T10:06:46.496623Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+----------+\n",
      "|   name|department|salary|row_number|\n",
      "+-------+----------+------+----------+\n",
      "|   Davd|   Finance| 10000|         1|\n",
      "| George|   Finance| 11000|         2|\n",
      "|  Fancy|   Finance| 12000|         3|\n",
      "|Haffman| Marketing|  7000|         1|\n",
      "|  Ilaja| Marketing|  8000|         2|\n",
      "|    Bob|     Sales|  7000|         1|\n",
      "|  Cindy|     Sales|  7500|         2|\n",
      "|    Ali|     Sales|  8000|         3|\n",
      "|  Elena|     Sales|  8000|         4|\n",
      "|   Joey|     Sales|  9000|         5|\n",
      "+-------+----------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 以 department 字段进行分组，以 salary 倒序排序\n",
    "# 按照部门对薪水排名，薪水最低的为第一名\n",
    "windowSpec = Window.partitionBy(\"department\").orderBy(F.asc(\"salary\"))\n",
    "# 分组内增加 row_number\n",
    "df_part = df.withColumn(\n",
    "    \"row_number\", \n",
    "    F.row_number().over(windowSpec)\n",
    ")\n",
    "df_part.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a88ca2e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T10:07:19.468051Z",
     "start_time": "2023-11-24T10:07:02.972042Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+------+----------+\n",
      "|  name|department|salary|row_number|\n",
      "+------+----------+------+----------+\n",
      "|George|   Finance| 11000|         2|\n",
      "| Ilaja| Marketing|  8000|         2|\n",
      "| Cindy|     Sales|  7500|         2|\n",
      "+------+----------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_part.where(F.col('row_number') == 2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6309b2",
   "metadata": {},
   "source": [
    "#### rank()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd4f7b2",
   "metadata": {},
   "source": [
    "rank()用来给按照指定列排序的分组窗增加一个排序的序号，\n",
    "\n",
    "如果有相同数值，则排序数相同，下一个序数顺延一位。来看如下代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ec0bd1f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T10:07:36.401426Z",
     "start_time": "2023-11-24T10:07:19.469797Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+----+\n",
      "|   name|department|salary|rank|\n",
      "+-------+----------+------+----+\n",
      "|  Fancy|   Finance| 12000|   1|\n",
      "| George|   Finance| 11000|   2|\n",
      "|   Davd|   Finance| 10000|   3|\n",
      "|  Ilaja| Marketing|  8000|   1|\n",
      "|Haffman| Marketing|  7000|   2|\n",
      "|   Joey|     Sales|  9000|   1|\n",
      "|    Ali|     Sales|  8000|   2|\n",
      "|  Elena|     Sales|  8000|   2|\n",
      "|  Cindy|     Sales|  7500|   4|\n",
      "|    Bob|     Sales|  7000|   5|\n",
      "+-------+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 使用 rank 排序，都是8000的薪水，就同列第二\n",
    "windowSpec = Window.partitionBy(\"department\").orderBy(F.desc(\"salary\"))\n",
    "df_rank = df.withColumn(\"rank\", F.rank().over(windowSpec))\n",
    "df_rank.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5b4e504",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T10:07:53.314386Z",
     "start_time": "2023-11-24T10:07:36.405177Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+----+\n",
      "|   name|department|salary|rank|\n",
      "+-------+----------+------+----+\n",
      "| George|   Finance| 11000|   2|\n",
      "|Haffman| Marketing|  7000|   2|\n",
      "|    Ali|     Sales|  8000|   2|\n",
      "|  Elena|     Sales|  8000|   2|\n",
      "+-------+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_rank.where(F.col(\"rank\")==\"2\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0fe188",
   "metadata": {},
   "source": [
    "#### dense_rank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d8fd6fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T10:08:09.937061Z",
     "start_time": "2023-11-24T10:07:53.314386Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+----------+\n",
      "|   name|department|salary|dense_rank|\n",
      "+-------+----------+------+----------+\n",
      "|  Fancy|   Finance| 12000|         1|\n",
      "| George|   Finance| 11000|         2|\n",
      "|   Davd|   Finance| 10000|         3|\n",
      "|  Ilaja| Marketing|  8000|         1|\n",
      "|Haffman| Marketing|  7000|         2|\n",
      "|   Joey|     Sales|  9000|         1|\n",
      "|    Ali|     Sales|  8000|         2|\n",
      "|  Elena|     Sales|  8000|         2|\n",
      "|  Cindy|     Sales|  7500|         3|\n",
      "|    Bob|     Sales|  7000|         4|\n",
      "+-------+----------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 注意 rank 排序，8000虽然为同列第二，但7500属于第4名\n",
    "# dense_rank()中， 8000同列第二后，7500属于第3名\n",
    "windowSpec  = Window.partitionBy(\"department\").orderBy(F.desc(\"salary\"))\n",
    "df.withColumn(\"dense_rank\", F.dense_rank().over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1707218a",
   "metadata": {},
   "source": [
    "#### percent_rank()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da269f2",
   "metadata": {},
   "source": [
    "计算不同数值的百分比排序数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c1962c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T10:08:26.751393Z",
     "start_time": "2023-11-24T10:08:09.938822Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+------------+\n",
      "|   name|department|salary|percent_rank|\n",
      "+-------+----------+------+------------+\n",
      "|  Fancy|   Finance| 12000|         0.0|\n",
      "| George|   Finance| 11000|         0.5|\n",
      "|   Davd|   Finance| 10000|         1.0|\n",
      "|  Ilaja| Marketing|  8000|         0.0|\n",
      "|Haffman| Marketing|  7000|         1.0|\n",
      "|   Joey|     Sales|  9000|         0.0|\n",
      "|    Ali|     Sales|  8000|        0.25|\n",
      "|  Elena|     Sales|  8000|        0.25|\n",
      "|  Cindy|     Sales|  7500|        0.75|\n",
      "|    Bob|     Sales|  7000|         1.0|\n",
      "+-------+----------+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "windowSpec  = Window.partitionBy(\"department\").orderBy(F.desc(\"salary\"))\n",
    "df.withColumn(\"percent_rank\",F.percent_rank().over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4336fbfa",
   "metadata": {},
   "source": [
    "上述结果可以理解为将 dense_rank() 的结果进行归一化， 即可得到0-1以内的百分数。percent_rank() 与 SQL 中的 PERCENT_RANK 函数效果一致。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0beed718",
   "metadata": {},
   "source": [
    "#### ntile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43c3f65",
   "metadata": {},
   "source": [
    "ntile()可将分组的数据按照指定数值n切分为n个部分， 每一部分按照行的先后给定相同的序数。例如n指定为2，则将组内数据分为两个部分， 第一部分序号为1，第二部分序号为2。理论上两部分数据行数是均等的， 但当数据为奇数行时，中间的那一行归到前一部分。\n",
    "\n",
    "按照部门对数据进行分组，然后在组内按照薪水高低进行排序， 再使用 ntile() 将组内数据切分为两个部分。结果如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "70bd6fe9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T10:08:55.263499Z",
     "start_time": "2023-11-24T10:08:38.247734Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+-----+\n",
      "|   name|department|salary|ntile|\n",
      "+-------+----------+------+-----+\n",
      "|  Fancy|   Finance| 12000|    1|\n",
      "| George|   Finance| 11000|    1|\n",
      "|   Davd|   Finance| 10000|    2|\n",
      "|  Ilaja| Marketing|  8000|    1|\n",
      "|Haffman| Marketing|  7000|    2|\n",
      "|   Joey|     Sales|  9000|    1|\n",
      "|    Ali|     Sales|  8000|    1|\n",
      "|  Elena|     Sales|  8000|    1|\n",
      "|  Cindy|     Sales|  7500|    2|\n",
      "|    Bob|     Sales|  7000|    2|\n",
      "+-------+----------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 按照部门对数据进行分组，然后在组内按照薪水高低进行排序 \n",
    "windowSpec = Window.partitionBy(\n",
    "    \"department\").orderBy(F.desc(\"salary\"))\n",
    "# 使用ntile() 将组内数据切分为两个部分\n",
    "df.withColumn(\"ntile\", F.ntile(2).over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b164342b",
   "metadata": {},
   "source": [
    "###  分析函数 Analytic functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edcf0d5",
   "metadata": {},
   "source": [
    "#### cume_dist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e06de27",
   "metadata": {},
   "source": [
    "cume_dist()函数用来获取数值的累进分布值，看如下例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "532e084c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T10:09:54.593116Z",
     "start_time": "2023-11-24T10:09:37.601198Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+------------------+\n",
      "|   name|department|salary|         cume_dist|\n",
      "+-------+----------+------+------------------+\n",
      "|  Fancy|   Finance| 12000|0.3333333333333333|\n",
      "| George|   Finance| 11000|0.6666666666666666|\n",
      "|   Davd|   Finance| 10000|               1.0|\n",
      "|  Ilaja| Marketing|  8000|               0.5|\n",
      "|Haffman| Marketing|  7000|               1.0|\n",
      "|   Joey|     Sales|  9000|               0.2|\n",
      "|    Ali|     Sales|  8000|               0.6|\n",
      "|  Elena|     Sales|  8000|               0.6|\n",
      "|  Cindy|     Sales|  7500|               0.8|\n",
      "|    Bob|     Sales|  7000|               1.0|\n",
      "+-------+----------+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "windowSpec = Window.partitionBy(\"department\").orderBy(F.desc(\"salary\"))\n",
    "df.withColumn(\"cume_dist\", F.cume_dist().over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "deefa82d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T10:10:11.383010Z",
     "start_time": "2023-11-24T10:09:54.593116Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+------------+\n",
      "|   name|department|salary|percent_rank|\n",
      "+-------+----------+------+------------+\n",
      "|  Fancy|   Finance| 12000|         0.0|\n",
      "| George|   Finance| 11000|         0.5|\n",
      "|   Davd|   Finance| 10000|         1.0|\n",
      "|  Ilaja| Marketing|  8000|         0.0|\n",
      "|Haffman| Marketing|  7000|         1.0|\n",
      "|   Joey|     Sales|  9000|         0.0|\n",
      "|    Ali|     Sales|  8000|        0.25|\n",
      "|  Elena|     Sales|  8000|        0.25|\n",
      "|  Cindy|     Sales|  7500|        0.75|\n",
      "|    Bob|     Sales|  7000|         1.0|\n",
      "+-------+----------+------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 和 percent_rank 对比一下\n",
    "df.withColumn(\n",
    "    'percent_rank',\n",
    "    F.percent_rank().over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaacff01",
   "metadata": {},
   "source": [
    "#### lag()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc92009",
   "metadata": {},
   "source": [
    "lag() 函数用于寻找按照指定列排好序的分组内每个数值的上一个数值，\n",
    "\n",
    "通俗的说，就是数值排好序以后，寻找排在每个数值的上一个数值。代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "52e0e92f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T10:10:44.621869Z",
     "start_time": "2023-11-24T10:10:28.051599Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+-----+\n",
      "|   name|department|salary|  lag|\n",
      "+-------+----------+------+-----+\n",
      "|  Fancy|   Finance| 12000| null|\n",
      "| George|   Finance| 11000|12000|\n",
      "|   Davd|   Finance| 10000|11000|\n",
      "|  Ilaja| Marketing|  8000| null|\n",
      "|Haffman| Marketing|  7000| 8000|\n",
      "|   Joey|     Sales|  9000| null|\n",
      "|    Ali|     Sales|  8000| 9000|\n",
      "|  Elena|     Sales|  8000| 8000|\n",
      "|  Cindy|     Sales|  7500| 8000|\n",
      "|    Bob|     Sales|  7000| 7500|\n",
      "+-------+----------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 相当于滞后项\n",
    "windowSpec  = Window.partitionBy(\"department\").orderBy(F.desc(\"salary\"))\n",
    "df.withColumn(\"lag\", F.lag(\"salary\",1).over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0f70ae",
   "metadata": {},
   "source": [
    "#### lead()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7a0b62",
   "metadata": {},
   "source": [
    "lead() 用于获取排序后的数值的下一个，代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa87ac80",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T10:11:28.859972Z",
     "start_time": "2023-11-24T10:11:12.096982Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+-----+\n",
      "|   name|department|salary| lead|\n",
      "+-------+----------+------+-----+\n",
      "|  Fancy|   Finance| 12000|11000|\n",
      "| George|   Finance| 11000|10000|\n",
      "|   Davd|   Finance| 10000| null|\n",
      "|  Ilaja| Marketing|  8000| 7000|\n",
      "|Haffman| Marketing|  7000| null|\n",
      "|   Joey|     Sales|  9000| 8000|\n",
      "|    Ali|     Sales|  8000| 8000|\n",
      "|  Elena|     Sales|  8000| 7500|\n",
      "|  Cindy|     Sales|  7500| 7000|\n",
      "|    Bob|     Sales|  7000| null|\n",
      "+-------+----------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 和滞后项相反，提前一位\n",
    "windowSpec  = Window.partitionBy(\"department\").orderBy(F.desc(\"salary\"))\n",
    "df.withColumn(\"lead\",F.lead(\"salary\",1).over(windowSpec)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d7f338",
   "metadata": {},
   "source": [
    "### 聚合函数 Aggregate Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8809407d",
   "metadata": {},
   "source": [
    "常见的聚合函数有avg, sum, min, max, count, approx_count_distinct()等，我们用如下代码来同时使用这些函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e2e97cf6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T10:12:12.091181Z",
     "start_time": "2023-11-24T10:11:55.153847Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\Lib\\site-packages\\pyspark\\sql\\functions.py:2610: FutureWarning: Deprecated in 2.1, use approx_count_distinct instead.\n",
      "  warnings.warn(\"Deprecated in 2.1, use approx_count_distinct instead.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+------+---+-------+-----+-----+-----+-----+--------------+\n",
      "|   name|department|salary|row|    avg|  sum|  min|  max|count|distinct_count|\n",
      "+-------+----------+------+---+-------+-----+-----+-----+-----+--------------+\n",
      "|  Fancy|   Finance| 12000|  1|11000.0|33000|10000|12000|    3|             3|\n",
      "| George|   Finance| 11000|  2|11000.0|33000|10000|12000|    3|             3|\n",
      "|   Davd|   Finance| 10000|  3|11000.0|33000|10000|12000|    3|             3|\n",
      "|  Ilaja| Marketing|  8000|  1| 7500.0|15000| 7000| 8000|    2|             2|\n",
      "|Haffman| Marketing|  7000|  2| 7500.0|15000| 7000| 8000|    2|             2|\n",
      "|   Joey|     Sales|  9000|  1| 7900.0|39500| 7000| 9000|    5|             4|\n",
      "|    Ali|     Sales|  8000|  2| 7900.0|39500| 7000| 9000|    5|             4|\n",
      "|  Elena|     Sales|  8000|  3| 7900.0|39500| 7000| 9000|    5|             4|\n",
      "|  Cindy|     Sales|  7500|  4| 7900.0|39500| 7000| 9000|    5|             4|\n",
      "|    Bob|     Sales|  7000|  5| 7900.0|39500| 7000| 9000|    5|             4|\n",
      "+-------+----------+------+---+-------+-----+-----+-----+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 分组，并对组内数据排序\n",
    "windowSpec  = Window.partitionBy(\"department\").orderBy(F.desc(\"salary\"))\n",
    "# 仅分组\n",
    "windowSpecAgg  = Window.partitionBy(\"department\")\n",
    "\n",
    "df.withColumn(\"row\", F.row_number().over(windowSpec)) \\\n",
    "  .withColumn(\"avg\", F.avg(\"salary\").over(windowSpecAgg)) \\\n",
    "  .withColumn(\"sum\", F.sum(\"salary\").over(windowSpecAgg)) \\\n",
    "  .withColumn(\"min\", F.min(\"salary\").over(windowSpecAgg)) \\\n",
    "  .withColumn(\"max\", F.max(\"salary\").over(windowSpecAgg)) \\\n",
    "  .withColumn(\"count\", F.count(\"salary\").over(windowSpecAgg)) \\\n",
    "  .withColumn(\"distinct_count\", F.approxCountDistinct(\"salary\").over(windowSpecAgg)) \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72138468",
   "metadata": {},
   "source": [
    "需要注意的是 approx_count_distinct() 函数适用与窗函数的统计， 而在groupby中通常用countDistinct()来代替该函数，用来求组内不重复的数值的条数。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948c031d",
   "metadata": {},
   "source": [
    "从结果来看，统计值基本上是按照部门分组，统计组内的salary情况。 如果我们只想要保留部门的统计结果，而将每个人的实际情况去掉，可以采用如下代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "04fe5b74",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T10:12:55.809700Z",
     "start_time": "2023-11-24T10:12:39.010112Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-----+-----+-----+-----+--------------+\n",
      "|department|    avg|  sum|  min|  max|count|distinct_count|\n",
      "+----------+-------+-----+-----+-----+-----+--------------+\n",
      "|   Finance|11000.0|33000|10000|12000|    3|             3|\n",
      "| Marketing| 7500.0|15000| 7000| 8000|    2|             2|\n",
      "|     Sales| 7900.0|39500| 7000| 9000|    5|             4|\n",
      "+----------+-------+-----+-----+-----+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "windowSpec  = Window.partitionBy(\"department\").orderBy(F.desc(\"salary\"))\n",
    "windowSpecAgg  = Window.partitionBy(\"department\")\n",
    "\n",
    "df = df.withColumn(\"row\", F.row_number().over(windowSpec)) \\\n",
    "  .withColumn(\"avg\", F.avg(\"salary\").over(windowSpecAgg)) \\\n",
    "  .withColumn(\"sum\", F.sum(\"salary\").over(windowSpecAgg)) \\\n",
    "  .withColumn(\"min\", F.min(\"salary\").over(windowSpecAgg)) \\\n",
    "  .withColumn(\"max\", F.max(\"salary\").over(windowSpecAgg)) \\\n",
    "  .withColumn(\"count\", F.count(\"salary\").over(windowSpecAgg)) \\\n",
    "  .withColumn(\"distinct_count\", F.approx_count_distinct(\"salary\").over(windowSpecAgg))\n",
    "\n",
    "# 仅选取分组第一行数据\n",
    "# 用F.col 去选row 行，怪怪的\n",
    "df_part  = df.where(F.col(\"row\")==1)\n",
    "df_part.select(\"department\",\"avg\",\"sum\",\"min\",\"max\",\"count\",\"distinct_count\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feeaff00",
   "metadata": {},
   "source": [
    "### 常用函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b5479c1e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T10:14:40.140446Z",
     "start_time": "2023-11-24T10:14:40.113754Z"
    }
   },
   "outputs": [],
   "source": [
    "# 加载和准备时间序列数据\n",
    "data = [(\"2022-01-01\", -10), (\"2022-01-02\", 15), (\"2022-01-03\", -20),\n",
    "        (\"2022-01-04\", -25), (\"2022-01-05\", -30), (\"2022-01-06\", -35),\n",
    "        (\"2022-01-07\", 40)]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"timestamp\", \"value\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1e974aa7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-24T10:14:58.141081Z",
     "start_time": "2023-11-24T10:14:41.925064Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+------------------+\n",
      "| timestamp|value|    moving_average|\n",
      "+----------+-----+------------------+\n",
      "|2022-01-01|  -10|              -5.0|\n",
      "|2022-01-02|   15|             -10.0|\n",
      "|2022-01-03|  -20|             -14.0|\n",
      "|2022-01-04|  -25|             -19.0|\n",
      "|2022-01-05|  -30|             -14.0|\n",
      "|2022-01-06|  -35|             -12.5|\n",
      "|2022-01-07|   40|-8.333333333333334|\n",
      "+----------+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col\n",
    "# 定义滑动窗口规范\n",
    "windowSpec = Window.orderBy(col(\"timestamp\")).rowsBetween(-2, 2)\n",
    "\n",
    "# 应用窗口规范和转换操作\n",
    "df1 = df.withColumn(\"moving_average\", F.avg(col(\"value\")).over(windowSpec))\n",
    "\n",
    "# 显示结果\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b550be3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "212.292px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
